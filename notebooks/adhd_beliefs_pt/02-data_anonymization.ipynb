{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb1692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, RecognizerResult\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities.engine import OperatorConfig\n",
    "from faker import Faker\n",
    "import spacy\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ff8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/adhd-beliefs-pt/adhd-beliefs-pt-prepared.pkl\"\n",
    "df = pd.read_pickle(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"pt\", \"model_name\": \"pt_core_news_lg\"}],\n",
    "}\n",
    "# Create NLP engine based on configuration\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine = provider.create_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers(nlp_engine=nlp_engine, languages=[\"pt\"])\n",
    "registry.recognizers = [\n",
    "    r for r in registry.recognizers\n",
    "    if not ((\"PERSON\" in r.supported_entities) and \"dictionary\" in r.__class__.__name__.lower())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the languages are needed to load country-specific recognizers \n",
    "# for finding phones, passport numbers, etc.\n",
    "analyzer = AnalyzerEngine(nlp_engine=nlp_engine,\n",
    "                          registry=registry)\n",
    "anonymizer = AnonymizerEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker(locale=['pt_PT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pt = nlp_engine.nlp[\"pt\"]  # spaCy Language instance\n",
    "PT_STOPWORDS = set(nlp_pt.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa70b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e139cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _match_casing(src: str, repl: str) -> str:\n",
    "    if src in [\"EUA\", \"UK\", \"EAU\"]:\n",
    "        return repl.title()\n",
    "    if src.isupper():\n",
    "        return repl.upper()\n",
    "    if src.istitle():\n",
    "        repl = repl.title()\n",
    "        # Lowercase specific Portuguese particles\n",
    "        repl = re.sub(r\"\\b(De|Da|Do|Das|Dos|E)\\b\", lambda m: m.group(1).lower(), repl)\n",
    "        return repl\n",
    "    if src.islower():\n",
    "        return repl.lower()\n",
    "    # Mixed / sentence case: title-case only if looks like a proper noun\n",
    "    return repl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a normalized set of country names\n",
    "COUNTRY_SET = set()\n",
    "COUNTRY_SYNONYMS = {_norm(n) for n in [\n",
    "    \"EUA\", \"Estados Unidos\", \"Reino Unido\", \"Inglaterra\", \"Escócia\",\n",
    "    \"País de Gales\", \"Irlanda do Norte\", \"República Checa\", \"Coreia do Sul\",\n",
    "    \"México\", \"Alemanha\", \"França\", \"Itália\", \"Espanha\",\n",
    "    \"Noruega\", \"Suécia\", \"Dinamarca\", \"Finlândia\", \"Islândia\",\n",
    "    \"Dubai\", \"Emirados Árabes Unidos\", \"Canadá\", \"Austrália\",\n",
    "]}\n",
    "try:\n",
    "    import pycountry\n",
    "    for c in pycountry.countries:\n",
    "        COUNTRY_SET.add(_norm(getattr(c, \"name\", \"\")))\n",
    "        if hasattr(c, \"official_name\"):\n",
    "            COUNTRY_SET.add(_norm(c.official_name))\n",
    "        # add common aliases\n",
    "        for attr in (\"common_name\",):\n",
    "            if hasattr(c, attr):\n",
    "                COUNTRY_SET.add(_norm(getattr(c, attr)))\n",
    "except Exception:\n",
    "    # no pycountry available -> rely on synonyms + heuristics\n",
    "    pass\n",
    "COUNTRY_SET |= COUNTRY_SYNONYMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_location(span_text: str) -> str:\n",
    "    s = _norm(span_text.rstrip(\".,;:!?)»”]\"))  # trim common trailing punct\n",
    "    if s in COUNTRY_SET:\n",
    "        return \"country\"\n",
    "    return \"city\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e213752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfilter_person(results, text, nlp_lang, min_score_person=0.72):\n",
    "    \"\"\"\n",
    "    Drop weak/false PERSON hits using:\n",
    "      - higher score requirement\n",
    "      - stopword/lowercase/too-short checks\n",
    "      - POS-based check: require proper nouns (PROPN). This blocks 'Política'.\n",
    "    \"\"\"\n",
    "    doc = nlp_lang(text)  # spaCy doc for POS/lemma checks\n",
    "    stopwords = nlp_lang.Defaults.stop_words\n",
    "\n",
    "    def span_tokens(r):\n",
    "        return [t for t in doc if t.idx >= r.start and t.idx + len(t) <= r.end]\n",
    "\n",
    "    filtered = []\n",
    "    for r in results:\n",
    "        if r.entity_type != \"PERSON\":\n",
    "            filtered.append(r)\n",
    "            continue\n",
    "\n",
    "        if r.score is not None and r.score < min_score_person:\n",
    "            continue\n",
    "\n",
    "        toks = span_tokens(r)\n",
    "        span_text = text[r.start:r.end]\n",
    "\n",
    "        # 1) Basic heuristics\n",
    "        if len(toks) == 1:\n",
    "            t = toks[0]\n",
    "            # reject all-lowercase or very short single tokens\n",
    "            if len(t.text) < 3:\n",
    "                continue\n",
    "\n",
    "        # 2) All tokens are stopwords? drop\n",
    "        if toks and all(t.text.lower() in stopwords for t in toks):\n",
    "            continue\n",
    "\n",
    "        # 3) POS-based rule: require PROPN for single-token names,\n",
    "        #    and majority PROPN for multi-token names\n",
    "        if len(toks) == 1:\n",
    "            if toks[0].pos_ != \"PROPN\":   # 'Política' is NOUN → drop\n",
    "                continue\n",
    "        else:\n",
    "            propn_ratio = sum(1 for t in toks if t.pos_ == \"PROPN\") / len(toks)\n",
    "            if propn_ratio < 0.5:\n",
    "                continue\n",
    "\n",
    "        # 4) Extra guard: if the span is sentence-initial and only ONE token,\n",
    "        #    and that token is a common noun (NOUN), drop.\n",
    "        sent = toks[0].sent if toks else None\n",
    "        if sent and len(toks) == 1:\n",
    "            if toks[0].i == sent.start and toks[0].pos_ != \"PROPN\":\n",
    "                continue\n",
    "\n",
    "        filtered.append(r)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_figures = [\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Cristiano Ronaldo\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"António Guterres\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Anitta\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Taylor Swift\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"J. D. Salinger\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Melville\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Osamu Dazai\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Jung\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Percy Jackson\"}\n",
    "]\n",
    "if \"entity_ruler\" not in nlp_pt.pipe_names:\n",
    "    ruler = nlp_pt.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "else:\n",
    "    ruler = nlp_pt.get_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(public_figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_names = [\n",
    "    {\"label\": \"PET_NAME\", \"pattern\": \"Vince\"},\n",
    "    {\"label\": \"PET_NAME\", \"pattern\": \"Kiko\"},\n",
    "    {\"label\": \"PET_NAME\", \"pattern\": \"Thor\"},\n",
    "    {\"label\": \"PET_NAME\", \"pattern\": \"Kylie\"},\n",
    "]\n",
    "if \"entity_ruler\" not in nlp_pt.pipe_names:\n",
    "    ruler = nlp_pt.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "else:\n",
    "    ruler = nlp_pt.get_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(pet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a59f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_persons_with_gendered_fakes(text: str, results, fake):\n",
    "    def _gender_from_article(txt: str, start_idx: int):\n",
    "        win = txt[max(0, start_idx-2):start_idx].lower()\n",
    "        if win.endswith(\"a \"): return \"f\"\n",
    "        if win.endswith(\"o \"): return \"m\"\n",
    "        return None\n",
    "\n",
    "    mapping = {}\n",
    "    non_person = [deepcopy(r) for r in results if r.entity_type != \"PERSON\"]\n",
    "    persons = sorted([r for r in results if r.entity_type == \"PERSON\"],\n",
    "                     key=lambda r: r.start, reverse=True)\n",
    "\n",
    "    new_text = text\n",
    "    replacements = []\n",
    "\n",
    "    for r in persons:\n",
    "        original_name = new_text[r.start:r.end]\n",
    "\n",
    "        if original_name in mapping:\n",
    "            replacement = mapping[original_name]\n",
    "        else:\n",
    "            gender = _gender_from_article(new_text, r.start)\n",
    "            if gender == \"f\":\n",
    "                replacement = f\"{fake.first_name_female()}\"\n",
    "            elif gender == \"m\":\n",
    "                replacement = f\"{fake.first_name_male()}\"\n",
    "            else:\n",
    "                replacement = fake.first_name()\n",
    "            mapping[original_name] = replacement\n",
    "        replacement = _match_casing(original_name, replacement)\n",
    "        \n",
    "        replacements.append({\n",
    "            \"entity_type\": \"PERSON\",\n",
    "            \"before\": original_name,\n",
    "            \"after\": replacement\n",
    "        })\n",
    "\n",
    "        # splice & adjust indices\n",
    "        original_len = r.end - r.start\n",
    "        new_text = new_text[:r.start] + replacement + new_text[r.end:]\n",
    "        delta = len(replacement) - original_len\n",
    "\n",
    "        if delta != 0:\n",
    "            for nr in non_person:\n",
    "                if nr.start >= r.end:\n",
    "                    nr.start += delta\n",
    "                    nr.end += delta\n",
    "\n",
    "        non_person = [nr for nr in non_person if not (nr.start < r.end and nr.end > r.start)]\n",
    "\n",
    "    return new_text, non_person, replacements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49004c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_ignored_names(results, text):\n",
    "    IGNORE_NAMES = {\"phda\", \"well\", \"PDAH\", \"anyways\", \"acalma-me\", \"catisfaction\", \"vivi\", \"Compal de pêra\", \"P.s\", \"humana\", \"Estimula-me\", \"Yoggi\", \"Deus\", \"bue giro\", \"bue\", \"Perturbação Obsessiva e Compulsiva\", \"AMVs\", \"Mochi donuts\", \"Mowgli\", \"Mãe\", \"Heck\", \"Rock in Rio\", \"Aspartame\", \"oubir\", \"Weeee\", \"Praxe\", \"Maybe\", \"Usagi\", \"Lucy\", \"Espiritualidade\", \"killer\"}\n",
    "    IGNORE_NAMES = {n.lower() for n in IGNORE_NAMES}\n",
    "    filtered = []\n",
    "    for r in results:\n",
    "        span = text[r.start:r.end].strip().lower()\n",
    "        if span in IGNORE_NAMES and r.entity_type in (\"PERSON\", \"LOCATION\"):\n",
    "            # skip this entity\n",
    "            continue\n",
    "        filtered.append(r)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_false_entities(results, text):\n",
    "    bad_locations = {\"campos de ferias\", \"viajar\", \"prontos\", \"phda\", \"sinto\", \"estive\", \"amanhã\", \"hiperfoca\", \"concentrar-me\", \"ir de erasmus\", \"🥹\", \"altos\", \"associam\", \"montes\", \"square\", \"entristece\", \"relembro-me\", \"fechei-me\", \"iria\", \"levo-os\", \"esteja<3\", \"volei\", \"gym\", \"tenho\", \"bombeiro\", \"deparome\", \"odevia\", \"go\", \"obrigada\", \"miuda\", \"backstory\", \"apeteceu-nos\", \"ocd\", \"terra\", \"usei-as\", \"regresso\", \"aquashow\", \"castração\", \"invençºao\", \"aikido\", \"aspartame\", \"coca cola\", \"procastinar\", \"fui\", \"uber\", \"desligar-me\", \"harvard\", \"calor\", \"overthinker\", \"canal panda\", \"lichia\", \"rua\", \"nonetheless\", \"mundo\", \"B\", \"beijinhos\", \"compaixão\", \"psicóloga\", \"fez-me\", \"preparámo-nos\", \"Constant brain chatter\\n•\", \"rua ignoram-me\", \"portugal\", \"europa\"}       # lowercase for comparison\n",
    "    bad_locations = {n.lower() for n in bad_locations}\n",
    "    \n",
    "    filtered = []\n",
    "    for r in results:\n",
    "        span = text[r.start:r.end].lower().strip()\n",
    "        if span == \"gonçalo\":\n",
    "            r.entity_type = \"PERSON\"\n",
    "        if r.entity_type == \"LOCATION\" and span in bad_locations:\n",
    "            continue  # drop\n",
    "        filtered.append(r)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e106ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_exemptions(text, score_threshold=0.65):\n",
    "    # 1) PUBLIC_FIGURE via spaCy EntityRuler (as you already have)\n",
    "    pf_results = []\n",
    "    doc_pf = nlp_pt(text)\n",
    "    for ent in doc_pf.ents:\n",
    "        if ent.label_ == \"PUBLIC_FIGURE\":\n",
    "            pf_results.append(RecognizerResult(\"PUBLIC_FIGURE\", ent.start_char, ent.end_char, 1.0))\n",
    "        if ent.label_ == \"PET_NAME\":\n",
    "            pf_results.append(RecognizerResult(\"PET_NAME\", ent.start_char, ent.end_char, 1.0))    \n",
    "\n",
    "    # 2) Presidio\n",
    "    pres_results = analyzer.analyze(text=text, language=\"pt\", score_threshold=score_threshold)\n",
    "\n",
    "    # 3) Post-filter PERSON with POS logic (<<< this is the new part)\n",
    "    pres_results = postfilter_person(pres_results, text, nlp_lang=nlp_pt, min_score_person=0.72)\n",
    "\n",
    "    # 4) Remove PERSON overlapping PUBLIC_FIGURE\n",
    "    pf_spans = [(r.start, r.end) for r in pf_results]\n",
    "    final = []\n",
    "    for r in pres_results:\n",
    "        if r.entity_type == \"PERSON\":\n",
    "            if any(not (r.end <= s or r.start >= e) for s, e in pf_spans):\n",
    "                continue\n",
    "        final.append(r)\n",
    "\n",
    "    final.extend(pf_results)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5110f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_non_person_with_logging(text, results, operators, *, fake, consistent_maps=None):\n",
    "    \"\"\"\n",
    "    Replace non-PERSON entities right-to-left with precise logging.\n",
    "    LOCATIONs are classified as city/country and mapped consistently per entry.\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "\n",
    "    def _run_operator(span_text, cfg):\n",
    "        name = getattr(cfg, \"operator_name\", None) or cfg.get(\"operator_name\")\n",
    "        params = getattr(cfg, \"params\", None) or cfg.get(\"params\", {}) or {}\n",
    "        if name == \"custom\":\n",
    "            fn = params.get(\"lambda\")\n",
    "            return fn(span_text) if fn else span_text\n",
    "        if name == \"replace\":\n",
    "            return str(params.get(\"new_value\", \"\"))\n",
    "        if name == \"mask\":\n",
    "            mchar = str(params.get(\"masking_char\", \"*\"))\n",
    "            n = int(params.get(\"chars_to_mask\", len(span_text)))\n",
    "            from_end = bool(params.get(\"from_end\", False))\n",
    "            n = max(0, min(n, len(span_text)))\n",
    "            if n == 0: return span_text\n",
    "            if n >= len(span_text): return mchar * len(span_text)\n",
    "            return (span_text[:-n] + mchar * n) if from_end else (mchar * n + span_text[n:])\n",
    "        return \"*\" * max(1, len(span_text))\n",
    "\n",
    "    # copy & sort right-to-left\n",
    "    res = [deepcopy(r) for r in results if r.entity_type != \"PERSON\"]\n",
    "    res.sort(key=lambda r: r.start, reverse=True)\n",
    "\n",
    "    out = text\n",
    "    logs = []\n",
    "\n",
    "    # per-entry maps\n",
    "    consistent_maps = consistent_maps or {}\n",
    "    city_map    = consistent_maps.setdefault(\"LOCATION_CITY\", {})     # norm(original) -> fake\n",
    "    country_map = consistent_maps.setdefault(\"LOCATION_COUNTRY\", {})  # norm(original) -> fake\n",
    "\n",
    "    for r in res:\n",
    "        etype = r.entity_type\n",
    "        before = out[r.start:r.end]\n",
    "        cfg = operators.get(etype, operators.get(\"DEFAULT\"))\n",
    "        if cfg is None:\n",
    "            continue\n",
    "\n",
    "        if etype == \"LOCATION\":\n",
    "            kind = classify_location(before)  # 'city' or 'country'\n",
    "            key  = _norm(before)\n",
    "            if kind == \"country\":\n",
    "                if key in country_map:\n",
    "                    after_raw = country_map[key]\n",
    "                else:\n",
    "                    # generate a country name\n",
    "                    after_raw = fake.country()\n",
    "                    country_map[key] = after_raw\n",
    "            else:\n",
    "                if key in city_map:\n",
    "                    after_raw = city_map[key]\n",
    "                else:\n",
    "                    # generate a city name\n",
    "                    after_raw = fake.city()\n",
    "                    city_map[key] = after_raw\n",
    "\n",
    "            after = _match_casing(before, after_raw)\n",
    "        else:\n",
    "            # non-LOCATION: use operator as-is\n",
    "            after = _run_operator(before, cfg)\n",
    "\n",
    "        out = out[:r.start] + after + out[r.end:]\n",
    "\n",
    "        logs.append({\"entity_type\": etype, \"before\": before, \"after\": after})\n",
    "\n",
    "    return out, logs, consistent_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b91145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anon = df[[\"special_interest\", \"diary_entry\", \"selfdefining_memory\", \"empty_sheet\"]]\n",
    "print(\"Columns to anonymize:\", df_anon.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_operators = {\n",
    "    \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.phone_number()}),\n",
    "    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.email()}),\n",
    "    \"LOCATION\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x}),\n",
    "    \"PUBLIC_FIGURE\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x}),\n",
    "    \"PET_NAME\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x}),\n",
    "    \"CREDIT_CARD\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.credit_card_number()}),\n",
    "    \"ORGANIZATION\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x}),\n",
    "    \"DEFAULT\": OperatorConfig(operator_name=\"mask\", params={'chars_to_mask': 10, 'masking_char': '*', 'from_end': False}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_count = 0\n",
    "entity_counter = Counter()\n",
    "change_log = []\n",
    "\n",
    "for idx, row in df_anon.iterrows():\n",
    "    for column in df_anon.columns:\n",
    "        cell_value = row[column]\n",
    "        if pd.notna(cell_value) and str(cell_value).strip():\n",
    "            text = str(cell_value)\n",
    "            \n",
    "            results = detect_with_exemptions(text)\n",
    "            results = drop_false_entities(results, text)\n",
    "            results = drop_ignored_names(results, text)\n",
    "            \n",
    "            text_after_person, results_without_person, person_repl = _replace_persons_with_gendered_fakes(\n",
    "                text, results, fake\n",
    "            )\n",
    "            \n",
    "            per_entry_maps = {}  # resets every entry; move outside loop for global consistency\n",
    "            text_final, non_person_repl, per_entry_maps = _apply_non_person_with_logging(\n",
    "                text_after_person,\n",
    "                results_without_person,\n",
    "                fake_operators,\n",
    "                fake=fake,\n",
    "                consistent_maps=per_entry_maps\n",
    "            )\n",
    "            \n",
    "            # result_obj = anonymizer.anonymize(\n",
    "            #     text=text_after_person,\n",
    "            #     analyzer_results=results_without_person,\n",
    "            #     operators=fake_operators\n",
    "            # )\n",
    "            # anonymized_text = result_obj.text\n",
    "            \n",
    "            anonymized_text = text_final\n",
    "            all_replacements = person_repl + non_person_repl\n",
    "            \n",
    "            # tmp_text = text_final\n",
    "            # items_desc = sorted(result_obj.items, key=lambda it: it.start, reverse=True)\n",
    "            \n",
    "            # presidio_repl = []\n",
    "            # for it in items_desc:\n",
    "            #     before_span = tmp_text[it.start:it.end]   # correct \"before\" in current state\n",
    "            #     presidio_repl.append({\n",
    "            #         \"entity_type\": it.entity_type,\n",
    "            #         \"before\": before_span,\n",
    "            #         \"after\": it.text\n",
    "            #     })\n",
    "            #     # apply the replacement so subsequent indices stay valid\n",
    "            #     tmp_text = tmp_text[:it.start] + it.text + tmp_text[it.end:]\n",
    "            # all_replacements = person_repl + presidio_repl\n",
    "            \n",
    "            if anonymized_text != cell_value:\n",
    "                modified_count += 1\n",
    "                for rep in all_replacements:\n",
    "                    entity_counter[rep[\"entity_type\"]] += 1\n",
    "                change_log.append({\n",
    "                    \"row\": idx,\n",
    "                    \"column\": column,\n",
    "                    \"original\": cell_value,\n",
    "                    \"anonymized\": anonymized_text,\n",
    "                    \"replacements\": all_replacements\n",
    "                })\n",
    "\n",
    "            df_anon.at[idx, column] = anonymized_text\n",
    "\n",
    "# ---- summary ----\n",
    "lines = []\n",
    "lines.append(\"=\"*60)\n",
    "lines.append(f\"Total modified entries: {modified_count}\")\n",
    "lines.append(\"Entities replaced (by type):\")\n",
    "for ent, count in entity_counter.most_common():\n",
    "    lines.append(f\"  {ent}: {count}\")\n",
    "\n",
    "lines.append(\"=\"*60)\n",
    "lines.append(\"Example changes:\")\n",
    "for log in change_log:\n",
    "    lines.append(f\"Row {log['row']} | Col {log['column']}\")\n",
    "    for rep in log[\"replacements\"]:\n",
    "        lines.append(f\"  {rep['entity_type']}: {rep['before']} -> {rep['after']}\")\n",
    "    lines.append(\"\\n\")\n",
    "    lines.append(f\"Original  : {log['original']}\\n\")\n",
    "    lines.append(f\"Anonymized: {log['anonymized']}\\n\")\n",
    "    lines.append(\"-\"*40)\n",
    "\n",
    "# Join as one string\n",
    "summary_text = \"\\n\".join(lines)\n",
    "# Save to Markdown file\n",
    "with open(\"data/anonymization_summary.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(\"Summary saved to data/anonymization_summary.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e30ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(columns=df_anon.columns), df_anon], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79888d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"../../data/adhd-beliefs-pt/adhd-beliefs-pt-anonymized.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adhd-linguistic-patterns-beliefs-portuguese-women",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
