{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4deb1692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, RecognizerResult\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities.engine import OperatorConfig\n",
    "from faker import Faker\n",
    "import spacy\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7ff8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>sex</th>\n",
       "      <th>adhd_diagnosis</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>occupation</th>\n",
       "      <th>dialect</th>\n",
       "      <th>forgetting_objects</th>\n",
       "      <th>forgetting_responsabilities</th>\n",
       "      <th>emotion_management</th>\n",
       "      <th>...</th>\n",
       "      <th>need_fast_talk_interest</th>\n",
       "      <th>need_fast_talk_information</th>\n",
       "      <th>speaking_before_thinking</th>\n",
       "      <th>something_to_add</th>\n",
       "      <th>something_to_add_timid</th>\n",
       "      <th>something_to_add_impulsive</th>\n",
       "      <th>special_interest</th>\n",
       "      <th>diary_entry</th>\n",
       "      <th>selfdefining_memory</th>\n",
       "      <th>empty_sheet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-29T18:09:34+01:00</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>21</td>\n",
       "      <td>Licenciatura</td>\n",
       "      <td>Estudante</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-30T16:29:47+02:00</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>22</td>\n",
       "      <td>Licenciatura</td>\n",
       "      <td>Estudante</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Adoro cozinhar, porque sinto que é a forma ide...</td>\n",
       "      <td>Hoje foi um dia bastante normal, como ontem fi...</td>\n",
       "      <td>Quando era mais nova, eu tinha uma professora ...</td>\n",
       "      <td>Recentemente tenho pensado muito no impacto qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-31T12:05:45+02:00</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>53</td>\n",
       "      <td>Licenciatura</td>\n",
       "      <td>Trabalhador</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-31T12:08:00+02:00</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>23</td>\n",
       "      <td>Mestrado</td>\n",
       "      <td>Trabalhador-estudante</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Gosto de ver séries porque relatam relações sa...</td>\n",
       "      <td>Os eventos que considero mais relevantes serão...</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>Este questionário foi um pouco extenso. Meu au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-31T12:21:38+02:00</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>20</td>\n",
       "      <td>Licenciatura</td>\n",
       "      <td>Estudante</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    datetime        sex adhd_diagnosis  age     education  \\\n",
       "0  2025-03-29T18:09:34+01:00  Masculino      Não tenho   21  Licenciatura   \n",
       "1  2025-03-30T16:29:47+02:00   Feminino      Não tenho   22  Licenciatura   \n",
       "2  2025-03-31T12:05:45+02:00   Feminino      Não tenho   53  Licenciatura   \n",
       "3  2025-03-31T12:08:00+02:00   Feminino      Não tenho   23      Mestrado   \n",
       "4  2025-03-31T12:21:38+02:00   Feminino      Não tenho   20  Licenciatura   \n",
       "\n",
       "              occupation                dialect forgetting_objects  \\\n",
       "0              Estudante  Português de Portugal                  2   \n",
       "1              Estudante  Português de Portugal                  3   \n",
       "2            Trabalhador  Português de Portugal                  3   \n",
       "3  Trabalhador-estudante  Português de Portugal                  2   \n",
       "4              Estudante  Português de Portugal                  4   \n",
       "\n",
       "  forgetting_responsabilities emotion_management  ... need_fast_talk_interest  \\\n",
       "0                           3                  2  ...                       2   \n",
       "1                           3                  4  ...                       1   \n",
       "2                           4                  4  ...                       1   \n",
       "3                           1                  4  ...                       1   \n",
       "4                           2                  2  ...                       2   \n",
       "\n",
       "  need_fast_talk_information speaking_before_thinking something_to_add  \\\n",
       "0                          1                        1                3   \n",
       "1                          1                        3                4   \n",
       "2                          1                        3                3   \n",
       "3                          1                        4                4   \n",
       "4                          2                        4                3   \n",
       "\n",
       "  something_to_add_timid something_to_add_impulsive  \\\n",
       "0                      4                          2   \n",
       "1                      4                          2   \n",
       "2                      1                          2   \n",
       "3                      4                          2   \n",
       "4                      4                          3   \n",
       "\n",
       "                                    special_interest  \\\n",
       "0                                                NaN   \n",
       "1  Adoro cozinhar, porque sinto que é a forma ide...   \n",
       "2                                                NaN   \n",
       "3  Gosto de ver séries porque relatam relações sa...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         diary_entry  \\\n",
       "0                                                NaN   \n",
       "1  Hoje foi um dia bastante normal, como ontem fi...   \n",
       "2                                                NaN   \n",
       "3  Os eventos que considero mais relevantes serão...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                 selfdefining_memory  \\\n",
       "0                                                NaN   \n",
       "1  Quando era mais nova, eu tinha uma professora ...   \n",
       "2                                                NaN   \n",
       "3                                          Não tenho   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         empty_sheet  \n",
       "0                                                NaN  \n",
       "1  Recentemente tenho pensado muito no impacto qu...  \n",
       "2                                                NaN  \n",
       "3  Este questionário foi um pouco extenso. Meu au...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"../../data/adhd-beliefs-pt/adhd-beliefs-pt-prepared.pkl\"\n",
    "df = pd.read_pickle(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4c83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"pt\", \"model_name\": \"pt_core_news_lg\"}],\n",
    "}\n",
    "# Create NLP engine based on configuration\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine = provider.create_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c4a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers(nlp_engine=nlp_engine, languages=[\"pt\"])\n",
    "registry.recognizers = [\n",
    "    r for r in registry.recognizers\n",
    "    if not ((\"PERSON\" in r.supported_entities) and \"dictionary\" in r.__class__.__name__.lower())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5cf6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the languages are needed to load country-specific recognizers \n",
    "# for finding phones, passport numbers, etc.\n",
    "analyzer = AnalyzerEngine(nlp_engine=nlp_engine,\n",
    "                          registry=registry)\n",
    "anonymizer = AnonymizerEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6729b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker(locale=['pt_PT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beca8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pt = nlp_engine.nlp[\"pt\"]  # spaCy Language instance\n",
    "PT_STOPWORDS = set(nlp_pt.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa70b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e139cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _match_casing(src: str, repl: str) -> str:\n",
    "    if src in [\"EUA\", \"UK\", \"EAU\"]:\n",
    "        return repl.title()\n",
    "    if src.isupper():\n",
    "        return repl.upper()\n",
    "    if src.istitle():\n",
    "        repl = repl.title()\n",
    "        # Lowercase specific Portuguese particles\n",
    "        repl = re.sub(r\"\\b(De|Da|Do|Das|Dos|E)\\b\", lambda m: m.group(1).lower(), repl)\n",
    "        return repl\n",
    "    if src.islower():\n",
    "        return repl.lower()\n",
    "    # Mixed / sentence case: title-case only if looks like a proper noun\n",
    "    return repl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4435419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a normalized set of country names\n",
    "COUNTRY_SET = set()\n",
    "COUNTRY_SYNONYMS = {_norm(n) for n in [\n",
    "    \"EUA\", \"Estados Unidos\", \"Reino Unido\", \"Inglaterra\", \"Escócia\",\n",
    "    \"País de Gales\", \"Irlanda do Norte\", \"República Checa\", \"Coreia do Sul\",\n",
    "    \"México\", \"Alemanha\", \"França\", \"Itália\", \"Espanha\",\n",
    "    \"Noruega\", \"Suécia\", \"Dinamarca\", \"Finlândia\", \"Islândia\",\n",
    "    \"Dubai\", \"Emirados Árabes Unidos\", \"Canadá\", \"Austrália\",\n",
    "]}\n",
    "try:\n",
    "    import pycountry\n",
    "    for c in pycountry.countries:\n",
    "        COUNTRY_SET.add(_norm(getattr(c, \"name\", \"\")))\n",
    "        if hasattr(c, \"official_name\"):\n",
    "            COUNTRY_SET.add(_norm(c.official_name))\n",
    "        # add common aliases\n",
    "        for attr in (\"common_name\",):\n",
    "            if hasattr(c, attr):\n",
    "                COUNTRY_SET.add(_norm(getattr(c, attr)))\n",
    "except Exception:\n",
    "    # no pycountry available -> rely on synonyms + heuristics\n",
    "    pass\n",
    "COUNTRY_SET |= COUNTRY_SYNONYMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "395c51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_location(span_text: str) -> str:\n",
    "    s = _norm(span_text.rstrip(\".,;:!?)»”]\"))  # trim common trailing punct\n",
    "    if s in COUNTRY_SET:\n",
    "        return \"country\"\n",
    "    return \"city\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e213752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfilter_person(results, text, nlp_lang, min_score_person=0.72):\n",
    "    \"\"\"\n",
    "    Drop weak/false PERSON hits using:\n",
    "      - higher score requirement\n",
    "      - stopword/lowercase/too-short checks\n",
    "      - POS-based check: require proper nouns (PROPN). This blocks 'Política'.\n",
    "    \"\"\"\n",
    "    doc = nlp_lang(text)  # spaCy doc for POS/lemma checks\n",
    "    stopwords = nlp_lang.Defaults.stop_words\n",
    "\n",
    "    def span_tokens(r):\n",
    "        return [t for t in doc if t.idx >= r.start and t.idx + len(t) <= r.end]\n",
    "\n",
    "    filtered = []\n",
    "    for r in results:\n",
    "        if r.entity_type != \"PERSON\":\n",
    "            filtered.append(r)\n",
    "            continue\n",
    "\n",
    "        if r.score is not None and r.score < min_score_person:\n",
    "            continue\n",
    "\n",
    "        toks = span_tokens(r)\n",
    "        span_text = text[r.start:r.end]\n",
    "\n",
    "        # 1) Basic heuristics\n",
    "        if len(toks) == 1:\n",
    "            t = toks[0]\n",
    "            # reject all-lowercase or very short single tokens\n",
    "            if len(t.text) < 3:\n",
    "                continue\n",
    "\n",
    "        # 2) All tokens are stopwords? drop\n",
    "        if toks and all(t.text.lower() in stopwords for t in toks):\n",
    "            continue\n",
    "\n",
    "        # 3) POS-based rule: require PROPN for single-token names,\n",
    "        #    and majority PROPN for multi-token names\n",
    "        if len(toks) == 1:\n",
    "            if toks[0].pos_ != \"PROPN\":   # 'Política' is NOUN → drop\n",
    "                continue\n",
    "        else:\n",
    "            propn_ratio = sum(1 for t in toks if t.pos_ == \"PROPN\") / len(toks)\n",
    "            if propn_ratio < 0.5:\n",
    "                continue\n",
    "\n",
    "        # 4) Extra guard: if the span is sentence-initial and only ONE token,\n",
    "        #    and that token is a common noun (NOUN), drop.\n",
    "        sent = toks[0].sent if toks else None\n",
    "        if sent and len(toks) == 1:\n",
    "            if toks[0].i == sent.start and toks[0].pos_ != \"PROPN\":\n",
    "                continue\n",
    "\n",
    "        filtered.append(r)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92dd5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_figures = [\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Cristiano Ronaldo\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"António Guterres\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Anitta\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Taylor Swift\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"J. D. Salinger\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Melville\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Osamu Dazai\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Jung\"},\n",
    "    {\"label\": \"PUBLIC_FIGURE\", \"pattern\": \"Percy Jackson\"}\n",
    "]\n",
    "if \"entity_ruler\" not in nlp_pt.pipe_names:\n",
    "    ruler = nlp_pt.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "else:\n",
    "    ruler = nlp_pt.get_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(public_figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af8c485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_names = [\n",
    "    {\"label\": \"PET_NAME\", \"pattern\": \"Vince\"},\n",
    "    {\"label\": \"PET_NAME\", \"pattern\": \"Kiko\"},\n",
    "    {\"label\": \"PET_NAME\", \"pattern\": \"Thor\"},\n",
    "    {\"label\": \"PET_NAME\", \"pattern\": \"Kylie\"},\n",
    "]\n",
    "if \"entity_ruler\" not in nlp_pt.pipe_names:\n",
    "    ruler = nlp_pt.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "else:\n",
    "    ruler = nlp_pt.get_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(pet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a59f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_persons_with_gendered_fakes(text: str, results, fake):\n",
    "    def _gender_from_article(txt: str, start_idx: int):\n",
    "        win = txt[max(0, start_idx-2):start_idx].lower()\n",
    "        if win.endswith(\"a \"): return \"f\"\n",
    "        if win.endswith(\"o \"): return \"m\"\n",
    "        return None\n",
    "\n",
    "    mapping = {}\n",
    "    non_person = [deepcopy(r) for r in results if r.entity_type != \"PERSON\"]\n",
    "    persons = sorted([r for r in results if r.entity_type == \"PERSON\"],\n",
    "                     key=lambda r: r.start, reverse=True)\n",
    "\n",
    "    new_text = text\n",
    "    replacements = []\n",
    "\n",
    "    for r in persons:\n",
    "        original_name = new_text[r.start:r.end]\n",
    "\n",
    "        if original_name in mapping:\n",
    "            replacement = mapping[original_name]\n",
    "        else:\n",
    "            gender = _gender_from_article(new_text, r.start)\n",
    "            if gender == \"f\":\n",
    "                replacement = f\"{fake.first_name_female()}\"\n",
    "            elif gender == \"m\":\n",
    "                replacement = f\"{fake.first_name_male()}\"\n",
    "            else:\n",
    "                replacement = fake.first_name()\n",
    "            mapping[original_name] = replacement\n",
    "        replacement = _match_casing(original_name, replacement)\n",
    "        \n",
    "        replacements.append({\n",
    "            \"entity_type\": \"PERSON\",\n",
    "            \"before\": original_name,\n",
    "            \"after\": replacement\n",
    "        })\n",
    "\n",
    "        # splice & adjust indices\n",
    "        original_len = r.end - r.start\n",
    "        new_text = new_text[:r.start] + replacement + new_text[r.end:]\n",
    "        delta = len(replacement) - original_len\n",
    "\n",
    "        if delta != 0:\n",
    "            for nr in non_person:\n",
    "                if nr.start >= r.end:\n",
    "                    nr.start += delta\n",
    "                    nr.end += delta\n",
    "\n",
    "        non_person = [nr for nr in non_person if not (nr.start < r.end and nr.end > r.start)]\n",
    "\n",
    "    return new_text, non_person, replacements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e49004c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_ignored_names(results, text):\n",
    "    IGNORE_NAMES = {\"phda\", \"well\", \"PDAH\", \"anyways\", \"acalma-me\", \"catisfaction\", \"vivi\", \"Compal de pêra\", \"P.s\", \"humana\", \"Estimula-me\", \"Yoggi\", \"Deus\", \"bue giro\", \"bue\", \"Perturbação Obsessiva e Compulsiva\", \"AMVs\", \"Mochi donuts\", \"Mowgli\", \"Mãe\", \"Heck\", \"Rock in Rio\", \"Aspartame\", \"oubir\", \"Weeee\", \"Praxe\", \"Maybe\", \"Usagi\", \"Lucy\", \"Espiritualidade\", \"killer\"}\n",
    "    IGNORE_NAMES = {n.lower() for n in IGNORE_NAMES}\n",
    "    filtered = []\n",
    "    for r in results:\n",
    "        span = text[r.start:r.end].strip().lower()\n",
    "        if span in IGNORE_NAMES and r.entity_type in (\"PERSON\", \"LOCATION\", \"ORG\"):\n",
    "            # skip this entity\n",
    "            continue\n",
    "        filtered.append(r)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b90e58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_false_entities(results, text):\n",
    "    bad_locations = {\"campos de ferias\", \"viajar\", \"prontos\", \"phda\", \"sinto\", \"estive\", \"amanhã\", \"hiperfoca\", \"concentrar-me\", \"ir de erasmus\", \"🥹\", \"altos\", \"associam\", \"montes\", \"square\", \"entristece\", \"relembro-me\", \"fechei-me\", \"iria\", \"levo-os\", \"esteja<3\", \"volei\", \"gym\", \"tenho\", \"bombeiro\", \"deparome\", \"odevia\", \"go\", \"obrigada\", \"miuda\", \"backstory\", \"apeteceu-nos\", \"ocd\", \"terra\", \"usei-as\", \"regresso\", \"aquashow\", \"castração\", \"invençºao\", \"aikido\", \"aspartame\", \"coca cola\", \"procastinar\", \"fui\", \"uber\", \"desligar-me\", \"harvard\", \"calor\", \"overthinker\", \"canal panda\", \"lichia\", \"rua\", \"nonetheless\", \"mundo\", \"B\", \"beijinhos\", \"compaixão\", \"psicóloga\", \"fez-me\", \"preparámo-nos\", \"Constant brain chatter\\n•\", \"rua ignoram-me\", \"portugal\", \"europa\"}       # lowercase for comparison\n",
    "    bad_orgs = {\"tds\", \"great day ^^ thank you for reading\", \"tp barata\", \"awe\", \"reels\", \"ps\", \"mdzs\", \"ap. fui\", \"aprender\", \"ko\", \"continuo\", \"tricot\", \"imenso\", \"f1\", \"motogp\", \"national geographic\", \"rádio\", \"adhd\", \"lavantei-me\", \"twitch\", \"starbucks\", \"pumpkin spice latte\", \"manchester united\", \"manchester city\", \"united\", \"linkin park\", \"adernalina\", \"poisoned world\", \"coca cola\", \"pepsi\", \"união europeia\", \"fda\", \"repetindo-me\", \"acalmei-me\", \"cp\", \"pir\", \"also\", \"ap\", \"impulsivo\", \"bubble tea\", \"miniso\", \"phda\", \"prd\"}\n",
    "\n",
    "    bad_locations = {n.lower() for n in bad_locations}\n",
    "    bad_orgs = {n.lower() for n in bad_orgs}\n",
    "    \n",
    "    filtered = []\n",
    "    for r in results:\n",
    "        span = text[r.start:r.end].lower().strip()\n",
    "        if span == \"gonçalo\":\n",
    "            r.entity_type = \"PERSON\"\n",
    "        if r.entity_type == \"LOCATION\" and span in bad_locations:\n",
    "            continue  # drop\n",
    "        if r.entity_type in (\"ORG\", \"ORGANIZATION\") and span in bad_orgs:                \n",
    "            continue  # drop\n",
    "        filtered.append(r)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00e106ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_exemptions(text, score_threshold=0.65):\n",
    "    # 1) PUBLIC_FIGURE via spaCy EntityRuler (as you already have)\n",
    "    pf_results = []\n",
    "    doc_pf = nlp_pt(text)\n",
    "    for ent in doc_pf.ents:\n",
    "        if ent.label_ == \"PUBLIC_FIGURE\":\n",
    "            pf_results.append(RecognizerResult(\"PUBLIC_FIGURE\", ent.start_char, ent.end_char, 1.0))\n",
    "        if ent.label_ == \"PET_NAME\":\n",
    "            pf_results.append(RecognizerResult(\"PET_NAME\", ent.start_char, ent.end_char, 1.0))    \n",
    "\n",
    "    # 2) Presidio\n",
    "    pres_results = analyzer.analyze(text=text, language=\"pt\", score_threshold=score_threshold)\n",
    "\n",
    "    # 3) Post-filter PERSON with POS logic (<<< this is the new part)\n",
    "    pres_results = postfilter_person(pres_results, text, nlp_lang=nlp_pt, min_score_person=0.72)\n",
    "\n",
    "    # 4) Remove PERSON overlapping PUBLIC_FIGURE\n",
    "    pf_spans = [(r.start, r.end) for r in pf_results]\n",
    "    final = []\n",
    "    for r in pres_results:\n",
    "        if r.entity_type == \"PERSON\":\n",
    "            if any(not (r.end <= s or r.start >= e) for s, e in pf_spans):\n",
    "                continue\n",
    "        final.append(r)\n",
    "\n",
    "    final.extend(pf_results)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a5110f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_non_person_with_logging(text, results, operators, *, fake, consistent_maps=None):\n",
    "    \"\"\"\n",
    "    Replace non-PERSON entities right-to-left with precise logging.\n",
    "    LOCATIONs are classified as city/country and mapped consistently per entry.\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "\n",
    "    def _run_operator(span_text, cfg):\n",
    "        name = getattr(cfg, \"operator_name\", None) or cfg.get(\"operator_name\")\n",
    "        params = getattr(cfg, \"params\", None) or cfg.get(\"params\", {}) or {}\n",
    "        if name == \"custom\":\n",
    "            fn = params.get(\"lambda\")\n",
    "            return fn(span_text) if fn else span_text\n",
    "        if name == \"replace\":\n",
    "            return str(params.get(\"new_value\", \"\"))\n",
    "        if name == \"mask\":\n",
    "            mchar = str(params.get(\"masking_char\", \"*\"))\n",
    "            n = int(params.get(\"chars_to_mask\", len(span_text)))\n",
    "            from_end = bool(params.get(\"from_end\", False))\n",
    "            n = max(0, min(n, len(span_text)))\n",
    "            if n == 0: return span_text\n",
    "            if n >= len(span_text): return mchar * len(span_text)\n",
    "            return (span_text[:-n] + mchar * n) if from_end else (mchar * n + span_text[n:])\n",
    "        return \"*\" * max(1, len(span_text))\n",
    "\n",
    "    # copy & sort right-to-left\n",
    "    res = [deepcopy(r) for r in results if r.entity_type != \"PERSON\"]\n",
    "    res.sort(key=lambda r: r.start, reverse=True)\n",
    "\n",
    "    out = text\n",
    "    logs = []\n",
    "\n",
    "    # per-entry maps\n",
    "    consistent_maps = consistent_maps or {}\n",
    "    city_map    = consistent_maps.setdefault(\"LOCATION_CITY\", {})     # norm(original) -> fake\n",
    "    country_map = consistent_maps.setdefault(\"LOCATION_COUNTRY\", {})  # norm(original) -> fake\n",
    "\n",
    "    for r in res:\n",
    "        etype = r.entity_type\n",
    "        before = out[r.start:r.end]\n",
    "        cfg = operators.get(etype, operators.get(\"DEFAULT\"))\n",
    "        if cfg is None:\n",
    "            continue\n",
    "\n",
    "        if etype == \"LOCATION\":\n",
    "            kind = classify_location(before)  # 'city' or 'country'\n",
    "            key  = _norm(before)\n",
    "            if kind == \"country\":\n",
    "                if key in country_map:\n",
    "                    after_raw = country_map[key]\n",
    "                else:\n",
    "                    # generate a country name\n",
    "                    after_raw = fake.country()\n",
    "                    country_map[key] = after_raw\n",
    "            else:\n",
    "                if key in city_map:\n",
    "                    after_raw = city_map[key]\n",
    "                else:\n",
    "                    # generate a city name\n",
    "                    after_raw = fake.city()\n",
    "                    city_map[key] = after_raw\n",
    "\n",
    "            after = _match_casing(before, after_raw)\n",
    "        else:\n",
    "            # non-LOCATION: use operator as-is\n",
    "            after = _run_operator(before, cfg)\n",
    "\n",
    "        out = out[:r.start] + after + out[r.end:]\n",
    "\n",
    "        logs.append({\"entity_type\": etype, \"before\": before, \"after\": after})\n",
    "\n",
    "    return out, logs, consistent_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4b91145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to anonymize: ['special_interest', 'diary_entry', 'selfdefining_memory', 'empty_sheet']\n"
     ]
    }
   ],
   "source": [
    "df_anon = df[[\"special_interest\", \"diary_entry\", \"selfdefining_memory\", \"empty_sheet\"]]\n",
    "print(\"Columns to anonymize:\", df_anon.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "798d23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_operators = {\n",
    "    \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.phone_number()}),\n",
    "    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.email()}),\n",
    "    \"LOCATION\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x}),\n",
    "    \"PUBLIC_FIGURE\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x}),\n",
    "    \"PET_NAME\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x}),\n",
    "    \"CREDIT_CARD\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.credit_card_number()}),\n",
    "    \"ORGANIZATION\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x}),\n",
    "    \"DEFAULT\": OperatorConfig(operator_name=\"mask\", params={'chars_to_mask': 10, 'masking_char': '*', 'from_end': False}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2b11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to anonymization_summary.md\n"
     ]
    }
   ],
   "source": [
    "modified_count = 0\n",
    "entity_counter = Counter()\n",
    "change_log = []\n",
    "\n",
    "for idx, row in df_anon.iterrows():\n",
    "    for column in df_anon.columns:\n",
    "        cell_value = row[column]\n",
    "        if pd.notna(cell_value) and str(cell_value).strip():\n",
    "            text = str(cell_value)\n",
    "            \n",
    "            results = detect_with_exemptions(text)\n",
    "            results = drop_false_entities(results, text)\n",
    "            results = drop_ignored_names(results, text)\n",
    "            \n",
    "            text_after_person, results_without_person, person_repl = _replace_persons_with_gendered_fakes(\n",
    "                text, results, fake\n",
    "            )\n",
    "            \n",
    "            per_entry_maps = {}  # resets every entry; move outside loop for global consistency\n",
    "            text_final, non_person_repl, per_entry_maps = _apply_non_person_with_logging(\n",
    "                text_after_person,\n",
    "                results_without_person,\n",
    "                fake_operators,\n",
    "                fake=fake,\n",
    "                consistent_maps=per_entry_maps\n",
    "            )\n",
    "            \n",
    "            # result_obj = anonymizer.anonymize(\n",
    "            #     text=text_after_person,\n",
    "            #     analyzer_results=results_without_person,\n",
    "            #     operators=fake_operators\n",
    "            # )\n",
    "            # anonymized_text = result_obj.text\n",
    "            \n",
    "            anonymized_text = text_final\n",
    "            all_replacements = person_repl + non_person_repl\n",
    "            \n",
    "            # tmp_text = text_final\n",
    "            # items_desc = sorted(result_obj.items, key=lambda it: it.start, reverse=True)\n",
    "            \n",
    "            # presidio_repl = []\n",
    "            # for it in items_desc:\n",
    "            #     before_span = tmp_text[it.start:it.end]   # correct \"before\" in current state\n",
    "            #     presidio_repl.append({\n",
    "            #         \"entity_type\": it.entity_type,\n",
    "            #         \"before\": before_span,\n",
    "            #         \"after\": it.text\n",
    "            #     })\n",
    "            #     # apply the replacement so subsequent indices stay valid\n",
    "            #     tmp_text = tmp_text[:it.start] + it.text + tmp_text[it.end:]\n",
    "            # all_replacements = person_repl + presidio_repl\n",
    "            \n",
    "            if anonymized_text != cell_value:\n",
    "                modified_count += 1\n",
    "                for rep in all_replacements:\n",
    "                    entity_counter[rep[\"entity_type\"]] += 1\n",
    "                change_log.append({\n",
    "                    \"row\": idx,\n",
    "                    \"column\": column,\n",
    "                    \"original\": cell_value,\n",
    "                    \"anonymized\": anonymized_text,\n",
    "                    \"replacements\": all_replacements\n",
    "                })\n",
    "\n",
    "            df_anon.at[idx, column] = anonymized_text\n",
    "\n",
    "# ---- summary ----\n",
    "lines = []\n",
    "lines.append(\"=\"*60)\n",
    "lines.append(f\"Total modified entries: {modified_count}\")\n",
    "lines.append(\"Entities replaced (by type):\")\n",
    "for ent, count in entity_counter.most_common():\n",
    "    lines.append(f\"  {ent}: {count}\")\n",
    "\n",
    "lines.append(\"=\"*60)\n",
    "lines.append(\"Example changes:\")\n",
    "for log in change_log:\n",
    "    lines.append(f\"Row {log['row']} | Col {log['column']}\")\n",
    "    for rep in log[\"replacements\"]:\n",
    "        lines.append(f\"  {rep['entity_type']}: {rep['before']} -> {rep['after']}\")\n",
    "    lines.append(\"\\n\")\n",
    "    lines.append(f\"Original  : {log['original']}\\n\")\n",
    "    lines.append(f\"Anonymized: {log['anonymized']}\\n\")\n",
    "    lines.append(\"-\"*40)\n",
    "\n",
    "# Join as one string\n",
    "summary_text = \"\\n\".join(lines)\n",
    "# Save to Markdown file\n",
    "with open(\"data/anonymization_summary.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(\"Summary saved to data/anonymization_summary.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42e30ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>sex</th>\n",
       "      <th>adhd_diagnosis</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>occupation</th>\n",
       "      <th>dialect</th>\n",
       "      <th>forgetting_objects</th>\n",
       "      <th>forgetting_responsabilities</th>\n",
       "      <th>emotion_management</th>\n",
       "      <th>...</th>\n",
       "      <th>need_fast_talk_interest</th>\n",
       "      <th>need_fast_talk_information</th>\n",
       "      <th>speaking_before_thinking</th>\n",
       "      <th>something_to_add</th>\n",
       "      <th>something_to_add_timid</th>\n",
       "      <th>something_to_add_impulsive</th>\n",
       "      <th>special_interest</th>\n",
       "      <th>diary_entry</th>\n",
       "      <th>selfdefining_memory</th>\n",
       "      <th>empty_sheet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-29T18:09:34+01:00</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>21</td>\n",
       "      <td>Licenciatura</td>\n",
       "      <td>Estudante</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-30T16:29:47+02:00</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>22</td>\n",
       "      <td>Licenciatura</td>\n",
       "      <td>Estudante</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Adoro cozinhar, porque sinto que é a forma ide...</td>\n",
       "      <td>Hoje foi um dia bastante normal, como ontem fi...</td>\n",
       "      <td>Quando era mais nova, eu tinha uma professora ...</td>\n",
       "      <td>Recentemente tenho pensado muito no impacto qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-31T12:05:45+02:00</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>53</td>\n",
       "      <td>Licenciatura</td>\n",
       "      <td>Trabalhador</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-31T12:08:00+02:00</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>23</td>\n",
       "      <td>Mestrado</td>\n",
       "      <td>Trabalhador-estudante</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Gosto de ver séries porque relatam relações sa...</td>\n",
       "      <td>Os eventos que considero mais relevantes serão...</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>Este questionário foi um pouco extenso. Meu au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-31T12:21:38+02:00</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Não tenho</td>\n",
       "      <td>20</td>\n",
       "      <td>Licenciatura</td>\n",
       "      <td>Estudante</td>\n",
       "      <td>Português de Portugal</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    datetime        sex adhd_diagnosis  age     education  \\\n",
       "0  2025-03-29T18:09:34+01:00  Masculino      Não tenho   21  Licenciatura   \n",
       "1  2025-03-30T16:29:47+02:00   Feminino      Não tenho   22  Licenciatura   \n",
       "2  2025-03-31T12:05:45+02:00   Feminino      Não tenho   53  Licenciatura   \n",
       "3  2025-03-31T12:08:00+02:00   Feminino      Não tenho   23      Mestrado   \n",
       "4  2025-03-31T12:21:38+02:00   Feminino      Não tenho   20  Licenciatura   \n",
       "\n",
       "              occupation                dialect forgetting_objects  \\\n",
       "0              Estudante  Português de Portugal                  2   \n",
       "1              Estudante  Português de Portugal                  3   \n",
       "2            Trabalhador  Português de Portugal                  3   \n",
       "3  Trabalhador-estudante  Português de Portugal                  2   \n",
       "4              Estudante  Português de Portugal                  4   \n",
       "\n",
       "  forgetting_responsabilities emotion_management  ... need_fast_talk_interest  \\\n",
       "0                           3                  2  ...                       2   \n",
       "1                           3                  4  ...                       1   \n",
       "2                           4                  4  ...                       1   \n",
       "3                           1                  4  ...                       1   \n",
       "4                           2                  2  ...                       2   \n",
       "\n",
       "  need_fast_talk_information speaking_before_thinking something_to_add  \\\n",
       "0                          1                        1                3   \n",
       "1                          1                        3                4   \n",
       "2                          1                        3                3   \n",
       "3                          1                        4                4   \n",
       "4                          2                        4                3   \n",
       "\n",
       "  something_to_add_timid something_to_add_impulsive  \\\n",
       "0                      4                          2   \n",
       "1                      4                          2   \n",
       "2                      1                          2   \n",
       "3                      4                          2   \n",
       "4                      4                          3   \n",
       "\n",
       "                                    special_interest  \\\n",
       "0                                                NaN   \n",
       "1  Adoro cozinhar, porque sinto que é a forma ide...   \n",
       "2                                                NaN   \n",
       "3  Gosto de ver séries porque relatam relações sa...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         diary_entry  \\\n",
       "0                                                NaN   \n",
       "1  Hoje foi um dia bastante normal, como ontem fi...   \n",
       "2                                                NaN   \n",
       "3  Os eventos que considero mais relevantes serão...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                 selfdefining_memory  \\\n",
       "0                                                NaN   \n",
       "1  Quando era mais nova, eu tinha uma professora ...   \n",
       "2                                                NaN   \n",
       "3                                          Não tenho   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         empty_sheet  \n",
       "0                                                NaN  \n",
       "1  Recentemente tenho pensado muito no impacto qu...  \n",
       "2                                                NaN  \n",
       "3  Este questionário foi um pouco extenso. Meu au...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df.drop(columns=df_anon.columns), df_anon], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a79888d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"../../data/adhd-beliefs-pt/adhd-beliefs-pt-anonymized.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adhd-linguistic-patterns-beliefs-portuguese-women",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
