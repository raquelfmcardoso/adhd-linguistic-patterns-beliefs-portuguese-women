{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f601a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from pingouin import bayesfactor_ttest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b173b38",
   "metadata": {},
   "source": [
    "## ADHD vs. Non-ADHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa32460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../data/adhd-beliefs-pt/adhd-beliefs-pt-liwc-proportional.pkl\")\n",
    "mask_adhd = (df['adhd_diagnosis']==\"Sim, diagnosticado\")\n",
    "mask_others = ~mask_adhd\n",
    "features = df.columns[-64:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea845ca",
   "metadata": {},
   "source": [
    "## Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b7f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_liwc(df, features, mask_g1, mask_g2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    For each LIWC feature:\n",
    "      - Welch’s t-test\n",
    "      - JZS Bayes factor\n",
    "      - Cohen’s d\n",
    "    Returns a DataFrame with p-values, BF10, d, FDR‐corrected p’s, etc.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for feat in features:\n",
    "        g1 = df.loc[mask_g1, feat].dropna()\n",
    "        g2 = df.loc[mask_g2, feat].dropna()\n",
    "        t_stat, p_val = stats.ttest_ind(g1, g2, equal_var=False)\n",
    "        n1, n2 = len(g1), len(g2)\n",
    "        bf10 = bayesfactor_ttest(t_stat, n1, n2, paired=False)\n",
    "        s1, s2 = g1.std(ddof=1), g2.std(ddof=1)\n",
    "        s_pool = np.sqrt(((n1-1)*s1**2 + (n2-1)*s2**2)/(n1+n2-2))\n",
    "        d = (g1.mean() - g2.mean())/s_pool\n",
    "        rows.append({\n",
    "            'feature': feat,\n",
    "            'mean_g1': g1.mean(),\n",
    "            'sd_g1': s1,\n",
    "            'mean_g2': g2.mean(),\n",
    "            'sd_g2': s2,\n",
    "            't_stat': t_stat,\n",
    "            'p_val': p_val,\n",
    "            'bf10': bf10,\n",
    "            'cohen_d': d\n",
    "        })\n",
    "    df_res = pd.DataFrame(rows)\n",
    "    _, p_corr, _, _ = multipletests(df_res['p_val'], method='fdr_bh')\n",
    "    df_res['p_fdr'] = p_corr\n",
    "    df_res['signif'] = df_res['p_fdr'] <= alpha\n",
    "    df_res['abs_cohen_d'] = df_res['cohen_d'].abs()\n",
    "    return df_res.sort_values('abs_cohen_d', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ece417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_group_diff(df, features, mask_g1, mask_g2, n_pc=5, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Standardize LIWC features, run PCA, perform Welch’s t-test on each PC\n",
    "    Returns a DataFrame of PC, explained_variance, t_stat, p_val, p_fdr.\n",
    "    \"\"\"\n",
    "    X = df[features].fillna(0).values\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=n_pc)\n",
    "    pcs = pca.fit_transform(Xs)\n",
    "    rows = []\n",
    "    for i in range(n_pc):\n",
    "        comp = pcs[:, i]\n",
    "        t, p = stats.ttest_ind(comp[mask_g1], comp[mask_g2], equal_var=False)\n",
    "        rows.append({\n",
    "            'PC': f'PC{i+1}',\n",
    "            'expl_var': pca.explained_variance_ratio_[i],\n",
    "            't_stat':   t,\n",
    "            'p_val':    p\n",
    "        })\n",
    "    df_pc = pd.DataFrame(rows)\n",
    "    _, p_corr, _, _ = multipletests(df_pc['p_val'], method='fdr_bh')\n",
    "    df_pc['p_fdr'] = p_corr\n",
    "    return df_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b234d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_pc1_loadings(df, features, n=10):\n",
    "    X = df[features].fillna(0).values\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(Xs)\n",
    "    load = pd.Series(pca.components_[0], index=features)\n",
    "    df_load = load.abs().sort_values(ascending=False).head(n).to_frame('abs_loading')\n",
    "    df_load['loading'] = load.loc[df_load.index]\n",
    "    return df_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f08ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _adaptive_inner_cv(y, target_splits=5):\n",
    "    \"\"\"Use 5 folds if every class has >=5 samples; otherwise back off to 3.\"\"\"\n",
    "    min_class = np.min(np.bincount(y))\n",
    "    n_splits = target_splits if min_class >= target_splits else 3\n",
    "    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297bf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logregcv_l1(inner_cv):\n",
    "    \"\"\"Configured L1-Logistic with explicit C grid and ROC-AUC tuning.\"\"\"\n",
    "    C_grid = np.logspace(-4, 4, 30)\n",
    "    return LogisticRegressionCV(\n",
    "        Cs=C_grid, cv=inner_cv, penalty=\"l1\", solver=\"saga\",\n",
    "        scoring=\"roc_auc\", class_weight=\"balanced\",\n",
    "        max_iter=5000, random_state=42, refit=True, n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b90d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stratified_bootstrap(X, y, rng):\n",
    "    \"\"\"Per-class resample with replacement; preserves class balance.\"\"\"\n",
    "    Xb_list, yb_list = [], []\n",
    "    for cls in np.unique(y):\n",
    "        idx = np.where(y == cls)[0]\n",
    "        samp = rng.choice(idx, size=len(idx), replace=True)\n",
    "        Xb_list.append(X[samp])\n",
    "        yb_list.append(y[samp])\n",
    "    Xb = np.vstack(Xb_list)\n",
    "    yb = np.concatenate(yb_list)\n",
    "    perm = rng.permutation(len(yb))\n",
    "    return Xb[perm], yb[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188dd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_logistic_top(df, features, mask_g1, n=10):\n",
    "    X = df[features].fillna(0).values\n",
    "    y = mask_g1.astype(int).values\n",
    "\n",
    "    inner_cv = _adaptive_inner_cv(y, target_splits=5)\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", _logregcv_l1(inner_cv))\n",
    "    ])\n",
    "    pipe.fit(X, y)\n",
    "\n",
    "    coef = pd.Series(pipe.named_steps[\"clf\"].coef_[0], index=features)\n",
    "    top = coef.abs().sort_values(ascending=False).head(n).index\n",
    "    return pd.DataFrame({\"coef\": coef.loc[top]}).sort_values(\"coef\", key=np.abs, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2280dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_auc(df, features, mask_g1):\n",
    "    X = df[features].fillna(0).values\n",
    "    y = mask_g1.astype(int).values\n",
    "\n",
    "    inner_cv = _adaptive_inner_cv(y, target_splits=5)\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", _logregcv_l1(inner_cv))\n",
    "    ])\n",
    "\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "    auc_scores = cross_val_score(pipe, X, y, cv=outer_cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "    return auc_scores.mean(), auc_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201ed227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_stability(df, features, mask_g1, n_boot=100, tol=1e-6):\n",
    "    X = df[features].fillna(0).values\n",
    "    y = mask_g1.astype(int).values\n",
    "\n",
    "    inner_cv = _adaptive_inner_cv(y, target_splits=5)\n",
    "\n",
    "    sel_counts = pd.Series(0, index=features, dtype=float)\n",
    "    pos_counts = pd.Series(0, index=features, dtype=float)\n",
    "    coef_sum   = pd.Series(0.0, index=features)\n",
    "\n",
    "    rng = np.random.RandomState(1000)\n",
    "\n",
    "    for b in range(n_boot):\n",
    "        Xb, yb = _stratified_bootstrap(X, y, rng)\n",
    "        if len(np.unique(yb)) < 2:\n",
    "            continue  # extreme edge case, but safe-guard\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", _logregcv_l1(inner_cv))\n",
    "        ])\n",
    "        pipe.fit(Xb, yb)\n",
    "\n",
    "        coef = pd.Series(pipe.named_steps[\"clf\"].coef_[0], index=features)\n",
    "        selected = coef.abs() > tol\n",
    "\n",
    "        sel_counts[selected] += 1\n",
    "        pos_counts[selected & (coef > 0)] += 1\n",
    "        coef_sum += coef\n",
    "\n",
    "    stability = sel_counts / n_boot\n",
    "    sign_consistency = (pos_counts / sel_counts.replace(0, np.nan))\n",
    "    mean_coef = coef_sum / n_boot\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"sel_prop\": stability,\n",
    "        \"mean_coef\": mean_coef,\n",
    "        \"pos_sign_prop\": sign_consistency\n",
    "    }).sort_values([\"sel_prop\", \"mean_coef\"], ascending=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8414a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d(x, y):\n",
    "    nx, ny = len(x), len(y)\n",
    "    sx, sy = np.std(x, ddof=1), np.std(y, ddof=1)\n",
    "    s_pooled = np.sqrt(((nx-1)*sx**2 + (ny-1)*sy**2) / (nx+ny-2))\n",
    "    return (np.mean(x) - np.mean(y)) / s_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c09f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_cohen_d(x, y):\n",
    "    return abs(cohen_d(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5cc0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(x, y, statfunc, n_boot=1000, ci=95):\n",
    "    boot_stats = []\n",
    "    for _ in range(n_boot):\n",
    "        bx = resample(x, replace=True)\n",
    "        by = resample(y, replace=True)\n",
    "        boot_stats.append(statfunc(bx, by))\n",
    "    lower = np.percentile(boot_stats, (100-ci)/2)\n",
    "    upper = np.percentile(boot_stats, 100-(100-ci)/2)\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "417875f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_priori_power(effect_size=0.6, alpha=0.05, power=0.8):\n",
    "    analysis = TTestIndPower()\n",
    "    return analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39da82a",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbdecb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LIWC dimensions |d| > 0.5:\n",
      "| feature   |   mean_g1 |   sd_g1 |   mean_g2 |   sd_g2 |   t_stat |   p_val |   bf10 |   cohen_d |   p_fdr | signif   |   abs_cohen_d |\n",
      "|:----------|----------:|--------:|----------:|--------:|---------:|--------:|-------:|----------:|--------:|:---------|--------------:|\n",
      "| cogmech   |     0.296 |   0.097 |     0.354 |   0.075 |   -3.174 |   0.003 | 17.400 |    -0.717 |   0.045 | True     |         0.717 |\n",
      "| excl      |     0.037 |   0.022 |     0.054 |   0.029 |   -3.687 |   0.000 | 79.128 |    -0.611 |   0.030 | True     |         0.611 |\n",
      "| tentat    |     0.063 |   0.040 |     0.085 |   0.041 |   -2.887 |   0.006 |  8.158 |    -0.558 |   0.069 | False    |         0.558 |\n",
      "| discrep   |     0.039 |   0.024 |     0.056 |   0.032 |   -3.328 |   0.001 | 26.793 |    -0.556 |   0.045 | True     |         0.556 |\n",
      "| relativ   |     0.163 |   0.059 |     0.196 |   0.061 |   -2.841 |   0.006 |  7.287 |    -0.549 |   0.069 | False    |         0.549 |\n",
      "| funct     |     0.387 |   0.107 |     0.434 |   0.080 |   -2.323 |   0.025 |  2.256 |    -0.538 |   0.201 | False    |         0.538 |\n",
      "| past      |     0.026 |   0.020 |     0.040 |   0.028 |   -3.180 |   0.002 | 17.698 |    -0.509 |   0.045 | True     |         0.509 |\n"
     ]
    }
   ],
   "source": [
    "# 1) Univariate LIWC\n",
    "uni = univariate_liwc(df, features, mask_adhd, mask_others)\n",
    "print(\"\\nLIWC dimensions |d| > 0.5:\")\n",
    "print(uni[uni['abs_cohen_d']>0.5].to_markdown(index=False, floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4149b146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrap 95% CIs for Cohen's d (|d| > 0.5):\n",
      "| feature   |      d |   ci_lower |   ci_upper |\n",
      "|:----------|-------:|-----------:|-----------:|\n",
      "| cogmech   | -0.717 |     -1.164 |     -0.335 |\n",
      "| excl      | -0.611 |     -0.949 |     -0.314 |\n",
      "| tentat    | -0.558 |     -0.932 |     -0.196 |\n",
      "| discrep   | -0.556 |     -0.892 |     -0.237 |\n",
      "| relativ   | -0.549 |     -0.957 |     -0.173 |\n",
      "| funct     | -0.538 |     -1.013 |     -0.128 |\n",
      "| past      | -0.509 |     -0.824 |     -0.216 |\n"
     ]
    }
   ],
   "source": [
    "# 2) Bootstrap CIs for features with |d| > 0.5\n",
    "top_feats = uni[uni['abs_cohen_d'] > 0.5]['feature']\n",
    "ci_list = []\n",
    "for feat in top_feats:\n",
    "    x = df.loc[mask_adhd, feat].dropna().values\n",
    "    y = df.loc[mask_others, feat].dropna().values\n",
    "    d_obs = cohen_d(x, y)\n",
    "    lo, hi = bootstrap_ci(x, y, cohen_d, n_boot=2000, ci=95)\n",
    "    ci_list.append({'feature': feat, 'd': d_obs, 'ci_lower': lo, 'ci_upper': hi})\n",
    "ci_df = pd.DataFrame(ci_list)\n",
    "print(\"\\nBootstrap 95% CIs for Cohen's d (|d| > 0.5):\")\n",
    "print(ci_df.to_markdown(index=False, floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2251f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Required N per group for d=0.6, α=0.05, 80% power: 44.6\n"
     ]
    }
   ],
   "source": [
    "# 3) A priori power\n",
    "req_n = a_priori_power(effect_size=0.6)\n",
    "print(f\"\\nRequired N per group for d=0.6, α=0.05, 80% power: {req_n:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d3fc96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LIWC dimensions |d| > 0.5:\n",
      "| feature   |   mean_g1 |   sd_g1 |   mean_g2 |   sd_g2 |   t_stat |   p_val |   bf10 |   cohen_d |   p_fdr | signif   |   abs_cohen_d |      N |\n",
      "|:----------|----------:|--------:|----------:|--------:|---------:|--------:|-------:|----------:|--------:|:---------|--------------:|-------:|\n",
      "| cogmech   |     0.296 |   0.097 |     0.354 |   0.075 |   -3.174 |   0.003 | 17.400 |    -0.717 |   0.045 | True     |         0.717 | 31.491 |\n",
      "| excl      |     0.037 |   0.022 |     0.054 |   0.029 |   -3.687 |   0.000 | 79.128 |    -0.611 |   0.030 | True     |         0.611 | 43.062 |\n",
      "| tentat    |     0.063 |   0.040 |     0.085 |   0.041 |   -2.887 |   0.006 |  8.158 |    -0.558 |   0.069 | False    |         0.558 | 51.448 |\n",
      "| discrep   |     0.039 |   0.024 |     0.056 |   0.032 |   -3.328 |   0.001 | 26.793 |    -0.556 |   0.045 | True     |         0.556 | 51.800 |\n",
      "| relativ   |     0.163 |   0.059 |     0.196 |   0.061 |   -2.841 |   0.006 |  7.287 |    -0.549 |   0.069 | False    |         0.549 | 53.055 |\n",
      "| funct     |     0.387 |   0.107 |     0.434 |   0.080 |   -2.323 |   0.025 |  2.256 |    -0.538 |   0.201 | False    |         0.538 | 55.309 |\n",
      "| past      |     0.026 |   0.020 |     0.040 |   0.028 |   -3.180 |   0.002 | 17.698 |    -0.509 |   0.045 | True     |         0.509 | 61.620 |\n",
      "\n",
      "Summary of required N per group:\n",
      "Highest N: 13470534.8 (feature: adverb)\n",
      "Lowest N: 31.5 (feature: cogmech)\n",
      "Mean N: 220308.3\n",
      "Median N: 451.9\n",
      "\n",
      "Summary of required N per group:\n",
      "Highest N: 61.6 (feature: past)\n",
      "Mean N: 49.7\n",
      "Median N: 51.8\n"
     ]
    }
   ],
   "source": [
    "# 3.5) A priori power\n",
    "uni = univariate_liwc(df, features, mask_adhd, mask_others)\n",
    "\n",
    "# Compute required N for each effect size\n",
    "uni['N'] = uni['abs_cohen_d'].apply(lambda d: a_priori_power(effect_size=d) if d > 0 else np.nan)\n",
    "\n",
    "print(\"\\nLIWC dimensions |d| > 0.5:\")\n",
    "subset = uni[uni['abs_cohen_d']>0.5]\n",
    "print(subset.to_markdown(index=False, floatfmt=\".3f\"))\n",
    "\n",
    "# Summary statistics for N\n",
    "valid_n = uni['N'].dropna()\n",
    "print(f\"\\nSummary of required N per group:\")\n",
    "print(f\"Highest N: {valid_n.max():.1f} (feature: {uni.loc[uni['N'].idxmax(), 'feature']})\")\n",
    "print(f\"Lowest N: {valid_n.min():.1f} (feature: {uni.loc[uni['N'].idxmin(), 'feature']})\")\n",
    "print(f\"Mean N: {valid_n.mean():.1f}\")\n",
    "print(f\"Median N: {valid_n.median():.1f}\")\n",
    "\n",
    "# Summary statistics for N in d > 0.5\n",
    "valid_n = subset['N'].dropna()\n",
    "if len(valid_n) > 0:\n",
    "    print(f\"\\nSummary of required N per group:\")\n",
    "    print(f\"Highest N: {valid_n.max():.1f} (feature: {subset.loc[subset['N'].idxmax(), 'feature']})\")\n",
    "    print(f\"Mean N: {valid_n.mean():.1f}\")\n",
    "    print(f\"Median N: {valid_n.median():.1f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid N values found for features with |d| > 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "183582a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA group differences:\n",
      "| PC   |   expl_var |   t_stat |   p_val |   p_fdr |\n",
      "|:-----|-----------:|---------:|--------:|--------:|\n",
      "| PC1  |      0.149 |   -2.625 |   0.012 |   0.058 |\n",
      "| PC2  |      0.081 |    1.433 |   0.158 |   0.395 |\n",
      "| PC3  |      0.069 |   -0.269 |   0.789 |   0.800 |\n",
      "| PC4  |      0.062 |    0.255 |   0.800 |   0.800 |\n",
      "| PC5  |      0.053 |    0.550 |   0.585 |   0.800 |\n"
     ]
    }
   ],
   "source": [
    "# 4) PCA group differences\n",
    "pc_res = pca_group_diff(df, features, mask_adhd, mask_others)\n",
    "print(\"\\nPCA group differences:\")\n",
    "print(pc_res.to_markdown(index=False, floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c499f195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top PC1 loadings:\n",
      "|         |   loading |\n",
      "|:--------|----------:|\n",
      "| ipron   |     0.275 |\n",
      "| funct   |     0.274 |\n",
      "| pronoun |     0.274 |\n",
      "| nonfl   |     0.241 |\n",
      "| shehe   |     0.238 |\n",
      "| article |     0.237 |\n",
      "| social  |     0.230 |\n",
      "| you     |     0.224 |\n",
      "| ppron   |     0.221 |\n",
      "| cogmech |     0.220 |\n"
     ]
    }
   ],
   "source": [
    "# 5) PCA group differences\n",
    "pc1_ld = top_pc1_loadings(df, features, n=10)\n",
    "print(\"\\nTop PC1 loadings:\")\n",
    "print(pc1_ld[['loading']].to_markdown(floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fae04433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 127)\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "df_sub = df[mask_adhd | mask_others]\n",
    "new_mask_adhd = (df['adhd_diagnosis']==\"Sim, diagnosticado\")\n",
    "print(df_sub.shape)\n",
    "print(new_mask_adhd.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7d2b034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         |   coef |\n",
      "|:--------|-------:|\n",
      "| cogmech | -2.412 |\n",
      "| conj    |  1.581 |\n",
      "| number  |  1.555 |\n",
      "| discrep | -1.538 |\n",
      "| relig   | -1.451 |\n",
      "| feel    |  1.135 |\n",
      "| certain |  1.107 |\n",
      "| swear   |  1.010 |\n",
      "| i       | -0.973 |\n",
      "| friend  | -0.967 |\n"
     ]
    }
   ],
   "source": [
    "top_coef = l1_logistic_top(df_sub, features, new_mask_adhd, n=10)\n",
    "print(top_coef.to_markdown(floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e32ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested CV AUC: mean=0.593, SD=0.108\n"
     ]
    }
   ],
   "source": [
    "mean_auc, sd_auc = nested_auc(df_sub, features, new_mask_adhd)\n",
    "print(f\"Nested CV AUC: mean={mean_auc:.3f}, SD={sd_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a2028dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         |   sel_prop |   mean_coef |   pos_sign_prop |\n",
      "|:--------|-----------:|------------:|----------------:|\n",
      "| cogmech |      0.950 |      -2.974 |           0.000 |\n",
      "| conj    |      0.900 |       2.516 |           1.000 |\n",
      "| number  |      0.930 |       2.218 |           1.000 |\n",
      "| discrep |      0.910 |      -2.505 |           0.000 |\n",
      "| relig   |      0.870 |      -1.517 |           0.023 |\n",
      "| feel    |      0.810 |       1.232 |           0.951 |\n",
      "| certain |      0.880 |       1.327 |           0.977 |\n",
      "| swear   |      0.960 |       1.833 |           0.990 |\n",
      "| i       |      0.810 |      -0.929 |           0.037 |\n",
      "| friend  |      0.910 |      -1.404 |           0.000 |\n"
     ]
    }
   ],
   "source": [
    "stab = l1_stability(df_sub, features, new_mask_adhd, n_boot=100)\n",
    "print(stab.loc[top_coef.index].to_markdown(floatfmt=\".3f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adhd-linguistic-patterns-beliefs-portuguese-women",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
