{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f601a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from pingouin import bayesfactor_ttest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b173b38",
   "metadata": {},
   "source": [
    "## Women with ADHD vs. Men with ADHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa32460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../data/adhd-beliefs-pt/adhd-beliefs-pt-liwc-proportional.pkl\")\n",
    "mask_women = (df['sex']==\"Feminino\") & (df['adhd_diagnosis']==\"Sim, diagnosticado\")\n",
    "mask_others = (df['sex']==\"Masculino\") & (df['adhd_diagnosis']==\"Sim, diagnosticado\")\n",
    "features = df.columns[-64:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of women with ADHD group\n",
    "mask_women_len = mask_women.sum()\n",
    "print(f\"Women with ADHD: {mask_women_len}\")\n",
    "print(f\"Men with ADHD: {mask_others.sum()}\")\n",
    "\n",
    "# Check if any LIWC features have constant values (zero variance)\n",
    "for feat in features[:5]:  # Check first 5 features as example\n",
    "    g1 = df.loc[mask_women, feat].dropna()\n",
    "    g2 = df.loc[mask_others, feat].dropna()\n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(f\"  Women - min: {g1.min():.4f}, max: {g1.max():.4f}, std: {g1.std():.4f}\")\n",
    "    print(f\"  Men - min: {g2.min():.4f}, max: {g2.max():.4f}, std: {g2.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['we'], errors='ignore')\n",
    "features.remove('we')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea845ca",
   "metadata": {},
   "source": [
    "## Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_liwc(df, features, mask_g1, mask_g2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    For each LIWC feature:\n",
    "      - Welch’s t-test\n",
    "      - JZS Bayes factor\n",
    "      - Cohen’s d\n",
    "    Returns a DataFrame with p-values, BF10, d, FDR‐corrected p’s, etc.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for feat in features:\n",
    "        g1 = df.loc[mask_g1, feat].dropna()\n",
    "        g2 = df.loc[mask_g2, feat].dropna()\n",
    "        t_stat, p_val = stats.ttest_ind(g1, g2, equal_var=False)\n",
    "        n1, n2 = len(g1), len(g2)\n",
    "        bf10 = bayesfactor_ttest(t_stat, n1, n2, paired=False)\n",
    "        s1, s2 = g1.std(ddof=1), g2.std(ddof=1)\n",
    "        s_pool = np.sqrt(((n1-1)*s1**2 + (n2-1)*s2**2)/(n1+n2-2))\n",
    "        d = (g1.mean() - g2.mean())/s_pool\n",
    "        rows.append({\n",
    "            'feature': feat,\n",
    "            'mean_g1': g1.mean(),\n",
    "            'sd_g1': s1,\n",
    "            'mean_g2': g2.mean(),\n",
    "            'sd_g2': s2,\n",
    "            't_stat': t_stat,\n",
    "            'p_val': p_val,\n",
    "            'bf10': bf10,\n",
    "            'cohen_d': d\n",
    "        })\n",
    "    df_res = pd.DataFrame(rows)\n",
    "    _, p_corr, _, _ = multipletests(df_res['p_val'], method='fdr_bh')\n",
    "    df_res['p_fdr'] = p_corr\n",
    "    df_res['signif'] = df_res['p_fdr'] <= alpha\n",
    "    df_res['abs_cohen_d'] = df_res['cohen_d'].abs()\n",
    "    return df_res.sort_values('abs_cohen_d', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_group_diff(df, features, mask_g1, mask_g2, n_pc=5, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Standardize LIWC features, run PCA, perform Welch’s t-test on each PC\n",
    "    Returns a DataFrame of PC, explained_variance, t_stat, p_val, p_fdr.\n",
    "    \"\"\"\n",
    "    X = df[features].fillna(0).values\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=n_pc)\n",
    "    pcs = pca.fit_transform(Xs)\n",
    "    rows = []\n",
    "    for i in range(n_pc):\n",
    "        comp = pcs[:, i]\n",
    "        t, p = stats.ttest_ind(comp[mask_g1], comp[mask_g2], equal_var=False)\n",
    "        rows.append({\n",
    "            'PC': f'PC{i+1}',\n",
    "            'expl_var': pca.explained_variance_ratio_[i],\n",
    "            't_stat':   t,\n",
    "            'p_val':    p\n",
    "        })\n",
    "    df_pc = pd.DataFrame(rows)\n",
    "    _, p_corr, _, _ = multipletests(df_pc['p_val'], method='fdr_bh')\n",
    "    df_pc['p_fdr'] = p_corr\n",
    "    return df_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_pc1_loadings(df, features, n=10):\n",
    "    X = df[features].fillna(0).values\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(Xs)\n",
    "    load = pd.Series(pca.components_[0], index=features)\n",
    "    df_load = load.abs().sort_values(ascending=False).head(n).to_frame('abs_loading')\n",
    "    df_load['loading'] = load.loc[df_load.index]\n",
    "    return df_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _adaptive_inner_cv(y, target_splits=5):\n",
    "    \"\"\"Use 5 folds if every class has >=5 samples; otherwise back off to 3.\"\"\"\n",
    "    min_class = np.min(np.bincount(y))\n",
    "    n_splits = target_splits if min_class >= target_splits else 3\n",
    "    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logregcv_l1(inner_cv):\n",
    "    \"\"\"Configured L1-Logistic with explicit C grid and ROC-AUC tuning.\"\"\"\n",
    "    C_grid = np.logspace(-4, 4, 30)\n",
    "    return LogisticRegressionCV(\n",
    "        Cs=C_grid, cv=inner_cv, penalty=\"l1\", solver=\"saga\",\n",
    "        scoring=\"roc_auc\", class_weight=\"balanced\",\n",
    "        max_iter=5000, random_state=42, refit=True, n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stratified_bootstrap(X, y, rng):\n",
    "    \"\"\"Per-class resample with replacement; preserves class balance.\"\"\"\n",
    "    Xb_list, yb_list = [], []\n",
    "    for cls in np.unique(y):\n",
    "        idx = np.where(y == cls)[0]\n",
    "        samp = rng.choice(idx, size=len(idx), replace=True)\n",
    "        Xb_list.append(X[samp])\n",
    "        yb_list.append(y[samp])\n",
    "    Xb = np.vstack(Xb_list)\n",
    "    yb = np.concatenate(yb_list)\n",
    "    perm = rng.permutation(len(yb))\n",
    "    return Xb[perm], yb[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc248db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_logistic_top(df, features, mask_g1, n=10):\n",
    "    X = df[features].fillna(0).values\n",
    "    y = mask_g1.astype(int).values\n",
    "\n",
    "    inner_cv = _adaptive_inner_cv(y, target_splits=5)\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", _logregcv_l1(inner_cv))\n",
    "    ])\n",
    "    pipe.fit(X, y)\n",
    "\n",
    "    coef = pd.Series(pipe.named_steps[\"clf\"].coef_[0], index=features)\n",
    "    top = coef.abs().sort_values(ascending=False).head(n).index\n",
    "    return pd.DataFrame({\"coef\": coef.loc[top]}).sort_values(\"coef\", key=np.abs, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_auc(df, features, mask_g1):\n",
    "    X = df[features].fillna(0).values\n",
    "    y = mask_g1.astype(int).values\n",
    "\n",
    "    inner_cv = _adaptive_inner_cv(y, target_splits=5)\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", _logregcv_l1(inner_cv))\n",
    "    ])\n",
    "\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "    auc_scores = cross_val_score(pipe, X, y, cv=outer_cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "    return auc_scores.mean(), auc_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe811f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_stability(df, features, mask_g1, n_boot=100, tol=1e-6):\n",
    "    X = df[features].fillna(0).values\n",
    "    y = mask_g1.astype(int).values\n",
    "\n",
    "    inner_cv = _adaptive_inner_cv(y, target_splits=5)\n",
    "\n",
    "    sel_counts = pd.Series(0, index=features, dtype=float)\n",
    "    pos_counts = pd.Series(0, index=features, dtype=float)\n",
    "    coef_sum   = pd.Series(0.0, index=features)\n",
    "\n",
    "    rng = np.random.RandomState(1000)\n",
    "\n",
    "    for b in range(n_boot):\n",
    "        Xb, yb = _stratified_bootstrap(X, y, rng)\n",
    "        if len(np.unique(yb)) < 2:\n",
    "            continue  # extreme edge case, but safe-guard\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", _logregcv_l1(inner_cv))\n",
    "        ])\n",
    "        pipe.fit(Xb, yb)\n",
    "\n",
    "        coef = pd.Series(pipe.named_steps[\"clf\"].coef_[0], index=features)\n",
    "        selected = coef.abs() > tol\n",
    "\n",
    "        sel_counts[selected] += 1\n",
    "        pos_counts[selected & (coef > 0)] += 1\n",
    "        coef_sum += coef\n",
    "\n",
    "    stability = sel_counts / n_boot\n",
    "    sign_consistency = (pos_counts / sel_counts.replace(0, np.nan))\n",
    "    mean_coef = coef_sum / n_boot\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"sel_prop\": stability,\n",
    "        \"mean_coef\": mean_coef,\n",
    "        \"pos_sign_prop\": sign_consistency\n",
    "    }).sort_values([\"sel_prop\", \"mean_coef\"], ascending=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d(x, y):\n",
    "    nx, ny = len(x), len(y)\n",
    "    sx, sy = np.std(x, ddof=1), np.std(y, ddof=1)\n",
    "    s_pooled = np.sqrt(((nx-1)*sx**2 + (ny-1)*sy**2) / (nx+ny-2))\n",
    "    return (np.mean(x) - np.mean(y)) / s_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_cohen_d(x, y):\n",
    "    return abs(cohen_d(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(x, y, statfunc, n_boot=1000, ci=95):\n",
    "    boot_stats = []\n",
    "    for _ in range(n_boot):\n",
    "        bx = resample(x, replace=True)\n",
    "        by = resample(y, replace=True)\n",
    "        boot_stats.append(statfunc(bx, by))\n",
    "    lower = np.percentile(boot_stats, (100-ci)/2)\n",
    "    upper = np.percentile(boot_stats, 100-(100-ci)/2)\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417875f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_priori_power(effect_size=0.6, alpha=0.05, power=0.8):\n",
    "    analysis = TTestIndPower()\n",
    "    return analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39da82a",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdecb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Univariate LIWC\n",
    "uni = univariate_liwc(df, features, mask_women, mask_others)\n",
    "print(\"\\nLIWC dimensions |d| > 0.5:\")\n",
    "print(uni[uni['abs_cohen_d']>0.5].to_markdown(index=False, floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954bafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Bootstrap CIs for features with |d| > 0.5\n",
    "top_feats = uni[uni['abs_cohen_d'] > 0.5]['feature']\n",
    "ci_list = []\n",
    "for feat in top_feats:\n",
    "    x = df.loc[mask_women, feat].dropna().values\n",
    "    y = df.loc[mask_others, feat].dropna().values\n",
    "    d_obs = cohen_d(x, y)\n",
    "    lo, hi = bootstrap_ci(x, y, cohen_d, n_boot=2000, ci=95)\n",
    "    ci_list.append({'feature': feat, 'd': d_obs, 'ci_lower': lo, 'ci_upper': hi})\n",
    "ci_df = pd.DataFrame(ci_list)\n",
    "print(\"\\nBootstrap 95% CIs for Cohen's d (|d| > 0.5):\")\n",
    "print(ci_df.to_markdown(index=False, floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) A priori power\n",
    "req_n = a_priori_power(effect_size=0.6)\n",
    "print(f\"\\nRequired N per group for d=0.6, α=0.05, 80% power: {req_n:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7dfd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5) A priori power\n",
    "uni = univariate_liwc(df, features, mask_women, mask_others)\n",
    "\n",
    "# Compute required N for each effect size\n",
    "uni['N'] = uni['abs_cohen_d'].apply(lambda d: a_priori_power(effect_size=d) if d > 0 else np.nan)\n",
    "\n",
    "print(\"\\nLIWC dimensions |d| > 0.5:\")\n",
    "subset = uni[uni['abs_cohen_d']>0.5]\n",
    "print(subset.to_markdown(index=False, floatfmt=\".3f\"))\n",
    "\n",
    "# Summary statistics for N\n",
    "valid_n = uni['N'].dropna()\n",
    "print(f\"\\nSummary of required N per group:\")\n",
    "print(f\"Highest N: {valid_n.max():.1f} (feature: {uni.loc[uni['N'].idxmax(), 'feature']})\")\n",
    "print(f\"Lowest N: {valid_n.min():.1f} (feature: {uni.loc[uni['N'].idxmin(), 'feature']})\")\n",
    "print(f\"Mean N: {valid_n.mean():.1f}\")\n",
    "print(f\"Median N: {valid_n.median():.1f}\")\n",
    "\n",
    "# Summary statistics for N in d > 0.5\n",
    "valid_n = subset['N'].dropna()\n",
    "if len(valid_n) > 0:\n",
    "    print(f\"\\nSummary of required N per group:\")\n",
    "    print(f\"Highest N: {valid_n.max():.1f} (feature: {subset.loc[subset['N'].idxmax(), 'feature']})\")\n",
    "    print(f\"Mean N: {valid_n.mean():.1f}\")\n",
    "    print(f\"Median N: {valid_n.median():.1f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid N values found for features with |d| > 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183582a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) PCA group differences\n",
    "pc_res = pca_group_diff(df, features, mask_women, mask_others)\n",
    "print(\"\\nPCA group differences:\")\n",
    "print(pc_res.to_markdown(index=False, floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) PCA group differences\n",
    "pc1_ld = top_pc1_loadings(df, features, n=10)\n",
    "print(\"\\nTop PC1 loadings:\")\n",
    "print(pc1_ld[['loading']].to_markdown(floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df[mask_women | mask_others]\n",
    "new_mask_women = (df_sub['sex']==\"Feminino\") & (df_sub['adhd_diagnosis']==\"Sim, diagnosticado\")\n",
    "print(df_sub.shape)\n",
    "print(new_mask_women.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ead45",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_coef = l1_logistic_top(df_sub, features, new_mask_women, n=10)\n",
    "print(top_coef.to_markdown(floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0efd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_auc, sd_auc = nested_auc(df_sub, features, new_mask_women)\n",
    "print(f\"Nested CV AUC: mean={mean_auc:.3f}, SD={sd_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stab = l1_stability(df_sub, features, new_mask_women, n_boot=100)\n",
    "print(stab.loc[top_coef.index].to_markdown(floatfmt=\".3f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adhd-linguistic-patterns-beliefs-portuguese-women",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
