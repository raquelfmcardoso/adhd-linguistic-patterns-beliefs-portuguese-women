{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce594e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bertopic import BERTopic\n",
    "from collections import Counter\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee0b4c",
   "metadata": {},
   "source": [
    "## Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = pd.read_pickle(\"../../data/adhd-beliefs-pt/adhd-beliefs-pt-embeddings-serafim-bertopic.pkl\")\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1145db9",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ccb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_word_weight_matrix(topic_model, top_n=30):\n",
    "    \"\"\"Extract topic-word weights for alignment\"\"\"\n",
    "    topics_info = topic_model.get_topics()\n",
    "    topic_ids = [tid for tid in topics_info.keys() if tid != -1]\n",
    "    t2weights = {}\n",
    "    for tid in topic_ids:\n",
    "        pairs = topics_info[tid][:top_n]\n",
    "        t2weights[tid] = dict(pairs)\n",
    "    return t2weights, topic_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_union_vocab(t2w_A, t2w_B):\n",
    "    \"\"\"Build union vocabulary from two topic models\"\"\"\n",
    "    vocab = sorted(set().union(*[set(d.keys()) for d in t2w_A.values()],\n",
    "                               *[set(d.keys()) for d in t2w_B.values()]))\n",
    "    index = {w:i for i,w in enumerate(vocab)}\n",
    "    return vocab, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb584730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_to_matrix(t2w, topic_ids, index):\n",
    "    \"\"\"Convert topic-word weights to normalized matrix\"\"\"\n",
    "    K, V = len(topic_ids), len(index)\n",
    "    M = np.zeros((K, V), dtype=float)\n",
    "    for r, tid in enumerate(topic_ids):\n",
    "        for w, wt in t2w[tid].items():\n",
    "            j = index.get(w)\n",
    "            if j is not None:\n",
    "                M[r, j] = wt\n",
    "    norms = np.linalg.norm(M, axis=1, keepdims=True) + 1e-12\n",
    "    return M / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_topics_by_cosine(model_A, model_B, top_n=30):\n",
    "    \"\"\"Align topics between two models using cosine similarity\"\"\"\n",
    "    t2w_A, tids_A = topic_word_weight_matrix(model_A, top_n=top_n)\n",
    "    t2w_B, tids_B = topic_word_weight_matrix(model_B, top_n=top_n)\n",
    "    vocab, index = build_union_vocab(t2w_A, t2w_B)\n",
    "    A = topics_to_matrix(t2w_A, tids_A, index)\n",
    "    B = topics_to_matrix(t2w_B, tids_B, index)\n",
    "    sim = cosine_similarity(A, B)\n",
    "    cost = 1.0 - sim\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "    matches = [(tids_A[i], tids_B[j], float(sim[i, j])) for i, j in zip(row_ind, col_ind)]\n",
    "    return tids_A, tids_B, matches, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fcb77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_per_topic(topic_ids_for_docs):\n",
    "    \"\"\"Count documents per topic\"\"\"\n",
    "    c = Counter([t for t in topic_ids_for_docs if t != -1])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_corrected_analysis(A_vec, B_vec, group_names=(\"ADHD\", \"non-ADHD\")):\n",
    "    \"\"\"\n",
    "    Statistically appropriate analysis for topic distribution data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"CORRECTED TOPIC ANALYSIS: {group_names[0]} vs {group_names[1]}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. EFFECT SIZE (Always valid)\n",
    "    jsd = jensenshannon(A_vec / A_vec.sum(), B_vec / B_vec.sum(), base=2)\n",
    "    print(f\"\\nüìä EFFECT SIZE:\")\n",
    "    print(f\"Jensen-Shannon Divergence: {jsd:.3f}\")\n",
    "    \n",
    "    if jsd < 0.1:\n",
    "        effect_size = \"Very small effect\"\n",
    "    elif jsd < 0.3:\n",
    "        effect_size = \"Small to moderate effect\"\n",
    "    elif jsd < 0.5:\n",
    "        effect_size = \"Large effect\"\n",
    "    else:\n",
    "        effect_size = \"Very large effect\"\n",
    "    print(f\"Interpretation: {effect_size}\")\n",
    "    \n",
    "    # 2. DATA STRUCTURE ANALYSIS\n",
    "    A_nonzero = np.sum(A_vec > 0)\n",
    "    B_nonzero = np.sum(B_vec > 0)\n",
    "    overlap = np.sum((A_vec > 0) & (B_vec > 0))\n",
    "    \n",
    "    print(f\"\\nüìà DATA STRUCTURE:\")\n",
    "    print(f\"{group_names[0]} uses {A_nonzero} topics\")\n",
    "    print(f\"{group_names[1]} uses {B_nonzero} topics\") \n",
    "    print(f\"Overlapping topics: {overlap}\")\n",
    "    print(f\"Total documents: {group_names[0]}={A_vec.sum()}, {group_names[1]}={B_vec.sum()}\")\n",
    "    \n",
    "    # 3. STATISTICAL SIGNIFICANCE (Fisher's exact test)\n",
    "    A_specific = np.sum(A_vec[B_vec == 0])  # Group A only\n",
    "    A_shared = np.sum(A_vec[B_vec > 0])     # Both groups\n",
    "    B_specific = np.sum(B_vec[A_vec == 0])  # Group B only  \n",
    "    B_shared = np.sum(B_vec[A_vec > 0])     # Both groups\n",
    "    \n",
    "    table_2x2 = np.array([[A_specific, A_shared], \n",
    "                          [B_specific, B_shared]])\n",
    "    \n",
    "    print(f\"\\nüéØ STATISTICAL SIGNIFICANCE:\")\n",
    "    print(f\"Strategy comparison table:\")\n",
    "    print(f\"                Group-Specific  Shared\")\n",
    "    print(f\"{group_names[0]:15s} {A_specific:13d} {A_shared:7d}\")\n",
    "    print(f\"{group_names[1]:15s} {B_specific:13d} {B_shared:7d}\")\n",
    "    \n",
    "    odds_ratio, p_fisher = fisher_exact(table_2x2)\n",
    "    print(f\"\\nFisher's exact test:\")\n",
    "    print(f\"p-value: {p_fisher:.6f}\")\n",
    "    print(f\"Odds ratio: {odds_ratio:.3f}\")\n",
    "    \n",
    "    if p_fisher < 0.001:\n",
    "        sig_level = \"Highly significant (p < 0.001)\"\n",
    "    elif p_fisher < 0.01:\n",
    "        sig_level = \"Very significant (p < 0.01)\"\n",
    "    elif p_fisher < 0.05:\n",
    "        sig_level = \"Significant (p < 0.05)\"\n",
    "    else:\n",
    "        sig_level = \"Not significant (p ‚â• 0.05)\"\n",
    "    \n",
    "    print(f\"Result: {sig_level}\")\n",
    "    \n",
    "    # 4. COMBINED INTERPRETATION\n",
    "    print(f\"\\nüéØ CONCLUSION:\")\n",
    "    if jsd >= 0.3 and p_fisher < 0.05:\n",
    "        conclusion = \"‚úÖ STRONG EVIDENCE: Groups have substantially different topic patterns\"\n",
    "    elif jsd >= 0.1 and p_fisher < 0.05:\n",
    "        conclusion = \"‚úÖ MODERATE EVIDENCE: Groups have different topic patterns\"\n",
    "    elif jsd >= 0.3:\n",
    "        conclusion = \"‚ö†Ô∏è  LARGE EFFECT: Substantial differences but not statistically significant\"\n",
    "    elif p_fisher < 0.05:\n",
    "        conclusion = \"‚ö†Ô∏è  SIGNIFICANT: Statistically different but small effect size\"\n",
    "    else:\n",
    "        conclusion = \"‚ùå NO EVIDENCE: Groups have similar topic patterns\"\n",
    "    \n",
    "    print(conclusion)\n",
    "    \n",
    "    return {\n",
    "        'jsd': jsd,\n",
    "        'fisher_p': p_fisher,\n",
    "        'odds_ratio': odds_ratio,\n",
    "        'effect_interpretation': effect_size,\n",
    "        'significance_interpretation': sig_level,\n",
    "        'overall_conclusion': conclusion\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88821ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_model(path):\n",
    "    \"\"\"Load a BERTopic model with the correct embedding model\"\"\"\n",
    "    return BERTopic.load(path, embedding_model=SentenceTransformer(\"PORTULAN/serafim-900m-portuguese-pt-sentence-encoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corresponding_df(df, group_name):\n",
    "    \"\"\"Filter dataframe for specific group\"\"\"\n",
    "    if group_name == \"Female_ADHD\":\n",
    "        return df[df[\"group\"] == \"Female_ADHD\"]\n",
    "    elif group_name == \"Female_noADHD\":\n",
    "        return df[df[\"group\"] == \"Female_noADHD\"]\n",
    "    elif group_name == \"ADHD\":\n",
    "        return df[df[\"group\"].isin([\"Male_ADHD\", \"Female_ADHD\"])]\n",
    "    elif group_name == \"noADHD\":\n",
    "        return df[df[\"group\"].isin([\"Male_noADHD\", \"Female_noADHD\"])]\n",
    "    elif group_name == \"Female\":\n",
    "        return df[df[\"group\"].isin([\"Female_ADHD\", \"Female_noADHD\"])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2344e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preliminary_steps(group_name, folder, topic_df):\n",
    "    \"\"\"Load model and prepare data for a specific group\"\"\"\n",
    "    print(f\"Loading {group_name} model from {folder}\")\n",
    "    df_group = get_corresponding_df(topic_df, group_name)\n",
    "    path = f\"../../data/adhd-beliefs-pt/bertopic_final/{group_name}/{folder}/\"\n",
    "    topic_model = load_bert_model(path)\n",
    "    return df_group, topic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea2b16",
   "metadata": {},
   "source": [
    "## Data Preparation & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae464b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find available model folders for each group\n",
    "female_adhd_folders = [name for name in os.listdir(f\"../../data/adhd-beliefs-pt/bertopic_final/Female_ADHD/\") \n",
    "                       if os.path.isdir(os.path.join(f\"../../data/adhd-beliefs-pt/bertopic_final/Female_ADHD/\", name))]\n",
    "female_noadhd_folders = [name for name in os.listdir(f\"../../data/adhd-beliefs-pt/bertopic_final/Female_noADHD/\") \n",
    "                         if os.path.isdir(os.path.join(f\"../../data/adhd-beliefs-pt/bertopic_final/Female_noADHD/\", name))]\n",
    "\n",
    "female_adhd_folder = female_adhd_folders[0] if female_adhd_folders else None\n",
    "female_noadhd_folder = female_noadhd_folders[0] if female_noadhd_folders else None\n",
    "\n",
    "print(f\"Female ADHD model folder: {female_adhd_folder}\")\n",
    "print(f\"Female non-ADHD model folder: {female_noadhd_folder}\")\n",
    "\n",
    "# Load the topic models\n",
    "adhd_df_group, adhd_topic_model = preliminary_steps(\"Female_ADHD\", female_adhd_folder, topic_df)\n",
    "noadhd_df_group, noadhd_topic_model = preliminary_steps(\"Female_noADHD\", female_noadhd_folder, topic_df)\n",
    "\n",
    "print(f\"\\nADHD group: {len(adhd_df_group)} documents, {len(adhd_topic_model.get_topics())} topics\")\n",
    "print(f\"Non-ADHD group: {len(noadhd_df_group)} documents, {len(noadhd_topic_model.get_topics())} topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee319c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_A_docs = adhd_topic_model.topics_\n",
    "topics_B_docs = noadhd_topic_model.topics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b436318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL QUALITY METRICS ===\")\n",
    "print(f\"ADHD Model:\")\n",
    "print(f\"  - Number of topics: {len(adhd_topic_model.get_topics())}\")\n",
    "print(f\"  - Outlier documents: {sum(1 for t in topics_A_docs if t == -1)} ({sum(1 for t in topics_A_docs if t == -1)/len(topics_A_docs)*100:.1f}%)\")\n",
    "print(f\"  - Average documents per topic: {len([t for t in topics_A_docs if t != -1]) / len(adhd_topic_model.get_topics()):.1f}\")\n",
    "\n",
    "print(f\"\\nNon-ADHD Model:\")\n",
    "print(f\"  - Number of topics: {len(noadhd_topic_model.get_topics())}\")\n",
    "print(f\"  - Outlier documents: {sum(1 for t in topics_B_docs if t == -1)} ({sum(1 for t in topics_B_docs if t == -1)/len(topics_B_docs)*100:.1f}%)\")\n",
    "print(f\"  - Average documents per topic: {len([t for t in topics_B_docs if t != -1]) / len(noadhd_topic_model.get_topics()):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b8085",
   "metadata": {},
   "source": [
    "## Analysis 1: Separate Models Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db84730",
   "metadata": {},
   "source": [
    "### 1.1 Similarity Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aligned_vectors(threshold, tids_A, tids_B, matches, topics_A_docs, topics_B_docs):\n",
    "    \"\"\"\n",
    "    Create aligned count vectors for a given similarity threshold\n",
    "    \n",
    "    Returns:\n",
    "        A_vec: Count vector for group A (ADHD)\n",
    "        B_vec: Count vector for group B (non-ADHD)  \n",
    "        aligned: List of topic pairs (ta, tb) where None means no match\n",
    "    \"\"\"\n",
    "    # Count documents per topic for each model\n",
    "    counts_A = counts_per_topic(topics_A_docs)\n",
    "    counts_B = counts_per_topic(topics_B_docs)\n",
    "    \n",
    "    # Create alignment based on threshold\n",
    "    aligned_pairs = [(ta, tb) for ta, tb, sim in matches if sim >= threshold]\n",
    "    \n",
    "    # Find topics that don't meet threshold\n",
    "    used_A = set(ta for ta, tb in aligned_pairs)\n",
    "    used_B = set(tb for ta, tb in aligned_pairs)\n",
    "    \n",
    "    unique_A = [ta for ta in tids_A if ta not in used_A]\n",
    "    unique_B = [tb for tb in tids_B if tb not in used_B]\n",
    "    \n",
    "    # Build count vectors\n",
    "    all_topics = aligned_pairs + [(ta, None) for ta in unique_A] + [(None, tb) for tb in unique_B]\n",
    "    \n",
    "    A_vec = []\n",
    "    B_vec = []\n",
    "    \n",
    "    for ta, tb in all_topics:\n",
    "        count_a = counts_A.get(ta, 0) if ta is not None else 0\n",
    "        count_b = counts_B.get(tb, 0) if tb is not None else 0\n",
    "        A_vec.append(count_a)\n",
    "        B_vec.append(count_b)\n",
    "    \n",
    "    return np.array(A_vec), np.array(B_vec), all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40]\n",
    "threshold_results = []\n",
    "\n",
    "# similarity matrix\n",
    "tids_A, tids_B, matches, base_sim_matrix = align_topics_by_cosine(\n",
    "    adhd_topic_model, noadhd_topic_model, top_n=30\n",
    ")\n",
    "\n",
    "print(f\"Similarity matrix stats:\")\n",
    "print(f\"  Min: {base_sim_matrix.min():.3f}\")\n",
    "print(f\"  Max: {base_sim_matrix.max():.3f}\")\n",
    "print(f\"  Mean: {base_sim_matrix.mean():.3f}\")\n",
    "\n",
    "# Test each threshold\n",
    "for thresh in thresholds:\n",
    "    try:\n",
    "        A_vec_t, B_vec_t, aligned_t = create_aligned_vectors(\n",
    "            thresh, tids_A, tids_B, matches, \n",
    "            topics_A_docs, topics_B_docs\n",
    "        )\n",
    "        \n",
    "        results = complete_corrected_analysis(A_vec_t, B_vec_t, (\"ADHD\", \"non-ADHD\"))\n",
    "        matched_similarities = [s for ta, tb, s in matches if s >= thresh]\n",
    "        avg_sim = np.mean(matched_similarities) if matched_similarities else 0\n",
    "        \n",
    "        num_pairs = sum(1 for ta, tb in aligned_t if ta is not None and tb is not None)\n",
    "        num_unique_A = sum(1 for ta, tb in aligned_t if tb is None and ta is not None)\n",
    "        num_unique_B = sum(1 for ta, tb in aligned_t if ta is None and tb is not None)\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': thresh,\n",
    "            'matched_pairs': num_pairs,\n",
    "            'unique_A': num_unique_A,\n",
    "            'unique_B': num_unique_B,\n",
    "            'fisher_p': results['fisher_p'],\n",
    "            'jsd': results['jsd'],\n",
    "            'avg_sim_matched_pairs': avg_sim,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error at threshold {thresh}: {e}\")\n",
    "\n",
    "# Select best threshold\n",
    "df_thresh = pd.DataFrame(threshold_results)\n",
    "print(\"\\nThreshold Analysis Results:\")\n",
    "print(df_thresh[['threshold', 'matched_pairs', 'jsd', 'fisher_p', 'avg_sim_matched_pairs']])\n",
    "\n",
    "if len(df_thresh) > 0:\n",
    "    df_thresh['score'] = (df_thresh['jsd'] * \n",
    "                         (1 - df_thresh['fisher_p']) * \n",
    "                         df_thresh['avg_sim_matched_pairs'] * \n",
    "                         np.log1p(df_thresh['matched_pairs']))\n",
    "    \n",
    "    best_threshold = df_thresh.loc[df_thresh['score'].idxmax(), 'threshold']\n",
    "    print(f\"\\n‚úÖ Recommended threshold: {best_threshold}\")\n",
    "else:\n",
    "    best_threshold = 0.10\n",
    "    print(f\"\\n‚ö†Ô∏è  Using default threshold: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_alignment_quality(matches, threshold):\n",
    "    \"\"\"Assess quality of topic alignment\"\"\"\n",
    "    similarities = [sim for _, _, sim in matches]\n",
    "    above_threshold = [sim for sim in similarities if sim >= threshold]\n",
    "    \n",
    "    print(f\"\\nALIGNMENT QUALITY ASSESSMENT:\")\n",
    "    print(f\"Similarity distribution:\")\n",
    "    print(f\"  - Mean similarity: {np.mean(similarities):.3f}\")\n",
    "    print(f\"  - Median similarity: {np.median(similarities):.3f}\")\n",
    "    print(f\"  - Min similarity: {np.min(similarities):.3f}\")\n",
    "    print(f\"  - Max similarity: {np.max(similarities):.3f}\")\n",
    "    \n",
    "    print(f\"\\nAlignment at threshold {threshold}:\")\n",
    "    print(f\"  - Topics aligned: {len(above_threshold)}\")\n",
    "    print(f\"  - Alignment rate: {len(above_threshold)/len(similarities)*100:.1f}%\")\n",
    "    print(f\"  - Mean aligned similarity: {np.mean(above_threshold):.3f}\")\n",
    "\n",
    "assess_alignment_quality(matches, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365bccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(base_sim_matrix, annot=True, fmt='.2f', cmap='RdYlBu_r', \n",
    "            xticklabels=range(len(base_sim_matrix[0])), \n",
    "            yticklabels=range(len(base_sim_matrix)))\n",
    "plt.title('Topic Similarity Matrix: Females with ADHD vs Females without ADHD Models')\n",
    "plt.xlabel('Females without ADHD Topics')\n",
    "plt.ylabel('Females with ADHD Topics')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show distribution of similarity scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "similarities = base_sim_matrix.flatten()\n",
    "plt.hist(similarities, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Use the optimal threshold\n",
    "threshold_to_show = best_threshold if 'best_threshold' in globals() else 0.10\n",
    "plt.axvline(threshold_to_show, color='red', linestyle='--', \n",
    "           label=f'Optimal Threshold ({threshold_to_show})')\n",
    "\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Topic Similarity Scores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Similarity statistics:\")\n",
    "print(f\"Mean: {similarities.mean():.3f}\")\n",
    "print(f\"Median: {np.median(similarities):.3f}\")\n",
    "print(f\"Std: {similarities.std():.3f}\")\n",
    "print(f\"% above threshold ({threshold_to_show}): {(similarities >= threshold_to_show).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb6ef5",
   "metadata": {},
   "source": [
    "### 1.2 Main Statistical Analysis (Separate Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MAIN STATISTICAL ANALYSIS: Female ADHD vs Female non-ADHD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# same function from threshold analysis\n",
    "threshold = best_threshold if 'best_threshold' in globals() else 0.10\n",
    "print(f\"    Threshold: {threshold}\")\n",
    "\n",
    "A_vec, B_vec, aligned = create_aligned_vectors(\n",
    "    threshold, tids_A, tids_B, matches, \n",
    "    topics_A_docs, topics_B_docs\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä ALIGNMENT SUMMARY:\")\n",
    "num_pairs = sum(1 for ta, tb in aligned if ta is not None and tb is not None)\n",
    "num_unique_A = sum(1 for ta, tb in aligned if tb is None and ta is not None)\n",
    "num_unique_B = sum(1 for ta, tb in aligned if ta is None and tb is not None)\n",
    "\n",
    "print(f\"  - Matched pairs: {num_pairs}\")\n",
    "print(f\"  - ADHD-unique topics: {num_unique_A}\")\n",
    "print(f\"  - Non-ADHD-unique topics: {num_unique_B}\")\n",
    "print(f\"  - Total topics in analysis: {len(aligned)}\")\n",
    "print(f\"  - Total documents: ADHD={A_vec.sum()}, Non-ADHD={B_vec.sum()}\")\n",
    "\n",
    "# Perform statistical analysis\n",
    "corrected_results = complete_corrected_analysis(A_vec, B_vec, (\"Female ADHD\", \"Female non-ADHD\"))\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"FINAL RESULTS SUMMARY\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"    Effect Size: JSD = {corrected_results['jsd']:.4f} ({corrected_results['effect_interpretation']})\")\n",
    "print(f\"    Significance: p = {corrected_results['fisher_p']:.6f} ({corrected_results['significance_interpretation']})\")\n",
    "print(f\"    Odds Ratio: {corrected_results['odds_ratio']:.3f}\")\n",
    "print(f\"    Conclusion: {corrected_results['overall_conclusion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26ce42",
   "metadata": {},
   "source": [
    "### 1.3 Topic Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for (ta, tb), ca, cb in zip(aligned, A_vec, B_vec):\n",
    "    rows.append({\n",
    "        \"topic_A\": ta, \"topic_B\": tb,\n",
    "        \"count_A\": int(ca), \"count_B\": int(cb)\n",
    "    })\n",
    "import pandas as pd\n",
    "df_aligned = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_top_words(model, tid, n=5):\n",
    "    if tid is None or pd.isna(tid):\n",
    "        return None\n",
    "    tid = int(tid)\n",
    "    pairs = model.get_topic(tid)\n",
    "    if not pairs:\n",
    "        return None\n",
    "    return \", \".join([w for w, _ in pairs[:n]])\n",
    "\n",
    "df_aligned[\"label_A\"] = df_aligned[\"topic_A\"].apply(lambda t: topic_top_words(adhd_topic_model, t))\n",
    "df_aligned[\"label_B\"] = df_aligned[\"topic_B\"].apply(lambda t: topic_top_words(noadhd_topic_model, t))\n",
    "df_aligned.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0eb13a",
   "metadata": {},
   "source": [
    "## Analysis 2: Combined Model Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "portuguese_stopwords = stopwords.words(\"portuguese\")\n",
    "additional_stopwords = [\n",
    "    \"pra\",\n",
    "    \"pro\",\n",
    "    \"t√°\",\n",
    "    \"j√°\",\n",
    "    \"ter\",\n",
    "    \"vai\",\n",
    "    \"vou\",\n",
    "    \"ent√£o\",\n",
    "    \"assim\",\n",
    "    \"a√≠\",\n",
    "    \"sobre\"\n",
    "]\n",
    "portuguese_stopwords.extend(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959013f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(adhd_texts, noadhd_texts, n_permutations=1000, random_seed=42):\n",
    "    \"\"\"\n",
    "    Fixed permutation test that creates a single combined topic model,\n",
    "    then tests if group differences could arise by chance.\n",
    "    \n",
    "    H0: Topic distributions are the same for both groups\n",
    "    H1: Topic distributions differ between groups\n",
    "    \"\"\"\n",
    "    from bertopic import BERTopic\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from umap import UMAP\n",
    "    from hdbscan import HDBSCAN\n",
    "    from bertopic.vectorizers import ClassTfidfTransformer\n",
    "    from bertopic.representation import KeyBERTInspired\n",
    "    \n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    \n",
    "    all_texts = adhd_texts + noadhd_texts\n",
    "    true_labels = ['ADHD'] * len(adhd_texts) + ['noADHD'] * len(noadhd_texts)\n",
    "    print(f\"Creating combined topic model for {len(all_texts)} documents...\")\n",
    "    \n",
    "    # Same setup as before\n",
    "    embedding_model = SentenceTransformer(\"PORTULAN/serafim-900m-portuguese-pt-sentence-encoder\")\n",
    "    umap_model = UMAP(n_neighbors=5, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=4, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    vectorizer_model = CountVectorizer(stop_words=portuguese_stopwords, min_df=2, ngram_range=(1, 2))\n",
    "    ctfidf_model = ClassTfidfTransformer(bm25_weighting=True)\n",
    "    \n",
    "    keybert_model = KeyBERTInspired()\n",
    "\n",
    "    representation_model = {\n",
    "        \"KeyBERT\": keybert_model,\n",
    "    }\n",
    "    \n",
    "    combined_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        language=\"multilingual\",\n",
    "        top_n_words=10,\n",
    "        verbose=True,\n",
    "        calculate_probabilities=True,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "    )\n",
    "    topics_all, _ = combined_model.fit_transform(all_texts)\n",
    "    \n",
    "\n",
    "    def calculate_group_difference(group_labels, topic_assignments):\n",
    "        \"\"\"Calculate JSD between two groups' topic distributions\"\"\"\n",
    "        adhd_mask = np.array(group_labels) == 'ADHD'\n",
    "        noadhd_mask = np.array(group_labels) == 'noADHD'\n",
    "        \n",
    "        adhd_topics = np.array(topic_assignments)[adhd_mask]\n",
    "        noadhd_topics = np.array(topic_assignments)[noadhd_mask]\n",
    "        \n",
    "        # Count topics for each group (excluding outliers = -1)\n",
    "        adhd_counts = Counter([t for t in adhd_topics if t != -1])\n",
    "        noadhd_counts = Counter([t for t in noadhd_topics if t != -1])\n",
    "        \n",
    "        # Get all unique topics\n",
    "        all_topic_ids = set(adhd_counts.keys()) | set(noadhd_counts.keys())\n",
    "        if len(all_topic_ids) < 2:\n",
    "            return np.nan\n",
    "        \n",
    "        # Create count vectors\n",
    "        adhd_vec = np.array([adhd_counts.get(tid, 0) for tid in sorted(all_topic_ids)])\n",
    "        noadhd_vec = np.array([noadhd_counts.get(tid, 0) for tid in sorted(all_topic_ids)])\n",
    "        \n",
    "        # Calculate JSD\n",
    "        if adhd_vec.sum() == 0 or noadhd_vec.sum() == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Normalize to probabilities\n",
    "        p = adhd_vec / adhd_vec.sum()\n",
    "        q = noadhd_vec / noadhd_vec.sum()\n",
    "        \n",
    "        return jensenshannon(p, q, base=2)\n",
    "    \n",
    "\n",
    "    jsd_observed = calculate_group_difference(true_labels, topics_all)\n",
    "    print(f\"Observed JSD with combined model: {jsd_observed:.4f}\")\n",
    "    \n",
    "    # Permutation test\n",
    "    print(f\"Running {n_permutations} permutations...\")\n",
    "    perm_jsds = []\n",
    "    \n",
    "    for i in range(n_permutations):\n",
    "        # Shuffle group labels while keeping same group sizes\n",
    "        shuffled_labels = true_labels.copy()\n",
    "        rng.shuffle(shuffled_labels)\n",
    "        \n",
    "        # Calculate JSD for shuffled groups\n",
    "        jsd_perm = calculate_group_difference(shuffled_labels, topics_all)\n",
    "        if not np.isnan(jsd_perm):\n",
    "            perm_jsds.append(jsd_perm)\n",
    "        \n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"  Completed {i + 1}/{n_permutations} permutations\")\n",
    "    \n",
    "    perm_jsds = np.array(perm_jsds)\n",
    "    \n",
    "    # Calculate p-value\n",
    "    if len(perm_jsds) > 0:\n",
    "        p_value = (perm_jsds >= jsd_observed).mean()\n",
    "    else:\n",
    "        p_value = np.nan\n",
    "    \n",
    "    return {\n",
    "        'observed_jsd': jsd_observed,\n",
    "        'permuted_jsds': perm_jsds,\n",
    "        'p_value': p_value,\n",
    "        'n_successful_permutations': len(perm_jsds),\n",
    "        'combined_model': combined_model,\n",
    "        'all_topic_assignments': topics_all\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adhd_texts = adhd_df_group[\"response\"].tolist()\n",
    "noadhd_texts = noadhd_df_group[\"response\"].tolist()\n",
    "\n",
    "print(f\"Running permutation test...\")\n",
    "print(f\"ADHD group: {len(adhd_texts)} documents\")\n",
    "print(f\"Non-ADHD group: {len(noadhd_texts)} documents\")\n",
    "\n",
    "# Permutation test\n",
    "perm_results = permutation_test(\n",
    "    adhd_texts, noadhd_texts, \n",
    "    n_permutations=1000, \n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\n=== PERMUTATION TEST RESULTS ===\")\n",
    "print(f\"Observed JSD: {perm_results['observed_jsd']:.4f}\")\n",
    "print(f\"Permutation p-value: {perm_results['p_value']:.4f}\")\n",
    "print(f\"Successful permutations: {perm_results['n_successful_permutations']}/1000\")\n",
    "\n",
    "# Interpretation\n",
    "if perm_results['p_value'] < 0.05:\n",
    "    print(f\"\\nSIGNIFICANT: p < 0.05\")\n",
    "    print(f\"   ‚Üí The observed difference is NOT due to random chance\")\n",
    "    print(f\"   ‚Üí Groups really do have different topic distributions\")\n",
    "else:\n",
    "    print(f\"\\nNOT SIGNIFICANT: p >= 0.05\") \n",
    "    print(f\"   ‚Üí The observed difference could be due to random chance\")\n",
    "    print(f\"   ‚Üí No strong evidence of real group differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507816ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "if perm_results['n_successful_permutations'] > 0:   \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Use seaborn for histogram\n",
    "    sns.histplot(perm_results['permuted_jsds'], bins=50, kde=True, \n",
    "                 color=sns.color_palette(\"rocket\", as_cmap=True)(0.4), \n",
    "                 label='Permutation distribution', stat='density')\n",
    "    \n",
    "    # Mark the observed value\n",
    "    plt.axvline(perm_results['observed_jsd'], color=sns.color_palette(\"rocket\")[1], \n",
    "                linestyle='--', linewidth=3, \n",
    "                label=f'Observed JSD: {perm_results[\"observed_jsd\"]:.3f}')\n",
    "    \n",
    "    plt.xlabel('Jensen-Shannon Divergence')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Permutation Test Results\\n(p-value = {perm_results[\"p_value\"]:.4f})',\n",
    "              fontsize=16, fontweight='bold', color='black')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add text annotation\n",
    "    if perm_results['p_value'] <= 0.05:\n",
    "        plt.text(0.885, 0.83, 'SIGNIFICANT, p > 0.05', \n",
    "                transform=plt.gca().transAxes, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=sns.color_palette(\"rocket\")[2]),\n",
    "                fontsize=12, ha='center')\n",
    "    else:\n",
    "        plt.text(0.855, 0.83, 'NOT SIGNIFICANT, p <= 0.05', \n",
    "                transform=plt.gca().transAxes,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=sns.color_palette(\"rocket\")[2]),\n",
    "                fontsize=12, ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(f\"\\n=== PERMUTATION STATISTICS ===\")\n",
    "    print(f\"Mean of null distribution: {perm_results['permuted_jsds'].mean():.4f}\")\n",
    "    print(f\"Std of null distribution: {perm_results['permuted_jsds'].std():.4f}\")\n",
    "    print(f\"95th percentile of null: {np.percentile(perm_results['permuted_jsds'], 95):.4f}\")\n",
    "    print(f\"99th percentile of null: {np.percentile(perm_results['permuted_jsds'], 99):.4f}\")\n",
    "else:\n",
    "    print(\"No successful permutations to visualize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04d93f",
   "metadata": {},
   "source": [
    "## Bootstrap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4caedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "B = 1000\n",
    "boot_jsd = []\n",
    "\n",
    "# Get original alignment\n",
    "cntA_orig = counts_per_topic(topics_A_docs)\n",
    "cntB_orig = counts_per_topic(topics_B_docs)\n",
    "\n",
    "for b in range(B):\n",
    "    # Resample document assignments, not alignments\n",
    "    idxA = rng.integers(0, len(topics_A_docs), len(topics_A_docs))\n",
    "    idxB = rng.integers(0, len(topics_B_docs), len(topics_B_docs))\n",
    "    \n",
    "    # Use the SAME alignment, but with resampled topic assignments\n",
    "    resampled_A_topics = np.array(topics_A_docs)[idxA]\n",
    "    resampled_B_topics = np.array(topics_B_docs)[idxB]\n",
    "    \n",
    "    # Count topics for resampled data\n",
    "    cntA_resamp = counts_per_topic(resampled_A_topics)\n",
    "    cntB_resamp = counts_per_topic(resampled_B_topics)\n",
    "    \n",
    "    # Use the ORIGINAL alignment structure with resampled counts\n",
    "    A_vec_b, B_vec_b = [], []\n",
    "    for ta, tb in aligned:  # ‚Üê Use the original 'aligned' from earlier\n",
    "        A_vec_b.append(cntA_resamp.get(ta, 0) if ta is not None else 0)\n",
    "        B_vec_b.append(cntB_resamp.get(tb, 0) if tb is not None else 0)\n",
    "    \n",
    "    try:\n",
    "        # Only calculate JSD for bootstrap\n",
    "        A_vec_b = np.array(A_vec_b)\n",
    "        B_vec_b = np.array(B_vec_b)\n",
    "        \n",
    "        if A_vec_b.sum() > 0 and B_vec_b.sum() > 0:\n",
    "            # Normalize to probabilities\n",
    "            p_b = A_vec_b / A_vec_b.sum()\n",
    "            q_b = B_vec_b / B_vec_b.sum()\n",
    "            jsd_b = jensenshannon(p_b, q_b, base=2)\n",
    "            boot_jsd.append(jsd_b)\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d93e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals and interpret results\n",
    "if len(boot_jsd) > 0:\n",
    "    boot_jsd = np.array(boot_jsd)\n",
    "    \n",
    "    jsd_ci_lower = np.percentile(boot_jsd, 2.5)\n",
    "    jsd_ci_upper = np.percentile(boot_jsd, 97.5)\n",
    "    \n",
    "    print(f\"\\nBootstrap 95% Confidence Intervals:\")\n",
    "    print(f\"JSD: [{jsd_ci_lower:.4f}, {jsd_ci_upper:.4f}]\")\n",
    "    \n",
    "    print(f\"\\nOriginal vs Bootstrap comparison:\")\n",
    "    original_jsd = corrected_results['jsd']\n",
    "    print(f\"Original JSD: {original_jsd:.4f} (bootstrap mean: {boot_jsd.mean():.4f})\")\n",
    "else:\n",
    "    print(\"No valid bootstrap samples were generated!\")\n",
    "    \n",
    "# Check if we have sufficient successful bootstrap samples\n",
    "success_rate = len(boot_jsd) / B\n",
    "print(f\"\\nBootstrap success rate: {success_rate:.1%} ({len(boot_jsd)}/{B} samples)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adhd-linguistic-patterns-beliefs-portuguese-women",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
