{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading model and tokenizer...\n",
      "/home/raquel/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "logger.info(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Setting device...\n",
      "INFO:__main__:Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU if available)\n",
    "logger.info(\"Setting device...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/adhd-posts.csv\"\n",
    "text_column = \"body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split long texts into chunks of up to 512 tokens\n",
    "def chunk_text(text, max_length=512):\n",
    "    logger.info(\"Chunking text...\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    logger.info(f\"Tokenized text: {tokens}\")\n",
    "    logger.info(f\"Number of tokens: {len(tokens)}\")\n",
    "    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the embedding of a list of tokens (a single chunk)\n",
    "def get_chunk_embedding(tokens):\n",
    "    logger.info(\"Getting chunk embedding...\")\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        tokens,\n",
    "        return_tensors='pt',\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length'\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full function to get embedding for any text by averaging chunk embeddings\n",
    "def get_full_embedding(text):\n",
    "    logger.info(\"Getting embedding...\")\n",
    "    text = str(text)  # in case of NaNs\n",
    "    chunks = chunk_text(text)\n",
    "    logger.info(f\"Number of chunks: {len(chunks)}\")\n",
    "    logger.info(f\"Chunks: {chunks}\")\n",
    "    chunk_embeddings = []\n",
    "\n",
    "    for tokens in chunks:\n",
    "        chunk_emb = get_chunk_embedding(tokens)\n",
    "        chunk_embeddings.append(chunk_emb)\n",
    "        logger.info(f\"Chunk embedding shape: {chunk_emb.shape}\")\n",
    "    # Average all chunk embeddings\n",
    "    full_embedding = torch.mean(torch.stack(chunk_embeddings), dim=0)\n",
    "    return full_embedding.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    logger.info(\"Loading dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    logger.info(f\"Dataset shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_embedding_workflow(file_path, text_column):\n",
    "    logger.info(\"Starting embedding workflow...\")\n",
    "    df = load_dataset(file_path)\n",
    "    # get only the first 5k rows\n",
    "    df = df.iloc[:5000]\n",
    "    logger.info(f\"Reduced dataset shape: {df.shape}\")\n",
    "    df[text_column] = df[text_column].astype(str)\n",
    "    # Apply the embedding function to the specified column\n",
    "    logger.info(f\"Applying embedding function to column: {text_column}\")\n",
    "    df['embedding'] = df[text_column].apply(get_full_embedding)\n",
    "    logger.info(\"Embedding completed.\")\n",
    "    # Save the DataFrame with embeddings to a new CSV file\n",
    "    logger.info(\"Saving DataFrame with embeddings...\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    logger.info(\"DataFrame saved.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting embedding workflow...\n",
      "INFO:__main__:Loading dataset...\n",
      "INFO:__main__:Dataset shape: (330693, 4)\n",
      "INFO:__main__:Reduced dataset shape: (5000, 4)\n",
      "INFO:__main__:Applying embedding function to column: body\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['android', 'app', 'to', 'strengthen', 'attention', '/', 'focus', 'hey', '/', 'r', '/', 'ad', '##hd', ',', 'check', 'out', 'my', 'simple', 'android', 'app', ':', '[', 'attention', 'exercise', ']', '(', 'https', ':', '/', '/', 'market', '.', 'android', '.', 'com', '/', 'details', '?', 'id', '=', 'com', '.', 'race', '##car', '##lab', '##s', '.', 'apps', '.', 'android', '.', 'attention', '##ex', '##er', '##cise', ')', 'it', \"'\", 's', 'just', 'a', 'series', 'of', 'simple', 'touch', '##screen', 'drawing', 'exercises', 'that', ',', 'with', 'practice', ',', 'noticeably', 'improve', 'attention', 'span', 'and', 'focus', '.', 'a', 'session', 'really', 'shouldn', \"'\", 't', 'take', 'more', 'than', 'a', 'couple', 'of', 'minutes', '!', 'if', 'you', \"'\", 've', 'heard', 'of', 'the', 'book', '[', 'delivered', 'from', 'distraction', ']', '(', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'delivered', '-', 'distraction', '-', 'getting', '-', 'attention', '-', 'disorder', '/', 'd', '##p', '/', '03', '##45', '##44', '##23', '##0', '##x', ')', ',', 'the', 'app', 'is', 'based', 'on', 'the', 'exercises', 'that', 'dr', '.', 'hall', '##owe', '##ll', 'recommends', 'in', 'the', 'book', '.', 'writing', 'code', 'while', 'managing', 'my', 'ad', '##hd', 'symptoms', 'hasn', \"'\", 't', 'been', 'easy', ',', 'but', 'i', 'hope', 'this', 'is', 'an', 'encouragement', 'to', 'you', '!', 'it', 'felt', 'really', 'good', 'to', 'ship', 'version', '1', '.', 'if', 'you', 'have', 'feature', 'ideas', 'or', 'bug', 'reports', ',', 'i', \"'\", 'd', 'love', 'to', 'hear', 'them', '!', 'on', 'my', 'road', '##ma', '##p', ':', '*', 'optional', 'daily', 'notification', '##s', '(', 'exercises', 'work', 'best', 'with', 'daily', 'practice', ')', '*', 'tracking', 'improvements', '*', 'making', 'it', 'pre', '##tti', '##er', '*', '*', 't', '##l', ';', 'dr', '*', '*', ':', 'android', 'users', 'try', 'my', 'app', '!', 'drawing', 'shapes', 'helps', 'you', 'focus', '!']\n",
      "INFO:__main__:Number of tokens: 256\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['android', 'app', 'to', 'strengthen', 'attention', '/', 'focus', 'hey', '/', 'r', '/', 'ad', '##hd', ',', 'check', 'out', 'my', 'simple', 'android', 'app', ':', '[', 'attention', 'exercise', ']', '(', 'https', ':', '/', '/', 'market', '.', 'android', '.', 'com', '/', 'details', '?', 'id', '=', 'com', '.', 'race', '##car', '##lab', '##s', '.', 'apps', '.', 'android', '.', 'attention', '##ex', '##er', '##cise', ')', 'it', \"'\", 's', 'just', 'a', 'series', 'of', 'simple', 'touch', '##screen', 'drawing', 'exercises', 'that', ',', 'with', 'practice', ',', 'noticeably', 'improve', 'attention', 'span', 'and', 'focus', '.', 'a', 'session', 'really', 'shouldn', \"'\", 't', 'take', 'more', 'than', 'a', 'couple', 'of', 'minutes', '!', 'if', 'you', \"'\", 've', 'heard', 'of', 'the', 'book', '[', 'delivered', 'from', 'distraction', ']', '(', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'delivered', '-', 'distraction', '-', 'getting', '-', 'attention', '-', 'disorder', '/', 'd', '##p', '/', '03', '##45', '##44', '##23', '##0', '##x', ')', ',', 'the', 'app', 'is', 'based', 'on', 'the', 'exercises', 'that', 'dr', '.', 'hall', '##owe', '##ll', 'recommends', 'in', 'the', 'book', '.', 'writing', 'code', 'while', 'managing', 'my', 'ad', '##hd', 'symptoms', 'hasn', \"'\", 't', 'been', 'easy', ',', 'but', 'i', 'hope', 'this', 'is', 'an', 'encouragement', 'to', 'you', '!', 'it', 'felt', 'really', 'good', 'to', 'ship', 'version', '1', '.', 'if', 'you', 'have', 'feature', 'ideas', 'or', 'bug', 'reports', ',', 'i', \"'\", 'd', 'love', 'to', 'hear', 'them', '!', 'on', 'my', 'road', '##ma', '##p', ':', '*', 'optional', 'daily', 'notification', '##s', '(', 'exercises', 'work', 'best', 'with', 'daily', 'practice', ')', '*', 'tracking', 'improvements', '*', 'making', 'it', 'pre', '##tti', '##er', '*', '*', 't', '##l', ';', 'dr', '*', '*', ':', 'android', 'users', 'try', 'my', 'app', '!', 'drawing', 'shapes', 'helps', 'you', 'focus', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'here', 'have', 'experience', 'with', 'im', '##ip', '##ram', '##ine', '?', 'my', 'doctor', 'has', 'suggested', 'it', 'for', 'add', 'with', 'anxiety', 'issues', 'and', 'i', 'think', 'i', 'may', 'try', 'it', '.', 'has', 'anyone', 'else', 'been', 'on', 'this', 'and', 'what', 'are', 'your', 'thoughts', 'about', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 43\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'here', 'have', 'experience', 'with', 'im', '##ip', '##ram', '##ine', '?', 'my', 'doctor', 'has', 'suggested', 'it', 'for', 'add', 'with', 'anxiety', 'issues', 'and', 'i', 'think', 'i', 'may', 'try', 'it', '.', 'has', 'anyone', 'else', 'been', 'on', 'this', 'and', 'what', 'are', 'your', 'thoughts', 'about', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'study', 'shows', 'that', 'for', 'people', 'with', 'ad', '##hd', ',', '\"', 'their', 'world', 'is', 'moving', 'at', 'a', 'much', 'faster', 'pace', 'and', 'there', '’', 's', 'always', 'going', 'to', 'be', 'a', 'mis', '##mat', '##ch', 'between', 'their', 'world', 'and', 'ours', '.', '”']\n",
      "INFO:__main__:Number of tokens: 39\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'study', 'shows', 'that', 'for', 'people', 'with', 'ad', '##hd', ',', '\"', 'their', 'world', 'is', 'moving', 'at', 'a', 'much', 'faster', 'pace', 'and', 'there', '’', 's', 'always', 'going', 'to', 'be', 'a', 'mis', '##mat', '##ch', 'between', 'their', 'world', 'and', 'ours', '.', '”']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'does', 'the', 'ad', '##hd', 'test', 'look', 'like', '?', 'i', \"'\", 'm', '21', 'and', 'didn', \"'\", 't', 'know', 'about', 'ad', '##hd', 'until', 'recently', '.', 'to', 'be', 'short', ':', 'i', \"'\", 've', 'found', 'myself', 'in', 'pretty', 'much', 'everything', 'you', 'people', 'write', 'here', 'and', 'other', 'stuff', 'that', \"'\", 's', 'on', 'the', 'web', '.', 'my', 'parents', 'think', 'that', 'it', \"'\", 's', 'not', 'a', 'condition', '(', 'if', 'it', \"'\", 's', 'even', 'ok', 'to', 'say', 'it', 'like', 'that', ')', 'and', 'that', 'everyone', 'can', 'have', 'attention', 'problems', 'but', 'if', 'i', 'want', 'i', 'can', 'go', 'talk', 'to', 'a', 'psychologist', 'or', 'someone', 'else', '.', 'so', 'now', 'i', \"'\", 'm', 'wondering', 'what', 'does', 'the', 'test', 'look', 'like', '?', 'i', \"'\", 'm', 'not', 'a', 'fan', 'of', 'any', 'medications', 'but', 'if', 'it', 'could', 'help', 'i', 'would', 'take', 'it', 'because', 'it', \"'\", 's', 'getting', 'pretty', 'hard', 'on', 'college', '.', 'thank', 'you', 'for', 'responses', '!', '*', '*', 'edit', ':', '*', '*', 'how', 'do', 'you', 'approach', 'a', 'psychiatrist', 'or', 'psychologist', 'with', 'the', 'problem', '?', 'do', 'you', 'just', 'tell', 'that', 'you', 'think', 'you', 'might', 'have', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 173\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'does', 'the', 'ad', '##hd', 'test', 'look', 'like', '?', 'i', \"'\", 'm', '21', 'and', 'didn', \"'\", 't', 'know', 'about', 'ad', '##hd', 'until', 'recently', '.', 'to', 'be', 'short', ':', 'i', \"'\", 've', 'found', 'myself', 'in', 'pretty', 'much', 'everything', 'you', 'people', 'write', 'here', 'and', 'other', 'stuff', 'that', \"'\", 's', 'on', 'the', 'web', '.', 'my', 'parents', 'think', 'that', 'it', \"'\", 's', 'not', 'a', 'condition', '(', 'if', 'it', \"'\", 's', 'even', 'ok', 'to', 'say', 'it', 'like', 'that', ')', 'and', 'that', 'everyone', 'can', 'have', 'attention', 'problems', 'but', 'if', 'i', 'want', 'i', 'can', 'go', 'talk', 'to', 'a', 'psychologist', 'or', 'someone', 'else', '.', 'so', 'now', 'i', \"'\", 'm', 'wondering', 'what', 'does', 'the', 'test', 'look', 'like', '?', 'i', \"'\", 'm', 'not', 'a', 'fan', 'of', 'any', 'medications', 'but', 'if', 'it', 'could', 'help', 'i', 'would', 'take', 'it', 'because', 'it', \"'\", 's', 'getting', 'pretty', 'hard', 'on', 'college', '.', 'thank', 'you', 'for', 'responses', '!', '*', '*', 'edit', ':', '*', '*', 'how', 'do', 'you', 'approach', 'a', 'psychiatrist', 'or', 'psychologist', 'with', 'the', 'problem', '?', 'do', 'you', 'just', 'tell', 'that', 'you', 'think', 'you', 'might', 'have', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['are', 'you', 'guys', 'good', 'with', 'maps', 'and', 'directions', '?', 'it', 'seems', 'like', 'one', 'thing', 'i', \"'\", 've', 'always', 'been', 'really', 'good', 'at', 'is', 'learning', 'my', 'way', 'around', 'places', 'and', 'learning', 'maps', '.', 'even', 'when', 'i', 'was', 'little', 'i', 'could', 'go', 'on', 'a', 'car', 'ride', 'and', 'remember', 'every', 'street', 'name', 'that', 'i', 'saw', 'along', 'the', 'way', '.', 'it', \"'\", 's', 'like', 'i', 'draw', 'a', 'map', 'in', 'my', 'head', 'without', 'even', 'thinking', 'about', 'it', '.', 'no', 'matter', 'where', 'i', 'am', ',', 'i', 'never', 'feel', 'like', 'i', \"'\", 'm', 'lost', ',', 'and', 'it', 'really', 'confuse', '##s', 'me', 'when', 'people', 'drive', 'one', 'block', 'off', 'the', 'beaten', 'path', 'and', 'say', 'that', 'they', \"'\", 're', 'completely', 'lost', '.', 'does', 'anybody', 'else', 'with', 'ad', '##hd', 'do', 'any', 'of', 'this', 'stuff', 'or', 'feel', 'this', 'way', 'when', 'it', 'comes', 'to', 'your', 'sense', 'of', 'direction', '?']\n",
      "INFO:__main__:Number of tokens: 136\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['are', 'you', 'guys', 'good', 'with', 'maps', 'and', 'directions', '?', 'it', 'seems', 'like', 'one', 'thing', 'i', \"'\", 've', 'always', 'been', 'really', 'good', 'at', 'is', 'learning', 'my', 'way', 'around', 'places', 'and', 'learning', 'maps', '.', 'even', 'when', 'i', 'was', 'little', 'i', 'could', 'go', 'on', 'a', 'car', 'ride', 'and', 'remember', 'every', 'street', 'name', 'that', 'i', 'saw', 'along', 'the', 'way', '.', 'it', \"'\", 's', 'like', 'i', 'draw', 'a', 'map', 'in', 'my', 'head', 'without', 'even', 'thinking', 'about', 'it', '.', 'no', 'matter', 'where', 'i', 'am', ',', 'i', 'never', 'feel', 'like', 'i', \"'\", 'm', 'lost', ',', 'and', 'it', 'really', 'confuse', '##s', 'me', 'when', 'people', 'drive', 'one', 'block', 'off', 'the', 'beaten', 'path', 'and', 'say', 'that', 'they', \"'\", 're', 'completely', 'lost', '.', 'does', 'anybody', 'else', 'with', 'ad', '##hd', 'do', 'any', 'of', 'this', 'stuff', 'or', 'feel', 'this', 'way', 'when', 'it', 'comes', 'to', 'your', 'sense', 'of', 'direction', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'started', 'concert', '##a', 'today', 'any', 'advice', '/', 'experience', 'from', 'long', 'time', 'adult', 'users', '?', 'i', 'am', 'now', '32', ',', 'i', 'was', 'on', 'rita', '##lin', 'from', 'the', 'age', 'of', '7', 'until', '##l', '14', 'and', 'then', 'again', 'briefly', 'when', 'i', 'was', '23', '.', 'i', 'have', 'been', 'having', 'tremendous', 'difficulty', 'functioning', 'over', 'the', 'past', 'few', 'months', 'so', 'i', 'asked', 'my', 'dr', 'if', 'i', 'could', 'try', 'concert', '##a', '.', 'i', 'like', 'the', 'idea', 'that', 'it', 'is', 'slow', 'release', 'and', 'so', 'in', 'theory', 'won', '##t', 'give', 'me', 'the', 'same', 'bump', '##y', 'ride', 'i', 'get', 'from', 'rita', '##lin', '.', 'my', 'first', 'day', 'has', 'been', 'great', '.', 'i', 'took', '27', '##mg', 'this', 'morning', 'and', 'have', 'managed', 'to', 'do', 'more', 'work', 'today', 'than', 'i', 'have', 'managed', 'in', 'the', 'last', '2', 'weeks', 'and', 'i', 'still', 'managed', 'to', 'look', 'at', 'red', '##dit', '.', 'if', 'anyone', 'has', 'any', 'experience', 'of', 'long', 'term', 'concert', '##a', 'use', 'as', 'an', 'adult', 'i', 'would', 'love', 'to', 'hear', 'about', 'it', 'as', 'well', 'as', 'any', 'tips', 'to', 'help', 'me', 'stay', 'balanced', '.', 'i', 'heard', 'omega', 'oils', 'help', 'quite', 'a', 'lot', '.', 'edit', ':', 'thanks', 'for', 'the', 'advice', 'everyone', '.', 'day', 'two', 'has', 'gone', 'well', '.', 'i', 'took', 'my', 'dose', 'earlier', 'today', 'and', 'did', 'start', 'losing', 'focus', 'a', 'bit', 'before', 'my', 'work', 'day', 'ended', '.', 'however', 'so', 'far', 'so', 'good', '.', 'i', 'think', 'i', 'will', 'try', 'taking', 'breaks', 'and', 'see', 'how', 'i', 'cope', 'with', 'that', '.', 'even', 'if', 'i', 'am', 'gr', '##og', '##gy', 'on', 'my', 'two', 'days', 'off', 'it', 'will', 'probably', 'be', 'better', 'than', 'do', '##pa', '##mine', 'burn', '##out', '.', 'thanks', 'again', 'and', 'i', 'will', 'keep', 'you', 'posted', 'about', 'day', '3', '.']\n",
      "INFO:__main__:Number of tokens: 265\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'started', 'concert', '##a', 'today', 'any', 'advice', '/', 'experience', 'from', 'long', 'time', 'adult', 'users', '?', 'i', 'am', 'now', '32', ',', 'i', 'was', 'on', 'rita', '##lin', 'from', 'the', 'age', 'of', '7', 'until', '##l', '14', 'and', 'then', 'again', 'briefly', 'when', 'i', 'was', '23', '.', 'i', 'have', 'been', 'having', 'tremendous', 'difficulty', 'functioning', 'over', 'the', 'past', 'few', 'months', 'so', 'i', 'asked', 'my', 'dr', 'if', 'i', 'could', 'try', 'concert', '##a', '.', 'i', 'like', 'the', 'idea', 'that', 'it', 'is', 'slow', 'release', 'and', 'so', 'in', 'theory', 'won', '##t', 'give', 'me', 'the', 'same', 'bump', '##y', 'ride', 'i', 'get', 'from', 'rita', '##lin', '.', 'my', 'first', 'day', 'has', 'been', 'great', '.', 'i', 'took', '27', '##mg', 'this', 'morning', 'and', 'have', 'managed', 'to', 'do', 'more', 'work', 'today', 'than', 'i', 'have', 'managed', 'in', 'the', 'last', '2', 'weeks', 'and', 'i', 'still', 'managed', 'to', 'look', 'at', 'red', '##dit', '.', 'if', 'anyone', 'has', 'any', 'experience', 'of', 'long', 'term', 'concert', '##a', 'use', 'as', 'an', 'adult', 'i', 'would', 'love', 'to', 'hear', 'about', 'it', 'as', 'well', 'as', 'any', 'tips', 'to', 'help', 'me', 'stay', 'balanced', '.', 'i', 'heard', 'omega', 'oils', 'help', 'quite', 'a', 'lot', '.', 'edit', ':', 'thanks', 'for', 'the', 'advice', 'everyone', '.', 'day', 'two', 'has', 'gone', 'well', '.', 'i', 'took', 'my', 'dose', 'earlier', 'today', 'and', 'did', 'start', 'losing', 'focus', 'a', 'bit', 'before', 'my', 'work', 'day', 'ended', '.', 'however', 'so', 'far', 'so', 'good', '.', 'i', 'think', 'i', 'will', 'try', 'taking', 'breaks', 'and', 'see', 'how', 'i', 'cope', 'with', 'that', '.', 'even', 'if', 'i', 'am', 'gr', '##og', '##gy', 'on', 'my', 'two', 'days', 'off', 'it', 'will', 'probably', 'be', 'better', 'than', 'do', '##pa', '##mine', 'burn', '##out', '.', 'thanks', 'again', 'and', 'i', 'will', 'keep', 'you', 'posted', 'about', 'day', '3', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['diagnosed', 'with', 'ad', '##hd', 'recently', '.', 'wrong', 'diagnosis', '?', 'i', 'was', 'diagnosed', 'with', 'adult', 'ad', '##hd', 'a', 'couple', 'of', 'months', 'ago', 'and', 'am', 'now', 'taking', '35', '##mg', 'of', 'rita', '##lin', 'twice', 'a', 'day', ',', 'i', 'usually', 'only', 'take', 'one', 'dose', '.', 'i', \"'\", 'm', 'not', 'sure', 'if', 'i', 'really', 'do', 'have', 'ad', '##hd', 'as', 'i', \"'\", 'm', 'not', 'usually', 'one', 'to', 'be', 'hyper', '##active', '.', 'i', 'am', 'very', 'forget', '##ful', 'and', 'have', 'no', 'attention', 'span', 'and', 'can', 'never', 'get', 'anything', 'done', 'but', 'could', 'that', 'just', 'be', 'a', 'result', 'of', 'pure', 'la', '##zine', '##ss', '?']\n",
      "INFO:__main__:Number of tokens: 94\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['diagnosed', 'with', 'ad', '##hd', 'recently', '.', 'wrong', 'diagnosis', '?', 'i', 'was', 'diagnosed', 'with', 'adult', 'ad', '##hd', 'a', 'couple', 'of', 'months', 'ago', 'and', 'am', 'now', 'taking', '35', '##mg', 'of', 'rita', '##lin', 'twice', 'a', 'day', ',', 'i', 'usually', 'only', 'take', 'one', 'dose', '.', 'i', \"'\", 'm', 'not', 'sure', 'if', 'i', 'really', 'do', 'have', 'ad', '##hd', 'as', 'i', \"'\", 'm', 'not', 'usually', 'one', 'to', 'be', 'hyper', '##active', '.', 'i', 'am', 'very', 'forget', '##ful', 'and', 'have', 'no', 'attention', 'span', 'and', 'can', 'never', 'get', 'anything', 'done', 'but', 'could', 'that', 'just', 'be', 'a', 'result', 'of', 'pure', 'la', '##zine', '##ss', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tried', 'every', 'ad', '##hd', 'drug', 'on', 'the', 'market', '-', 'no', 'help', '.', 'what', 'now', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tried', 'every', 'ad', '##hd', 'drug', 'on', 'the', 'market', '-', 'no', 'help', '.', 'what', 'now', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'some', 'common', 'problems', 'in', 'your', 'relationship', 'with', 'your', 'spouse', 'that', 'are', 'related', 'to', '/', 'caused', 'by', 'ad', '##hd', '?', 'just', 'looking', 'for', 'some', 'answers', 'as', 'to', 'why', 'they', 'never', 'last', 'for', 'me', '.', '.']\n",
      "INFO:__main__:Number of tokens: 36\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'some', 'common', 'problems', 'in', 'your', 'relationship', 'with', 'your', 'spouse', 'that', 'are', 'related', 'to', '/', 'caused', 'by', 'ad', '##hd', '?', 'just', 'looking', 'for', 'some', 'answers', 'as', 'to', 'why', 'they', 'never', 'last', 'for', 'me', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'not', 'sure', 'what', 'to', 'do', '.']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'not', 'sure', 'what', 'to', 'do', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['no', 'luck', 'with', 'v', '##y', '##van', '##se', ',', 'rita', '##lin', '(', 'ir', '/', 'la', ')', ',', 'focal', '##in', '-', 'has', 'anyone', 'experienced', 'similar', '?', '-', 'cross', '##post', 'from', '/', 'r', '/', 'add']\n",
      "INFO:__main__:Number of tokens: 32\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['no', 'luck', 'with', 'v', '##y', '##van', '##se', ',', 'rita', '##lin', '(', 'ir', '/', 'la', ')', ',', 'focal', '##in', '-', 'has', 'anyone', 'experienced', 'similar', '?', '-', 'cross', '##post', 'from', '/', 'r', '/', 'add']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'received', 'my', 'prescription', 'today', 'i', 'was', 'curious', 'to', 'a', 'few', 'things', '.', 'i', 'looked', 'at', 'the', 'bottle', 'and', 'it', 'said', 'd', '-', 'amp', '##het', '##amine', 'salt', 'combo', 'is', 'there', 'a', 'difference', 'between', 'that', 'and', 'actual', 'ad', '##eral', '##l', '.', 'how', 'long', 'does', 'it', 'take', 'to', 'work', '?', 'and', 'how', 'does', 'it', 'effect', 'you', 'normally', '?']\n",
      "INFO:__main__:Number of tokens: 56\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'received', 'my', 'prescription', 'today', 'i', 'was', 'curious', 'to', 'a', 'few', 'things', '.', 'i', 'looked', 'at', 'the', 'bottle', 'and', 'it', 'said', 'd', '-', 'amp', '##het', '##amine', 'salt', 'combo', 'is', 'there', 'a', 'difference', 'between', 'that', 'and', 'actual', 'ad', '##eral', '##l', '.', 'how', 'long', 'does', 'it', 'take', 'to', 'work', '?', 'and', 'how', 'does', 'it', 'effect', 'you', 'normally', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['shout', 'to', 'r', '/', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['shout', 'to', 'r', '/', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "INFO:__main__:Tokenized text: ['26', ',', 'no', 'idea', 'what', 'to', 'do', ',', 'could', 'use', 'advice', 'so', 'i', 'have', 'ad', '##hd', 'and', 'have', 'been', 'having', 'to', 'deal', 'with', 'it', 'my', 'whole', 'life', ',', 'i', \"'\", 've', 'never', 'really', 'been', 'on', 'med', '##s', 'for', 'it', ',', 'and', 'it', 'seems', 'the', 'way', 'to', 'go', ',', 'but', 'my', 'main', 'question', 'for', 'you', 'guys', 'is', 'how', 'to', 'deal', 'before', 'you', 'have', 'a', 'chance', 'to', 'get', 'to', 'a', 'pre', '##scribe', '##r', '.', 'i', \"'\", 'm', 'currently', 'unemployed', ',', 'and', 'therefore', 'no', 'health', 'insurance', 'and', 'the', 'ad', '##hd', 'is', 'severely', 'impact', '##ing', 'my', 'job', 'search', 'as', 'well', 'as', 'the', 'rest', 'of', 'my', 'life', '.', 'background', ':', 'ad', '##hd', 'has', 'been', 'a', 'major', 'factor', 'in', 'my', 'inability', 'to', 'focus', 'and', 'get', 'things', 'done', 'since', 'i', 'was', 'in', '1st', 'grade', '.', 'i', '’', 'm', 'not', 'hyper', '##active', ',', 'my', 'main', 'problem', 'is', 'my', 'inability', 'to', 'do', 'things', 'i', 'don', '’', 't', 'find', 'intriguing', ',', 'even', 'if', 'i', 'can', 'start', 'it', 'doesn', '’', 't', 'last', 'more', 'than', 'a', 'couple', 'minutes', 'before', 'i', 'do', 'something', 'else', '.', 'throughout', 'grade', 'school', 'and', 'college', 'my', 'grades', 'suffered', 'because', 'i', 'just', 'never', 'did', 'the', 'work', 'or', 'studied', ',', 'i', 'did', 'poorly', 'in', 'college', 'because', 'of', 'it', ',', 'and', 'at', 'my', 'last', 'job', 'it', 'was', 'a', 'constant', 'problem', '.', 'the', 'only', 'reason', 'i', 'didn', '’', 't', 'get', 'fired', 'is', 'my', 'ability', 'to', 'talk', 'my', 'way', 'out', 'of', 'most', 'problems', ',', 'i', 'constantly', 'ignored', 'my', 'responsibilities', 'or', 'even', 'forgot', 'about', 'them', '.', 'about', '6', 'years', 'ago', 'i', 'started', 'to', 'accept', 'what', 'ad', '##hd', 'was', 'and', 'that', 'i', 'had', 'it', ',', 'i', 'had', 'been', 'in', 'denial', 'and', 'one', 'of', 'those', 'asshole', '##s', 'that', 'claim', 'it', 'doesn', '’', 't', 'really', 'exist', ',', 'or', 'at', 'the', 'very', 'least', 'i', 'didn', '’', 't', 'have', 'it', '.', 'so', 'about', 'halfway', 'into', 'doing', 'really', 'shitty', 'in', 'college', 'i', 'wanted', 'to', 'get', 'help', ',', 'i', 'went', 'to', 'my', 'primary', 'doc', 'and', 'asked', 'if', 'add', '##eral', '##l', 'was', 'the', 'way', 'to', 'go', '.', 'he', 'said', 'he', 'didn', '’', 't', 'like', 'add', '##eral', '##l', 'and', 'prescribed', 'concert', '##a', ',', 'so', 'i', 'went', 'and', 'got', 'that', 'prescription', 'filled', 'and', 'took', 'it', 'a', 'couple', 'times', ',', 'i', 'ended', 'up', 'freaking', 'out', 'because', 'my', 'heart', 'was', 'pounding', 'so', 'hard', 'because', 'of', 'it', 'and', 'never', 'took', 'it', 'again', '.', 'a', 'couple', 'years', 'ago', 'i', 'knew', 'i', 'had', 'to', 'do', 'something', 'after', 'almost', 'getting', 'fired', 'a', 'couple', 'times', '(', 'in', 'all', 'honesty', 'i', 'should', 'have', 'been', ')', ',', 'so', 'i', 'went', 'to', 'a', 'new', 'doc', 'who', 'said', 'to', 'try', 'add', '##eral', '##l', '.', 'i', 'got', 'the', 'prescription', 'and', 'took', 'it', 'to', 'cv', '##s', ',', 'they', 'said', 'the', 'insurance', 'company', 'needed', 'a', 'reason', 'for', 'it', 'to', 'be', 'filled', ',', 'which', 'was', 'surprising', 'to', 'me', 'because', 'you', '’', 'd', 'think', 'that', 'the', 'doc', 'pre', '##sc', '##ri', '##bing', 'it', 'would', 'be', 'reason', 'enough', ',', 'didn', '’', 't', 'pursue', 'it', 'beyond', 'that', '.', 'that', 'was', 'the', 'last', 'time', 'i', 'did', 'anything', ',', 'it', '’', 's', 'years', 'in', 'between', 'tries', 'because', 'i', '’', 'm', 'so', 'lax', 'about', 'it', '.', 'i', '’', 'm', 'even', 'using', 'this', 'as', 'something', 'i', 'feel', 'a', 'need', 'to', 'do', 'other', 'than', 'what', 'i', 'should', 'be', 'doing', ',', 'applying', 'for', 'jobs', '.', 't', '##l', ';', 'dr', '–', 'ad', '##hd', 'has', 'a', 'hold', 'on', 'my', 'life', ',', 'i', 'have', 'no', 'insurance', ',', 'how', 'to', 'i', 'get', 'to', 'a', 'point', 'to', 'where', 'i', 'can', 'get', 'on', 'med', '##s', '?', ',', 'and', 'how', 'do', 'i', 'actually', 'stay', 'on', 'them', '?', 'any', 'help', 'is', 'appreciated']\n",
      "INFO:__main__:Number of tokens: 575\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['26', ',', 'no', 'idea', 'what', 'to', 'do', ',', 'could', 'use', 'advice', 'so', 'i', 'have', 'ad', '##hd', 'and', 'have', 'been', 'having', 'to', 'deal', 'with', 'it', 'my', 'whole', 'life', ',', 'i', \"'\", 've', 'never', 'really', 'been', 'on', 'med', '##s', 'for', 'it', ',', 'and', 'it', 'seems', 'the', 'way', 'to', 'go', ',', 'but', 'my', 'main', 'question', 'for', 'you', 'guys', 'is', 'how', 'to', 'deal', 'before', 'you', 'have', 'a', 'chance', 'to', 'get', 'to', 'a', 'pre', '##scribe', '##r', '.', 'i', \"'\", 'm', 'currently', 'unemployed', ',', 'and', 'therefore', 'no', 'health', 'insurance', 'and', 'the', 'ad', '##hd', 'is', 'severely', 'impact', '##ing', 'my', 'job', 'search', 'as', 'well', 'as', 'the', 'rest', 'of', 'my', 'life', '.', 'background', ':', 'ad', '##hd', 'has', 'been', 'a', 'major', 'factor', 'in', 'my', 'inability', 'to', 'focus', 'and', 'get', 'things', 'done', 'since', 'i', 'was', 'in', '1st', 'grade', '.', 'i', '’', 'm', 'not', 'hyper', '##active', ',', 'my', 'main', 'problem', 'is', 'my', 'inability', 'to', 'do', 'things', 'i', 'don', '’', 't', 'find', 'intriguing', ',', 'even', 'if', 'i', 'can', 'start', 'it', 'doesn', '’', 't', 'last', 'more', 'than', 'a', 'couple', 'minutes', 'before', 'i', 'do', 'something', 'else', '.', 'throughout', 'grade', 'school', 'and', 'college', 'my', 'grades', 'suffered', 'because', 'i', 'just', 'never', 'did', 'the', 'work', 'or', 'studied', ',', 'i', 'did', 'poorly', 'in', 'college', 'because', 'of', 'it', ',', 'and', 'at', 'my', 'last', 'job', 'it', 'was', 'a', 'constant', 'problem', '.', 'the', 'only', 'reason', 'i', 'didn', '’', 't', 'get', 'fired', 'is', 'my', 'ability', 'to', 'talk', 'my', 'way', 'out', 'of', 'most', 'problems', ',', 'i', 'constantly', 'ignored', 'my', 'responsibilities', 'or', 'even', 'forgot', 'about', 'them', '.', 'about', '6', 'years', 'ago', 'i', 'started', 'to', 'accept', 'what', 'ad', '##hd', 'was', 'and', 'that', 'i', 'had', 'it', ',', 'i', 'had', 'been', 'in', 'denial', 'and', 'one', 'of', 'those', 'asshole', '##s', 'that', 'claim', 'it', 'doesn', '’', 't', 'really', 'exist', ',', 'or', 'at', 'the', 'very', 'least', 'i', 'didn', '’', 't', 'have', 'it', '.', 'so', 'about', 'halfway', 'into', 'doing', 'really', 'shitty', 'in', 'college', 'i', 'wanted', 'to', 'get', 'help', ',', 'i', 'went', 'to', 'my', 'primary', 'doc', 'and', 'asked', 'if', 'add', '##eral', '##l', 'was', 'the', 'way', 'to', 'go', '.', 'he', 'said', 'he', 'didn', '’', 't', 'like', 'add', '##eral', '##l', 'and', 'prescribed', 'concert', '##a', ',', 'so', 'i', 'went', 'and', 'got', 'that', 'prescription', 'filled', 'and', 'took', 'it', 'a', 'couple', 'times', ',', 'i', 'ended', 'up', 'freaking', 'out', 'because', 'my', 'heart', 'was', 'pounding', 'so', 'hard', 'because', 'of', 'it', 'and', 'never', 'took', 'it', 'again', '.', 'a', 'couple', 'years', 'ago', 'i', 'knew', 'i', 'had', 'to', 'do', 'something', 'after', 'almost', 'getting', 'fired', 'a', 'couple', 'times', '(', 'in', 'all', 'honesty', 'i', 'should', 'have', 'been', ')', ',', 'so', 'i', 'went', 'to', 'a', 'new', 'doc', 'who', 'said', 'to', 'try', 'add', '##eral', '##l', '.', 'i', 'got', 'the', 'prescription', 'and', 'took', 'it', 'to', 'cv', '##s', ',', 'they', 'said', 'the', 'insurance', 'company', 'needed', 'a', 'reason', 'for', 'it', 'to', 'be', 'filled', ',', 'which', 'was', 'surprising', 'to', 'me', 'because', 'you', '’', 'd', 'think', 'that', 'the', 'doc', 'pre', '##sc', '##ri', '##bing', 'it', 'would', 'be', 'reason', 'enough', ',', 'didn', '’', 't', 'pursue', 'it', 'beyond', 'that', '.', 'that', 'was', 'the', 'last', 'time', 'i', 'did', 'anything', ',', 'it', '’', 's', 'years', 'in', 'between', 'tries', 'because', 'i', '’', 'm', 'so', 'lax', 'about', 'it', '.', 'i', '’', 'm', 'even', 'using', 'this', 'as', 'something', 'i', 'feel', 'a', 'need'], ['to', 'do', 'other', 'than', 'what', 'i', 'should', 'be', 'doing', ',', 'applying', 'for', 'jobs', '.', 't', '##l', ';', 'dr', '–', 'ad', '##hd', 'has', 'a', 'hold', 'on', 'my', 'life', ',', 'i', 'have', 'no', 'insurance', ',', 'how', 'to', 'i', 'get', 'to', 'a', 'point', 'to', 'where', 'i', 'can', 'get', 'on', 'med', '##s', '?', ',', 'and', 'how', 'do', 'i', 'actually', 'stay', 'on', 'them', '?', 'any', 'help', 'is', 'appreciated']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['adult', 'add', 'no', 'insurance', 'diagnosed', 'as', 'a', 'child', 'and', 'was', 'med', '##icated', 'until', '15', '.', 'i', 'stopped', 'taking', 'med', '##s', 'and', 'found', 'ways', 'to', 'cope', '.', 'i', 'am', '30', 'now', 'and', 'the', 'brain', 'is', 'not', 'under', 'my', 'control', 'any', 'more', '.', 'have', 'al', '##ot', 'of', 'lost', 'moments', 'and', 'time', 'stuck', 'on', 'the', 'dumb', '.', 'i', 'am', 'a', 'contractor', 'and', 'have', 'no', 'insurance', '.', 'i', 'can', '##t', 'afford', 'a', 'ps', '##ych', 'dr', 'and', 'med', '##s', 'but', 'need', 'to', 'do', 'something', '.', 'i', 'have', 'read', 'zinc', ',', 'fish', 'oil', 'and', 'morning', 'pri', '##m', '##rose', 'oil', 'can', 'do', 'wonders', 'as', 'a', 'natural', 'alternative', 'to', 'various', 'medications', '.', 'does', 'anyone', 'have', 'experience', 'with', 'these', 'things', '.', 'also', 'my', 'son', '(', 'not', 'genetic', ')', 'is', 'diagnosed', 'bi', '-', 'polar', 'and', 'i', 'have', 'come', 'to', 'learn', 'al', '##ot', 'about', 'it', 'over', 'the', 'last', '10', 'years', '.', 'i', 'have', 'a', 'real', 'short', 'fuse', 'and', 'can', 'go', 'from', '0', '-', 'totally', 'pissed', 'with', 'heart', 'rate', 'up', 'in', 'seconds', 'then', 'calm', 'back', 'down', 'in', 'minutes', '.', 'don', \"'\", 't', 'know', 'if', 'i', 'could', 'also', 'be', 'bi', '-', 'polar', 'or', 'if', 'this', 'is', 'happening', 'because', 'my', 'brain', 'isn', \"'\", 't', 'functioning', 'correctly', '.', 'i', 'know', 'i', 'could', 'find', 'a', 'dr', '.', 'and', 'probably', 'be', 'put', 'on', 'disability', 'to', 'get', 'free', 'medical', 'care', 'and', 'pills', 'but', 'frankly', 'i', 'can', \"'\", 't', 'live', 'on', 'disability', 'it', 'doesn', '##t', 'pay', 'enough', 'so', 'i', 'have', 'to', 'work', '.']\n",
      "INFO:__main__:Number of tokens: 234\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['adult', 'add', 'no', 'insurance', 'diagnosed', 'as', 'a', 'child', 'and', 'was', 'med', '##icated', 'until', '15', '.', 'i', 'stopped', 'taking', 'med', '##s', 'and', 'found', 'ways', 'to', 'cope', '.', 'i', 'am', '30', 'now', 'and', 'the', 'brain', 'is', 'not', 'under', 'my', 'control', 'any', 'more', '.', 'have', 'al', '##ot', 'of', 'lost', 'moments', 'and', 'time', 'stuck', 'on', 'the', 'dumb', '.', 'i', 'am', 'a', 'contractor', 'and', 'have', 'no', 'insurance', '.', 'i', 'can', '##t', 'afford', 'a', 'ps', '##ych', 'dr', 'and', 'med', '##s', 'but', 'need', 'to', 'do', 'something', '.', 'i', 'have', 'read', 'zinc', ',', 'fish', 'oil', 'and', 'morning', 'pri', '##m', '##rose', 'oil', 'can', 'do', 'wonders', 'as', 'a', 'natural', 'alternative', 'to', 'various', 'medications', '.', 'does', 'anyone', 'have', 'experience', 'with', 'these', 'things', '.', 'also', 'my', 'son', '(', 'not', 'genetic', ')', 'is', 'diagnosed', 'bi', '-', 'polar', 'and', 'i', 'have', 'come', 'to', 'learn', 'al', '##ot', 'about', 'it', 'over', 'the', 'last', '10', 'years', '.', 'i', 'have', 'a', 'real', 'short', 'fuse', 'and', 'can', 'go', 'from', '0', '-', 'totally', 'pissed', 'with', 'heart', 'rate', 'up', 'in', 'seconds', 'then', 'calm', 'back', 'down', 'in', 'minutes', '.', 'don', \"'\", 't', 'know', 'if', 'i', 'could', 'also', 'be', 'bi', '-', 'polar', 'or', 'if', 'this', 'is', 'happening', 'because', 'my', 'brain', 'isn', \"'\", 't', 'functioning', 'correctly', '.', 'i', 'know', 'i', 'could', 'find', 'a', 'dr', '.', 'and', 'probably', 'be', 'put', 'on', 'disability', 'to', 'get', 'free', 'medical', 'care', 'and', 'pills', 'but', 'frankly', 'i', 'can', \"'\", 't', 'live', 'on', 'disability', 'it', 'doesn', '##t', 'pay', 'enough', 'so', 'i', 'have', 'to', 'work', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['na', '##rco', '##le', '##psy', 'or', 'just', 'bad', 'sleep', 'habits', '?', '(', 'x', '-', 'post', 'from', '/', 'r', '/', 'na', '##rco', '##le', '##psy', ')', 'for', 'the', 'past', 'several', 'years', '(', 'i', \"'\", 'm', 'in', 'high', 'school', ')', 'i', \"'\", 've', 'always', 'had', 'trouble', 'staying', 'awake', 'during', 'my', 'school', 'days', 'and', 'even', 'at', 'home', 'some', 'times', '.', 'to', 'me', ',', 'i', 'was', 'normal', ',', 'because', 'i', 'know', 'everyone', 'falls', 'asleep', 'at', 'school', '.', 'but', 'the', 'thing', 'is', 'i', 'recently', 'experienced', 'sleep', 'paralysis', 'for', 'the', 'first', 'time', ',', 'and', 'this', 'lead', 'me', 'to', 'look', 'at', 'na', '##rco', '##le', '##psy', 'as', 'a', 'possibility', '.', 'does', 'na', '##rco', '##le', '##psy', 'ever', 'group', 'with', 'ad', '/', 'hd', '?', 'i', 'was', 'diagnosed', 'with', 'depression', 'but', 'that', 'diagnosis', 'was', ',', 'to', 'me', ',', 'incorrect', 'and', 'it', 'now', 'looks', 'like', 'ad', '/', 'hd', 'to', 'me', ',', 'but', 'as', 'i', 'stated', 'above', ',', 'the', 'symptoms', 'for', 'na', '##rco', '##le', '##psy', 'have', 'made', 'themselves', 'clear', 'and', 'i', \"'\", 'm', 'just', 'not', 'really', 'sure', 'what', 'to', 'think', '.', 'if', 'it', 'matters', 'my', 'ad', '/', 'hd', '(', 'if', 'that', \"'\", 's', 'really', 'it', ')', 'symptoms', 'have', 'had', 'to', 'do', 'with', 'ina', '##tten', '##tion', ',', 'inability', 'to', 'focus', 'on', 'a', 'topic', 'which', 'leads', 'me', 'to', 'trouble', 'grasping', 'concepts', 'and', 'therefore', 'academic', 'failure', ',', 'and', 'the', 'inability', 'to', 'be', 'still', 'or', 'be', 'inactive', 'to', 'some', 'extent', '.', 'all', 'of', 'this', ',', 'coupled', 'with', 'my', 'falling', 'asleep', 'at', 'inappropriate', 'times', ',', 'have', 'just', 'made', 'my', 'grades', ',', 'and', 'mood', ',', 'drop', '.', 'please', 'help', ',', 'red', '##dit', '.', 'what', 'are', 'your', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 257\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['na', '##rco', '##le', '##psy', 'or', 'just', 'bad', 'sleep', 'habits', '?', '(', 'x', '-', 'post', 'from', '/', 'r', '/', 'na', '##rco', '##le', '##psy', ')', 'for', 'the', 'past', 'several', 'years', '(', 'i', \"'\", 'm', 'in', 'high', 'school', ')', 'i', \"'\", 've', 'always', 'had', 'trouble', 'staying', 'awake', 'during', 'my', 'school', 'days', 'and', 'even', 'at', 'home', 'some', 'times', '.', 'to', 'me', ',', 'i', 'was', 'normal', ',', 'because', 'i', 'know', 'everyone', 'falls', 'asleep', 'at', 'school', '.', 'but', 'the', 'thing', 'is', 'i', 'recently', 'experienced', 'sleep', 'paralysis', 'for', 'the', 'first', 'time', ',', 'and', 'this', 'lead', 'me', 'to', 'look', 'at', 'na', '##rco', '##le', '##psy', 'as', 'a', 'possibility', '.', 'does', 'na', '##rco', '##le', '##psy', 'ever', 'group', 'with', 'ad', '/', 'hd', '?', 'i', 'was', 'diagnosed', 'with', 'depression', 'but', 'that', 'diagnosis', 'was', ',', 'to', 'me', ',', 'incorrect', 'and', 'it', 'now', 'looks', 'like', 'ad', '/', 'hd', 'to', 'me', ',', 'but', 'as', 'i', 'stated', 'above', ',', 'the', 'symptoms', 'for', 'na', '##rco', '##le', '##psy', 'have', 'made', 'themselves', 'clear', 'and', 'i', \"'\", 'm', 'just', 'not', 'really', 'sure', 'what', 'to', 'think', '.', 'if', 'it', 'matters', 'my', 'ad', '/', 'hd', '(', 'if', 'that', \"'\", 's', 'really', 'it', ')', 'symptoms', 'have', 'had', 'to', 'do', 'with', 'ina', '##tten', '##tion', ',', 'inability', 'to', 'focus', 'on', 'a', 'topic', 'which', 'leads', 'me', 'to', 'trouble', 'grasping', 'concepts', 'and', 'therefore', 'academic', 'failure', ',', 'and', 'the', 'inability', 'to', 'be', 'still', 'or', 'be', 'inactive', 'to', 'some', 'extent', '.', 'all', 'of', 'this', ',', 'coupled', 'with', 'my', 'falling', 'asleep', 'at', 'inappropriate', 'times', ',', 'have', 'just', 'made', 'my', 'grades', ',', 'and', 'mood', ',', 'drop', '.', 'please', 'help', ',', 'red', '##dit', '.', 'what', 'are', 'your', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['medication', 'from', 'the', 'coffee', 'shop', '?', 'examining', 'caf', '##fe', '##ine', 'for', 'the', 'treatment', 'of', 'ad', '##hd', '.']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['medication', 'from', 'the', 'coffee', 'shop', '?', 'examining', 'caf', '##fe', '##ine', 'for', 'the', 'treatment', 'of', 'ad', '##hd', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'as', 'a', 'child', 'but', 'stopped', 'taking', 'medication', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'as', 'a', 'child', 'but', 'stopped', 'taking', 'medication', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##eral', '##l', 'and', 'tolerance', 'i', 'am', 'so', 'happy', 'that', 'i', 'found', 'this', 'sub', '##red', '##dit', 'out', 'you', 'guys', 'have', 'been', 'light', 'years', 'of', 'help', '!', 'i', 'now', 'have', 'a', 'new', 'set', 'of', 'questions', '.', 'i', 'am', 'highly', 'enjoying', 'how', 'productive', 'and', 'focused', 'i', 'have', 'been', 'but', 'i', 'have', 'a', 'worry', 'that', 'i', 'might', 'build', 'a', 'tolerance', 'too', 'this', '.', 'so', 'i', 'have', '3', 'questions', '!', '1', '.', 'how', 'long', 'before', 'my', 'body', 'begins', 'to', 'adapt', 'and', 'build', 'a', 'tolerance', '?', '2', '.', 'how', 'do', 'i', 'prevent', 'my', 'body', 'from', 'building', 'the', 'tolerance', '?', 'and', '3', '.', 'if', 'i', 'do', 'build', 'a', 'tolerance', 'how', 'do', 'i', 'basically', 'reset', '?']\n",
      "INFO:__main__:Number of tokens: 109\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##eral', '##l', 'and', 'tolerance', 'i', 'am', 'so', 'happy', 'that', 'i', 'found', 'this', 'sub', '##red', '##dit', 'out', 'you', 'guys', 'have', 'been', 'light', 'years', 'of', 'help', '!', 'i', 'now', 'have', 'a', 'new', 'set', 'of', 'questions', '.', 'i', 'am', 'highly', 'enjoying', 'how', 'productive', 'and', 'focused', 'i', 'have', 'been', 'but', 'i', 'have', 'a', 'worry', 'that', 'i', 'might', 'build', 'a', 'tolerance', 'too', 'this', '.', 'so', 'i', 'have', '3', 'questions', '!', '1', '.', 'how', 'long', 'before', 'my', 'body', 'begins', 'to', 'adapt', 'and', 'build', 'a', 'tolerance', '?', '2', '.', 'how', 'do', 'i', 'prevent', 'my', 'body', 'from', 'building', 'the', 'tolerance', '?', 'and', '3', '.', 'if', 'i', 'do', 'build', 'a', 'tolerance', 'how', 'do', 'i', 'basically', 'reset', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['remember', 'the', 'motor', '##cy', '##cl', '##ist', 'who', 'was', 'pulled', 'out', 'from', 'underneath', 'that', 'burning', 'car', '?', 'well', '.', '.', '.', 'he', 'survived', '!']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['remember', 'the', 'motor', '##cy', '##cl', '##ist', 'who', 'was', 'pulled', 'out', 'from', 'underneath', 'that', 'burning', 'car', '?', 'well', '.', '.', '.', 'he', 'survived', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'x', '-', 'post', 'from', 'add', ']', 'add', '##eral', '##l', ',', 'food', ',', 'and', 'ph', ':', 'there', 'seems', 'to', 'be', 'no', 'consensus', 'as', 'to', 'how', 'to', 'get', 'the', 'most', 'out', 'of', 'your', 'medicine', '.', 'i', \"'\", 've', 'read', 'hundreds', 'of', 'an', '##ec', '##dote', '##s', 'in', 'support', 'of', 'dozens', 'of', 'conflicting', 'beliefs', '.', 'please', 'use', 'science', 'to', 'explain', 'how', 'these', '3', 'interact', '.', '[', 'will', 'x', '-', 'post', 'to', 'asks', '##oc', '##ials', '##cie', '##nce', '/', 'asks', '##cie', '##nce', ']']\n",
      "INFO:__main__:Number of tokens: 78\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'x', '-', 'post', 'from', 'add', ']', 'add', '##eral', '##l', ',', 'food', ',', 'and', 'ph', ':', 'there', 'seems', 'to', 'be', 'no', 'consensus', 'as', 'to', 'how', 'to', 'get', 'the', 'most', 'out', 'of', 'your', 'medicine', '.', 'i', \"'\", 've', 'read', 'hundreds', 'of', 'an', '##ec', '##dote', '##s', 'in', 'support', 'of', 'dozens', 'of', 'conflicting', 'beliefs', '.', 'please', 'use', 'science', 'to', 'explain', 'how', 'these', '3', 'interact', '.', '[', 'will', 'x', '-', 'post', 'to', 'asks', '##oc', '##ials', '##cie', '##nce', '/', 'asks', '##cie', '##nce', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['before', 'and', 'after', 'medications', 'so', ',', 'here', \"'\", 's', 'the', 'deal', '.', 'all', 'my', 'life', ',', 'i', \"'\", 've', 'been', 'kinda', 'fl', '##ak', '##y', '/', 'flight', '##y', '.', 'shrugged', 'off', 'homework', ',', 'ditch', '##ed', 'plans', 'with', 'friends', ',', 'etc', '.', 'my', 'parents', 'and', 'teachers', 'always', 'attributed', 'it', 'to', 'my', 'intelligence', ',', 'and', 'as', 'they', 'were', 'essentially', 'calling', 'me', 'smart', ',', 'i', 'went', 'with', 'it', '.', 'but', ',', 'in', 'recent', 'years', ',', 'i', \"'\", 've', 'started', 'to', 'suspect', 'it', 'was', 'ad', '##hd', '.', 'i', \"'\", 've', 'had', 'problems', 'with', 'several', 'drugs', ',', 'all', 'of', 'which', 'i', 'ended', 'up', 'just', 'walking', 'away', 'from', 'for', 'a', 'new', 'cr', '##ut', '##ch', '.', 'my', 'mind', 'never', 'stops', 'moving', ',', 'even', 'when', 'i', \"'\", 'm', 'trying', 'to', 'sleep', '.', 'it', 'also', 'just', 'kind', 'of', 'wander', '##s', ',', 'often', 'ponder', '##ing', 'random', 'things', 'that', 'nobody', 'would', 'ever', 'care', 'about', ',', 'but', 'which', 'amused', 'me', 'in', 'the', 'moment', '.', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'hold', 'a', 'job', 'for', 'long', ',', 'and', 'whenever', 'i', 'get', 'paid', ',', 'i', 'tend', 'to', 'spend', 'it', 'on', 'shit', 'i', 'don', \"'\", 't', 'need', ';', 'i', 'just', 'say', 'fuck', 'it', 'and', 'buy', 'stuff', '.', 'i', \"'\", 'm', 'divorced', 'at', '22', ',', 'and', 'none', 'of', 'my', 'other', 'relationships', 'have', 'lasted', 'long', '.', 'anyway', '##s', ',', 'i', 'ram', '##ble', '.', 'i', \"'\", 'm', 'going', 'in', 'to', 'get', 'it', 'diagnosed', 'monday', 'morning', ',', 'and', 'while', 'i', \"'\", 'll', 'let', 'the', 'doctors', 'worry', 'about', 'whether', 'i', 'have', 'it', 'or', 'not', ',', 'my', 'question', 'is', ',', 'did', 'you', 'feel', 'any', 'different', 'after', 'starting', 'treatment', '?', 'like', ',', 'did', 'you', 'look', 'at', 'things', 'differently', ',', 'or', 'have', 'a', 'different', 'thought', 'process', 'or', 'anything', '?']\n",
      "INFO:__main__:Number of tokens: 276\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['before', 'and', 'after', 'medications', 'so', ',', 'here', \"'\", 's', 'the', 'deal', '.', 'all', 'my', 'life', ',', 'i', \"'\", 've', 'been', 'kinda', 'fl', '##ak', '##y', '/', 'flight', '##y', '.', 'shrugged', 'off', 'homework', ',', 'ditch', '##ed', 'plans', 'with', 'friends', ',', 'etc', '.', 'my', 'parents', 'and', 'teachers', 'always', 'attributed', 'it', 'to', 'my', 'intelligence', ',', 'and', 'as', 'they', 'were', 'essentially', 'calling', 'me', 'smart', ',', 'i', 'went', 'with', 'it', '.', 'but', ',', 'in', 'recent', 'years', ',', 'i', \"'\", 've', 'started', 'to', 'suspect', 'it', 'was', 'ad', '##hd', '.', 'i', \"'\", 've', 'had', 'problems', 'with', 'several', 'drugs', ',', 'all', 'of', 'which', 'i', 'ended', 'up', 'just', 'walking', 'away', 'from', 'for', 'a', 'new', 'cr', '##ut', '##ch', '.', 'my', 'mind', 'never', 'stops', 'moving', ',', 'even', 'when', 'i', \"'\", 'm', 'trying', 'to', 'sleep', '.', 'it', 'also', 'just', 'kind', 'of', 'wander', '##s', ',', 'often', 'ponder', '##ing', 'random', 'things', 'that', 'nobody', 'would', 'ever', 'care', 'about', ',', 'but', 'which', 'amused', 'me', 'in', 'the', 'moment', '.', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'hold', 'a', 'job', 'for', 'long', ',', 'and', 'whenever', 'i', 'get', 'paid', ',', 'i', 'tend', 'to', 'spend', 'it', 'on', 'shit', 'i', 'don', \"'\", 't', 'need', ';', 'i', 'just', 'say', 'fuck', 'it', 'and', 'buy', 'stuff', '.', 'i', \"'\", 'm', 'divorced', 'at', '22', ',', 'and', 'none', 'of', 'my', 'other', 'relationships', 'have', 'lasted', 'long', '.', 'anyway', '##s', ',', 'i', 'ram', '##ble', '.', 'i', \"'\", 'm', 'going', 'in', 'to', 'get', 'it', 'diagnosed', 'monday', 'morning', ',', 'and', 'while', 'i', \"'\", 'll', 'let', 'the', 'doctors', 'worry', 'about', 'whether', 'i', 'have', 'it', 'or', 'not', ',', 'my', 'question', 'is', ',', 'did', 'you', 'feel', 'any', 'different', 'after', 'starting', 'treatment', '?', 'like', ',', 'did', 'you', 'look', 'at', 'things', 'differently', ',', 'or', 'have', 'a', 'different', 'thought', 'process', 'or', 'anything', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'can', 'only', 'enjoy', 'lyrics', 'in', 'music', 'while', 'on', 'a', 'substance', '.']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'can', 'only', 'enjoy', 'lyrics', 'in', 'music', 'while', 'on', 'a', 'substance', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['did', '##ae', 'watch', '\"', 'limit', '##less', '\"', ',', 'personally', 'relate', 'to', 'it', 'and', 'feel', 'kind', 'of', 'guilty', 'about', 'it', '?', '[', 'trailer', ']']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['did', '##ae', 'watch', '\"', 'limit', '##less', '\"', ',', 'personally', 'relate', 'to', 'it', 'and', 'feel', 'kind', 'of', 'guilty', 'about', 'it', '?', '[', 'trailer', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dear', 'un', '##med', '##icated', ':', 'how', 'do', 'you', 'fall', 'asleep', '?', 'i', \"'\", 'm', 'waiting', 'for', 'health', 'benefits', 'at', 'work', 'to', 'kick', 'in', 'before', 'i', 'start', 'going', 'to', 'therapy', ',', 'so', 'until', 'then', ',', 'i', \"'\", 'm', 'kinda', 'stuck', '.', 'i', 'work', 'overnight', ',', 'so', 'i', 'generally', 'come', 'home', 'when', 'the', 'sun', 'is', 'already', 'up', '.', 'i', 'have', 'nice', 'curtains', 'that', 'block', 'out', 'the', 'sun', ',', 'but', 'that', 'only', 'does', 'so', 'much', 'for', 'me', '.', 'despite', 'being', 'clearly', 'physically', 'tired', ',', 'i', 'find', 'things', 'to', 'do', 'until', 'i', \"'\", 've', 'wasted', 'my', 'whole', 'day', 'in', 'a', '\"', 'no', 'hold', 'on', ',', 'i', \"'\", 'm', 'about', 'to', 'sleep', '\"', 'phase', '.', 'does', 'anyone', 'have', 'any', 'tips', '?', 'what', 'do', 'you', 'do', 'when', 'you', 'can', \"'\", 't', 'sleep', '?']\n",
      "INFO:__main__:Number of tokens: 127\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dear', 'un', '##med', '##icated', ':', 'how', 'do', 'you', 'fall', 'asleep', '?', 'i', \"'\", 'm', 'waiting', 'for', 'health', 'benefits', 'at', 'work', 'to', 'kick', 'in', 'before', 'i', 'start', 'going', 'to', 'therapy', ',', 'so', 'until', 'then', ',', 'i', \"'\", 'm', 'kinda', 'stuck', '.', 'i', 'work', 'overnight', ',', 'so', 'i', 'generally', 'come', 'home', 'when', 'the', 'sun', 'is', 'already', 'up', '.', 'i', 'have', 'nice', 'curtains', 'that', 'block', 'out', 'the', 'sun', ',', 'but', 'that', 'only', 'does', 'so', 'much', 'for', 'me', '.', 'despite', 'being', 'clearly', 'physically', 'tired', ',', 'i', 'find', 'things', 'to', 'do', 'until', 'i', \"'\", 've', 'wasted', 'my', 'whole', 'day', 'in', 'a', '\"', 'no', 'hold', 'on', ',', 'i', \"'\", 'm', 'about', 'to', 'sleep', '\"', 'phase', '.', 'does', 'anyone', 'have', 'any', 'tips', '?', 'what', 'do', 'you', 'do', 'when', 'you', 'can', \"'\", 't', 'sleep', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'just', 'took', 'my', 'first', 'pill', 'of', 'v', '##y', '##van', '##se', 'today', ',', 'what', 'can', 'i', 'expect', 'in', 'the', 'coming', 'days', '?', 'i', 'took', 'my', 'first', 'dose', 'of', 'v', '##yan', '##se', '70', '##mg', 'today', '.', 'i', 'was', 'diagnosed', 'last', 'week', '.', 'today', ',', 'energetic', 'yet', 'relaxed', ',', 'i', 'feel', 'like', 'i', 'want', 'to', 'do', 'things', 'but', 'i', 'am', 'not', 'anxious', 'if', 'i', 'don', \"'\", 't', '.', 'i', 'have', 'dry', 'mouth', 'and', 'i', 'still', 'am', 'getting', 'distracted', 'but', 'i', 'don', \"'\", 't', 'have', 'the', 'resistance', 'to', 'the', 'tasks', 'that', 'used', 'to', 'take', 'so', 'much', 'wasted', 'energy', '.', 'i', 'feel', 'like', 'this', ':', 'http', ':', '/', '/', 'www', '.', 'ro', '##swell', '##pa', '##int', '##ing', '##con', '##tra', '##ctor', '.', 'com', '/', 'w', '##p', '-', 'content', '/', 'up', '##load', '##s', '/', '2010', '/', '05', '/', 'man', '_', 'relieved', '##1', '.', 'jp', '##g', '_', 'opt', '##1', '.', 'jp', '##g', 'i', 'have', 'another', 'appointment', 'with', 'my', 'doctor', 'in', 'a', 'week', ',', 'but', 'ut', '##il', 'then', ':', 'so', ',', 'in', 'the', 'next', 'few', 'days', 'what', 'should', 'i', 'look', 'for', 'to', 'make', 'sure', 'i', \"'\", 'm', 'taking', 'the', 'right', 'dose', '?', 'i', 'still', 'am', 'getting', 'distracted', 'a', 'lot', ',', 'does', 'that', 'improve', 'over', 'the', 'long', 'term', '?', 'what', 'happens', 'if', 'i', 'decide', 'to', 'not', 'take', 'them', 'this', 'weekend', '?', 'any', 'warning', 'signs', 'i', 'should', 'look', 'for', '?', 'also', ',', 'how', 'long', 'does', 'the', 'effects', 'last', 'in', 'the', 'day', '?', 'thank', 'you', 'for', 'any', 'advice', 'or', 'experiences', 'you', 'share', '.']\n",
      "INFO:__main__:Number of tokens: 241\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'just', 'took', 'my', 'first', 'pill', 'of', 'v', '##y', '##van', '##se', 'today', ',', 'what', 'can', 'i', 'expect', 'in', 'the', 'coming', 'days', '?', 'i', 'took', 'my', 'first', 'dose', 'of', 'v', '##yan', '##se', '70', '##mg', 'today', '.', 'i', 'was', 'diagnosed', 'last', 'week', '.', 'today', ',', 'energetic', 'yet', 'relaxed', ',', 'i', 'feel', 'like', 'i', 'want', 'to', 'do', 'things', 'but', 'i', 'am', 'not', 'anxious', 'if', 'i', 'don', \"'\", 't', '.', 'i', 'have', 'dry', 'mouth', 'and', 'i', 'still', 'am', 'getting', 'distracted', 'but', 'i', 'don', \"'\", 't', 'have', 'the', 'resistance', 'to', 'the', 'tasks', 'that', 'used', 'to', 'take', 'so', 'much', 'wasted', 'energy', '.', 'i', 'feel', 'like', 'this', ':', 'http', ':', '/', '/', 'www', '.', 'ro', '##swell', '##pa', '##int', '##ing', '##con', '##tra', '##ctor', '.', 'com', '/', 'w', '##p', '-', 'content', '/', 'up', '##load', '##s', '/', '2010', '/', '05', '/', 'man', '_', 'relieved', '##1', '.', 'jp', '##g', '_', 'opt', '##1', '.', 'jp', '##g', 'i', 'have', 'another', 'appointment', 'with', 'my', 'doctor', 'in', 'a', 'week', ',', 'but', 'ut', '##il', 'then', ':', 'so', ',', 'in', 'the', 'next', 'few', 'days', 'what', 'should', 'i', 'look', 'for', 'to', 'make', 'sure', 'i', \"'\", 'm', 'taking', 'the', 'right', 'dose', '?', 'i', 'still', 'am', 'getting', 'distracted', 'a', 'lot', ',', 'does', 'that', 'improve', 'over', 'the', 'long', 'term', '?', 'what', 'happens', 'if', 'i', 'decide', 'to', 'not', 'take', 'them', 'this', 'weekend', '?', 'any', 'warning', 'signs', 'i', 'should', 'look', 'for', '?', 'also', ',', 'how', 'long', 'does', 'the', 'effects', 'last', 'in', 'the', 'day', '?', 'thank', 'you', 'for', 'any', 'advice', 'or', 'experiences', 'you', 'share', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['should', 'i', 'get', 'tested', '?', 'focus', 'issues', ',', 'fi', '##dget', '##y', 'most', 'of', 'the', 'time', ',', 'tin', '##ni', '##tus', '(', 'probably', 'unrelated', ',', 'but', 'who', 'the', 'hell', 'knows', '.', '.', '.', ')', 'also', ',', 'i', 'occasionally', 'feel', 'a', 'buzz', 'on', 'my', 'leg', 'as', 'if', 'my', 'phone', 'just', 'vibrated', ',', 'but', 'i', 'will', 'sometimes', 'get', 'that', 'feeling', 'even', 'when', 'my', 'phone', 'isn', \"'\", 't', 'in', 'my', 'pocket', '.', 'i', \"'\", 've', 'suspected', 'for', 'a', 'while', 'that', 'i', 'am', 'probably', 'add', '/', 'ad', '##hd', ',', 'but', 'have', 'not', 'yet', 'been', 'tested', '.', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 92\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['should', 'i', 'get', 'tested', '?', 'focus', 'issues', ',', 'fi', '##dget', '##y', 'most', 'of', 'the', 'time', ',', 'tin', '##ni', '##tus', '(', 'probably', 'unrelated', ',', 'but', 'who', 'the', 'hell', 'knows', '.', '.', '.', ')', 'also', ',', 'i', 'occasionally', 'feel', 'a', 'buzz', 'on', 'my', 'leg', 'as', 'if', 'my', 'phone', 'just', 'vibrated', ',', 'but', 'i', 'will', 'sometimes', 'get', 'that', 'feeling', 'even', 'when', 'my', 'phone', 'isn', \"'\", 't', 'in', 'my', 'pocket', '.', 'i', \"'\", 've', 'suspected', 'for', 'a', 'while', 'that', 'i', 'am', 'probably', 'add', '/', 'ad', '##hd', ',', 'but', 'have', 'not', 'yet', 'been', 'tested', '.', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['raised', 'on', 'rita', '##lin', 'chapter', '2', '(', 'feedback', 'and', 'comments', 'welcome', '!', ')']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['raised', 'on', 'rita', '##lin', 'chapter', '2', '(', 'feedback', 'and', 'comments', 'welcome', '!', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'got', 'asked', 'this', 'question', ',', 'i', 'was', 'wondering', 'if', 'you', 'guys', 'had', 'stumbled', 'into', 'the', 'same', 'problem', '?', 'background', '(', 'short', ')', ':', '-', 'being', 'a', 'slack', '##er', ',', 'depression', ',', 'bad', 'breakup', 'with', 'g', '##f', 'i', 'obsessed', 'over', 'lead', 'to', 'me', 'getting', 'kicked', 'out', '.', '-', 'i', 'get', 'better', ',', 'find', 'out', 'i', 'have', 'had', 'ad', '##hd', 'since', 'grade', '8', 'most', 'likely', ',', 'start', 'treatment', '(', 'meditation', ',', 'books', ',', 'and', 'methyl', '##ph', '##eni', '##date', 'derivative', ')', ',', 'lose', 'weight', ',', 'become', 'very', 'fit', ',', 'back', 'in', 'un', '##i', 'now', '.', '-', 'i', 'also', 'have', 'developed', 'better', 'habits', 'and', 'social', 'skills', ',', 'lots', 'of', 'girls', 'approach', 'me', 'now', 'and', 'frankly', 'i', 'feel', 'like', 'many', 'are', \"'\", 'after', 'me', \"'\", 'lo', '##lo', '##lo', '##l', '.', 'im', 'a', 'part', 'of', 'a', 'club', ',', 'and', 'in', 'a', 'meeting', 'one', 'of', 'the', 'guys', '(', 'and', 'then', 'one', 'of', 'the', 'ladies', ')', 'asked', 'me', 'why', 'i', 'don', '##t', 'have', 'a', 'girlfriend', '.', 'i', 'had', 'no', 'answer', '.', '.', 'i', 'guess', 'i', 'don', \"'\", 't', 'feel', 'like', 'i', 'could', 'really', 'share', 'my', 'life', 'with', 'someone', 'without', 'scar', '##ing', 'them', 'off', '?', 'i', 'can', \"'\", 't', 'even', 'come', 'up', 'with', 'a', 'reason', '!', 'i', 'would', 'like', 'to', 'find', 'a', 'partner', 'to', 'share', 'my', 'life', 'with', ',', 'but', 'i', 'somehow', 'feel', 'unable', '.', 'anyone', 'out', 'there', 'experienced', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 223\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'got', 'asked', 'this', 'question', ',', 'i', 'was', 'wondering', 'if', 'you', 'guys', 'had', 'stumbled', 'into', 'the', 'same', 'problem', '?', 'background', '(', 'short', ')', ':', '-', 'being', 'a', 'slack', '##er', ',', 'depression', ',', 'bad', 'breakup', 'with', 'g', '##f', 'i', 'obsessed', 'over', 'lead', 'to', 'me', 'getting', 'kicked', 'out', '.', '-', 'i', 'get', 'better', ',', 'find', 'out', 'i', 'have', 'had', 'ad', '##hd', 'since', 'grade', '8', 'most', 'likely', ',', 'start', 'treatment', '(', 'meditation', ',', 'books', ',', 'and', 'methyl', '##ph', '##eni', '##date', 'derivative', ')', ',', 'lose', 'weight', ',', 'become', 'very', 'fit', ',', 'back', 'in', 'un', '##i', 'now', '.', '-', 'i', 'also', 'have', 'developed', 'better', 'habits', 'and', 'social', 'skills', ',', 'lots', 'of', 'girls', 'approach', 'me', 'now', 'and', 'frankly', 'i', 'feel', 'like', 'many', 'are', \"'\", 'after', 'me', \"'\", 'lo', '##lo', '##lo', '##l', '.', 'im', 'a', 'part', 'of', 'a', 'club', ',', 'and', 'in', 'a', 'meeting', 'one', 'of', 'the', 'guys', '(', 'and', 'then', 'one', 'of', 'the', 'ladies', ')', 'asked', 'me', 'why', 'i', 'don', '##t', 'have', 'a', 'girlfriend', '.', 'i', 'had', 'no', 'answer', '.', '.', 'i', 'guess', 'i', 'don', \"'\", 't', 'feel', 'like', 'i', 'could', 'really', 'share', 'my', 'life', 'with', 'someone', 'without', 'scar', '##ing', 'them', 'off', '?', 'i', 'can', \"'\", 't', 'even', 'come', 'up', 'with', 'a', 'reason', '!', 'i', 'would', 'like', 'to', 'find', 'a', 'partner', 'to', 'share', 'my', 'life', 'with', ',', 'but', 'i', 'somehow', 'feel', 'unable', '.', 'anyone', 'out', 'there', 'experienced', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['r', '/', 'ad', '##hd', ',', 'da', '##e', 'suffer', 'from', 'a', 'highly', 'addict', '##ive', 'personality', '?', 'i', \"'\", 'm', 'convinced', 'i', 'can', 'get', 'addicted', 'to', 'anything', 'and', 'everything', 'even', 'remotely', 'addict', '##ive', '.', 'this', 'is', 'something', 'i', 'have', 'come', 'to', 'attribute', 'to', 'my', 'ad', '##hd', 'and', 'the', 'imp', '##ulsive', '##ness', 'that', 'comes', 'with', 'it', '.', 'some', 'of', 'my', 'addiction', '##s', '(', 'past', 'and', 'present', ')', ':', '-', 'caf', '##fe', '##ine', '(', 'present', ')', ':', 'i', 'have', 'since', 'greatly', 'reduced', 'my', 'intake', '.', '-', 'running', '(', 'present', ')', ':', 'i', 'can', \"'\", 't', 'get', 'through', 'the', 'day', 'without', 'going', 'for', 'at', 'least', '10', 'miles', '.', '-', 'v', '##y', '##van', '##se', '(', 'past', ')', ':', 'i', 'wasn', \"'\", 't', 'so', 'much', 'addicted', 'to', 'a', 'buzz', 'or', 'high', 'from', 'it', ',', 'rather', 'what', 'i', 'found', 'addict', '##ive', 'was', 'the', 'focus', 'which', 'i', 'had', 'never', 'had', 'before', '.', 'eventually', ',', 'i', 'reformed', 'my', 'relationship', 'with', 'this', 'medicine', 'by', 'gaining', 'respect', 'and', 'enjoyment', 'from', 'my', 'ad', '##hd', 'self', '.', '-', 'heroin', '/', 'op', '##iate', '##s', '(', 'past', ')', ':', 'yes', ',', 'i', 'did', 'indeed', 'da', '##ble', 'in', 'this', 'terribly', 'addict', '##ive', 'and', 'destructive', 'drug', '.', 'it', 'was', 'short', 'lived', '(', 'about', '2', 'months', ')', 'but', '*', 'very', '*', 'intense', '.', 'luckily', 'i', 'had', 'enough', 'sense', 'to', 'stop', 'when', 'i', 'saw', 'myself', 'at', 'the', 'pre', '##ci', '##pic', '##e', 'of', 'what', 'could', 'have', 'very', 'well', 'been', 'a', 'very', 'serious', 'habit', '.', '-', 'cannabis', '(', 'past', ')', 'does', 'anyone', 'share', 'this', 'trait', '?']\n",
      "INFO:__main__:Number of tokens: 244\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['r', '/', 'ad', '##hd', ',', 'da', '##e', 'suffer', 'from', 'a', 'highly', 'addict', '##ive', 'personality', '?', 'i', \"'\", 'm', 'convinced', 'i', 'can', 'get', 'addicted', 'to', 'anything', 'and', 'everything', 'even', 'remotely', 'addict', '##ive', '.', 'this', 'is', 'something', 'i', 'have', 'come', 'to', 'attribute', 'to', 'my', 'ad', '##hd', 'and', 'the', 'imp', '##ulsive', '##ness', 'that', 'comes', 'with', 'it', '.', 'some', 'of', 'my', 'addiction', '##s', '(', 'past', 'and', 'present', ')', ':', '-', 'caf', '##fe', '##ine', '(', 'present', ')', ':', 'i', 'have', 'since', 'greatly', 'reduced', 'my', 'intake', '.', '-', 'running', '(', 'present', ')', ':', 'i', 'can', \"'\", 't', 'get', 'through', 'the', 'day', 'without', 'going', 'for', 'at', 'least', '10', 'miles', '.', '-', 'v', '##y', '##van', '##se', '(', 'past', ')', ':', 'i', 'wasn', \"'\", 't', 'so', 'much', 'addicted', 'to', 'a', 'buzz', 'or', 'high', 'from', 'it', ',', 'rather', 'what', 'i', 'found', 'addict', '##ive', 'was', 'the', 'focus', 'which', 'i', 'had', 'never', 'had', 'before', '.', 'eventually', ',', 'i', 'reformed', 'my', 'relationship', 'with', 'this', 'medicine', 'by', 'gaining', 'respect', 'and', 'enjoyment', 'from', 'my', 'ad', '##hd', 'self', '.', '-', 'heroin', '/', 'op', '##iate', '##s', '(', 'past', ')', ':', 'yes', ',', 'i', 'did', 'indeed', 'da', '##ble', 'in', 'this', 'terribly', 'addict', '##ive', 'and', 'destructive', 'drug', '.', 'it', 'was', 'short', 'lived', '(', 'about', '2', 'months', ')', 'but', '*', 'very', '*', 'intense', '.', 'luckily', 'i', 'had', 'enough', 'sense', 'to', 'stop', 'when', 'i', 'saw', 'myself', 'at', 'the', 'pre', '##ci', '##pic', '##e', 'of', 'what', 'could', 'have', 'very', 'well', 'been', 'a', 'very', 'serious', 'habit', '.', '-', 'cannabis', '(', 'past', ')', 'does', 'anyone', 'share', 'this', 'trait', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'get', 'headache', '##s', 'almost', 'everyday', 'from', 'st', '##im', '##ula', '##nts', '?', 'if', 'so', ',', 'how', 'do', 'you', 'manage', 'them', '?']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'get', 'headache', '##s', 'almost', 'everyday', 'from', 'st', '##im', '##ula', '##nts', '?', 'if', 'so', ',', 'how', 'do', 'you', 'manage', 'them', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'me', '-', 'adult', 'experiences', 'of', 'ad', '##hd', '[', 'uk', ']']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'me', '-', 'adult', 'experiences', 'of', 'ad', '##hd', '[', 'uk', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'i', 'feel', 'going', 'to', 'work', 'after', 'getting', 'back', 'on', 'st', '##rat', '##tera']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'i', 'feel', 'going', 'to', 'work', 'after', 'getting', 'back', 'on', 'st', '##rat', '##tera']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '/', 'ad', '##hd', 'coaching', ',', 'worth', 'the', 'money', '?', 'has', 'anyone', 'here', 'had', 'any', 'experience', 'with', 'coaching', 'as', 'a', 'means', 'of', 'managing', 'their', 'add', 'or', 'ad', '##hd', '?', 'i', 'was', 'referred', 'to', 'the', 'hall', '##owe', '##ll', 'center', 'in', 'nyc', 'by', 'my', 'school', ',', 'but', 'they', 'have', 'referred', 'me', 'to', 'an', 'outside', 'counsel', '##lor', 'who', 'charges', '850', '$', 'for', 'the', 'first', 'month', '(', 'initial', '\"', 'discovery', 'interview', '\"', 'and', '3', 'weekly', 'follow', 'up', 'calls', ')', '.', 'seems', 'very', 'expensive', 'and', 'would', 'love', 'to', 'know', 'if', 'anyone', 'has', 'had', 'results', 'from', 'coaching', '.', 't', '##l', ';', 'dr', ':', 'coaching', 'seems', 'really', '$', '$', '$', '!', 'is', 'it', 'worth', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 109\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '/', 'ad', '##hd', 'coaching', ',', 'worth', 'the', 'money', '?', 'has', 'anyone', 'here', 'had', 'any', 'experience', 'with', 'coaching', 'as', 'a', 'means', 'of', 'managing', 'their', 'add', 'or', 'ad', '##hd', '?', 'i', 'was', 'referred', 'to', 'the', 'hall', '##owe', '##ll', 'center', 'in', 'nyc', 'by', 'my', 'school', ',', 'but', 'they', 'have', 'referred', 'me', 'to', 'an', 'outside', 'counsel', '##lor', 'who', 'charges', '850', '$', 'for', 'the', 'first', 'month', '(', 'initial', '\"', 'discovery', 'interview', '\"', 'and', '3', 'weekly', 'follow', 'up', 'calls', ')', '.', 'seems', 'very', 'expensive', 'and', 'would', 'love', 'to', 'know', 'if', 'anyone', 'has', 'had', 'results', 'from', 'coaching', '.', 't', '##l', ';', 'dr', ':', 'coaching', 'seems', 'really', '$', '$', '$', '!', 'is', 'it', 'worth', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['government', 'restrict', '##s', 'medication', '.', 'legal', '?', 'humane', '?', '[', 'norway', ']']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['government', 'restrict', '##s', 'medication', '.', 'legal', '?', 'humane', '?', '[', 'norway', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['went', 'off', 'st', '##rat', '##tera', 'because', 'of', 'price', ',', 'just', 'taking', 'add', '##eral', '##l', '(', 'generic', ')', '.', 'not', 'sure', 'its', 'working', 'alone', '.', '.', '.', 'advice', '?', 'i', 'was', 'on', '60', '##mg', 'of', 'st', '##rat', '##tera', 'and', '20', '##mg', 'of', 'add', '##eral', '##l', 'daily', ',', 'for', 'the', 'past', '9', 'years', 'with', 'a', 'little', 'up', '-', 'dos', '##ing', 'or', 'down', 'dos', '##ing', 'over', 'that', 'period', 'of', 'time', 'but', 'pretty', 'regular', '.', 'i', 'have', 'weird', 'insurance', '.', 'i', 'pay', 'full', 'price', 'for', 'prescription', '##s', 'but', 'then', 'my', 'work', 'will', 'rei', '##mb', '##urse', 'me', 'for', '50', '%', '.', 'the', 'st', '##rat', '##tera', '(', 'no', 'generic', 'available', ')', 'was', 'close', 'to', '$', '200', '/', 'mo', '.', 'the', 'add', '##eral', '##l', '(', 'generic', ')', '$', '35', '.', 'i', 'am', 'at', 'a', 'particularly', 'tight', 'point', 'financially', '(', 'story', 'of', 'my', 'life', ')', 'and', 'decided', 'to', 'just', 'not', 'get', 'st', '##rat', '##tera', 'for', 'a', 'couple', 'months', '.', 'the', 'first', 'couple', 'of', 'weeks', 'off', 'it', 'was', 'ha', '##aar', '##rd', '##dd', '.', '.', '.', 'i', 'wanted', 'to', 'just', 'die', '.', 'but', 'i', 'knew', 'that', 'was', 'going', 'to', 'happen', 'and', 'i', 'fought', 'very', 'hard', 'through', 'the', 'intense', 'feelings', 'of', 'hopeless', '##ness', '.', 'i', 'still', 'struggle', 'but', 'not', 'as', 'bad', ',', 'normal', ',', 'i', 'have', 'always', 'had', 'depression', 'but', 'treating', 'the', 'add', 'has', 'treated', 'the', 'depression', '(', 'for', 'the', 'most', 'part', ')', '.', 'it', 'has', 'been', 'almost', '2', 'months', 'now', 'taking', 'only', 'add', '##eral', '##l', '.', 'i', 'am', 'struggling', 'at', 'work', '.', 'it', 'is', 'really', 'hard', 'to', 'get', 'through', 'the', 'day', '.', 'in', 'the', 'morning', 'i', 'do', 'my', 'usual', 'sprint', 'through', 'tasks', '.', 'i', 'have', 'a', 'very', 'good', 'job', ',', 'very', 'creative', 'type', 'thing', ',', 'a', 'lot', 'of', 'responsibility', 'and', 'flexibility', '.', 'i', 'feel', 'lately', 'like', 'i', 'am', 'slowing', 'down', 'and', 'i', 'am', 'definitely', 'having', 'a', 'hard', 'time', 'focusing', '.', 'my', 'house', 'is', 'a', 'mess', '.', 'it', 'is', 'the', 'mess', '##iest', 'its', 'been', 'in', 'a', 'while', '.', 'i', 'feel', 'like', 'i', 'am', 'letting', 'everyone', 'down', 'around', 'me', '.', 'do', 'i', 'get', 'back', 'on', 'st', '##rat', '##tera', 'and', 'just', 'deal', 'with', 'the', 'cost', '?', 'do', 'i', 'up', 'my', 'add', '##eral', '##l', 'dos', '##age', '?', 'is', 'there', 'other', ',', 'cheaper', 'drugs', 'to', 'try', '?', 'any', 'advice', 'is', 'appreciated', '.', 'i', 'feel', 'like', 'i', 'am', 'pretty', 'much', 'on', 'my', 'own', 'when', 'it', 'comes', 'to', 'bringing', 'this', 'info', 'to', 'my', 'doctor', '.', 'since', 'being', 'diagnosed', '9', 'years', 'ago', ',', 'she', 'pretty', 'much', 'just', 'stuck', 'with', 'my', 'original', 'doctors', 'recommendations', 'and', 'lets', 'me', 'decide', 'if', 'it', 'needs', 'adjustment', 'based', 'on', 'how', 'i', 'feel', '.', 'she', 'is', 'a', 'family', 'doctor', '&', 'g', '##yne', '##col', '##ogist', ',', 'and', 'i', 'do', 'not', 'get', 'the', 'impression', 'she', 'is', 'entirely', 'up', 'on', 'psychiatric', 'drugs', '(', 'don', \"'\", 't', 'get', 'me', 'wrong', ',', 'she', 'is', 'otherwise', 'a', 'really', 'good', 'doctor', ')', '.', 'i', 'just', 'can', \"'\", 't', 'go', 'to', 'her', 'with', '\"', 'i', 'don', '##t', 'know', 'what', 'to', 'do', 'but', 'i', 'need', 'to', 'do', 'something', '\"', 'i', 'need', 'to', 'saying', 'something', 'more', 'pro', '##active', 'like', '\"', 'i', 'think', 'it', 'would', 'be', 'good', 'if', 'i', 'take', 'xx', 'dose', 'and', 'possibly', 'switch', 'to', 'b', '##lab', '##la', 'drug', ',', 'what', 'do', 'you', 'think', '?', '\"', 'but', 'i', 'am', 'at', 'a', 'loss', ',', 'since', 'i', 'have', 'been', 'doing', 'the', 'same', 'thing', 'for', '9', 'years', 'i', 'do', 'not', 'even', 'know', 'if', 'there', 'might', 'be', 'something', 'better', 'for', 'me', 'i', 'am', 'missing', 'out', 'on', '.']\n",
      "INFO:__main__:Number of tokens: 558\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['went', 'off', 'st', '##rat', '##tera', 'because', 'of', 'price', ',', 'just', 'taking', 'add', '##eral', '##l', '(', 'generic', ')', '.', 'not', 'sure', 'its', 'working', 'alone', '.', '.', '.', 'advice', '?', 'i', 'was', 'on', '60', '##mg', 'of', 'st', '##rat', '##tera', 'and', '20', '##mg', 'of', 'add', '##eral', '##l', 'daily', ',', 'for', 'the', 'past', '9', 'years', 'with', 'a', 'little', 'up', '-', 'dos', '##ing', 'or', 'down', 'dos', '##ing', 'over', 'that', 'period', 'of', 'time', 'but', 'pretty', 'regular', '.', 'i', 'have', 'weird', 'insurance', '.', 'i', 'pay', 'full', 'price', 'for', 'prescription', '##s', 'but', 'then', 'my', 'work', 'will', 'rei', '##mb', '##urse', 'me', 'for', '50', '%', '.', 'the', 'st', '##rat', '##tera', '(', 'no', 'generic', 'available', ')', 'was', 'close', 'to', '$', '200', '/', 'mo', '.', 'the', 'add', '##eral', '##l', '(', 'generic', ')', '$', '35', '.', 'i', 'am', 'at', 'a', 'particularly', 'tight', 'point', 'financially', '(', 'story', 'of', 'my', 'life', ')', 'and', 'decided', 'to', 'just', 'not', 'get', 'st', '##rat', '##tera', 'for', 'a', 'couple', 'months', '.', 'the', 'first', 'couple', 'of', 'weeks', 'off', 'it', 'was', 'ha', '##aar', '##rd', '##dd', '.', '.', '.', 'i', 'wanted', 'to', 'just', 'die', '.', 'but', 'i', 'knew', 'that', 'was', 'going', 'to', 'happen', 'and', 'i', 'fought', 'very', 'hard', 'through', 'the', 'intense', 'feelings', 'of', 'hopeless', '##ness', '.', 'i', 'still', 'struggle', 'but', 'not', 'as', 'bad', ',', 'normal', ',', 'i', 'have', 'always', 'had', 'depression', 'but', 'treating', 'the', 'add', 'has', 'treated', 'the', 'depression', '(', 'for', 'the', 'most', 'part', ')', '.', 'it', 'has', 'been', 'almost', '2', 'months', 'now', 'taking', 'only', 'add', '##eral', '##l', '.', 'i', 'am', 'struggling', 'at', 'work', '.', 'it', 'is', 'really', 'hard', 'to', 'get', 'through', 'the', 'day', '.', 'in', 'the', 'morning', 'i', 'do', 'my', 'usual', 'sprint', 'through', 'tasks', '.', 'i', 'have', 'a', 'very', 'good', 'job', ',', 'very', 'creative', 'type', 'thing', ',', 'a', 'lot', 'of', 'responsibility', 'and', 'flexibility', '.', 'i', 'feel', 'lately', 'like', 'i', 'am', 'slowing', 'down', 'and', 'i', 'am', 'definitely', 'having', 'a', 'hard', 'time', 'focusing', '.', 'my', 'house', 'is', 'a', 'mess', '.', 'it', 'is', 'the', 'mess', '##iest', 'its', 'been', 'in', 'a', 'while', '.', 'i', 'feel', 'like', 'i', 'am', 'letting', 'everyone', 'down', 'around', 'me', '.', 'do', 'i', 'get', 'back', 'on', 'st', '##rat', '##tera', 'and', 'just', 'deal', 'with', 'the', 'cost', '?', 'do', 'i', 'up', 'my', 'add', '##eral', '##l', 'dos', '##age', '?', 'is', 'there', 'other', ',', 'cheaper', 'drugs', 'to', 'try', '?', 'any', 'advice', 'is', 'appreciated', '.', 'i', 'feel', 'like', 'i', 'am', 'pretty', 'much', 'on', 'my', 'own', 'when', 'it', 'comes', 'to', 'bringing', 'this', 'info', 'to', 'my', 'doctor', '.', 'since', 'being', 'diagnosed', '9', 'years', 'ago', ',', 'she', 'pretty', 'much', 'just', 'stuck', 'with', 'my', 'original', 'doctors', 'recommendations', 'and', 'lets', 'me', 'decide', 'if', 'it', 'needs', 'adjustment', 'based', 'on', 'how', 'i', 'feel', '.', 'she', 'is', 'a', 'family', 'doctor', '&', 'g', '##yne', '##col', '##ogist', ',', 'and', 'i', 'do', 'not', 'get', 'the', 'impression', 'she', 'is', 'entirely', 'up', 'on', 'psychiatric', 'drugs', '(', 'don', \"'\", 't', 'get', 'me', 'wrong', ',', 'she', 'is', 'otherwise', 'a', 'really', 'good', 'doctor', ')', '.', 'i', 'just', 'can', \"'\", 't', 'go', 'to', 'her', 'with', '\"', 'i', 'don', '##t', 'know', 'what', 'to', 'do', 'but', 'i', 'need', 'to', 'do', 'something', '\"', 'i', 'need', 'to', 'saying', 'something', 'more', 'pro', '##active', 'like', '\"', 'i', 'think', 'it', 'would', 'be', 'good', 'if', 'i', 'take', 'xx', 'dose', 'and', 'possibly', 'switch', 'to', 'b', '##lab'], ['##la', 'drug', ',', 'what', 'do', 'you', 'think', '?', '\"', 'but', 'i', 'am', 'at', 'a', 'loss', ',', 'since', 'i', 'have', 'been', 'doing', 'the', 'same', 'thing', 'for', '9', 'years', 'i', 'do', 'not', 'even', 'know', 'if', 'there', 'might', 'be', 'something', 'better', 'for', 'me', 'i', 'am', 'missing', 'out', 'on', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['apologies', 'if', 'this', 'is', 'not', 'the', 'right', 'place', 'for', 'this', 'post', ',', 'but', 'i', 'could', 'use', 'a', 'bit', 'of', 'advice', '.', 'hi', 'red', '##dit', ',', 'i', 'have', 'been', 'diagnosed', 'with', 'add', '(', 'imp', '##ulsive', ',', 'not', 'hyper', '##active', ')', 'since', '7th', 'grade', '.', 'i', 'have', 'been', 'struggling', ',', 'but', 'have', 'made', 'it', 'through', 'college', 'and', '~', '5', 'years', 'of', 'work', 'without', 'getting', 'fired', '.', 'until', 'today', '.', 'i', 'had', 'a', 'new', 'job', 'in', 'finance', 'for', 'about', 'a', 'month', '(', 'i', 'narrowly', 'escaped', 'getting', 'fired', 'from', 'the', 'last', 'job', ')', '.', 'i', 'know', ',', 'deep', 'down', ',', 'it', \"'\", 's', 'probably', 'for', 'the', 'best', 'because', 'the', '#', '1', 'requirement', 'for', 'accountants', '/', 'financial', 'analysts', 'is', 'attention', 'to', 'detail', '.', 'i', 'will', 'try', 'not', 'to', 'bore', 'you', 'with', 'too', 'many', 'details', 'about', 'why', 'this', 'makes', 'me', 'feel', 'like', 'a', 'terrible', 'failure', ',', 'but', 'the', 'bottom', 'line', 'is', 'that', 'i', 'no', 'longer', 'have', 'an', 'income', 'and', 'just', 'signed', 'a', '12', 'month', 'lease', 'in', 'a', 'new', 'city', '(', 'where', 'i', 'relocated', 'for', 'work', ')', '.', 'i', 'am', 'not', 'sure', 'what', 'type', 'of', 'advice', 'it', 'is', 'that', 'i', 'am', 'looking', 'for', ',', 'but', 'i', \"'\", 'm', 'just', 'trying', 'to', 'figure', 'out', 'what', 'to', 'do', 'next', '.', 'it', 'seems', 'clear', 'that', 'corporate', 'finance', 'is', 'not', 'the', 'right', 'field', 'for', 'me', '(', 'or', 'anyone', 'with', 'add', ')', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'how', 'i', \"'\", 'm', 'supposed', 'to', 'survive', 'out', 'here', '.', 'i', \"'\", 'm', '29', 'years', 'old', 'and', 'sometimes', 'i', 'just', 'feel', 'like', 'a', 'child', 'that', 'can', \"'\", 't', 'get', 'himself', 'together', '.', 'i', 'genuinely', 'tried', 'checking', 'my', 'work', 'to', 'the', 'best', 'of', 'my', 'ability', 'every', 'time', ',', 'but', 'i', 'still', 'made', 'mistakes', ',', 'which', 'led', 'to', 'me', 'being', 'let', 'go', 'today', '.', 'can', 'anyone', 'give', 'some', 'helpful', 'advice', '?', 'i', 'want', 'to', 'be', 'successful', 'in', 'life', 'and', 'i', \"'\", 'm', 'no', 'longer', 'sure', 'how', 'to', 'do', 'it', '.', 'if', 'you', \"'\", 've', 'read', 'this', 'far', ',', 'i', 'would', 'just', 'like', 'to', 'genuinely', 'say', '\"', 'thank', 'you', '\"', 'from', 'one', 'internet', 'stranger', 'to', 'another', '.', 'also', ',', 'if', 'there', 'is', 'a', 'better', 'place', 'for', 'this', 'post', 'just', 'let', 'me', 'know', 'and', 'i', \"'\", 'll', 'x', '-', 'post', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 365\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['apologies', 'if', 'this', 'is', 'not', 'the', 'right', 'place', 'for', 'this', 'post', ',', 'but', 'i', 'could', 'use', 'a', 'bit', 'of', 'advice', '.', 'hi', 'red', '##dit', ',', 'i', 'have', 'been', 'diagnosed', 'with', 'add', '(', 'imp', '##ulsive', ',', 'not', 'hyper', '##active', ')', 'since', '7th', 'grade', '.', 'i', 'have', 'been', 'struggling', ',', 'but', 'have', 'made', 'it', 'through', 'college', 'and', '~', '5', 'years', 'of', 'work', 'without', 'getting', 'fired', '.', 'until', 'today', '.', 'i', 'had', 'a', 'new', 'job', 'in', 'finance', 'for', 'about', 'a', 'month', '(', 'i', 'narrowly', 'escaped', 'getting', 'fired', 'from', 'the', 'last', 'job', ')', '.', 'i', 'know', ',', 'deep', 'down', ',', 'it', \"'\", 's', 'probably', 'for', 'the', 'best', 'because', 'the', '#', '1', 'requirement', 'for', 'accountants', '/', 'financial', 'analysts', 'is', 'attention', 'to', 'detail', '.', 'i', 'will', 'try', 'not', 'to', 'bore', 'you', 'with', 'too', 'many', 'details', 'about', 'why', 'this', 'makes', 'me', 'feel', 'like', 'a', 'terrible', 'failure', ',', 'but', 'the', 'bottom', 'line', 'is', 'that', 'i', 'no', 'longer', 'have', 'an', 'income', 'and', 'just', 'signed', 'a', '12', 'month', 'lease', 'in', 'a', 'new', 'city', '(', 'where', 'i', 'relocated', 'for', 'work', ')', '.', 'i', 'am', 'not', 'sure', 'what', 'type', 'of', 'advice', 'it', 'is', 'that', 'i', 'am', 'looking', 'for', ',', 'but', 'i', \"'\", 'm', 'just', 'trying', 'to', 'figure', 'out', 'what', 'to', 'do', 'next', '.', 'it', 'seems', 'clear', 'that', 'corporate', 'finance', 'is', 'not', 'the', 'right', 'field', 'for', 'me', '(', 'or', 'anyone', 'with', 'add', ')', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'how', 'i', \"'\", 'm', 'supposed', 'to', 'survive', 'out', 'here', '.', 'i', \"'\", 'm', '29', 'years', 'old', 'and', 'sometimes', 'i', 'just', 'feel', 'like', 'a', 'child', 'that', 'can', \"'\", 't', 'get', 'himself', 'together', '.', 'i', 'genuinely', 'tried', 'checking', 'my', 'work', 'to', 'the', 'best', 'of', 'my', 'ability', 'every', 'time', ',', 'but', 'i', 'still', 'made', 'mistakes', ',', 'which', 'led', 'to', 'me', 'being', 'let', 'go', 'today', '.', 'can', 'anyone', 'give', 'some', 'helpful', 'advice', '?', 'i', 'want', 'to', 'be', 'successful', 'in', 'life', 'and', 'i', \"'\", 'm', 'no', 'longer', 'sure', 'how', 'to', 'do', 'it', '.', 'if', 'you', \"'\", 've', 'read', 'this', 'far', ',', 'i', 'would', 'just', 'like', 'to', 'genuinely', 'say', '\"', 'thank', 'you', '\"', 'from', 'one', 'internet', 'stranger', 'to', 'another', '.', 'also', ',', 'if', 'there', 'is', 'a', 'better', 'place', 'for', 'this', 'post', 'just', 'let', 'me', 'know', 'and', 'i', \"'\", 'll', 'x', '-', 'post', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'get', 'anxiety', 'from', 'concert', '##a', '?', 'i', 'have', 'never', 'had', 'anxiety', 'in', 'my', 'entire', 'life', '.', 'but', 'lately', ',', 'i', \"'\", 've', 'been', 'getting', 'this', 'feeling', 'of', 'nervous', '##ness', 'and', 'my', 'body', 'tense', '##s', 'up', 'and', 'it', \"'\", 's', 'de', '##bil', '##itating', '.', 'i', 'can', \"'\", 't', 'even', 'write', 'an', '800', 'word', 'article', 'in', '3', 'hours', 'because', 'of', 'it', '.', 'i', 'have', 'a', 'doctors', 'appointment', 'tomorrow', '.', 'wondering', 'if', 'i', 'should', 'bring', 'it', 'up', 'to', 'my', 'doctor', '.']\n",
      "INFO:__main__:Number of tokens: 79\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'get', 'anxiety', 'from', 'concert', '##a', '?', 'i', 'have', 'never', 'had', 'anxiety', 'in', 'my', 'entire', 'life', '.', 'but', 'lately', ',', 'i', \"'\", 've', 'been', 'getting', 'this', 'feeling', 'of', 'nervous', '##ness', 'and', 'my', 'body', 'tense', '##s', 'up', 'and', 'it', \"'\", 's', 'de', '##bil', '##itating', '.', 'i', 'can', \"'\", 't', 'even', 'write', 'an', '800', 'word', 'article', 'in', '3', 'hours', 'because', 'of', 'it', '.', 'i', 'have', 'a', 'doctors', 'appointment', 'tomorrow', '.', 'wondering', 'if', 'i', 'should', 'bring', 'it', 'up', 'to', 'my', 'doctor', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['am', 'i', 'depressed', 'because', 'i', 'have', 'ad', '##hd', ',', 'or', 'is', 'my', 'lack', 'of', 'focus', 'just', 'because', 'of', 'depression', '?', '(', 'x', '-', 'posted', 'from', 'r', '/', 'depression', 'i', 'was', 'diagnosed', 'as', 'having', 'depression', 'at', '16', ',', 'and', 'have', 'been', 'on', '5', 'different', 'ssr', '##is', 'over', 'the', 'last', '4', 'years', '.', 'i', 'always', 'wind', 'up', 'quit', '##ting', 'them', 'because', 'although', 'they', 'do', 'work', 'to', 'make', 'me', 'less', 'depressed', ',', 'they', 'also', 'make', 'me', 'less', '.', '.', '.', '.', 'everything', '.', 'they', 'make', 'me', 'feel', 'like', 'an', 'emotion', '##less', 'robot', '/', 'zombie', 'that', 'could', 'not', 'be', 'brought', 'to', 'give', 'a', 'shit', 'about', 'anything', '.', 'most', 'of', 'the', 'ssr', '##is', 'also', 'made', 'me', 'feel', 'slug', '##gis', '##h', ',', 'tired', ',', 'and', 'un', '##fo', '##cus', '##ed', '.', 'i', 'know', 'lack', 'of', 'motivation', ',', 'sleep', 'problems', ',', 'fatigue', ',', 'etc', '.', 'are', 'symptoms', 'of', 'depression', ',', 'but', 'the', 'ssr', '##is', 'only', 'made', 'these', 'worse', ',', 'while', 'numb', '##ing', 'me', 'emotionally', '.', 'nowadays', ',', 'i', 'feel', 'like', 'these', 'physical', 'symptoms', 'are', 'what', 'hurt', 'me', 'the', 'most', '.', 'i', 'was', 'a', 'decent', 'student', 'through', 'most', 'of', 'grade', 'school', ',', 'with', 'parents', 'i', 'didn', \"'\", 't', 'dare', 'di', '##so', '##bey', 'or', 'di', '##sa', '##pp', '##oint', '(', 'mostly', 'scared', 'of', 'my', 'dad', \"'\", 's', 'wrath', ')', '.', 'so', 'maybe', 'that', \"'\", 's', 'why', 'i', 'wasn', \"'\", 't', 'considered', 'to', 'have', 'add', '/', 'ad', '##hd', '.', 'but', 'now', 'the', 'signs', 'and', 'symptoms', 'seem', 'very', 'apparent', 'to', 'me', ':', 'i', 'can', \"'\", 't', 'focus', '.', 'at', 'all', '.', 'it', \"'\", 's', 'like', 'my', 'mind', 'is', 'running', 'too', 'fast', 'for', 'what', 'i', \"'\", 'm', 'doing', '.', 'i', 'start', 'reading', 'some', 'text', ',', 'and', 'before', 'i', 'know', 'it', ',', 'i', \"'\", 'm', 'ski', '##mming', '.', 'i', 'start', 'watching', 'a', 'movie', 'or', 'tv', 'program', ',', 'and', 'next', 'thing', 'you', 'know', ',', 'i', \"'\", 'm', 'on', 'my', 'cell', 'phone', 'or', 'in', 'the', 'kitchen', 'wondering', 'what', 'there', 'is', 'to', 'eat', '.', 'this', 'is', 'what', \"'\", 's', 'killing', 'me', 'in', 'school', '.', 'i', \"'\", 'm', 'always', 'tense', '/', 'wound', 'up', '/', 'restless', '.', 'this', 'is', 'one', 'thing', 'i', \"'\", 've', 'never', 'noticed', 'myself', ',', 'but', 'people', 'always', 'comment', 'on', '.', 'i', \"'\", 'm', 'always', 'tapping', 'my', 'foot', 'or', 'clicking', 'my', 'pen', 'or', 'drumming', 'my', 'fingers', 'on', 'the', 'table', '.', 'if', 'someone', 'tries', 'to', 'get', 'my', 'attention', ',', 'the', 'response', 'will', 'be', 'less', 'like', ',', '\"', 'oh', ',', 'hi', ',', 'what', \"'\", 's', 'up', '?', '\"', 'and', 'more', '\"', 'yeah', '##w', '##hat', '?', '\"', '(', 'not', 'so', 'much', 'angrily', ',', 'but', 'more', 'in', 'a', 'snap', '-', 'my', '-', 'head', '-', 'around', '-', 'and', '-', 'answer', '-', 'before', '-', 'they', '-', 'even', '-', 'finished', '-', 'their', '-', 'sentence', 'kind', 'of', 'way', ')', '.', 'i', 'cannot', 'be', 'organized', '.', 'i', 'cannot', 'finish', 'a', 'task', 'after', 'i', \"'\", 've', 'started', 'it', '.', 'i', 'have', 'problems', 'getting', 'started', 'on', 'anything', 'that', 'i', 'know', 'will', 'require', 'more', 'than', '30', 'sec', '.', 'of', 'work', 'or', 'attention', '.', 'these', 'are', 'just', 'a', 'few', 'examples', 'of', 'the', \"'\", 'symptoms', \"'\", 'that', 'lead', 'me', 'to', 'believe', 'i', 'may', 'suffer', 'from', 'a', 'tad', 'of', 'the', 'ol', \"'\", 'ad', '##hd', '.', 'i', 'get', 'frustrated', 'with', 'my', 'inability', 'to', 'focus', 'or', 'accomplish', 'things', 'i', 'am', 'otherwise', 'perfectly', 'capable', 'of', '.', 'i', 'get', 'down', 'on', 'myself', ',', 'and', 'then', 'i', 'go', ',', '\"', 'oh', ',', 'gee', '##z', ',', 'i', \"'\", 'm', 'worthless', '/', 'depressed', '/', 'a', 'failure', ',', 'etc', '.', '\"', 'and', 'that', \"'\", 's', 'where', 'the', \"'\", 'depression', \"'\", 'comes', 'from', '.', 'i', 'can', \"'\", 't', 'keep', 'friendships', ',', 'because', 'it', \"'\", 's', 'hard', 'for', 'me', 'to', 'plan', 'outing', '##s', ',', 'be', 'motivated', 'to', 'do', 'anything', 'in', 'the', 'first', 'place', 'that', 'would', 'require', 'more', 'than', '5', 'min', '.', 'of', 'driving', ',', 'or', 'calm', 'myself', 'down', 'enough', 'to', 'make', 'myself', 'easy', 'to', 'be', 'around', '(', 'no', 'one', 'likes', 'to', 'be', 'around', 'a', 'shaky', ',', 'volatile', ',', 'tense', ',', 'fi', '##dget', '##y', 'person', ')', '.', 'this', 'also', 'de', '##press', '##es', 'me', '.', 't', '##l', ';', 'dr', '-', 'i', 'think', 'my', 'depression', 'symptoms', 'may', 'actually', 'be', 'more', 'an', 'effect', 'caused', 'by', 'ad', '##hd', ',', 'and', 'not', 'the', 'other', 'way', 'around', '.', 'thanks', 'for', 'reading', ',', 'your', 'thoughts', 'and', 'insight', 'would', 'be', 'greatly', 'appreciated', '.', '(', 'edit', ':', 'spaced', 'out', 'the', 'text', '.', ')']\n",
      "INFO:__main__:Number of tokens: 701\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['am', 'i', 'depressed', 'because', 'i', 'have', 'ad', '##hd', ',', 'or', 'is', 'my', 'lack', 'of', 'focus', 'just', 'because', 'of', 'depression', '?', '(', 'x', '-', 'posted', 'from', 'r', '/', 'depression', 'i', 'was', 'diagnosed', 'as', 'having', 'depression', 'at', '16', ',', 'and', 'have', 'been', 'on', '5', 'different', 'ssr', '##is', 'over', 'the', 'last', '4', 'years', '.', 'i', 'always', 'wind', 'up', 'quit', '##ting', 'them', 'because', 'although', 'they', 'do', 'work', 'to', 'make', 'me', 'less', 'depressed', ',', 'they', 'also', 'make', 'me', 'less', '.', '.', '.', '.', 'everything', '.', 'they', 'make', 'me', 'feel', 'like', 'an', 'emotion', '##less', 'robot', '/', 'zombie', 'that', 'could', 'not', 'be', 'brought', 'to', 'give', 'a', 'shit', 'about', 'anything', '.', 'most', 'of', 'the', 'ssr', '##is', 'also', 'made', 'me', 'feel', 'slug', '##gis', '##h', ',', 'tired', ',', 'and', 'un', '##fo', '##cus', '##ed', '.', 'i', 'know', 'lack', 'of', 'motivation', ',', 'sleep', 'problems', ',', 'fatigue', ',', 'etc', '.', 'are', 'symptoms', 'of', 'depression', ',', 'but', 'the', 'ssr', '##is', 'only', 'made', 'these', 'worse', ',', 'while', 'numb', '##ing', 'me', 'emotionally', '.', 'nowadays', ',', 'i', 'feel', 'like', 'these', 'physical', 'symptoms', 'are', 'what', 'hurt', 'me', 'the', 'most', '.', 'i', 'was', 'a', 'decent', 'student', 'through', 'most', 'of', 'grade', 'school', ',', 'with', 'parents', 'i', 'didn', \"'\", 't', 'dare', 'di', '##so', '##bey', 'or', 'di', '##sa', '##pp', '##oint', '(', 'mostly', 'scared', 'of', 'my', 'dad', \"'\", 's', 'wrath', ')', '.', 'so', 'maybe', 'that', \"'\", 's', 'why', 'i', 'wasn', \"'\", 't', 'considered', 'to', 'have', 'add', '/', 'ad', '##hd', '.', 'but', 'now', 'the', 'signs', 'and', 'symptoms', 'seem', 'very', 'apparent', 'to', 'me', ':', 'i', 'can', \"'\", 't', 'focus', '.', 'at', 'all', '.', 'it', \"'\", 's', 'like', 'my', 'mind', 'is', 'running', 'too', 'fast', 'for', 'what', 'i', \"'\", 'm', 'doing', '.', 'i', 'start', 'reading', 'some', 'text', ',', 'and', 'before', 'i', 'know', 'it', ',', 'i', \"'\", 'm', 'ski', '##mming', '.', 'i', 'start', 'watching', 'a', 'movie', 'or', 'tv', 'program', ',', 'and', 'next', 'thing', 'you', 'know', ',', 'i', \"'\", 'm', 'on', 'my', 'cell', 'phone', 'or', 'in', 'the', 'kitchen', 'wondering', 'what', 'there', 'is', 'to', 'eat', '.', 'this', 'is', 'what', \"'\", 's', 'killing', 'me', 'in', 'school', '.', 'i', \"'\", 'm', 'always', 'tense', '/', 'wound', 'up', '/', 'restless', '.', 'this', 'is', 'one', 'thing', 'i', \"'\", 've', 'never', 'noticed', 'myself', ',', 'but', 'people', 'always', 'comment', 'on', '.', 'i', \"'\", 'm', 'always', 'tapping', 'my', 'foot', 'or', 'clicking', 'my', 'pen', 'or', 'drumming', 'my', 'fingers', 'on', 'the', 'table', '.', 'if', 'someone', 'tries', 'to', 'get', 'my', 'attention', ',', 'the', 'response', 'will', 'be', 'less', 'like', ',', '\"', 'oh', ',', 'hi', ',', 'what', \"'\", 's', 'up', '?', '\"', 'and', 'more', '\"', 'yeah', '##w', '##hat', '?', '\"', '(', 'not', 'so', 'much', 'angrily', ',', 'but', 'more', 'in', 'a', 'snap', '-', 'my', '-', 'head', '-', 'around', '-', 'and', '-', 'answer', '-', 'before', '-', 'they', '-', 'even', '-', 'finished', '-', 'their', '-', 'sentence', 'kind', 'of', 'way', ')', '.', 'i', 'cannot', 'be', 'organized', '.', 'i', 'cannot', 'finish', 'a', 'task', 'after', 'i', \"'\", 've', 'started', 'it', '.', 'i', 'have', 'problems', 'getting', 'started', 'on', 'anything', 'that', 'i', 'know', 'will', 'require', 'more', 'than', '30', 'sec', '.', 'of', 'work', 'or', 'attention', '.', 'these', 'are', 'just', 'a', 'few', 'examples', 'of', 'the', \"'\", 'symptoms', \"'\", 'that', 'lead', 'me', 'to', 'believe', 'i', 'may', 'suffer', 'from', 'a', 'tad', 'of', 'the', 'ol', \"'\", 'ad', '##hd'], ['.', 'i', 'get', 'frustrated', 'with', 'my', 'inability', 'to', 'focus', 'or', 'accomplish', 'things', 'i', 'am', 'otherwise', 'perfectly', 'capable', 'of', '.', 'i', 'get', 'down', 'on', 'myself', ',', 'and', 'then', 'i', 'go', ',', '\"', 'oh', ',', 'gee', '##z', ',', 'i', \"'\", 'm', 'worthless', '/', 'depressed', '/', 'a', 'failure', ',', 'etc', '.', '\"', 'and', 'that', \"'\", 's', 'where', 'the', \"'\", 'depression', \"'\", 'comes', 'from', '.', 'i', 'can', \"'\", 't', 'keep', 'friendships', ',', 'because', 'it', \"'\", 's', 'hard', 'for', 'me', 'to', 'plan', 'outing', '##s', ',', 'be', 'motivated', 'to', 'do', 'anything', 'in', 'the', 'first', 'place', 'that', 'would', 'require', 'more', 'than', '5', 'min', '.', 'of', 'driving', ',', 'or', 'calm', 'myself', 'down', 'enough', 'to', 'make', 'myself', 'easy', 'to', 'be', 'around', '(', 'no', 'one', 'likes', 'to', 'be', 'around', 'a', 'shaky', ',', 'volatile', ',', 'tense', ',', 'fi', '##dget', '##y', 'person', ')', '.', 'this', 'also', 'de', '##press', '##es', 'me', '.', 't', '##l', ';', 'dr', '-', 'i', 'think', 'my', 'depression', 'symptoms', 'may', 'actually', 'be', 'more', 'an', 'effect', 'caused', 'by', 'ad', '##hd', ',', 'and', 'not', 'the', 'other', 'way', 'around', '.', 'thanks', 'for', 'reading', ',', 'your', 'thoughts', 'and', 'insight', 'would', 'be', 'greatly', 'appreciated', '.', '(', 'edit', ':', 'spaced', 'out', 'the', 'text', '.', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['one', 'per', '##k', 'of', 'not', 'being', 'diagnosed', 'with', 'ad', '##hd', 'until', 'i', 'was', 'an', 'adult', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['one', 'per', '##k', 'of', 'not', 'being', 'diagnosed', 'with', 'ad', '##hd', 'until', 'i', 'was', 'an', 'adult', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['announcement', 'regarding', 'the', 'ownership', 'of', 'this', 'sub', '##red', '##dit', 'changing', 'hands', ',', 'and', 'what', 'to', 'expect', 'in', 'the', 'future', '!', '*', '*', 't', '##ld', '##r', ':', 'i', 'took', 'over', 'the', 'sub', '##red', '##dit', 'from', 'inactive', 'mod', '##s', 'and', ',', 'as', 'i', 'have', 'time', ',', 'i', \"'\", 'm', 'going', 'to', 'make', 'some', 'improvements', 'around', 'here', ',', 'such', 'as', 'adding', 'relevant', 'information', 'to', 'the', 'side', '##bar', '.', 'if', 'you', 'have', 'any', 'suggestions', 'for', 'that', 'content', ',', 'or', 'any', 'other', 'improvements', 'you', \"'\", 'd', 'like', 'to', 'see', 'in', '/', 'r', '/', 'ad', '##hd', ',', 'reply', 'to', 'this', 'post', '!', '*', '*', '*', '*', 'edit', ':', 'update', 'as', 'of', 'saturday', 'oct', '.', '1st', ':', 'have', 'the', 'fa', '##q', 'about', 'half', '-', 'written', ',', 'just', 'been', 'busy', 'with', 'school', '##work', ',', 'etc', '.', '*', '*', 'hey', 'all', ',', 'since', 'the', 'previous', 'mod', '##s', 'of', '/', 'r', '/', 'ad', '##hd', 'had', 'been', 'inactive', 'for', 'a', 'year', ',', 'i', 'submitted', 'a', 'request', 'to', 'take', 'it', 'over', ',', 'and', 'i', \"'\", 've', 'been', 'approved', '!', 'the', 'reason', 'i', 'wanted', 'to', 'do', 'this', 'is', 'mainly', 'because', 'i', 'felt', 'the', 'main', 'page', 'isn', \"'\", 't', 'living', 'up', 'to', 'its', 'potential', '.', 'i', 'want', 'to', 'com', '##pile', 'a', 'lot', 'of', 'useful', 'information', 'to', 'link', 'in', 'the', 'side', '##bar', ',', 'so', 'that', 'all', 'the', '\"', 'i', 'think', 'i', 'might', 'have', 'ad', '##hd', '\"', 'posters', 'have', 'a', 'list', 'of', 'high', '-', 'quality', 'information', 'to', 'start', 'with', '.', 'i', \"'\", 'd', 'like', 'to', 'see', 'this', 'sub', '##red', '##dit', 'be', 'both', 'a', 'supportive', 'community', 'and', 'a', 'hub', 'of', 'reliable', 'information', '.', 'my', 'approach', 'to', 'ad', '##hd', 'is', 'one', 'that', \"'\", 's', 'fairly', 'scientific', 'and', 'pro', '-', 'medication', '.', 'i', 'haven', \"'\", 't', 'yet', 'mentally', 'iron', '##ed', 'out', 'what', 'my', 'policies', 'will', 'be', ',', 'but', 'right', 'now', 'i', 'think', 'it', 'will', 'be', 'like', 'this', ':', 'i', 'may', 'control', 'what', 'links', 'go', 'in', 'the', 'side', '##bar', ',', 'but', 'i', 'don', \"'\", 't', 'down', '##vot', '##e', 'people', 'with', 'whom', 'i', 'disagree', '.', '(', 'i', 'might', 'speak', 'out', 'against', 'your', 'ideas', 'in', 'a', 'public', 'reply', ',', 'though', '!', ')', 'as', 'moderator', ',', 'i', 'have', 'no', 'plans', 'on', 'tam', '##per', '##ing', 'with', 'anyone', \"'\", 's', 'posts', 'or', 'comments', ',', 'unless', 'you', \"'\", 're', 'spreading', 'mis', '##in', '##form', '##ation', 'to', 'a', 'dangerous', 'degree', ',', 'or', 'otherwise', 'being', 'a', 'dick', '.', 'standard', 'red', '##dit', 'code', 'of', 'conduct', 'will', 'apply', ',', 'and', 'i', \"'\", 'm', 'sure', 'i', 'don', \"'\", 't', 'have', 'to', 'explain', 'that', '.', 'a', 'note', 'about', 'merging', 'with', '/', 'r', '/', 'add', ',', 'because', 'i', 'know', 'people', 'will', 'be', 'asking', ':', 'my', 'answer', 'is', ',', 'i', 'don', \"'\", 't', 'know', 'how', 'or', 'if', 'that', 'works', ',', 'but', 'i', \"'\", 'll', 'talk', 'to', '/', 'r', '/', 'add', \"'\", 's', 'mod', '##s', 'and', 'we', \"'\", 'll', 'see', 'what', 'we', 'can', 'find', 'out', '.', 'i', \"'\", 'll', 'update', 'you', 'when', 'i', 'have', 'more', 'to', 'report', '.', 'i', \"'\", 'm', 'reasonably', 'busy', ',', 'and', 'this', 'is', 'also', 'my', 'first', 'time', 'mode', '##rating', 'on', 'red', '##dit', ',', 'so', 'i', \"'\", 'll', 'have', 'to', 'ask', 'you', 'to', 'bear', 'with', 'me', ',', 'especially', 'at', 'first', ',', 'while', 'i', 'get', 'the', 'hang', 'of', 'things', 'and', 'roll', 'out', 'my', 'planned', 'improvements', 'for', 'this', 'sub', '##red', '##dit', 'as', 'i', \"'\", 'm', 'able', '.', 'having', 'said', 'all', 'that', ',', 'feel', 'free', 'to', 'start', 'commenting', 'on', 'this', 'post', 'with', 'ideas', 'for', 'what', 'you', 'think', 'should', 'go', 'into', 'our', 'fa', '##q', ',', 'list', 'of', 'helpful', 'links', ',', 'books', ',', 'etc', '.', '!', 'and', 'if', 'you', 'don', \"'\", 't', 'mind', ',', 'up', '##vot', '##e', 'this', 'so', 'more', 'people', 'see', 'it', '!', 'thanks', 'everyone', '.']\n",
      "INFO:__main__:Number of tokens: 589\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['announcement', 'regarding', 'the', 'ownership', 'of', 'this', 'sub', '##red', '##dit', 'changing', 'hands', ',', 'and', 'what', 'to', 'expect', 'in', 'the', 'future', '!', '*', '*', 't', '##ld', '##r', ':', 'i', 'took', 'over', 'the', 'sub', '##red', '##dit', 'from', 'inactive', 'mod', '##s', 'and', ',', 'as', 'i', 'have', 'time', ',', 'i', \"'\", 'm', 'going', 'to', 'make', 'some', 'improvements', 'around', 'here', ',', 'such', 'as', 'adding', 'relevant', 'information', 'to', 'the', 'side', '##bar', '.', 'if', 'you', 'have', 'any', 'suggestions', 'for', 'that', 'content', ',', 'or', 'any', 'other', 'improvements', 'you', \"'\", 'd', 'like', 'to', 'see', 'in', '/', 'r', '/', 'ad', '##hd', ',', 'reply', 'to', 'this', 'post', '!', '*', '*', '*', '*', 'edit', ':', 'update', 'as', 'of', 'saturday', 'oct', '.', '1st', ':', 'have', 'the', 'fa', '##q', 'about', 'half', '-', 'written', ',', 'just', 'been', 'busy', 'with', 'school', '##work', ',', 'etc', '.', '*', '*', 'hey', 'all', ',', 'since', 'the', 'previous', 'mod', '##s', 'of', '/', 'r', '/', 'ad', '##hd', 'had', 'been', 'inactive', 'for', 'a', 'year', ',', 'i', 'submitted', 'a', 'request', 'to', 'take', 'it', 'over', ',', 'and', 'i', \"'\", 've', 'been', 'approved', '!', 'the', 'reason', 'i', 'wanted', 'to', 'do', 'this', 'is', 'mainly', 'because', 'i', 'felt', 'the', 'main', 'page', 'isn', \"'\", 't', 'living', 'up', 'to', 'its', 'potential', '.', 'i', 'want', 'to', 'com', '##pile', 'a', 'lot', 'of', 'useful', 'information', 'to', 'link', 'in', 'the', 'side', '##bar', ',', 'so', 'that', 'all', 'the', '\"', 'i', 'think', 'i', 'might', 'have', 'ad', '##hd', '\"', 'posters', 'have', 'a', 'list', 'of', 'high', '-', 'quality', 'information', 'to', 'start', 'with', '.', 'i', \"'\", 'd', 'like', 'to', 'see', 'this', 'sub', '##red', '##dit', 'be', 'both', 'a', 'supportive', 'community', 'and', 'a', 'hub', 'of', 'reliable', 'information', '.', 'my', 'approach', 'to', 'ad', '##hd', 'is', 'one', 'that', \"'\", 's', 'fairly', 'scientific', 'and', 'pro', '-', 'medication', '.', 'i', 'haven', \"'\", 't', 'yet', 'mentally', 'iron', '##ed', 'out', 'what', 'my', 'policies', 'will', 'be', ',', 'but', 'right', 'now', 'i', 'think', 'it', 'will', 'be', 'like', 'this', ':', 'i', 'may', 'control', 'what', 'links', 'go', 'in', 'the', 'side', '##bar', ',', 'but', 'i', 'don', \"'\", 't', 'down', '##vot', '##e', 'people', 'with', 'whom', 'i', 'disagree', '.', '(', 'i', 'might', 'speak', 'out', 'against', 'your', 'ideas', 'in', 'a', 'public', 'reply', ',', 'though', '!', ')', 'as', 'moderator', ',', 'i', 'have', 'no', 'plans', 'on', 'tam', '##per', '##ing', 'with', 'anyone', \"'\", 's', 'posts', 'or', 'comments', ',', 'unless', 'you', \"'\", 're', 'spreading', 'mis', '##in', '##form', '##ation', 'to', 'a', 'dangerous', 'degree', ',', 'or', 'otherwise', 'being', 'a', 'dick', '.', 'standard', 'red', '##dit', 'code', 'of', 'conduct', 'will', 'apply', ',', 'and', 'i', \"'\", 'm', 'sure', 'i', 'don', \"'\", 't', 'have', 'to', 'explain', 'that', '.', 'a', 'note', 'about', 'merging', 'with', '/', 'r', '/', 'add', ',', 'because', 'i', 'know', 'people', 'will', 'be', 'asking', ':', 'my', 'answer', 'is', ',', 'i', 'don', \"'\", 't', 'know', 'how', 'or', 'if', 'that', 'works', ',', 'but', 'i', \"'\", 'll', 'talk', 'to', '/', 'r', '/', 'add', \"'\", 's', 'mod', '##s', 'and', 'we', \"'\", 'll', 'see', 'what', 'we', 'can', 'find', 'out', '.', 'i', \"'\", 'll', 'update', 'you', 'when', 'i', 'have', 'more', 'to', 'report', '.', 'i', \"'\", 'm', 'reasonably', 'busy', ',', 'and', 'this', 'is', 'also', 'my', 'first', 'time', 'mode', '##rating', 'on', 'red', '##dit', ',', 'so', 'i', \"'\", 'll', 'have', 'to', 'ask', 'you', 'to', 'bear', 'with', 'me', ',', 'especially', 'at', 'first', ',', 'while', 'i', 'get', 'the'], ['hang', 'of', 'things', 'and', 'roll', 'out', 'my', 'planned', 'improvements', 'for', 'this', 'sub', '##red', '##dit', 'as', 'i', \"'\", 'm', 'able', '.', 'having', 'said', 'all', 'that', ',', 'feel', 'free', 'to', 'start', 'commenting', 'on', 'this', 'post', 'with', 'ideas', 'for', 'what', 'you', 'think', 'should', 'go', 'into', 'our', 'fa', '##q', ',', 'list', 'of', 'helpful', 'links', ',', 'books', ',', 'etc', '.', '!', 'and', 'if', 'you', 'don', \"'\", 't', 'mind', ',', 'up', '##vot', '##e', 'this', 'so', 'more', 'people', 'see', 'it', '!', 'thanks', 'everyone', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['realized', 'i', 'have', 'ad', '##hd', 'last', 'year', '.', 'i', 'am', '20', 'and', 'senior', 'in', 'university', '.', 'please', 'help', '!', '!', '!']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['realized', 'i', 'have', 'ad', '##hd', 'last', 'year', '.', 'i', 'am', '20', 'and', 'senior', 'in', 'university', '.', 'please', 'help', '!', '!', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['prescription', 'discount', '##s', 'for', '(', 'generic', ')', 'add', '##eral', '##l', '?', 'yes', ',', 'my', 'friend', 'has', 'a', 'prescription', '.', '(', 'my', 'prescription', 'is', 'for', 'concert', '##a', ',', 'because', 'there', \"'\", 's', 'a', 'manufacturer', 'patient', 'assistance', 'program', 'for', 'it', '.', ')', 'my', 'ad', '##hd', 'friend', 'takes', 'generic', 'add', '##eral', '##l', ',', 'but', 'there', 'was', 'a', 'four', '-', 'fold', 'price', 'jump', 'in', 'the', 'cash', 'price', 'at', 'cost', '##co', '.', '(', 'no', 'insurance', '.', ')', 'i', 'quickly', 'checked', 'shire', ',', 'to', 'discover', 'their', 'program', 'has', 'des', '##iste', '##d', ',', 'in', 'favor', 'of', 'one', 'for', 'v', '##y', '##vana', '##se', '.', 'can', 'anyone', 'recommend', 'the', 'best', 'way', 'to', 'decrease', 'the', 'cost', '?', 'any', 'experience', 'with', 'prescription', 'discount', 'cards', '?', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 116\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['prescription', 'discount', '##s', 'for', '(', 'generic', ')', 'add', '##eral', '##l', '?', 'yes', ',', 'my', 'friend', 'has', 'a', 'prescription', '.', '(', 'my', 'prescription', 'is', 'for', 'concert', '##a', ',', 'because', 'there', \"'\", 's', 'a', 'manufacturer', 'patient', 'assistance', 'program', 'for', 'it', '.', ')', 'my', 'ad', '##hd', 'friend', 'takes', 'generic', 'add', '##eral', '##l', ',', 'but', 'there', 'was', 'a', 'four', '-', 'fold', 'price', 'jump', 'in', 'the', 'cash', 'price', 'at', 'cost', '##co', '.', '(', 'no', 'insurance', '.', ')', 'i', 'quickly', 'checked', 'shire', ',', 'to', 'discover', 'their', 'program', 'has', 'des', '##iste', '##d', ',', 'in', 'favor', 'of', 'one', 'for', 'v', '##y', '##vana', '##se', '.', 'can', 'anyone', 'recommend', 'the', 'best', 'way', 'to', 'decrease', 'the', 'cost', '?', 'any', 'experience', 'with', 'prescription', 'discount', 'cards', '?', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sun', '##the', '##ani', '##ne']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sun', '##the', '##ani', '##ne']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['process', 'for', 'ad', '##hd', 'diagnosis', 'in', 'sydney', '/', 'australia', 'posting', 'to', 'ask', 'how', 'the', 'process', 'should', 'go', '.', 'i', 'went', 'to', 'my', 'gp', ',', 'voiced', 'my', 'concerns', ',', 'he', 'was', 'rather', 'dismiss', '##ive', 'but', 'still', 'gave', 'me', 'a', 'refer', '##ral', '.', 'problem', 'now', 'is', 'that', 'the', 'refer', '##ral', 'is', 'to', 'a', 'psychologist', 'and', 'not', 'a', 'psychiatrist', ',', 'in', 'australia', ',', 'this', 'means', 'that', 'even', 'if', 'they', 'dia', '##gno', '##se', 'me', 'with', 'add', ',', 'they', 'won', \"'\", 't', 'be', 'able', 'to', 'pre', '##scribe', 'medication', 'or', 'even', 'be', 'able', 'to', 'refer', 'me', 'to', 'a', 'psychiatrist', '(', 'in', 'the', 'sense', 'that', 'i', \"'\", 'll', 'be', 'able', 'to', 'get', 'a', 'healthcare', 're', '##bate', ')', '.', 'http', ':', '/', '/', 'www', '.', 'health', '.', 'nsw', '.', 'gov', '.', 'au', '/', 'public', '##hea', '##lth', '/', 'pharmaceutical', '/', 'ad', '##hd', '/', 'fa', '##q', '##s', '.', 'as', '##p', '#', 'para', '_', '1', 'http', ':', '/', '/', 'www', '.', 'health', '.', 'nsw', '.', 'gov', '.', 'au', '/', 'resources', '/', 'public', '##hea', '##lth', '/', 'pharmaceutical', '/', 'pdf', '/', 'ad', '##hd', '_', 'criteria', '_', 'adult', '.', 'pdf', 'my', 'gp', 'won', \"'\", 't', 'be', 'able', 'to', 'pre', '##scribe', 'either', 'unless', 'under', 'very', 'specific', 'circumstances', '(', 'of', 'which', 'i', 'do', 'not', 'know', ')', '.', 'i', 'realise', 'how', 'odd', 'this', 'may', 'sound', 'since', 'it', 'feels', 'like', 'i', 'may', 'just', 'be', 'fishing', 'for', 'drugs', 'but', 'my', 'concerns', 'about', 'myself', 'and', 'having', 'add', 'is', 'quite', 'real', '(', 'i', 'had', 'put', 'off', 'going', 'to', 'the', 'gp', 'just', 'so', 'i', 'could', 'objective', '##ly', 'review', 'myself', ',', 'as', 'wrong', 'as', 'that', 'sounds', ')', '.', 'does', 'anyone', 'have', 'any', 'experience', 'in', 'regards', 'to', 'this', ',', 'especially', 'in', 'australia', '?', 'is', 'it', 'normal', 'for', 'me', 'to', 'be', 'referred', 'to', 'someone', 'who', 'can', \"'\", 't', 'actually', 'pre', '##scribe', '?', 'what', 'are', 'my', 'choices', 'here', '?']\n",
      "INFO:__main__:Number of tokens: 293\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['process', 'for', 'ad', '##hd', 'diagnosis', 'in', 'sydney', '/', 'australia', 'posting', 'to', 'ask', 'how', 'the', 'process', 'should', 'go', '.', 'i', 'went', 'to', 'my', 'gp', ',', 'voiced', 'my', 'concerns', ',', 'he', 'was', 'rather', 'dismiss', '##ive', 'but', 'still', 'gave', 'me', 'a', 'refer', '##ral', '.', 'problem', 'now', 'is', 'that', 'the', 'refer', '##ral', 'is', 'to', 'a', 'psychologist', 'and', 'not', 'a', 'psychiatrist', ',', 'in', 'australia', ',', 'this', 'means', 'that', 'even', 'if', 'they', 'dia', '##gno', '##se', 'me', 'with', 'add', ',', 'they', 'won', \"'\", 't', 'be', 'able', 'to', 'pre', '##scribe', 'medication', 'or', 'even', 'be', 'able', 'to', 'refer', 'me', 'to', 'a', 'psychiatrist', '(', 'in', 'the', 'sense', 'that', 'i', \"'\", 'll', 'be', 'able', 'to', 'get', 'a', 'healthcare', 're', '##bate', ')', '.', 'http', ':', '/', '/', 'www', '.', 'health', '.', 'nsw', '.', 'gov', '.', 'au', '/', 'public', '##hea', '##lth', '/', 'pharmaceutical', '/', 'ad', '##hd', '/', 'fa', '##q', '##s', '.', 'as', '##p', '#', 'para', '_', '1', 'http', ':', '/', '/', 'www', '.', 'health', '.', 'nsw', '.', 'gov', '.', 'au', '/', 'resources', '/', 'public', '##hea', '##lth', '/', 'pharmaceutical', '/', 'pdf', '/', 'ad', '##hd', '_', 'criteria', '_', 'adult', '.', 'pdf', 'my', 'gp', 'won', \"'\", 't', 'be', 'able', 'to', 'pre', '##scribe', 'either', 'unless', 'under', 'very', 'specific', 'circumstances', '(', 'of', 'which', 'i', 'do', 'not', 'know', ')', '.', 'i', 'realise', 'how', 'odd', 'this', 'may', 'sound', 'since', 'it', 'feels', 'like', 'i', 'may', 'just', 'be', 'fishing', 'for', 'drugs', 'but', 'my', 'concerns', 'about', 'myself', 'and', 'having', 'add', 'is', 'quite', 'real', '(', 'i', 'had', 'put', 'off', 'going', 'to', 'the', 'gp', 'just', 'so', 'i', 'could', 'objective', '##ly', 'review', 'myself', ',', 'as', 'wrong', 'as', 'that', 'sounds', ')', '.', 'does', 'anyone', 'have', 'any', 'experience', 'in', 'regards', 'to', 'this', ',', 'especially', 'in', 'australia', '?', 'is', 'it', 'normal', 'for', 'me', 'to', 'be', 'referred', 'to', 'someone', 'who', 'can', \"'\", 't', 'actually', 'pre', '##scribe', '?', 'what', 'are', 'my', 'choices', 'here', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', 'some', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', 'some', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['music', 'to', 'stay', 'focused', '?', 'i', 'find', 'that', 'some', 'types', 'of', 'music', 'help', 'me', 'concentrate', '.', 'classical', ',', 'instrumental', ',', 'or', 'music', 'with', 'subdued', 'vocals', 'in', 'a', 'language', 'i', 'can', \"'\", 't', 'understand', '.', '.', 'like', 'japanese', '.', 'for', 'example', 'harry', 'potter', 'soundtracks', '.', 'the', 'problem', 'is', 'that', 'it', 'doesn', \"'\", 't', 'work', 'if', 'i', 'use', 'the', 'same', 'album', 'all', 'the', 'time', '.', 'another', 'problem', 'is', 'that', 'some', 'parts', 'of', 'some', 'tracks', 'actually', 'start', 'giving', 'me', 'a', 'headache', ',', 'or', 'raise', 'my', 'heartbeat', '.', 'well', 'it', \"'\", 's', 'composed', 'to', 'do', 'so', 'usually', ',', 'some', 'scene', 'in', 'the', 'movie', 'where', 'someone', 'is', 'about', 'to', 'be', 'eaten', 'or', 'something', '.', '.', 'can', 'you', 'think', 'of', 'anything', 'else', 'that', \"'\", 's', 'a', 'bit', 'more', 'mono', '##ton', '##ous', ',', 'yet', 'keeps', 'you', 'in', 'some', 'sort', 'of', 'a', 'trance', '?', 'yet', 'not', 'int', '##rus', '##ive', 'to', 'studying', '/', 'working', '.', 'thanks', '.']\n",
      "INFO:__main__:Number of tokens: 148\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['music', 'to', 'stay', 'focused', '?', 'i', 'find', 'that', 'some', 'types', 'of', 'music', 'help', 'me', 'concentrate', '.', 'classical', ',', 'instrumental', ',', 'or', 'music', 'with', 'subdued', 'vocals', 'in', 'a', 'language', 'i', 'can', \"'\", 't', 'understand', '.', '.', 'like', 'japanese', '.', 'for', 'example', 'harry', 'potter', 'soundtracks', '.', 'the', 'problem', 'is', 'that', 'it', 'doesn', \"'\", 't', 'work', 'if', 'i', 'use', 'the', 'same', 'album', 'all', 'the', 'time', '.', 'another', 'problem', 'is', 'that', 'some', 'parts', 'of', 'some', 'tracks', 'actually', 'start', 'giving', 'me', 'a', 'headache', ',', 'or', 'raise', 'my', 'heartbeat', '.', 'well', 'it', \"'\", 's', 'composed', 'to', 'do', 'so', 'usually', ',', 'some', 'scene', 'in', 'the', 'movie', 'where', 'someone', 'is', 'about', 'to', 'be', 'eaten', 'or', 'something', '.', '.', 'can', 'you', 'think', 'of', 'anything', 'else', 'that', \"'\", 's', 'a', 'bit', 'more', 'mono', '##ton', '##ous', ',', 'yet', 'keeps', 'you', 'in', 'some', 'sort', 'of', 'a', 'trance', '?', 'yet', 'not', 'int', '##rus', '##ive', 'to', 'studying', '/', 'working', '.', 'thanks', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', '/', 'r', '/', 'ad', '##hd', 'fa', '##q', '!', 'if', 'you', \"'\", 're', 'new', 'here', ',', 'please', 'read', 'this', 'before', 'posting', '!', '*', '*', '/', 'r', '/', 'ad', '##hd', 'frequently', 'asked', 'questions', '!', '*', '*', '*', '*', '*', 'this', 'fa', '##q', 'is', 'a', 'work', 'in', 'progress', '.', 'i', 'will', 'add', 'more', 'to', 'it', 'as', 'i', 'have', 'time', '!', 'please', 'feel', 'free', 'to', 'contribute', 'suggestions', 'or', 'other', 'information', '.', 'as', 'should', 'be', 'the', 'policy', 'in', 'all', 'contexts', 'within', 'this', 'sub', '##red', '##dit', ',', 'please', 'tell', 'me', 'if', 'i', \"'\", 'm', 'wrong', 'about', 'something', ',', 'cite', 'your', 'source', ',', 'and', 'i', \"'\", 'll', 'fix', 'it', 'as', 'soon', 'as', 'possible', '!', 'also', ',', 'i', \"'\", 'm', 'not', 'the', 'best', 'at', 'format', '##ting', 'on', 'red', '##dit', 'but', 'if', 'it', 'looks', 'terrible', 'when', 'i', 'first', 'post', 'it', ',', 'hopefully', 'it', \"'\", 's', 'on', 'the', 'way', 'to', 'looking', 'better', 'as', 'i', 't', '##we', '##ak', 'it', '.', '*', '*', '*', '*', '*', '1', '.', 'i', 'have', 'symptoms', 'x', ',', 'y', ',', 'and', 'z', '!', 'does', 'this', 'mean', 'i', 'have', 'ad', '##hd', '?', '*', '*', 'short', 'answer', ':', 'maybe', '.', '[', 'these', 'are', 'the', 'criteria', 'by', 'which', 'a', 'medical', 'professional', 'will', 'assess', 'you', '.', ']', '(', 'https', ':', '/', '/', 'www', '.', 'ms', '##u', '.', 'ed', '##u', '/', 'course', '/', 'ce', '##p', '/', '88', '##8', '/', 'ad', '##hd', '%', '20', '##fi', '##les', '/', 'ds', '##m', '-', 'iv', '.', 'h', '##tm', ')', 'read', 'and', 'understand', 'this', 'carefully', ',', 'don', '’', 't', 'just', 'skip', 'to', 'the', 'list', 'of', 'symptoms', 'and', 'ignore', 'the', 'rest', '.', 'long', '-', 'wind', '##ed', 'answer', ':', 'ad', '##hd', 'and', 'its', 'suffer', '##ers', 'carry', 'an', 'unfortunate', 'stigma', 'of', 'being', 'labelled', 'illegitimate', '.', 'it', '’', 's', 'been', 'called', 'a', 'diagnosis', '/', 'excuse', 'for', 'normal', 'la', '##zine', '##ss', ',', 'a', 'sc', '##am', 'by', 'big', 'ph', '##arm', '##a', ',', 'an', 'excuse', 'to', 'be', 'prescribed', 'st', '##im', '##ula', '##nts', ',', 'a', 'by', '-', 'product', 'of', 'over', '##ex', '##po', '##sure', 'to', 'tv', '/', 'video', 'games', '/', 'cell', 'phones', '/', 'advertising', ',', 'and', 'more', '.', 'it', '’', 's', 'all', 'too', 'common', 'to', 'be', 'met', 'with', 'contempt', 'and', 'resistance', 'when', 'trying', 'to', 'get', 'accommodations', 'from', 'school', 'faculty', ',', 'even', 'when', 'the', 'institution', 'is', 'legally', 'required', 'to', 'provide', 'it', '.', 'we', 'face', 'the', 'same', 'doubt', 'from', 'our', 'friends', ',', 'loved', 'ones', ',', 'and', 'cow', '##or', '##kers', '.', 'i', 'believe', 'the', 'reason', 'this', 'happens', 'is', 'that', 'the', 'symptoms', 'of', 'ad', '##hd', 'are', 'experiences', 'that', 'most', 'people', 'experience', 'to', 'some', 'degree', ',', 'some', 'of', 'the', 'time', ',', 'or', 'even', 'much', 'of', 'the', 'time', '.', 'everyone', 'can', 'relate', 'to', 'pro', '##cr', '##ast', '##inating', ',', 'being', 'forget', '##ful', ',', '(', '“', 'um', ',', 'what', 'did', 'i', 'come', 'in', 'this', 'room', 'to', 'get', '?', '”', ')', ',', 'being', 'impatient', ',', 'hyper', ',', 'and', 'so', 'on', '.', 'as', 'a', 'contrasting', 'example', ',', 'schizophrenia', ',', 'bearing', 'in', 'mind', 'that', 'it', 'has', 'severe', 'social', 'stigma', '##s', 'of', 'its', 'own', ',', 'is', 'not', 'questioned', 'in', 'this', 'way', ',', 'because', 'when', 'was', 'the', 'last', 'time', 'you', 'heard', 'voices', 'in', 'your', 'head', 'that', 'told', 'you', 'to', 'do', 'things', '?', 'most', 'people', 'do', 'not', 'relate', 'to', 'this', 'experience', ',', 'so', 'they', 'don', '’', 't', 'question', 'its', 'legitimacy', 'as', 'being', 'a', 'disorder', '.', 'we', 'cannot', 'dia', '##gno', '##se', 'you', '.', 'that', '’', 's', 'the', 'job', 'of', 'a', 'medical', 'professional', '.', 'however', ',', 'looking', 'at', 'the', 'criteria', 'for', 'diagnosis', 'in', 'the', 'ds', '##m', 'iv', 'should', 'give', 'you', 'a', 'pretty', 'good', 'idea', 'of', 'whether', 'or', 'not', 'a', 'doctor', 'will', 'dia', '##gno', '##se', 'you', ',', 'since', 'that', 'is', 'the', 'standard', 'they', 'are', 'going', 'by', '.', '(', 'at', 'least', 'in', 'north', 'america', '.', ')', 'taken', 'from', 'the', 'university', 'of', 'michigan', '’', 's', 'website', ',', 'and', 'linked', 'just', 'above', ',', 'here', 'is', 'what', 'is', 'printed', 'in', 'the', 'ds', '##m', 'iv', ':', '>', '”', '*', 'essential', 'features', ':', '>', '*', 'a', '.', 'persistent', 'pattern', 'of', 'ina', '##tten', '##tion', 'and', '/', 'or', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', 'that', 'is', 'more', 'frequently', 'displayed', 'and', 'is', 'more', 'severe', 'than', 'is', 'typically', 'observed', 'in', 'individuals', 'at', 'comparable', 'level', 'of', 'development', '.', '*', '>', '*', 'b', '.', 'some', 'hyper', '##active', '-', 'imp', '##ulsive', 'or', 'ina', '##tten', '##tive', 'symptoms', 'must', 'have', 'been', 'present', 'before', 'seven', 'years', 'of', 'age', '.', '*', '>', '*', 'c', '.', 'some', 'impairment', 'from', 'the', 'symptoms', 'must', 'be', 'present', 'in', 'at', 'least', 'two', 'settings', '.', '*', '>', '*', 'd', '.', 'there', 'must', 'be', 'clear', 'evidence', 'of', 'interference', 'with', 'developmental', '##ly', 'appropriate', 'social', ',', 'academic', 'or', 'occupational', 'functioning', '.', '*', '>', '*', 'e', '.', 'the', 'disturbance', 'does', 'not', 'occur', 'exclusively', 'during', 'the', 'course', 'of', 'a', 'per', '##vas', '##ive', 'developmental', 'disorder', ',', 'schizophrenia', ',', 'or', 'other', 'psychotic', 'disorders', 'and', 'is', 'not', 'better', 'accounted', 'for', 'by', 'another', 'mental', 'disorder', '.', '*', '>', '*', '*', '*', 'three', 'sub', '##type', '##s', ':', '*', '*', '*', '>', '*', '*', '*', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', 'predominantly', 'ina', '##tten', '##tive', 'type', ':', '*', '*', '*', '*', 'this', 'sub', '##type', 'is', 'used', 'if', 'six', '(', 'or', 'more', ')', 'symptoms', 'of', 'ina', '##tten', '##tion', '(', 'but', 'fewer', 'than', 'six', 'symptoms', 'of', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', ')', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', '.', '*', '>', '*', '*', '*', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', 'predominantly', 'hyper', '##active', '-', 'imp', '##ulsive', 'type', ':', '*', '*', '*', '*', 'this', 'sub', '##type', 'should', 'be', 'used', 'if', 'six', '(', 'or', 'more', ')', 'symptoms', 'of', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', '(', 'but', 'fewer', 'than', 'six', 'of', 'ina', '##tten', '##tion', ')', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', '.', '*', '>', '*', '*', '*', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', 'combined', 'type', ':', '*', '*', '*', '*', 'this', 'sub', '##type', 'should', 'be', 'used', 'if', 'six', '(', 'or', 'more', ')', 'symptoms', 'of', 'ina', '##tten', '##tion', 'and', 'six', '(', 'or', 'more', ')', 'symptoms', 'of', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', '.', '*', '>', '*', '*', '*', 'diagnostic', 'criteria', 'for', 'the', 'three', 'sub', '##type', '##s', 'of', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', 'according', 'to', 'ds', '##m', '-', 'iv', ':', '*', '*', '*', '>', '*', 'a', '.', '“', 'persistent', 'pattern', 'of', 'ina', '##tten', '##tion', 'and', '/', 'or', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', 'that', 'is', 'more', 'frequently', 'displayed', 'and', 'is', 'more', 'severe', 'than', 'is', 'typically', 'observed', 'in', 'individuals', 'at', 'comparable', 'level', 'of', 'development', '.', '”', 'individual', 'must', 'meet', 'criteria', 'for', 'either', '(', '1', ')', 'or', '(', '2', ')', ':', '*', '>', '*', '(', '1', ')', 'six', '(', 'or', 'more', ')', 'of', 'the', 'following', 'symptoms', 'of', 'ina', '##tten', '##tion', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', 'to', 'a', 'degree', 'that', 'is', 'mala', '##da', '##ptive', 'and', 'inconsistent', 'with', 'developmental', 'level', ':', '*', '>', '*', '*', '*', 'ina', '##tten', '##tion', '*', '*', '*', '>', '*', '(', 'a', ')', 'often', 'fails', 'to', 'give', 'close', 'attention', 'to', 'details', 'or', 'makes', 'careless', 'mistakes', 'in', 'school', '##work', ',', 'work', 'or', 'other', 'activities', '*', '>', '*', '(', 'b', ')', 'often', 'has', 'difficulty', 'sustaining', 'attention', 'in', 'tasks', 'or', 'play', 'activity', '*', '>', '*', '(', 'c', ')', 'often', 'does', 'not', 'seem', 'to', 'listen', 'when', 'spoken', 'to', 'directly', '*', '>', '*', '(', 'd', ')', 'often', 'does', 'not', 'follow', 'through', 'on', 'instructions', 'and', 'fails', 'to', 'finish', 'school', '##work', ',', 'chores', 'or', 'duties', 'in', 'the', 'workplace', '(', 'not', 'due', 'to', 'opposition', '##al', 'behavior', 'or', 'failure', 'to', 'understand', 'instructions', ')', '*', '>', '*', '(', 'e', ')', 'often', 'has', 'difficulty', 'organizing', 'tasks', 'and', 'activities', '*', '>', '*', '(', 'f', ')', 'often', 'avoids', ',', 'dislike', '##s', ',', 'or', 'is', 'reluctant', 'to', 'engage', 'in', 'tasks', 'that', 'require', 'sustained', 'mental', 'effort', '(', 'such', 'as', 'school', '##work', 'or', 'homework', ')', '*', '>', '*', '(', 'g', ')', 'often', 'loose', '##s', 'things', 'necessary', 'for', 'tasks', 'or', 'activities', '(', 'e', '.', 'g', '.', ',', 'toys', ',', 'school', 'assignments', ',', 'pencil', '##s', ',', 'books', 'or', 'tools', ')', '*', '>', '*', '(', 'h', ')', 'is', 'often', 'easily', 'distracted', 'by', 'extra', '##neo', '##us', 'stimuli', '*', '>', '*', '(', 'i', ')', 'is', 'often', 'forget', '##ful', 'in', 'daily', 'activities', '*', '>', '*', '(', '2', ')', 'six', '(', 'or', 'more', ')', 'of', 'the', 'following', 'symptoms', 'of', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', 'to', 'a', 'degree', 'that', 'is', 'mala', '##da', '##ptive', 'and', 'inconsistent', 'with', 'developmental', 'level', ':', '*', '>', '*', '*', '*', 'hyper', '##act', '##ivity', '*', '*', '*', '>', '*', '(', 'a', ')', 'often', 'fi', '##dgets', 'with', 'hands', 'or', 'feet', 'or', 'sq', '##ui', '##rm', '##s', 'in', 'seat', '*', '>', '*', '(', 'b', ')', 'often', 'leaves', 'seat', 'in', 'classroom', 'or', 'in', 'other', 'situations', 'in', 'which', 'remaining', 'seated', 'is', 'expected', '*', '>', '*', '(', 'c', ')', 'often', 'runs', 'about', 'or', 'climbs', 'excessive', '##ly', 'in', 'situations', 'in', 'which', 'it', 'is', 'inappropriate', '(', 'in', 'adolescents', 'or', 'adults', ',', 'may', 'be', 'limited', 'to', 'subjective', 'feelings', 'of', 'restless', '##ness', ')', '*', '>', '*', '(', 'd', ')', 'often', 'has', 'difficulty', 'playing', 'or', 'engaging', 'in', 'leisure', 'activities', 'quietly', '*', '>', '*', '(', 'e', ')', 'is', 'often', '“', 'on', 'the', 'go', '”', 'or', 'often', 'acts', 'as', 'if', '“', 'driven', 'by', 'a', 'motor', '”', '*', '>', '*', '(', 'f', ')', 'often', 'talks', 'excessive', '##ly', '*', '>', '*', '*', '*', 'imp', '##uls', '##ivity', '*', '*', '*', '>', '*', '(', 'g', ')', 'often', 'blur', '##ts', 'out', 'answers', 'before', 'questions', 'have', 'been', 'completed', '*', '>', '*', '(', 'h', ')', 'often', 'has', 'difficulty', 'awaiting', 'turn', '*', '>', '*', '(', 'i', ')', 'often', 'interrupt', '##s', 'or', 'int', '##rud', '##es', 'on', 'others', '(', 'e', '.', 'g', '.', ',', 'butt', '##s', 'into', 'conversations', 'or', 'games', ')', '*', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '>', '*', 'b', '.', 'some', 'hyper', '##active', '-', 'imp', '##ulsive', 'or', 'ina', '##tten', '##tive', 'symptoms', 'must', 'have', 'been', 'present', 'before', 'age', '7', 'years', '.', '*', '>', '*', 'c', '.', 'some', 'impairment', 'from', 'the', 'symptoms', 'is', 'present', 'in', 'at', 'least', 'two', 'settings', '(', 'e', '.', 'g', '.', ',', 'at', 'school', '[', 'or', 'work', ']', 'and', 'at', 'home', ')', '.', '*', '>', '*', 'd', '.', 'there', 'must', 'be', 'clear', 'evidence', 'of', 'interference', 'with', 'developmental', '##ly', 'appropriate', 'social', ',', 'academic', 'or', 'occupational', 'functioning', '.', '*', '>', '*', 'e', '.', 'the', 'disturbance', 'does', 'not', 'occur', 'exclusively', 'during', 'the', 'course', 'of', 'a', 'per', '##vas', '##ive', 'developmental', 'disorder', ',', 'schizophrenia', ',', 'or', 'other', 'psychotic', 'disorders', 'and', 'is', 'not', 'better', 'accounted', 'for', 'by', 'another', 'mental', 'disorder', '(', 'e', '.', 'g', '.', ',', 'mood', 'disorder', ',', 'anxiety', 'disorder', ',', 'di', '##sso', '##cia', '##tive', 'disorder', ',', 'or', 'a', 'personality', 'disorder', ')', '.', '”', '*', '*', '*', 'continued', 'in', 'comments', '!', '*', '*']\n",
      "INFO:__main__:Number of tokens: 1776\n",
      "INFO:__main__:Number of chunks: 4\n",
      "INFO:__main__:Chunks: [['the', '/', 'r', '/', 'ad', '##hd', 'fa', '##q', '!', 'if', 'you', \"'\", 're', 'new', 'here', ',', 'please', 'read', 'this', 'before', 'posting', '!', '*', '*', '/', 'r', '/', 'ad', '##hd', 'frequently', 'asked', 'questions', '!', '*', '*', '*', '*', '*', 'this', 'fa', '##q', 'is', 'a', 'work', 'in', 'progress', '.', 'i', 'will', 'add', 'more', 'to', 'it', 'as', 'i', 'have', 'time', '!', 'please', 'feel', 'free', 'to', 'contribute', 'suggestions', 'or', 'other', 'information', '.', 'as', 'should', 'be', 'the', 'policy', 'in', 'all', 'contexts', 'within', 'this', 'sub', '##red', '##dit', ',', 'please', 'tell', 'me', 'if', 'i', \"'\", 'm', 'wrong', 'about', 'something', ',', 'cite', 'your', 'source', ',', 'and', 'i', \"'\", 'll', 'fix', 'it', 'as', 'soon', 'as', 'possible', '!', 'also', ',', 'i', \"'\", 'm', 'not', 'the', 'best', 'at', 'format', '##ting', 'on', 'red', '##dit', 'but', 'if', 'it', 'looks', 'terrible', 'when', 'i', 'first', 'post', 'it', ',', 'hopefully', 'it', \"'\", 's', 'on', 'the', 'way', 'to', 'looking', 'better', 'as', 'i', 't', '##we', '##ak', 'it', '.', '*', '*', '*', '*', '*', '1', '.', 'i', 'have', 'symptoms', 'x', ',', 'y', ',', 'and', 'z', '!', 'does', 'this', 'mean', 'i', 'have', 'ad', '##hd', '?', '*', '*', 'short', 'answer', ':', 'maybe', '.', '[', 'these', 'are', 'the', 'criteria', 'by', 'which', 'a', 'medical', 'professional', 'will', 'assess', 'you', '.', ']', '(', 'https', ':', '/', '/', 'www', '.', 'ms', '##u', '.', 'ed', '##u', '/', 'course', '/', 'ce', '##p', '/', '88', '##8', '/', 'ad', '##hd', '%', '20', '##fi', '##les', '/', 'ds', '##m', '-', 'iv', '.', 'h', '##tm', ')', 'read', 'and', 'understand', 'this', 'carefully', ',', 'don', '’', 't', 'just', 'skip', 'to', 'the', 'list', 'of', 'symptoms', 'and', 'ignore', 'the', 'rest', '.', 'long', '-', 'wind', '##ed', 'answer', ':', 'ad', '##hd', 'and', 'its', 'suffer', '##ers', 'carry', 'an', 'unfortunate', 'stigma', 'of', 'being', 'labelled', 'illegitimate', '.', 'it', '’', 's', 'been', 'called', 'a', 'diagnosis', '/', 'excuse', 'for', 'normal', 'la', '##zine', '##ss', ',', 'a', 'sc', '##am', 'by', 'big', 'ph', '##arm', '##a', ',', 'an', 'excuse', 'to', 'be', 'prescribed', 'st', '##im', '##ula', '##nts', ',', 'a', 'by', '-', 'product', 'of', 'over', '##ex', '##po', '##sure', 'to', 'tv', '/', 'video', 'games', '/', 'cell', 'phones', '/', 'advertising', ',', 'and', 'more', '.', 'it', '’', 's', 'all', 'too', 'common', 'to', 'be', 'met', 'with', 'contempt', 'and', 'resistance', 'when', 'trying', 'to', 'get', 'accommodations', 'from', 'school', 'faculty', ',', 'even', 'when', 'the', 'institution', 'is', 'legally', 'required', 'to', 'provide', 'it', '.', 'we', 'face', 'the', 'same', 'doubt', 'from', 'our', 'friends', ',', 'loved', 'ones', ',', 'and', 'cow', '##or', '##kers', '.', 'i', 'believe', 'the', 'reason', 'this', 'happens', 'is', 'that', 'the', 'symptoms', 'of', 'ad', '##hd', 'are', 'experiences', 'that', 'most', 'people', 'experience', 'to', 'some', 'degree', ',', 'some', 'of', 'the', 'time', ',', 'or', 'even', 'much', 'of', 'the', 'time', '.', 'everyone', 'can', 'relate', 'to', 'pro', '##cr', '##ast', '##inating', ',', 'being', 'forget', '##ful', ',', '(', '“', 'um', ',', 'what', 'did', 'i', 'come', 'in', 'this', 'room', 'to', 'get', '?', '”', ')', ',', 'being', 'impatient', ',', 'hyper', ',', 'and', 'so', 'on', '.', 'as', 'a', 'contrasting', 'example', ',', 'schizophrenia', ',', 'bearing', 'in', 'mind', 'that', 'it', 'has', 'severe', 'social', 'stigma', '##s', 'of', 'its', 'own', ',', 'is', 'not', 'questioned', 'in', 'this', 'way', ',', 'because', 'when', 'was', 'the', 'last', 'time', 'you', 'heard', 'voices', 'in', 'your', 'head', 'that', 'told', 'you', 'to', 'do', 'things', '?', 'most', 'people', 'do', 'not', 'relate', 'to', 'this', 'experience', ','], ['so', 'they', 'don', '’', 't', 'question', 'its', 'legitimacy', 'as', 'being', 'a', 'disorder', '.', 'we', 'cannot', 'dia', '##gno', '##se', 'you', '.', 'that', '’', 's', 'the', 'job', 'of', 'a', 'medical', 'professional', '.', 'however', ',', 'looking', 'at', 'the', 'criteria', 'for', 'diagnosis', 'in', 'the', 'ds', '##m', 'iv', 'should', 'give', 'you', 'a', 'pretty', 'good', 'idea', 'of', 'whether', 'or', 'not', 'a', 'doctor', 'will', 'dia', '##gno', '##se', 'you', ',', 'since', 'that', 'is', 'the', 'standard', 'they', 'are', 'going', 'by', '.', '(', 'at', 'least', 'in', 'north', 'america', '.', ')', 'taken', 'from', 'the', 'university', 'of', 'michigan', '’', 's', 'website', ',', 'and', 'linked', 'just', 'above', ',', 'here', 'is', 'what', 'is', 'printed', 'in', 'the', 'ds', '##m', 'iv', ':', '>', '”', '*', 'essential', 'features', ':', '>', '*', 'a', '.', 'persistent', 'pattern', 'of', 'ina', '##tten', '##tion', 'and', '/', 'or', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', 'that', 'is', 'more', 'frequently', 'displayed', 'and', 'is', 'more', 'severe', 'than', 'is', 'typically', 'observed', 'in', 'individuals', 'at', 'comparable', 'level', 'of', 'development', '.', '*', '>', '*', 'b', '.', 'some', 'hyper', '##active', '-', 'imp', '##ulsive', 'or', 'ina', '##tten', '##tive', 'symptoms', 'must', 'have', 'been', 'present', 'before', 'seven', 'years', 'of', 'age', '.', '*', '>', '*', 'c', '.', 'some', 'impairment', 'from', 'the', 'symptoms', 'must', 'be', 'present', 'in', 'at', 'least', 'two', 'settings', '.', '*', '>', '*', 'd', '.', 'there', 'must', 'be', 'clear', 'evidence', 'of', 'interference', 'with', 'developmental', '##ly', 'appropriate', 'social', ',', 'academic', 'or', 'occupational', 'functioning', '.', '*', '>', '*', 'e', '.', 'the', 'disturbance', 'does', 'not', 'occur', 'exclusively', 'during', 'the', 'course', 'of', 'a', 'per', '##vas', '##ive', 'developmental', 'disorder', ',', 'schizophrenia', ',', 'or', 'other', 'psychotic', 'disorders', 'and', 'is', 'not', 'better', 'accounted', 'for', 'by', 'another', 'mental', 'disorder', '.', '*', '>', '*', '*', '*', 'three', 'sub', '##type', '##s', ':', '*', '*', '*', '>', '*', '*', '*', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', 'predominantly', 'ina', '##tten', '##tive', 'type', ':', '*', '*', '*', '*', 'this', 'sub', '##type', 'is', 'used', 'if', 'six', '(', 'or', 'more', ')', 'symptoms', 'of', 'ina', '##tten', '##tion', '(', 'but', 'fewer', 'than', 'six', 'symptoms', 'of', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', ')', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', '.', '*', '>', '*', '*', '*', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', 'predominantly', 'hyper', '##active', '-', 'imp', '##ulsive', 'type', ':', '*', '*', '*', '*', 'this', 'sub', '##type', 'should', 'be', 'used', 'if', 'six', '(', 'or', 'more', ')', 'symptoms', 'of', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', '(', 'but', 'fewer', 'than', 'six', 'of', 'ina', '##tten', '##tion', ')', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', '.', '*', '>', '*', '*', '*', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', 'combined', 'type', ':', '*', '*', '*', '*', 'this', 'sub', '##type', 'should', 'be', 'used', 'if', 'six', '(', 'or', 'more', ')', 'symptoms', 'of', 'ina', '##tten', '##tion', 'and', 'six', '(', 'or', 'more', ')', 'symptoms', 'of', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', '.', '*', '>', '*', '*', '*', 'diagnostic', 'criteria', 'for', 'the', 'three', 'sub', '##type', '##s', 'of', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', 'according', 'to', 'ds', '##m', '-', 'iv', ':', '*', '*', '*', '>', '*', 'a', '.', '“', 'persistent', 'pattern', 'of', 'ina', '##tten', '##tion', 'and', '/', 'or', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', 'that'], ['is', 'more', 'frequently', 'displayed', 'and', 'is', 'more', 'severe', 'than', 'is', 'typically', 'observed', 'in', 'individuals', 'at', 'comparable', 'level', 'of', 'development', '.', '”', 'individual', 'must', 'meet', 'criteria', 'for', 'either', '(', '1', ')', 'or', '(', '2', ')', ':', '*', '>', '*', '(', '1', ')', 'six', '(', 'or', 'more', ')', 'of', 'the', 'following', 'symptoms', 'of', 'ina', '##tten', '##tion', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', 'to', 'a', 'degree', 'that', 'is', 'mala', '##da', '##ptive', 'and', 'inconsistent', 'with', 'developmental', 'level', ':', '*', '>', '*', '*', '*', 'ina', '##tten', '##tion', '*', '*', '*', '>', '*', '(', 'a', ')', 'often', 'fails', 'to', 'give', 'close', 'attention', 'to', 'details', 'or', 'makes', 'careless', 'mistakes', 'in', 'school', '##work', ',', 'work', 'or', 'other', 'activities', '*', '>', '*', '(', 'b', ')', 'often', 'has', 'difficulty', 'sustaining', 'attention', 'in', 'tasks', 'or', 'play', 'activity', '*', '>', '*', '(', 'c', ')', 'often', 'does', 'not', 'seem', 'to', 'listen', 'when', 'spoken', 'to', 'directly', '*', '>', '*', '(', 'd', ')', 'often', 'does', 'not', 'follow', 'through', 'on', 'instructions', 'and', 'fails', 'to', 'finish', 'school', '##work', ',', 'chores', 'or', 'duties', 'in', 'the', 'workplace', '(', 'not', 'due', 'to', 'opposition', '##al', 'behavior', 'or', 'failure', 'to', 'understand', 'instructions', ')', '*', '>', '*', '(', 'e', ')', 'often', 'has', 'difficulty', 'organizing', 'tasks', 'and', 'activities', '*', '>', '*', '(', 'f', ')', 'often', 'avoids', ',', 'dislike', '##s', ',', 'or', 'is', 'reluctant', 'to', 'engage', 'in', 'tasks', 'that', 'require', 'sustained', 'mental', 'effort', '(', 'such', 'as', 'school', '##work', 'or', 'homework', ')', '*', '>', '*', '(', 'g', ')', 'often', 'loose', '##s', 'things', 'necessary', 'for', 'tasks', 'or', 'activities', '(', 'e', '.', 'g', '.', ',', 'toys', ',', 'school', 'assignments', ',', 'pencil', '##s', ',', 'books', 'or', 'tools', ')', '*', '>', '*', '(', 'h', ')', 'is', 'often', 'easily', 'distracted', 'by', 'extra', '##neo', '##us', 'stimuli', '*', '>', '*', '(', 'i', ')', 'is', 'often', 'forget', '##ful', 'in', 'daily', 'activities', '*', '>', '*', '(', '2', ')', 'six', '(', 'or', 'more', ')', 'of', 'the', 'following', 'symptoms', 'of', 'hyper', '##act', '##ivity', '-', 'imp', '##uls', '##ivity', 'have', 'persisted', 'for', 'at', 'least', 'six', 'months', 'to', 'a', 'degree', 'that', 'is', 'mala', '##da', '##ptive', 'and', 'inconsistent', 'with', 'developmental', 'level', ':', '*', '>', '*', '*', '*', 'hyper', '##act', '##ivity', '*', '*', '*', '>', '*', '(', 'a', ')', 'often', 'fi', '##dgets', 'with', 'hands', 'or', 'feet', 'or', 'sq', '##ui', '##rm', '##s', 'in', 'seat', '*', '>', '*', '(', 'b', ')', 'often', 'leaves', 'seat', 'in', 'classroom', 'or', 'in', 'other', 'situations', 'in', 'which', 'remaining', 'seated', 'is', 'expected', '*', '>', '*', '(', 'c', ')', 'often', 'runs', 'about', 'or', 'climbs', 'excessive', '##ly', 'in', 'situations', 'in', 'which', 'it', 'is', 'inappropriate', '(', 'in', 'adolescents', 'or', 'adults', ',', 'may', 'be', 'limited', 'to', 'subjective', 'feelings', 'of', 'restless', '##ness', ')', '*', '>', '*', '(', 'd', ')', 'often', 'has', 'difficulty', 'playing', 'or', 'engaging', 'in', 'leisure', 'activities', 'quietly', '*', '>', '*', '(', 'e', ')', 'is', 'often', '“', 'on', 'the', 'go', '”', 'or', 'often', 'acts', 'as', 'if', '“', 'driven', 'by', 'a', 'motor', '”', '*', '>', '*', '(', 'f', ')', 'often', 'talks', 'excessive', '##ly', '*', '>', '*', '*', '*', 'imp', '##uls', '##ivity', '*', '*', '*', '>', '*', '(', 'g', ')', 'often', 'blur', '##ts', 'out', 'answers', 'before', 'questions', 'have', 'been', 'completed', '*', '>', '*', '(', 'h', ')', 'often', 'has', 'difficulty', 'awaiting', 'turn', '*', '>', '*', '(', 'i', ')'], ['often', 'interrupt', '##s', 'or', 'int', '##rud', '##es', 'on', 'others', '(', 'e', '.', 'g', '.', ',', 'butt', '##s', 'into', 'conversations', 'or', 'games', ')', '*', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '>', '*', 'b', '.', 'some', 'hyper', '##active', '-', 'imp', '##ulsive', 'or', 'ina', '##tten', '##tive', 'symptoms', 'must', 'have', 'been', 'present', 'before', 'age', '7', 'years', '.', '*', '>', '*', 'c', '.', 'some', 'impairment', 'from', 'the', 'symptoms', 'is', 'present', 'in', 'at', 'least', 'two', 'settings', '(', 'e', '.', 'g', '.', ',', 'at', 'school', '[', 'or', 'work', ']', 'and', 'at', 'home', ')', '.', '*', '>', '*', 'd', '.', 'there', 'must', 'be', 'clear', 'evidence', 'of', 'interference', 'with', 'developmental', '##ly', 'appropriate', 'social', ',', 'academic', 'or', 'occupational', 'functioning', '.', '*', '>', '*', 'e', '.', 'the', 'disturbance', 'does', 'not', 'occur', 'exclusively', 'during', 'the', 'course', 'of', 'a', 'per', '##vas', '##ive', 'developmental', 'disorder', ',', 'schizophrenia', ',', 'or', 'other', 'psychotic', 'disorders', 'and', 'is', 'not', 'better', 'accounted', 'for', 'by', 'another', 'mental', 'disorder', '(', 'e', '.', 'g', '.', ',', 'mood', 'disorder', ',', 'anxiety', 'disorder', ',', 'di', '##sso', '##cia', '##tive', 'disorder', ',', 'or', 'a', 'personality', 'disorder', ')', '.', '”', '*', '*', '*', 'continued', 'in', 'comments', '!', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['switch', 'from', 'concert', '##a', 'to', 'add', '##eral', '##l', 'hello', ',', 'my', 'doctor', 'and', 'i', 'have', 'decided', 'to', 'switch', 'me', 'from', '54', '##mg', 'of', 'concert', '##a', 'to', 'add', '##eral', '##l', '.', 'he', \"'\", 's', 'starting', 'me', 'off', 'on', '20', '##mg', '2', '##x', '/', 'day', '.', 'the', 'reason', 'for', 'the', 'switch', 'was', 'purely', 'monetary', ',', 'my', 'job', 'switched', 'insurance', 'co', '##s', 'so', 'my', 'concert', '##a', 'went', 'from', '$', '35', 'to', '$', '120', '/', 'month', '!', 'i', \"'\", 'm', 'not', 'on', 'the', 'add', '##eral', '##l', 'x', '##r', ',', 'so', 'it', \"'\", 'll', 'be', 'a', 'change', 'for', 'me', 'to', 'have', 'to', 'remember', 'to', 'take', 'the', 'medicine', 'twice', 'a', 'day', ',', 'but', 'other', 'then', 'that', ',', 'does', 'anyone', 'have', 'any', 'experience', 'switching', 'and', 'if', 'so', ',', 'any', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 124\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['switch', 'from', 'concert', '##a', 'to', 'add', '##eral', '##l', 'hello', ',', 'my', 'doctor', 'and', 'i', 'have', 'decided', 'to', 'switch', 'me', 'from', '54', '##mg', 'of', 'concert', '##a', 'to', 'add', '##eral', '##l', '.', 'he', \"'\", 's', 'starting', 'me', 'off', 'on', '20', '##mg', '2', '##x', '/', 'day', '.', 'the', 'reason', 'for', 'the', 'switch', 'was', 'purely', 'monetary', ',', 'my', 'job', 'switched', 'insurance', 'co', '##s', 'so', 'my', 'concert', '##a', 'went', 'from', '$', '35', 'to', '$', '120', '/', 'month', '!', 'i', \"'\", 'm', 'not', 'on', 'the', 'add', '##eral', '##l', 'x', '##r', ',', 'so', 'it', \"'\", 'll', 'be', 'a', 'change', 'for', 'me', 'to', 'have', 'to', 'remember', 'to', 'take', 'the', 'medicine', 'twice', 'a', 'day', ',', 'but', 'other', 'then', 'that', ',', 'does', 'anyone', 'have', 'any', 'experience', 'switching', 'and', 'if', 'so', ',', 'any', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['not', 'feeling', 'affects', 'of', 'ad', '##eral', '##l', '.', 'i', \"'\", 've', 'been', 'on', 'ad', '##eral', '##l', 'for', 'a', 'couple', 'weeks', 'now', 'and', 'it', 'seems', 'like', 'the', 'boost', 'in', 'my', 'ability', 'to', 'pay', 'attention', 'has', 'already', 'faded', '.', 'could', 'this', 'be', 'a', 'cause', 'because', 'of', 'incorrect', 'dos', '##age', '?', 'i', \"'\", 'm', 'currently', 'taking', '10', '##mg', '##s', 'twice', 'a', 'day', '.', 'if', 'anyone', 'can', 'give', 'me', 'any', 'advice', 'it', 'would', 'be', 'appreciated', '.']\n",
      "INFO:__main__:Number of tokens: 72\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['not', 'feeling', 'affects', 'of', 'ad', '##eral', '##l', '.', 'i', \"'\", 've', 'been', 'on', 'ad', '##eral', '##l', 'for', 'a', 'couple', 'weeks', 'now', 'and', 'it', 'seems', 'like', 'the', 'boost', 'in', 'my', 'ability', 'to', 'pay', 'attention', 'has', 'already', 'faded', '.', 'could', 'this', 'be', 'a', 'cause', 'because', 'of', 'incorrect', 'dos', '##age', '?', 'i', \"'\", 'm', 'currently', 'taking', '10', '##mg', '##s', 'twice', 'a', 'day', '.', 'if', 'anyone', 'can', 'give', 'me', 'any', 'advice', 'it', 'would', 'be', 'appreciated', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['angry', ',', 'and', 'still', 'un', '##tre', '##ated', '.', 'okay', '.', 'my', 'story', '.', 'as', 'a', 'child', ',', 'when', 'my', 'parents', 'and', 'teachers', 'noticed', 'my', 'bad', 'grades', 'and', 'difficulties', 'paying', 'attention', ',', 'they', 'explained', 'it', 'away', 'as', 'an', 'intelligent', 'child', 'who', 'wasn', \"'\", 't', 'being', 'challenged', '.', 'i', 'accepted', 'this', '(', 'and', 'my', 'bad', 'grades', ')', 'until', 'i', 'was', '17', 'or', 'so', '.', 'i', 'began', 'suspect', '##ing', 'i', 'had', 'ad', '##hd', 'at', 'about', 'that', 'time', ',', 'but', 'was', 'afraid', 'to', 'act', 'on', 'my', 'suspicions', ',', 'due', 'to', 'a', 'fear', 'of', 'people', 'calling', 'me', 'lazy', ';', 'people', 'accusing', 'me', 'as', 'using', 'the', 'disease', 'as', 'an', 'excuse', '.', 'flash', 'forward', 'to', 'now', '.', 'after', 'much', 'study', ',', 'i', \"'\", 'm', 'convinced', 'i', 'have', 'ad', '##hd', '.', 'i', 'use', 'medicinal', 'marijuana', 'for', 'my', 'cr', '##oh', '##n', \"'\", 's', 'disease', ',', 'and', 'it', 'seems', 'to', 'help', 'slow', 'down', 'my', 'thoughts', ',', 'so', 'i', 'use', 'it', 'for', 'that', ',', 'as', 'well', '.', 'however', ',', 'i', \"'\", 'm', '22', ',', 'job', '##less', ';', 'i', 'can', 'never', 'keep', 'a', 'job', '.', 'already', 'married', 'and', 'divorced', '.', 'living', 'in', 'my', 'grandmother', \"'\", 's', 'barn', '.', 'and', ',', 'as', 'much', 'as', 'i', 'hate', 'my', 'situation', ',', 'i', 'cannot', 'seem', 'to', 'get', 'myself', 'out', '.', 'so', ',', 'as', 'a', 'desperate', 'act', ',', 'i', 'pursue', 'a', 'medical', 'opinion', 'on', 'my', 'perceived', 'ad', '##hd', '.', 'the', 'psychologist', 'agrees', 'that', 'i', 'have', 'ad', '##hd', 'and', 'refers', 'me', 'to', 'the', 'psychologist', 'for', 'treatment', 'options', '.', 'for', 'the', 'first', 'time', 'in', 'a', 'long', 'time', ',', 'i', 'feel', 'truly', 'hopeful', '.', 'maybe', ',', 'once', 'i', 'begin', 'treatment', ',', 'i', 'can', 'start', 'my', 'life', 'over', '.', 'start', 'college', 'again', ',', 'except', 'this', 'time', ',', 'i', \"'\", 'll', 'get', 'my', 'school', '##work', 'done', ',', 'and', 'make', 'something', 'of', 'myself', '.', 'i', \"'\", 'm', 'very', 'excited', 'about', 'the', 'possibilities', '.', 'the', 'psychologist', 'requested', 'blood', '##work', 'before', 'i', 'see', 'the', 'psychiatrist', '.', 'i', 'get', 'that', ';', 'they', 'don', \"'\", 't', 'want', 'to', 'give', 'st', '##im', '##ula', '##nts', 'to', 'speed', 'addict', '##s', '.', 'i', 'get', 'that', ',', 'so', 'i', 'give', 'blood', 'the', 'very', 'next', 'day', 'and', 'wait', '.', 'for', 'two', 'weeks', '.', 'today', ',', 'i', 'call', 'the', 'psychiatrist', ',', 'hoping', 'to', 'get', 'an', 'app', '##t', 'and', 'speed', 'the', 'process', 'along', '.', 'they', \"'\", 're', 'willing', 'to', 'schedule', 'the', 'appointment', ',', 'but', 'tell', 'me', 'that', 'since', 'my', 'blood', 'showing', 'marijuana', '(', 'which', 'i', 'told', 'them', 'they', 'would', ',', 'and', 'no', 'other', 'drugs', 'showed', 'up', 'in', 'my', 'blood', ')', ',', 'they', 'won', \"'\", 't', 'help', 'me', '(', 'yet', ')', '.', 'i', 'have', 'two', 'choices', ':', 'either', 'wait', 'three', 'months', 'without', 'treatment', 'until', 'the', 'marijuana', 'is', 'out', 'of', 'my', 'system', 'and', 're', '-', 'do', 'my', 'blood', 'tests', ',', 'or', 'check', 'into', 'their', '\"', 'chemical', 'dependency', 'program', '\"', '.', 'this', 'seems', 'wrong', 'to', 'me', '.', 'if', 'i', 'tested', 'positive', 'for', 'met', '##h', ',', 'sure', ',', 'but', 'cannabis', '?', 'i', 'really', 'feel', 'like', 'i', \"'\", 'm', 'not', 'getting', 'the', 'treatment', 'i', 'deserve', '.', 'am', 'i', 'wrong', '?']\n",
      "INFO:__main__:Number of tokens: 490\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['angry', ',', 'and', 'still', 'un', '##tre', '##ated', '.', 'okay', '.', 'my', 'story', '.', 'as', 'a', 'child', ',', 'when', 'my', 'parents', 'and', 'teachers', 'noticed', 'my', 'bad', 'grades', 'and', 'difficulties', 'paying', 'attention', ',', 'they', 'explained', 'it', 'away', 'as', 'an', 'intelligent', 'child', 'who', 'wasn', \"'\", 't', 'being', 'challenged', '.', 'i', 'accepted', 'this', '(', 'and', 'my', 'bad', 'grades', ')', 'until', 'i', 'was', '17', 'or', 'so', '.', 'i', 'began', 'suspect', '##ing', 'i', 'had', 'ad', '##hd', 'at', 'about', 'that', 'time', ',', 'but', 'was', 'afraid', 'to', 'act', 'on', 'my', 'suspicions', ',', 'due', 'to', 'a', 'fear', 'of', 'people', 'calling', 'me', 'lazy', ';', 'people', 'accusing', 'me', 'as', 'using', 'the', 'disease', 'as', 'an', 'excuse', '.', 'flash', 'forward', 'to', 'now', '.', 'after', 'much', 'study', ',', 'i', \"'\", 'm', 'convinced', 'i', 'have', 'ad', '##hd', '.', 'i', 'use', 'medicinal', 'marijuana', 'for', 'my', 'cr', '##oh', '##n', \"'\", 's', 'disease', ',', 'and', 'it', 'seems', 'to', 'help', 'slow', 'down', 'my', 'thoughts', ',', 'so', 'i', 'use', 'it', 'for', 'that', ',', 'as', 'well', '.', 'however', ',', 'i', \"'\", 'm', '22', ',', 'job', '##less', ';', 'i', 'can', 'never', 'keep', 'a', 'job', '.', 'already', 'married', 'and', 'divorced', '.', 'living', 'in', 'my', 'grandmother', \"'\", 's', 'barn', '.', 'and', ',', 'as', 'much', 'as', 'i', 'hate', 'my', 'situation', ',', 'i', 'cannot', 'seem', 'to', 'get', 'myself', 'out', '.', 'so', ',', 'as', 'a', 'desperate', 'act', ',', 'i', 'pursue', 'a', 'medical', 'opinion', 'on', 'my', 'perceived', 'ad', '##hd', '.', 'the', 'psychologist', 'agrees', 'that', 'i', 'have', 'ad', '##hd', 'and', 'refers', 'me', 'to', 'the', 'psychologist', 'for', 'treatment', 'options', '.', 'for', 'the', 'first', 'time', 'in', 'a', 'long', 'time', ',', 'i', 'feel', 'truly', 'hopeful', '.', 'maybe', ',', 'once', 'i', 'begin', 'treatment', ',', 'i', 'can', 'start', 'my', 'life', 'over', '.', 'start', 'college', 'again', ',', 'except', 'this', 'time', ',', 'i', \"'\", 'll', 'get', 'my', 'school', '##work', 'done', ',', 'and', 'make', 'something', 'of', 'myself', '.', 'i', \"'\", 'm', 'very', 'excited', 'about', 'the', 'possibilities', '.', 'the', 'psychologist', 'requested', 'blood', '##work', 'before', 'i', 'see', 'the', 'psychiatrist', '.', 'i', 'get', 'that', ';', 'they', 'don', \"'\", 't', 'want', 'to', 'give', 'st', '##im', '##ula', '##nts', 'to', 'speed', 'addict', '##s', '.', 'i', 'get', 'that', ',', 'so', 'i', 'give', 'blood', 'the', 'very', 'next', 'day', 'and', 'wait', '.', 'for', 'two', 'weeks', '.', 'today', ',', 'i', 'call', 'the', 'psychiatrist', ',', 'hoping', 'to', 'get', 'an', 'app', '##t', 'and', 'speed', 'the', 'process', 'along', '.', 'they', \"'\", 're', 'willing', 'to', 'schedule', 'the', 'appointment', ',', 'but', 'tell', 'me', 'that', 'since', 'my', 'blood', 'showing', 'marijuana', '(', 'which', 'i', 'told', 'them', 'they', 'would', ',', 'and', 'no', 'other', 'drugs', 'showed', 'up', 'in', 'my', 'blood', ')', ',', 'they', 'won', \"'\", 't', 'help', 'me', '(', 'yet', ')', '.', 'i', 'have', 'two', 'choices', ':', 'either', 'wait', 'three', 'months', 'without', 'treatment', 'until', 'the', 'marijuana', 'is', 'out', 'of', 'my', 'system', 'and', 're', '-', 'do', 'my', 'blood', 'tests', ',', 'or', 'check', 'into', 'their', '\"', 'chemical', 'dependency', 'program', '\"', '.', 'this', 'seems', 'wrong', 'to', 'me', '.', 'if', 'i', 'tested', 'positive', 'for', 'met', '##h', ',', 'sure', ',', 'but', 'cannabis', '?', 'i', 'really', 'feel', 'like', 'i', \"'\", 'm', 'not', 'getting', 'the', 'treatment', 'i', 'deserve', '.', 'am', 'i', 'wrong', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['feel', 'dependent', 'on', 'medication', 'and', 'scared', 'to', 'go', 'off', '.', '.', '.', 'what', 'do', 'i', 'do', '?', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'and', 'd', '##ys', '##cal', '##cu', '##lia', 'when', 'i', 'was', 'a', 'senior', 'in', 'highs', '##cho', '##ol', '(', '18', 'years', 'old', ')', '.', 'i', 'was', 'put', 'on', 'medication', '(', 'v', '##y', '##vance', ')', 'which', 'completely', 'changed', 'my', 'life', '-', 'i', 'could', 'study', ',', 'i', 'could', 'focus', 'on', 'conversations', ',', 'i', 'didn', \"'\", 't', 'feel', 'socially', 'awkward', 'anymore', 'and', 'therefore', 'became', 'more', 'social', 'and', 'gained', 'more', 'friends', ',', 'i', 'remembered', 'things', 'better', '.', '.', '.', 'but', 'it', 'was', 'only', 'temporary', '.', 'after', 'two', 'years', 'of', 'steadily', 'increasing', 'my', 'dose', ',', 'v', '##y', '##vance', 'simply', 'wasn', \"'\", 't', 'working', 'for', 'me', 'at', 'all', 'anymore', '.', 'i', 'was', 'put', 'on', 'add', '##eral', '##l', 'x', '##r', ',', 'but', 'now', 'my', 'tolerance', 'for', 'that', 'is', 'starting', 'to', 'increase', 'too', '.', 'i', 'feel', 'almost', 'afraid', 'to', 'live', 'my', 'life', 'now', 'without', 'medication', '.', 'i', \"'\", 'm', 'scared', 'to', 'go', 'back', 'to', 'a', 'time', 'when', 'i', 'couldn', \"'\", 't', 'handle', 'anything', 'because', 'of', 'my', 'symptoms', '.', 'but', 'i', 'also', 'feel', 'that', 'it', 'must', 'be', 'un', '##real', '##istic', 'to', '(', 'a', ')', 'keep', 'increasing', 'my', 'dose', 'indefinitely', 'and', '(', 'b', ')', 'be', 'on', 'medication', 'forever', 'and', 'ever', '.', 'so', 'what', 'do', 'i', 'do', '?', 'i', 'feel', 'like', 'i', \"'\", 've', 'become', 'so', 'dependent', 'on', 'it', ',', 'that', 'i', \"'\", 'm', 'afraid', 'of', 'what', 'would', 'happen', 'if', 'i', 'just', 'stopped', '.', 'i', 'have', 'no', 'confidence', 'whatsoever', 'in', 'my', 'un', '##med', '##icated', 'self', '.', 'i', 'really', 'feel', 'like', 'i', \"'\", 'm', 'stuck', 'between', 'a', 'rock', 'and', 'a', 'hard', 'place', ',', 'red', '##dit', '.', '.', '.', 'any', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 279\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['feel', 'dependent', 'on', 'medication', 'and', 'scared', 'to', 'go', 'off', '.', '.', '.', 'what', 'do', 'i', 'do', '?', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'and', 'd', '##ys', '##cal', '##cu', '##lia', 'when', 'i', 'was', 'a', 'senior', 'in', 'highs', '##cho', '##ol', '(', '18', 'years', 'old', ')', '.', 'i', 'was', 'put', 'on', 'medication', '(', 'v', '##y', '##vance', ')', 'which', 'completely', 'changed', 'my', 'life', '-', 'i', 'could', 'study', ',', 'i', 'could', 'focus', 'on', 'conversations', ',', 'i', 'didn', \"'\", 't', 'feel', 'socially', 'awkward', 'anymore', 'and', 'therefore', 'became', 'more', 'social', 'and', 'gained', 'more', 'friends', ',', 'i', 'remembered', 'things', 'better', '.', '.', '.', 'but', 'it', 'was', 'only', 'temporary', '.', 'after', 'two', 'years', 'of', 'steadily', 'increasing', 'my', 'dose', ',', 'v', '##y', '##vance', 'simply', 'wasn', \"'\", 't', 'working', 'for', 'me', 'at', 'all', 'anymore', '.', 'i', 'was', 'put', 'on', 'add', '##eral', '##l', 'x', '##r', ',', 'but', 'now', 'my', 'tolerance', 'for', 'that', 'is', 'starting', 'to', 'increase', 'too', '.', 'i', 'feel', 'almost', 'afraid', 'to', 'live', 'my', 'life', 'now', 'without', 'medication', '.', 'i', \"'\", 'm', 'scared', 'to', 'go', 'back', 'to', 'a', 'time', 'when', 'i', 'couldn', \"'\", 't', 'handle', 'anything', 'because', 'of', 'my', 'symptoms', '.', 'but', 'i', 'also', 'feel', 'that', 'it', 'must', 'be', 'un', '##real', '##istic', 'to', '(', 'a', ')', 'keep', 'increasing', 'my', 'dose', 'indefinitely', 'and', '(', 'b', ')', 'be', 'on', 'medication', 'forever', 'and', 'ever', '.', 'so', 'what', 'do', 'i', 'do', '?', 'i', 'feel', 'like', 'i', \"'\", 've', 'become', 'so', 'dependent', 'on', 'it', ',', 'that', 'i', \"'\", 'm', 'afraid', 'of', 'what', 'would', 'happen', 'if', 'i', 'just', 'stopped', '.', 'i', 'have', 'no', 'confidence', 'whatsoever', 'in', 'my', 'un', '##med', '##icated', 'self', '.', 'i', 'really', 'feel', 'like', 'i', \"'\", 'm', 'stuck', 'between', 'a', 'rock', 'and', 'a', 'hard', 'place', ',', 'red', '##dit', '.', '.', '.', 'any', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'diagnosed', ',', 'advice', '?', 'i', 'was', 'recently', '(', 'a', 'week', 'ago', ')', 'diagnosed', 'with', 'ad', '##hd', 'after', 'taking', 'extensive', 'educational', 'psychological', 'testing', '.', 'i', 'didn', \"'\", 't', 'even', 'think', 'about', 'having', 'ad', '##hd', 'until', 'my', 'psychiatrist', 'started', 'noticing', 'things', '.', 'then', 'i', 'saw', 'that', 'they', 'were', 'apparent', 'all', 'throughout', 'my', 'childhood', '.', 'the', 'main', 'reason', 'i', 'took', 'the', 'test', 'was', 'test', 'time', 'extensions', 'in', 'college', '.', 'my', 'grades', 'went', 'from', 'c', '-', \"'\", 's', '/', 'd', \"'\", 's', 'to', 'b', '+', \"'\", 's', '/', 'a', \"'\", 's', 'on', 'tests', '(', 'they', 'let', 'me', 'do', 'this', 'one', 'semester', 'without', 'being', 'diagnosed', 'but', 'said', 'i', 'had', 'to', 'get', 'tested', 'for', 'continuing', 'the', 'accommodations', ')', '.', 'i', 'am', 'currently', 'attending', 'vt', 'major', '##ing', 'in', 'software', 'engineering', 'so', 'my', 'gp', '##a', 'is', 'important', 'for', 'a', 'job', 'out', 'of', 'college', '.', 'i', 'have', 'yet', 'to', 'schedule', 'a', 'follow', 'up', 'with', 'my', 'psychiatrist', '.', 'i', 'would', 'like', 'to', 'know', 'what', 'to', 'say', 'to', 'my', 'psychiatrist', 'about', 'my', 'recent', 'diagnosis', 'which', 'is', 'here', ':', '\"', 'good', 'morning', ',', 'let', 'me', 'just', 'give', 'you', 'an', 'overview', '.', 'you', 'definitely', 'exhibit', 'ad', '##hd', 'symptoms', ',', 'as', 'well', 'as', ',', 'significant', 'anxiety', 'that', 'probably', 'is', 'associated', 'with', 'the', 'frustration', 'of', 'the', 'symptoms', '.', 'in', 'addition', ',', 'you', 'produced', 'a', 'high', 'score', 'for', 'ne', '##uro', '##tic', '##ism', 'scales', 'that', 'are', 'also', 'associated', 'with', 'the', 'anxiety', 'and', 'attention', 'deficit', '##s', '(', 'and', 'with', 'depression', ',', 'anger', '/', 'hostility', ',', 'and', 'vulnerability', ')', '.', 'you', 'mentioned', 'that', 'you', 'have', 'noticed', 'mood', 'swings', 'recently', '.', 'i', 'think', 'with', 'your', 'family', 'history', 'of', 'depression', ',', 'you', 'should', 'address', 'this', 'during', 'the', 'time', 'that', 'you', 'are', 'here', 'in', 'raleigh', '.', 'it', 'will', 'be', 'a', 'good', 'time', 'for', 'you', 'to', 'focus', 'on', 'your', 'emotional', 'status', 'and', 'get', 'the', 'ad', '##hd', 'regulated', '.', 'one', 'thing', 'we', 'recognize', 'in', 'males', 'is', 'that', 'depression', 'often', 'manifest', '##s', 'in', 'anger', ',', 'so', 'this', 'should', 'be', 'addressed', 'in', 'therapy', '.', 'i', 'am', 'glad', 'you', 'have', 'decided', 'to', 'wait', 'until', 'january', 'to', 'go', 'back', 'to', 'school', '.', 'you', 'are', 'very', 'intelligent', 'and', 'should', 'be', 'commended', 'for', 'keeping', 'such', 'a', 'good', 'gp', '##a', '.', 'although', 'i', 'did', 'not', 'see', 'a', 'need', 'to', 'do', 'a', 'complete', 'cognitive', 'work', '-', 'up', ',', 'i', 'gave', 'you', '3', 'tests', 'during', 'which', 'i', 'was', 'able', 'to', 'observe', 'your', 'attention', '.', 'you', 'missed', 'specific', 'instructions', ',', 'another', 'indication', 'of', 'the', 'attention', 'deficit', '##s', '.', '\"', 'i', 'need', 'advice', 'on', 'medication', 'po', '##ssi', '##bil', '##ites', ',', 'etc', '.', 'any', 'help', 'would', 'be', 'appreciated', '.', 'thanks', '.']\n",
      "INFO:__main__:Number of tokens: 416\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'diagnosed', ',', 'advice', '?', 'i', 'was', 'recently', '(', 'a', 'week', 'ago', ')', 'diagnosed', 'with', 'ad', '##hd', 'after', 'taking', 'extensive', 'educational', 'psychological', 'testing', '.', 'i', 'didn', \"'\", 't', 'even', 'think', 'about', 'having', 'ad', '##hd', 'until', 'my', 'psychiatrist', 'started', 'noticing', 'things', '.', 'then', 'i', 'saw', 'that', 'they', 'were', 'apparent', 'all', 'throughout', 'my', 'childhood', '.', 'the', 'main', 'reason', 'i', 'took', 'the', 'test', 'was', 'test', 'time', 'extensions', 'in', 'college', '.', 'my', 'grades', 'went', 'from', 'c', '-', \"'\", 's', '/', 'd', \"'\", 's', 'to', 'b', '+', \"'\", 's', '/', 'a', \"'\", 's', 'on', 'tests', '(', 'they', 'let', 'me', 'do', 'this', 'one', 'semester', 'without', 'being', 'diagnosed', 'but', 'said', 'i', 'had', 'to', 'get', 'tested', 'for', 'continuing', 'the', 'accommodations', ')', '.', 'i', 'am', 'currently', 'attending', 'vt', 'major', '##ing', 'in', 'software', 'engineering', 'so', 'my', 'gp', '##a', 'is', 'important', 'for', 'a', 'job', 'out', 'of', 'college', '.', 'i', 'have', 'yet', 'to', 'schedule', 'a', 'follow', 'up', 'with', 'my', 'psychiatrist', '.', 'i', 'would', 'like', 'to', 'know', 'what', 'to', 'say', 'to', 'my', 'psychiatrist', 'about', 'my', 'recent', 'diagnosis', 'which', 'is', 'here', ':', '\"', 'good', 'morning', ',', 'let', 'me', 'just', 'give', 'you', 'an', 'overview', '.', 'you', 'definitely', 'exhibit', 'ad', '##hd', 'symptoms', ',', 'as', 'well', 'as', ',', 'significant', 'anxiety', 'that', 'probably', 'is', 'associated', 'with', 'the', 'frustration', 'of', 'the', 'symptoms', '.', 'in', 'addition', ',', 'you', 'produced', 'a', 'high', 'score', 'for', 'ne', '##uro', '##tic', '##ism', 'scales', 'that', 'are', 'also', 'associated', 'with', 'the', 'anxiety', 'and', 'attention', 'deficit', '##s', '(', 'and', 'with', 'depression', ',', 'anger', '/', 'hostility', ',', 'and', 'vulnerability', ')', '.', 'you', 'mentioned', 'that', 'you', 'have', 'noticed', 'mood', 'swings', 'recently', '.', 'i', 'think', 'with', 'your', 'family', 'history', 'of', 'depression', ',', 'you', 'should', 'address', 'this', 'during', 'the', 'time', 'that', 'you', 'are', 'here', 'in', 'raleigh', '.', 'it', 'will', 'be', 'a', 'good', 'time', 'for', 'you', 'to', 'focus', 'on', 'your', 'emotional', 'status', 'and', 'get', 'the', 'ad', '##hd', 'regulated', '.', 'one', 'thing', 'we', 'recognize', 'in', 'males', 'is', 'that', 'depression', 'often', 'manifest', '##s', 'in', 'anger', ',', 'so', 'this', 'should', 'be', 'addressed', 'in', 'therapy', '.', 'i', 'am', 'glad', 'you', 'have', 'decided', 'to', 'wait', 'until', 'january', 'to', 'go', 'back', 'to', 'school', '.', 'you', 'are', 'very', 'intelligent', 'and', 'should', 'be', 'commended', 'for', 'keeping', 'such', 'a', 'good', 'gp', '##a', '.', 'although', 'i', 'did', 'not', 'see', 'a', 'need', 'to', 'do', 'a', 'complete', 'cognitive', 'work', '-', 'up', ',', 'i', 'gave', 'you', '3', 'tests', 'during', 'which', 'i', 'was', 'able', 'to', 'observe', 'your', 'attention', '.', 'you', 'missed', 'specific', 'instructions', ',', 'another', 'indication', 'of', 'the', 'attention', 'deficit', '##s', '.', '\"', 'i', 'need', 'advice', 'on', 'medication', 'po', '##ssi', '##bil', '##ites', ',', 'etc', '.', 'any', 'help', 'would', 'be', 'appreciated', '.', 'thanks', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'participating', 'in', 'a', 'research', 'study', 'involving', 'ad', '##hd', '.', 'i', 'was', 'asked', 'if', 'i', 'interrupt', 'a', 'lot', ',', 'or', 'talk', 'over', 'people', '.', 'i', 'do', ',', 'but', 'why', 'is', 'that', 'considered', 'a', 'characteristic', 'of', 'folks', 'with', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 42\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'participating', 'in', 'a', 'research', 'study', 'involving', 'ad', '##hd', '.', 'i', 'was', 'asked', 'if', 'i', 'interrupt', 'a', 'lot', ',', 'or', 'talk', 'over', 'people', '.', 'i', 'do', ',', 'but', 'why', 'is', 'that', 'considered', 'a', 'characteristic', 'of', 'folks', 'with', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'add', 'prevents', 'me', 'from', 'looking', 'at', 'this', 'sub', '##red', '##dit', 'for', 'more', 'than', '12', 'seconds', '.', '.', '.', '.', 'so', 'instead', 'of', 'any', 'post', 'that', 'has', 'meaning', ',', 'here', \"'\", 's', 'a', 'lighthouse', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '.', '-', '.', '_', '_', '_', '_', '_', '.', '.', '.', '.', '|', '|', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '.', '.', '.', '.', '-', '-', '-', '-', '-', '-', '\"', '\"', '\"', '\"', 'u', '##u', '##u', '##u', '##u', '|', '=', '=', '=', '|', '|', '=', '=', '=', '|', '|', '=', '=', '=', '|', '|', '=', '=', '=', '|', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'it', \"'\", 's', 'a', 'lighthouse', '.', 'just', 'trust', 'me', 'on', 'this', 'one']\n",
      "INFO:__main__:Number of tokens: 212\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'add', 'prevents', 'me', 'from', 'looking', 'at', 'this', 'sub', '##red', '##dit', 'for', 'more', 'than', '12', 'seconds', '.', '.', '.', '.', 'so', 'instead', 'of', 'any', 'post', 'that', 'has', 'meaning', ',', 'here', \"'\", 's', 'a', 'lighthouse', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '.', '-', '.', '_', '_', '_', '_', '_', '.', '.', '.', '.', '|', '|', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '.', '.', '.', '.', '-', '-', '-', '-', '-', '-', '\"', '\"', '\"', '\"', 'u', '##u', '##u', '##u', '##u', '|', '=', '=', '=', '|', '|', '=', '=', '=', '|', '|', '=', '=', '=', '|', '|', '=', '=', '=', '|', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'it', \"'\", 's', 'a', 'lighthouse', '.', 'just', 'trust', 'me', 'on', 'this', 'one']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['got', 'laid', 'off', ',', 'cannot', 'relax', '.', '.', '.', 'just', 'got', 'a', 'phone', 'call', 'from', 'my', 'boss', 'today', ',', 'due', 'to', 'the', 'local', 'economy', 'of', 'the', 'neighbourhood', 'of', 'where', 'my', 'job', 'is', 'located', ',', 'they', 'have', 'to', 'downs', '##ize', '.', 'since', 'i', 'am', 'the', 'employee', 'with', 'few', '##est', 'hours', 'and', 'highest', 'compared', 'salary', ',', 'i', 'was', 'a', 'necessary', 'cut', '.', 'but', 'that', 'is', 'not', 'my', 'issue', ',', 'my', 'issue', 'is', 'that', 'i', 'cannot', 'un', '##wind', ',', 'normally', 'i', 'can', 'control', 'my', 'stress', 'levels', ',', 'and', 'with', 'my', 'medication', 'i', 'can', 'focus', '.', 'now', 'it', 'seems', 'though', 'that', 'my', 'medication', 'is', 'adding', 'to', 'the', 'stress', ',', 'my', 'heart', 'is', 'racing', '.', 'any', 'suggestions', '?', 'notes', ':', 'this', 'has', 'no', 'significant', 'impact', 'on', 'my', 'own', 'personal', 'economy', 'at', 'all', '.', 'so', 'i', 'am', 'not', 'worried', 'about', 'loss', 'of', 'money', '.', 'it', 'is', 'just', ',', 'the', 'fact', 'i', 'lost', 'my', 'job', ',', 'i', 'have', 'had', 'for', 'years', '.', 'and', 'i', 'haven', \"'\", 't', 'been', 'without', 'a', 'job', 'completely', 'for', 'several', 'years', '.']\n",
      "INFO:__main__:Number of tokens: 169\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['got', 'laid', 'off', ',', 'cannot', 'relax', '.', '.', '.', 'just', 'got', 'a', 'phone', 'call', 'from', 'my', 'boss', 'today', ',', 'due', 'to', 'the', 'local', 'economy', 'of', 'the', 'neighbourhood', 'of', 'where', 'my', 'job', 'is', 'located', ',', 'they', 'have', 'to', 'downs', '##ize', '.', 'since', 'i', 'am', 'the', 'employee', 'with', 'few', '##est', 'hours', 'and', 'highest', 'compared', 'salary', ',', 'i', 'was', 'a', 'necessary', 'cut', '.', 'but', 'that', 'is', 'not', 'my', 'issue', ',', 'my', 'issue', 'is', 'that', 'i', 'cannot', 'un', '##wind', ',', 'normally', 'i', 'can', 'control', 'my', 'stress', 'levels', ',', 'and', 'with', 'my', 'medication', 'i', 'can', 'focus', '.', 'now', 'it', 'seems', 'though', 'that', 'my', 'medication', 'is', 'adding', 'to', 'the', 'stress', ',', 'my', 'heart', 'is', 'racing', '.', 'any', 'suggestions', '?', 'notes', ':', 'this', 'has', 'no', 'significant', 'impact', 'on', 'my', 'own', 'personal', 'economy', 'at', 'all', '.', 'so', 'i', 'am', 'not', 'worried', 'about', 'loss', 'of', 'money', '.', 'it', 'is', 'just', ',', 'the', 'fact', 'i', 'lost', 'my', 'job', ',', 'i', 'have', 'had', 'for', 'years', '.', 'and', 'i', 'haven', \"'\", 't', 'been', 'without', 'a', 'job', 'completely', 'for', 'several', 'years', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'a', 'thought', '.', 'ad', '##hd', 'as', 'a', 'brief', 'anxiety', '?', 'we', 'want', 'to', 'be', 'up', 'to', 'people', \"'\", 's', 'expectations', '.', 'could', 'hd', '##hd', 'be', 'like', 'a', 'tension', '?', 'like', 'a', 'brief', 'anxiety', ',', 'the', 'worry', 'that', 'we', \"'\", 'll', 'again', 'not', 'be', 'able', 'to', 'focus', ',', 'understand', 'or', 'remember', 'all', 'the', 'things', 'that', 'go', 'through', 'our', 'brain', 'and', 'that', 'we', 'wish', 'we', 'could', 'sort', 'and', 'remember', '?', 'all', 'the', 'information', 'we', '*', 'should', '*', 'be', 'remembering', '.', 'could', 'it', 'be', 'that', 'feeling', 'this', 'short', 'and', 'un', '##not', '##ice', '##able', 'anxiety', 'while', 'for', 'example', 'listening', 'to', 'someone', 'talking', ',', 'distract', '##s', 'us', 'and', 'makes', 'us', 'unable', 'to', 'focus', 'understand', 'or', 'remember', '?', 'this', 'is', 'just', 'a', 'thought', '.', 'i', \"'\", 'm', 'experimenting', 'these', 'days', ',', 'trying', 'to', 'let', 'go', 'of', 'this', 'anxiety', 'of', 'not', 'managing', 'to', 'focus', ',', 'i', \"'\", 'm', 'trying', 'to', 'let', 'all', 'the', 'thoughts', 'and', 'ideas', 'come', 'and', 'go', ',', 'even', 'if', 'it', \"'\", 's', 'overwhelming', 'at', 'first', 'and', 'see', 'if', 'it', 'helps', 'focusing', 'after', 'a', 'while', '.', 'thinking', 'i', 'am', 'ad', '##hd', '-', 'pi', 'myself', '.']\n",
      "INFO:__main__:Number of tokens: 181\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'a', 'thought', '.', 'ad', '##hd', 'as', 'a', 'brief', 'anxiety', '?', 'we', 'want', 'to', 'be', 'up', 'to', 'people', \"'\", 's', 'expectations', '.', 'could', 'hd', '##hd', 'be', 'like', 'a', 'tension', '?', 'like', 'a', 'brief', 'anxiety', ',', 'the', 'worry', 'that', 'we', \"'\", 'll', 'again', 'not', 'be', 'able', 'to', 'focus', ',', 'understand', 'or', 'remember', 'all', 'the', 'things', 'that', 'go', 'through', 'our', 'brain', 'and', 'that', 'we', 'wish', 'we', 'could', 'sort', 'and', 'remember', '?', 'all', 'the', 'information', 'we', '*', 'should', '*', 'be', 'remembering', '.', 'could', 'it', 'be', 'that', 'feeling', 'this', 'short', 'and', 'un', '##not', '##ice', '##able', 'anxiety', 'while', 'for', 'example', 'listening', 'to', 'someone', 'talking', ',', 'distract', '##s', 'us', 'and', 'makes', 'us', 'unable', 'to', 'focus', 'understand', 'or', 'remember', '?', 'this', 'is', 'just', 'a', 'thought', '.', 'i', \"'\", 'm', 'experimenting', 'these', 'days', ',', 'trying', 'to', 'let', 'go', 'of', 'this', 'anxiety', 'of', 'not', 'managing', 'to', 'focus', ',', 'i', \"'\", 'm', 'trying', 'to', 'let', 'all', 'the', 'thoughts', 'and', 'ideas', 'come', 'and', 'go', ',', 'even', 'if', 'it', \"'\", 's', 'overwhelming', 'at', 'first', 'and', 'see', 'if', 'it', 'helps', 'focusing', 'after', 'a', 'while', '.', 'thinking', 'i', 'am', 'ad', '##hd', '-', 'pi', 'myself', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['little', 'survey', 'about', 'marijuana', 'and', 'ad', '##hd', '.']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['little', 'survey', 'about', 'marijuana', 'and', 'ad', '##hd', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'can', \"'\", 't', 'control', 'my', 'life', 'and', 'i', \"'\", 've', 'done', 'just', 'about', 'everything', 'possible', ',', 'i', \"'\", 'm', 'exhausted', '.']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'can', \"'\", 't', 'control', 'my', 'life', 'and', 'i', \"'\", 've', 'done', 'just', 'about', 'everything', 'possible', ',', 'i', \"'\", 'm', 'exhausted', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'with', 'ad', '##hd', 'find', 'it', 'easier', 'to', 'concentrate', 'if', 'they', 'close', 'their', 'eyes', 'to', 'mentally', 'block', 'everything', 'around', 'them', '?', 'sometimes', 'at', 'work', 'i', 'have', 'a', 'lot', 'of', 'trouble', 'remembering', 'things', 'and', 'recently', 'i', 'started', 'closing', 'my', 'eyes', 'and', 'although', 'it', 'takes', 'a', 'second', 'or', 'two', 'it', 'comes', 'to', 'me', '.', 'i', 'think', 'its', 'because', 'whenever', 'i', 'was', 'doing', 'whatever', 'i', \"'\", 'm', 'trying', 'to', 'remember', 'my', 'mind', 'was', 'racing', 'and', 'i', 'wasn', \"'\", 't', 'fully', 'in', 'the', 'moment', '.', 'anyone', 'else', 'experience', 'this', '?', 'also', 'anyone', 'know', 'if', 'there', 'is', 'a', 'correlation', 'between', 'loss', 'of', 'memory', 'and', 'ad', '##hd', '?', 'i', 'find', 'that', 'since', 'it', \"'\", 's', 'hard', 'to', 'concentrate', 'then', 'it', \"'\", 'll', 'be', 'even', 'harder', 'to', 'remember', 'what', 'you', 'were', 'trying', 'to', 'concentrate', 'on', 'in', 'the', 'first', 'place', '.', '*', 'edited', 'for', 'grammar']\n",
      "INFO:__main__:Number of tokens: 138\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'with', 'ad', '##hd', 'find', 'it', 'easier', 'to', 'concentrate', 'if', 'they', 'close', 'their', 'eyes', 'to', 'mentally', 'block', 'everything', 'around', 'them', '?', 'sometimes', 'at', 'work', 'i', 'have', 'a', 'lot', 'of', 'trouble', 'remembering', 'things', 'and', 'recently', 'i', 'started', 'closing', 'my', 'eyes', 'and', 'although', 'it', 'takes', 'a', 'second', 'or', 'two', 'it', 'comes', 'to', 'me', '.', 'i', 'think', 'its', 'because', 'whenever', 'i', 'was', 'doing', 'whatever', 'i', \"'\", 'm', 'trying', 'to', 'remember', 'my', 'mind', 'was', 'racing', 'and', 'i', 'wasn', \"'\", 't', 'fully', 'in', 'the', 'moment', '.', 'anyone', 'else', 'experience', 'this', '?', 'also', 'anyone', 'know', 'if', 'there', 'is', 'a', 'correlation', 'between', 'loss', 'of', 'memory', 'and', 'ad', '##hd', '?', 'i', 'find', 'that', 'since', 'it', \"'\", 's', 'hard', 'to', 'concentrate', 'then', 'it', \"'\", 'll', 'be', 'even', 'harder', 'to', 'remember', 'what', 'you', 'were', 'trying', 'to', 'concentrate', 'on', 'in', 'the', 'first', 'place', '.', '*', 'edited', 'for', 'grammar']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'self', '-', 'organize', 'projects', '?', 'i', 'have', 'a', 'giant', 'project', '(', 'a', 'thesis', ')', 'that', 'is', 'just', 'overwhelming', ',', 'especially', 'since', 'i', \"'\", 've', 'never', 'done', 'anything', 'at', 'all', 'like', 'this', '.', 'it', \"'\", 's', 'anxiety', '-', 'inducing', ',', 'which', 'is', 'counter', '##pro', '##ductive', ',', 'and', 'makes', 'me', 'app', '##re', '##hen', '##sive', 'that', 'i', \"'\", 'm', 'about', 'to', 'crash', 'and', 'burn', 'my', 'whole', 'life', '.']\n",
      "INFO:__main__:Number of tokens: 68\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'self', '-', 'organize', 'projects', '?', 'i', 'have', 'a', 'giant', 'project', '(', 'a', 'thesis', ')', 'that', 'is', 'just', 'overwhelming', ',', 'especially', 'since', 'i', \"'\", 've', 'never', 'done', 'anything', 'at', 'all', 'like', 'this', '.', 'it', \"'\", 's', 'anxiety', '-', 'inducing', ',', 'which', 'is', 'counter', '##pro', '##ductive', ',', 'and', 'makes', 'me', 'app', '##re', '##hen', '##sive', 'that', 'i', \"'\", 'm', 'about', 'to', 'crash', 'and', 'burn', 'my', 'whole', 'life', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mornings', 'with', 'ad', '##hd', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mornings', 'with', 'ad', '##hd', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', '2nd', 'day', 'on', 'add', '##eral', '##l', 'and', 'pro', '##za', '##c', '.', '.', '.', 'i', 'feel', 'electric', '.', 'for', 'as', 'long', 'as', 'i', 'can', 'remember', 'iv', '##e', 'tapped', 'my', 'feet', 'or', 'bounced', 'my', 'legs', 'up', 'and', 'down', 'or', 'tapping', 'my', 'fingers', 'on', 'the', 'desk', ',', 'etc', '.', 'that', 'seems', 'to', 'be', '10', '##x', 'worse', 'now', 'but', 'im', 'able', 'to', 'focus', 'while', 'at', 'work', '.', 'im', 'currently', 'a', 'tattoo', 'apprentice', 'and', 'my', 'i', 'can', 'crank', 'out', 'pictures', 'but', 'my', 'hands', 'are', 'so', 'shaky', 'i', 'don', '##t', 'feel', 'good', 'with', 'a', 'machine', 'in', 'my', 'hands', '.', 'does', 'this', 'go', 'away', 'or', 'does', 'it', 'get', 'worse', '?']\n",
      "INFO:__main__:Number of tokens: 104\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', '2nd', 'day', 'on', 'add', '##eral', '##l', 'and', 'pro', '##za', '##c', '.', '.', '.', 'i', 'feel', 'electric', '.', 'for', 'as', 'long', 'as', 'i', 'can', 'remember', 'iv', '##e', 'tapped', 'my', 'feet', 'or', 'bounced', 'my', 'legs', 'up', 'and', 'down', 'or', 'tapping', 'my', 'fingers', 'on', 'the', 'desk', ',', 'etc', '.', 'that', 'seems', 'to', 'be', '10', '##x', 'worse', 'now', 'but', 'im', 'able', 'to', 'focus', 'while', 'at', 'work', '.', 'im', 'currently', 'a', 'tattoo', 'apprentice', 'and', 'my', 'i', 'can', 'crank', 'out', 'pictures', 'but', 'my', 'hands', 'are', 'so', 'shaky', 'i', 'don', '##t', 'feel', 'good', 'with', 'a', 'machine', 'in', 'my', 'hands', '.', 'does', 'this', 'go', 'away', 'or', 'does', 'it', 'get', 'worse', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['did', 'you', 'have', 'ad', '##hd', 'as', 'a', 'kid', '?', 'how', 'did', 'you', 'do', 'on', 'overnight', '##s', 'away', 'from', 'home', '?', 'son', 'going', 'away', 'for', '1st', 'time', '.', 'mom', 'worried', '.', 'my', 'son', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'he', 'was', 'in', 'second', 'grade', '.', 'rita', '##lin', 'has', 'made', 'a', 'huge', 'difference', 'in', 'his', 'life', ',', 'and', 'ours', 'as', 'a', 'family', '.', 'he', 'is', 'now', 'in', '5th', 'grade', 'and', 'preparing', 'to', 'go', 'on', 'a', '3', 'day', 'overnight', 'trip', 'with', 'his', 'class', '.', 'i', \"'\", 'm', 'worried', '.', 'as', 'most', 'of', 'you', 'know', ',', 'there', \"'\", 's', 'always', 'that', 'time', 'of', 'night', 'when', 'the', 'medication', 'wears', 'off', 'and', 'self', '-', 'control', 'goes', 'out', 'the', 'window', '.', 'at', 'home', 'we', 'can', 'give', 'him', 'a', 'short', '-', 'acting', 'pill', 'in', 'late', '-', 'afternoon', 'if', 'he', 'has', 'an', 'activity', ',', 'or', 'we', 'just', 'manage', 'the', 'rebound', 'ourselves', '.', 'we', \"'\", 've', 'had', 'lots', 'of', 'practice', '.', 'so', ',', 'for', 'those', 'of', 'you', 'who', 'had', 'ad', '##hd', 'when', 'you', 'were', 'young', '.', 'did', 'you', 'go', 'on', 'overnight', 'trips', '?', 'did', 'you', 'get', 'in', 'trouble', 'when', 'your', 'med', '##s', 'wore', 'off', '?', 'how', 'did', 'you', 'cope', '?', 'my', 'son', 'tends', 'to', 'be', 'very', 'hard', 'on', 'himself', 'and', 'i', 'so', 'want', 'this', 'to', 'be', 'a', 'positive', 'experience', 'for', 'him', '!']\n",
      "INFO:__main__:Number of tokens: 212\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['did', 'you', 'have', 'ad', '##hd', 'as', 'a', 'kid', '?', 'how', 'did', 'you', 'do', 'on', 'overnight', '##s', 'away', 'from', 'home', '?', 'son', 'going', 'away', 'for', '1st', 'time', '.', 'mom', 'worried', '.', 'my', 'son', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'he', 'was', 'in', 'second', 'grade', '.', 'rita', '##lin', 'has', 'made', 'a', 'huge', 'difference', 'in', 'his', 'life', ',', 'and', 'ours', 'as', 'a', 'family', '.', 'he', 'is', 'now', 'in', '5th', 'grade', 'and', 'preparing', 'to', 'go', 'on', 'a', '3', 'day', 'overnight', 'trip', 'with', 'his', 'class', '.', 'i', \"'\", 'm', 'worried', '.', 'as', 'most', 'of', 'you', 'know', ',', 'there', \"'\", 's', 'always', 'that', 'time', 'of', 'night', 'when', 'the', 'medication', 'wears', 'off', 'and', 'self', '-', 'control', 'goes', 'out', 'the', 'window', '.', 'at', 'home', 'we', 'can', 'give', 'him', 'a', 'short', '-', 'acting', 'pill', 'in', 'late', '-', 'afternoon', 'if', 'he', 'has', 'an', 'activity', ',', 'or', 'we', 'just', 'manage', 'the', 'rebound', 'ourselves', '.', 'we', \"'\", 've', 'had', 'lots', 'of', 'practice', '.', 'so', ',', 'for', 'those', 'of', 'you', 'who', 'had', 'ad', '##hd', 'when', 'you', 'were', 'young', '.', 'did', 'you', 'go', 'on', 'overnight', 'trips', '?', 'did', 'you', 'get', 'in', 'trouble', 'when', 'your', 'med', '##s', 'wore', 'off', '?', 'how', 'did', 'you', 'cope', '?', 'my', 'son', 'tends', 'to', 'be', 'very', 'hard', 'on', 'himself', 'and', 'i', 'so', 'want', 'this', 'to', 'be', 'a', 'positive', 'experience', 'for', 'him', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'should', 'be', 'going', 'to', 'sleep', 'right', 'now', '.', '.', '.', '.', '.', '.', 'but', 'instead', 'i', \"'\", 'm', 'filled', 'with', 'energy', 'and', 'want', 'to', 'go', 'outside', 'and', 'do', 'everything', 'possible', '.', 'ga', '##dda', '##m', '##n', 'brain', '.', 'where', 'were', 'you', 'when', 'i', 'had', 'to', 'write', 'that', 'paper', '?']\n",
      "INFO:__main__:Number of tokens: 49\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'should', 'be', 'going', 'to', 'sleep', 'right', 'now', '.', '.', '.', '.', '.', '.', 'but', 'instead', 'i', \"'\", 'm', 'filled', 'with', 'energy', 'and', 'want', 'to', 'go', 'outside', 'and', 'do', 'everything', 'possible', '.', 'ga', '##dda', '##m', '##n', 'brain', '.', 'where', 'were', 'you', 'when', 'i', 'had', 'to', 'write', 'that', 'paper', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hunters', 'and', 'farmers']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hunters', 'and', 'farmers']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['school', 'kills', 'creativity', '-', 'ken', 'robinson']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['school', 'kills', 'creativity', '-', 'ken', 'robinson']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['school', 'kills', 'creativity', '-', 'ken', 'robinson', '(', 'fixed', ')']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['school', 'kills', 'creativity', '-', 'ken', 'robinson', '(', 'fixed', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['diagnosis', 'from', 'general', 'doctor', '?']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['diagnosis', 'from', 'general', 'doctor', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['psychiatrist', 'prescribed', 'st', '##rat', '##tera', 'does', 'anyone', 'take', 'this', '?', 'i', 'am', 'just', 'kinda', 'worried', 'about', 'the', 'effectiveness', '.', 'i', 'want', 'something', 'that', 'actually', 'works', ',', 'not', 'something', 'that', 'takes', '4', 'weeks', 'just', 'to', 'notice', 'a', 'little', 'difference', '?', 'has', 'anyone', 'taken', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 44\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['psychiatrist', 'prescribed', 'st', '##rat', '##tera', 'does', 'anyone', 'take', 'this', '?', 'i', 'am', 'just', 'kinda', 'worried', 'about', 'the', 'effectiveness', '.', 'i', 'want', 'something', 'that', 'actually', 'works', ',', 'not', 'something', 'that', 'takes', '4', 'weeks', 'just', 'to', 'notice', 'a', 'little', 'difference', '?', 'has', 'anyone', 'taken', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mental', 'health', 'day', ':', 'regarding', 'mental', 'health', 'stigma']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mental', 'health', 'day', ':', 'regarding', 'mental', 'health', 'stigma']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ph', '##arm', '##acies', 'experience', 'shortage', 'of', 'popular', 'ad', '##hd', 'drug']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ph', '##arm', '##acies', 'experience', 'shortage', 'of', 'popular', 'ad', '##hd', 'drug']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'or', 'la', '##zine', '##ss', '?', 'how', 'does', 'one', 'tell', 'the', 'difference', '?', 'i', 'think', 'i', 'have', 'some', 'of', 'the', 'symptoms', 'of', 'ad', '##hd', ',', 'but', 'i', 'can', \"'\", 't', 'be', 'sure', 'until', 'i', 'get', 'diagnosed', ',', 'which', 'could', 'take', 'a', 'while', '.', 'in', 'the', 'meantime', ',', 'i', 'have', 'to', 'keep', 'working', 'on', 'homework', 'and', 'such', ',', 'which', 'i', 'am', 'constantly', 'pro', '##cr', '##ast', '##inating', 'on', '.', 'so', 'while', 'i', 'do', 'that', ',', 'i', 'keep', 'thinking', 'to', 'myself', 'how', 'ridiculous', 'it', 'is', 'that', 'i', 'can', \"'\", 't', 'do', 'the', 'time', '-', 'consuming', 'but', 'simple', 'stuff', '(', 'due', 'tomorrow', ')', 'i', 'have', 'to', 'do', 'to', 'keep', 'my', 'grade', 'up', ',', 'which', 'makes', 'me', 'feel', 'like', 'i', \"'\", 'm', 'lazy', 'and', 'stupid', 'and', 'like', 'i', 'just', 'found', 'some', 'disease', 'on', 'the', 'internet', 'that', 'describes', 'some', 'of', 'what', 'i', \"'\", 'm', 'going', 'through', 'and', 'used', 'that', 'as', 'an', 'excuse', 'for', 'why', 'i', \"'\", 'm', 'not', 'doing', 'my', 'work', '.', '(', 'and', 'my', 'parents', 'and', 'such', 'don', \"'\", 't', 'think', 'that', 'i', 'have', 'ad', '##hd', '.', ')', 'so', 'how', 'do', 'i', 'tell', 'the', 'difference', 'between', 'ad', '##hd', 'and', 'la', '##zine', '##ss', '?']\n",
      "INFO:__main__:Number of tokens: 189\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'or', 'la', '##zine', '##ss', '?', 'how', 'does', 'one', 'tell', 'the', 'difference', '?', 'i', 'think', 'i', 'have', 'some', 'of', 'the', 'symptoms', 'of', 'ad', '##hd', ',', 'but', 'i', 'can', \"'\", 't', 'be', 'sure', 'until', 'i', 'get', 'diagnosed', ',', 'which', 'could', 'take', 'a', 'while', '.', 'in', 'the', 'meantime', ',', 'i', 'have', 'to', 'keep', 'working', 'on', 'homework', 'and', 'such', ',', 'which', 'i', 'am', 'constantly', 'pro', '##cr', '##ast', '##inating', 'on', '.', 'so', 'while', 'i', 'do', 'that', ',', 'i', 'keep', 'thinking', 'to', 'myself', 'how', 'ridiculous', 'it', 'is', 'that', 'i', 'can', \"'\", 't', 'do', 'the', 'time', '-', 'consuming', 'but', 'simple', 'stuff', '(', 'due', 'tomorrow', ')', 'i', 'have', 'to', 'do', 'to', 'keep', 'my', 'grade', 'up', ',', 'which', 'makes', 'me', 'feel', 'like', 'i', \"'\", 'm', 'lazy', 'and', 'stupid', 'and', 'like', 'i', 'just', 'found', 'some', 'disease', 'on', 'the', 'internet', 'that', 'describes', 'some', 'of', 'what', 'i', \"'\", 'm', 'going', 'through', 'and', 'used', 'that', 'as', 'an', 'excuse', 'for', 'why', 'i', \"'\", 'm', 'not', 'doing', 'my', 'work', '.', '(', 'and', 'my', 'parents', 'and', 'such', 'don', \"'\", 't', 'think', 'that', 'i', 'have', 'ad', '##hd', '.', ')', 'so', 'how', 'do', 'i', 'tell', 'the', 'difference', 'between', 'ad', '##hd', 'and', 'la', '##zine', '##ss', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'not', 'really', 'doing', 'much']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'not', 'really', 'doing', 'much']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'my', 'first', 'academic', 'test', 'ci', '##rce', 'diagnosis', 'and', 'treatment', 'tomorrow', '!', 'i', 'feel', 'pretty', 'good', '.', '.', 'i', 'think', 'i', 'will', 'do', 'well', '.', 'i', 'am', 'a', 'bit', 'anxious', '.', 'anyway', '.', 'thanks', 'for', 'all', 'the', 'help', 'throughout', 'the', 'year', 'guys', '.', 'really', 'in', '##val', '##ua', '##ble', '.', 'will', 'update', ':', '-', ')']\n",
      "INFO:__main__:Number of tokens: 55\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'my', 'first', 'academic', 'test', 'ci', '##rce', 'diagnosis', 'and', 'treatment', 'tomorrow', '!', 'i', 'feel', 'pretty', 'good', '.', '.', 'i', 'think', 'i', 'will', 'do', 'well', '.', 'i', 'am', 'a', 'bit', 'anxious', '.', 'anyway', '.', 'thanks', 'for', 'all', 'the', 'help', 'throughout', 'the', 'year', 'guys', '.', 'really', 'in', '##val', '##ua', '##ble', '.', 'will', 'update', ':', '-', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['technology', 'and', 'the', 'adult', 'ad', '##hd', 'mom', ':', 'my', 'top', '5', 'life', '##sa', '##vers']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['technology', 'and', 'the', 'adult', 'ad', '##hd', 'mom', ':', 'my', 'top', '5', 'life', '##sa', '##vers']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['2nd', 'annual', 'ad', '##hd', 'awareness', 'expo', '(', 'upcoming', 'ad', '##hd', 'awareness', 'week', ',', 'starting', '16', 'oct', ')']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['2nd', 'annual', 'ad', '##hd', 'awareness', 'expo', '(', 'upcoming', 'ad', '##hd', 'awareness', 'week', ',', 'starting', '16', 'oct', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'approach', 'your', 'doctor', 'with', 'the', 'problem', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'approach', 'your', 'doctor', 'with', 'the', 'problem', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 'diagnosed', 'in', 'london', '(', 'the', 'uk', 'one', ')']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 'diagnosed', 'in', 'london', '(', 'the', 'uk', 'one', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'la', '##5', '##c', 'project', 'at', 'ucla', 'is', 'currently', 'looking', 'for', 'individuals', 'between', 'the', 'ages', 'of', '21', 'and', '50', 'who', 'have', 'been', 'diagnosed', 'with', 'schizophrenia', ',', 'bipolar', 'disorder', 'or', 'ad', '##hd', '.']\n",
      "INFO:__main__:Number of tokens: 32\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'la', '##5', '##c', 'project', 'at', 'ucla', 'is', 'currently', 'looking', 'for', 'individuals', 'between', 'the', 'ages', 'of', '21', 'and', '50', 'who', 'have', 'been', 'diagnosed', 'with', 'schizophrenia', ',', 'bipolar', 'disorder', 'or', 'ad', '##hd', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'in', 'a', 'relationship', '|', 'ad', '##hd', 'acceptance', 'blog', '-', 'add', '##itude']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'in', 'a', 'relationship', '|', 'ad', '##hd', 'acceptance', 'blog', '-', 'add', '##itude']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['attention', 'called', 'to', 'ad', '##hd', '-', 'opinion', '-', 'ho', '##usa', '##tonic', 'times', '(', 'might', 'be', 'a', 'good', 'one', 'to', 'share', 'to', 'non', '-', 'ad', '##hd', 'friends', '/', 'family', ')']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['attention', 'called', 'to', 'ad', '##hd', '-', 'opinion', '-', 'ho', '##usa', '##tonic', 'times', '(', 'might', 'be', 'a', 'good', 'one', 'to', 'share', 'to', 'non', '-', 'ad', '##hd', 'friends', '/', 'family', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', 'someone', 'help', 'me', 'with', 'ad', '##hd', 'med', '##s', '?', '3', 'months', 'ago', ',', 'i', 'underwent', '8', 'hours', 'of', 'psychological', 'testing', '.', 'it', 'was', 'determined', 'that', 'i', 'had', 'moderate', 'ad', '##hd', ',', 'mild', 'depression', 'and', 'mild', 'hopeless', '##ness', '.', 'i', 'think', 'everyone', 'knew', 'i', 'had', 'ad', '##hd', 'as', 'i', 'am', 'a', '24', 'year', 'old', 'male', 'that', 'is', 'super', 'hyper', '##active', 'and', 'i', 'also', 'have', 'been', '.', 'i', 'am', 'constantly', 'bored', 'and', 'i', 'am', 'habit', '##ually', 'late', '.', 'i', 'am', 'very', 'forget', '##ful', 'and', 'want', 'i', 'would', 'talk', 'your', 'ear', 'off', 'if', 'i', 'can', '.', 'i', 'have', 'to', 'constantly', 'stop', 'myself', 'from', 'dominating', 'a', 'conversation', '.', 'i', 'also', 'have', 'a', 'very', 'low', '\"', 'brain', 'to', 'mouth', 'filter', '\"', ',', 'in', 'other', 'words', ',', 'i', 'speak', 'my', 'mind', 'without', 'fear', 'of', 'backlash', '.', 'i', 'also', 'respond', 'to', 'feeling', 'like', 'i', 'am', 'in', 'trouble', '.', 'i', 'think', 'my', 'biggest', 'issue', 'is', 'that', 'as', 'i', 'get', 'older', ',', 'i', 'am', 'getting', 'more', 'and', 'more', 'angry', 'and', 'frustrated', '.', 'i', 'am', 'at', 'the', 'point', 'in', 'which', 'driving', 'makes', 'me', 'in', 'a', 'horrible', 'mood', '.', 'i', 'made', 'it', 'through', 'college', 'and', 'while', 'i', 'was', 'on', 'my', 'parents', 'insurance', ',', 'my', 'parents', 'told', 'me', 'i', 'couldn', \"'\", 't', 'go', 'see', 'a', 'psychologist', 'for', 'help', 'as', '\"', 'there', 'was', 'nothing', 'wrong', 'with', 'me', '\"', 'and', 'i', 'would', 'never', 'be', 'able', 'to', 'get', 'insurance', 'again', '.', 'i', 'got', 'a', 'really', 'nice', 'job', 'out', 'of', 'school', 'and', 'decided', 'i', 'needed', 'to', 'handle', 'some', 'personal', 'problems', 'with', 'my', 'past', 'with', 'a', 'therapist', '.', 'i', 'was', 'referred', 'to', 'around', 'and', 'we', 'are', 'still', 'trying', 'different', 'things', '.', 'i', 'first', 'tried', 'rita', '##lin', 'which', 'i', 'thought', 'worked', 'in', 'a', 'way', 'so', 'we', 'switched', 'to', 'the', 'extended', 'release', 'concert', '##a', '.', 'it', 'started', 'to', 'give', 'me', 'side', 'effects', 'like', 'headache', '##s', 'and', 'i', 'felt', 'super', 'ir', '##rita', '##ble', 'as', 'i', 'kept', 'taking', 'it', '.', 'i', 'was', 'switched', 'to', 'add', '##er', '##ral', 'extended', 'release', 'now', 'and', 'i', 'it', 'makes', 'me', 'super', 'wired', 'and', 'ji', '##tter', '##y', '.', 'the', 'focus', 'is', 'there', ',', 'but', 'man', 'do', 'i', 'feel', 'like', 'i', 'could', 'run', '10', 'miles', 'and', 'still', 'be', 'ready', 'to', 'go', '.', 'the', 'problem', 'is', 'when', 'it', 'wears', 'off', '.', 'i', 'am', 'just', 'in', 'a', 'horrible', 'mood', '.', 'i', 'am', 'pissed', 'off', 'at', 'the', 'world', '.', 'i', 'am', 'just', 'angry', 'and', 'nothing', 'is', 'right', '.', 'everything', 'sucks', '.', 'i', 'have', 'started', 'taking', 'it', 'at', 'noon', 'so', 'that', 'my', 'evenings', 'are', 'better', 'and', 'i', 'can', 'try', 'to', 'wind', 'down', 'without', 'piss', '##ing', 'off', 'my', 'girlfriend', '.', 'luckily', ',', 'she', 'somewhat', 'understands', 'how', 'i', 'operate', 'since', 'we', 'have', 'been', 'together', 'for', '5', 'years', '.', 'we', 'started', 'living', 'together', 'this', 'past', 'year', 'and', 'i', 'really', 'don', \"'\", 't', 'want', 'her', 'to', 'hate', 'being', 'around', 'me', 'because', 'my', 'mood', 'is', 'so', 'bad', 'when', 'i', 'get', 'home', '.', 'has', 'anyone', 'had', 'an', 'experience', 'like', 'this', 'and', 'what', 'have', 'you', 'done', '/', 'take', 'to', 'make', 'yourself', 'not', 'so', 'angry', '?', 'i', 'think', 'the', 'add', '##eral', '##l', '/', 'rita', '##lin', 'help', 'in', 'a', 'way', ',', 'but', 'i', 'want', 'to', 'be', 'able', 'to', 'calm', 'down', 'and', 'relax', '.', 'i', 'have', 'smoked', 'weed', 'in', 'the', 'past', 'and', 'i', 'enjoy', 'that', 'calm', 'focused', 'feeling', 'tremendous', '##ly', ',', 'but', 'i', 'tend', 'to', 'only', 'do', 'it', 'once', 'in', 'awhile', 'now', '.', 'i', 'also', 'don', \"'\", 't', 'want', 'to', 'be', 'stone', '##d', 'all', 'the', 'time', 'as', 'i', 'have', 'to', 'work', 'with', 'people', 'that', 'would', 'be', 'able', 'to', 'tell', '.', 'i', 'seem', 'my', 'psychologist', 'on', 'tuesday', 'and', 'i', 'would', 'appreciate', 'any', 'suggestions', 'of', 'med', '##s', 'or', 'treatments', '?', 'thanks', '!', 'edit', ':', 'grammar', 'and', 'spelling']\n",
      "INFO:__main__:Number of tokens: 594\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['can', 'someone', 'help', 'me', 'with', 'ad', '##hd', 'med', '##s', '?', '3', 'months', 'ago', ',', 'i', 'underwent', '8', 'hours', 'of', 'psychological', 'testing', '.', 'it', 'was', 'determined', 'that', 'i', 'had', 'moderate', 'ad', '##hd', ',', 'mild', 'depression', 'and', 'mild', 'hopeless', '##ness', '.', 'i', 'think', 'everyone', 'knew', 'i', 'had', 'ad', '##hd', 'as', 'i', 'am', 'a', '24', 'year', 'old', 'male', 'that', 'is', 'super', 'hyper', '##active', 'and', 'i', 'also', 'have', 'been', '.', 'i', 'am', 'constantly', 'bored', 'and', 'i', 'am', 'habit', '##ually', 'late', '.', 'i', 'am', 'very', 'forget', '##ful', 'and', 'want', 'i', 'would', 'talk', 'your', 'ear', 'off', 'if', 'i', 'can', '.', 'i', 'have', 'to', 'constantly', 'stop', 'myself', 'from', 'dominating', 'a', 'conversation', '.', 'i', 'also', 'have', 'a', 'very', 'low', '\"', 'brain', 'to', 'mouth', 'filter', '\"', ',', 'in', 'other', 'words', ',', 'i', 'speak', 'my', 'mind', 'without', 'fear', 'of', 'backlash', '.', 'i', 'also', 'respond', 'to', 'feeling', 'like', 'i', 'am', 'in', 'trouble', '.', 'i', 'think', 'my', 'biggest', 'issue', 'is', 'that', 'as', 'i', 'get', 'older', ',', 'i', 'am', 'getting', 'more', 'and', 'more', 'angry', 'and', 'frustrated', '.', 'i', 'am', 'at', 'the', 'point', 'in', 'which', 'driving', 'makes', 'me', 'in', 'a', 'horrible', 'mood', '.', 'i', 'made', 'it', 'through', 'college', 'and', 'while', 'i', 'was', 'on', 'my', 'parents', 'insurance', ',', 'my', 'parents', 'told', 'me', 'i', 'couldn', \"'\", 't', 'go', 'see', 'a', 'psychologist', 'for', 'help', 'as', '\"', 'there', 'was', 'nothing', 'wrong', 'with', 'me', '\"', 'and', 'i', 'would', 'never', 'be', 'able', 'to', 'get', 'insurance', 'again', '.', 'i', 'got', 'a', 'really', 'nice', 'job', 'out', 'of', 'school', 'and', 'decided', 'i', 'needed', 'to', 'handle', 'some', 'personal', 'problems', 'with', 'my', 'past', 'with', 'a', 'therapist', '.', 'i', 'was', 'referred', 'to', 'around', 'and', 'we', 'are', 'still', 'trying', 'different', 'things', '.', 'i', 'first', 'tried', 'rita', '##lin', 'which', 'i', 'thought', 'worked', 'in', 'a', 'way', 'so', 'we', 'switched', 'to', 'the', 'extended', 'release', 'concert', '##a', '.', 'it', 'started', 'to', 'give', 'me', 'side', 'effects', 'like', 'headache', '##s', 'and', 'i', 'felt', 'super', 'ir', '##rita', '##ble', 'as', 'i', 'kept', 'taking', 'it', '.', 'i', 'was', 'switched', 'to', 'add', '##er', '##ral', 'extended', 'release', 'now', 'and', 'i', 'it', 'makes', 'me', 'super', 'wired', 'and', 'ji', '##tter', '##y', '.', 'the', 'focus', 'is', 'there', ',', 'but', 'man', 'do', 'i', 'feel', 'like', 'i', 'could', 'run', '10', 'miles', 'and', 'still', 'be', 'ready', 'to', 'go', '.', 'the', 'problem', 'is', 'when', 'it', 'wears', 'off', '.', 'i', 'am', 'just', 'in', 'a', 'horrible', 'mood', '.', 'i', 'am', 'pissed', 'off', 'at', 'the', 'world', '.', 'i', 'am', 'just', 'angry', 'and', 'nothing', 'is', 'right', '.', 'everything', 'sucks', '.', 'i', 'have', 'started', 'taking', 'it', 'at', 'noon', 'so', 'that', 'my', 'evenings', 'are', 'better', 'and', 'i', 'can', 'try', 'to', 'wind', 'down', 'without', 'piss', '##ing', 'off', 'my', 'girlfriend', '.', 'luckily', ',', 'she', 'somewhat', 'understands', 'how', 'i', 'operate', 'since', 'we', 'have', 'been', 'together', 'for', '5', 'years', '.', 'we', 'started', 'living', 'together', 'this', 'past', 'year', 'and', 'i', 'really', 'don', \"'\", 't', 'want', 'her', 'to', 'hate', 'being', 'around', 'me', 'because', 'my', 'mood', 'is', 'so', 'bad', 'when', 'i', 'get', 'home', '.', 'has', 'anyone', 'had', 'an', 'experience', 'like', 'this', 'and', 'what', 'have', 'you', 'done', '/', 'take', 'to', 'make', 'yourself', 'not', 'so', 'angry', '?', 'i', 'think', 'the', 'add', '##eral', '##l', '/', 'rita', '##lin', 'help', 'in', 'a', 'way', ',', 'but', 'i', 'want', 'to', 'be', 'able', 'to', 'calm', 'down', 'and', 'relax'], ['.', 'i', 'have', 'smoked', 'weed', 'in', 'the', 'past', 'and', 'i', 'enjoy', 'that', 'calm', 'focused', 'feeling', 'tremendous', '##ly', ',', 'but', 'i', 'tend', 'to', 'only', 'do', 'it', 'once', 'in', 'awhile', 'now', '.', 'i', 'also', 'don', \"'\", 't', 'want', 'to', 'be', 'stone', '##d', 'all', 'the', 'time', 'as', 'i', 'have', 'to', 'work', 'with', 'people', 'that', 'would', 'be', 'able', 'to', 'tell', '.', 'i', 'seem', 'my', 'psychologist', 'on', 'tuesday', 'and', 'i', 'would', 'appreciate', 'any', 'suggestions', 'of', 'med', '##s', 'or', 'treatments', '?', 'thanks', '!', 'edit', ':', 'grammar', 'and', 'spelling']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sh', '##hh', '##hh', '##h', '!', 'i', 'have', 'ad', '##hd', '!', 'can', 'you', 'keep', 'a', 'secret', '?', '|', 'ad', '##hd', 'man', 'of', 'distraction']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sh', '##hh', '##hh', '##h', '!', 'i', 'have', 'ad', '##hd', '!', 'can', 'you', 'keep', 'a', 'secret', '?', '|', 'ad', '##hd', 'man', 'of', 'distraction']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['counter', 'to', 'dry', '-', 'mouth', '?', 'has', 'anyone', 'found', 'something', 'effective', 'for', 'counter', '##ing', 'dry', '-', 'mouth', 'when', 'taking', 'add', '##eral', '##l', '?', 'i', 'know', 'the', 'general', 'stuff', ',', 'drink', 'plenty', 'of', 'fluids', ',', 'etc', '.', 'does', 'anyone', 'have', 'any', 'other', 'recommendations', 'that', 'may', 'help', 'them', '?']\n",
      "INFO:__main__:Number of tokens: 47\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['counter', 'to', 'dry', '-', 'mouth', '?', 'has', 'anyone', 'found', 'something', 'effective', 'for', 'counter', '##ing', 'dry', '-', 'mouth', 'when', 'taking', 'add', '##eral', '##l', '?', 'i', 'know', 'the', 'general', 'stuff', ',', 'drink', 'plenty', 'of', 'fluids', ',', 'etc', '.', 'does', 'anyone', 'have', 'any', 'other', 'recommendations', 'that', 'may', 'help', 'them', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'awareness', 'events', 'planned', '-', 'phil', '##ly', '##bu', '##rb', '##s', '.', 'com', ':', 'community', ':', 'doyle', '##stown']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'awareness', 'events', 'planned', '-', 'phil', '##ly', '##bu', '##rb', '##s', '.', 'com', ':', 'community', ':', 'doyle', '##stown']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'ad', '##hd', 'drugs', 'boost', 'do', '##pa', '##mine', 'levels', '?', '|', 'ps', '##ych', 'central', 'news']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'ad', '##hd', 'drugs', 'boost', 'do', '##pa', '##mine', 'levels', '?', '|', 'ps', '##ych', 'central', 'news']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'american', 'academy', 'of', 'pediatric', '##s', 'has', 'expanded', 'the', 'age', 'range', 'for', 'the', 'diagnosis', 'and', 'treatment', 'of', 'ad', '##hd', 'to', 'children', 'as', 'young', 'as', '4', 'and', 'as', 'old', 'as', '18']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'american', 'academy', 'of', 'pediatric', '##s', 'has', 'expanded', 'the', 'age', 'range', 'for', 'the', 'diagnosis', 'and', 'treatment', 'of', 'ad', '##hd', 'to', 'children', 'as', 'young', 'as', '4', 'and', 'as', 'old', 'as', '18']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'manual', 'points', 'to', 'a', 'fresh', 'approach', 'to', 'd', '##ys', '##le', '##xia']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'manual', 'points', 'to', 'a', 'fresh', 'approach', 'to', 'd', '##ys', '##le', '##xia']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['happy', 'ad', '##hd', 'awareness', 'week', '!']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['happy', 'ad', '##hd', 'awareness', 'week', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['blend', '##tec', 'guy', 'blend', '##s', 'at', 'uv', '##u', '(', 'the', '\"', 'will', 'it', 'blend', '?', '\"', 'inventor', 'has', 'ad', '##hd', ')']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['blend', '##tec', 'guy', 'blend', '##s', 'at', 'uv', '##u', '(', 'the', '\"', 'will', 'it', 'blend', '?', '\"', 'inventor', 'has', 'ad', '##hd', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'ad', '##hd', 'and', 'i', 'am', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'ad', '##hd', 'and', 'i', 'am', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'multi', '##mo', '##dal', 'treatment', ':', 'soul', 'food', ',', 'part', 'ii']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'multi', '##mo', '##dal', 'treatment', ':', 'soul', 'food', ',', 'part', 'ii']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'am', 'pretty', 'sure', 'now', 'that', 'i', 'have', 'ad', '##hd', ',', 'please', 'help', 'me']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'am', 'pretty', 'sure', 'now', 'that', 'i', 'have', 'ad', '##hd', ',', 'please', 'help', 'me']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'aa', '##p', 'ad', '##hd', 'guide', '##line', 'expands', 'age', 'range', ',', 'scope']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'aa', '##p', 'ad', '##hd', 'guide', '##line', 'expands', 'age', 'range', ',', 'scope']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'left', 'myself', 'logged', 'into', 'red', '##dit', 'and', 'my', 'friend', 'saw', 'and', 'decided', 'to', 'make', 'a', 'self', 'post', 'in', 'my', 'name', '.']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'left', 'myself', 'logged', 'into', 'red', '##dit', 'and', 'my', 'friend', 'saw', 'and', 'decided', 'to', 'make', 'a', 'self', 'post', 'in', 'my', 'name', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '.', '.', '.', 'a', 'g', '##lor', '##ified', 'breathing', 'problem', 'for', 'most', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '.', '.', '.', 'a', 'g', '##lor', '##ified', 'breathing', 'problem', 'for', 'most', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['17', 'year', 'old', ',', 'inhibitor', '##y', 'ad', '##hd', '.']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['17', 'year', 'old', ',', 'inhibitor', '##y', 'ad', '##hd', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['syn', '##ap', '##to', '##l', 'add', 'ad', '##hd', 'sy', '##mpt', '##om', 'relief', 'medicine', 'for', 'children', 'and', 'adults', '.', 'all', '-', 'natural', 'home', '##opa', '##thic', 'medicine', 'quickly', 'relieve', '##s', 'add', 'ad', '##hd', 'symptoms', 'including', 'hyper', '##act', '##ivity', ',', 'ina', '##tten', '##tive', '##ness', ',', 'and', 'difficulty', 'concentrating', '-', '-', '-', 'i', 'feel', 'bad', 'for', 'the', 'kids', 'who', \"'\", 's', 'parents', 'think', 'this', 'will', 'actually', 'help', '.']\n",
      "INFO:__main__:Number of tokens: 63\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['syn', '##ap', '##to', '##l', 'add', 'ad', '##hd', 'sy', '##mpt', '##om', 'relief', 'medicine', 'for', 'children', 'and', 'adults', '.', 'all', '-', 'natural', 'home', '##opa', '##thic', 'medicine', 'quickly', 'relieve', '##s', 'add', 'ad', '##hd', 'symptoms', 'including', 'hyper', '##act', '##ivity', ',', 'ina', '##tten', '##tive', '##ness', ',', 'and', 'difficulty', 'concentrating', '-', '-', '-', 'i', 'feel', 'bad', 'for', 'the', 'kids', 'who', \"'\", 's', 'parents', 'think', 'this', 'will', 'actually', 'help', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['learning', '-', 'disabled', 'students', 'get', 'firm', '##er', 'grip', 'on', 'college', '–', 'usa', '##to', '##day', '.', 'com']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['learning', '-', 'disabled', 'students', 'get', 'firm', '##er', 'grip', 'on', 'college', '–', 'usa', '##to', '##day', '.', 'com']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['robert', 'to', '##th', 'on', 'ad', '##hd', 'and', 'developing', 'creativity', '|', 'the', 'creative', 'mind']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['robert', 'to', '##th', 'on', 'ad', '##hd', 'and', 'developing', 'creativity', '|', 'the', 'creative', 'mind']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['card', '##io', 'and', 'ad', '##hd', 'i', 'talked', 'to', 'my', 'nutrition', 'professor', 'one', 'day', 'about', 'ad', '##hd', 'and', 'the', 'side', 'effects', 'of', 'drugs', 'and', 'she', 'recommended', 'that', 'i', 'do', 'intensive', 'card', '##io', 'for', 'an', 'hour', '(', 'about', '75', '%', 'to', '85', '%', 'of', 'your', 'max', 'heart', 'rate', ')', 'every', 'day', 'and', 'see', 'how', 'if', 'it', 'helps', '.', '*', 'maximum', 'heart', 'rate', 'is', '220', '-', 'age', '.', '*', '{', '[', '(', 'max', 'heart', 'rate', '-', 'resting', 'heart', 'rate', ')', 'x', 'the', 'intensity', '(', '.', '75', 'to', '.', '85', ')', ']', '+', 'resting', 'heart', 'rate', '}', 'well', ',', 'i', \"'\", 've', 'actually', 'tried', 'and', 'it', '(', 'albeit', ',', 'not', 'regularly', 'cause', 'i', \"'\", 'm', 'a', 'bum', ')', 'it', 'does', 'seem', 'to', 'help', ',', 'especially', 'with', 'the', 'mental', 'exhaustion', 'and', 'fatigue', 'after', 'a', 'long', 'day', 'on', 'add', '##eral', '##l', 'x', '##r', '.', 'i', 'don', \"'\", 't', 'recommend', 'doing', 'this', 'while', 'on', 'the', 'effects', 'of', 'st', '##im', '##ula', '##nt', 'medications', 'such', 'as', 'add', '##eral', '##l', ',', 'v', '##y', '##van', '##se', ',', 'etc', '.', 'hope', 'this', 'helps', '!']\n",
      "INFO:__main__:Number of tokens: 172\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['card', '##io', 'and', 'ad', '##hd', 'i', 'talked', 'to', 'my', 'nutrition', 'professor', 'one', 'day', 'about', 'ad', '##hd', 'and', 'the', 'side', 'effects', 'of', 'drugs', 'and', 'she', 'recommended', 'that', 'i', 'do', 'intensive', 'card', '##io', 'for', 'an', 'hour', '(', 'about', '75', '%', 'to', '85', '%', 'of', 'your', 'max', 'heart', 'rate', ')', 'every', 'day', 'and', 'see', 'how', 'if', 'it', 'helps', '.', '*', 'maximum', 'heart', 'rate', 'is', '220', '-', 'age', '.', '*', '{', '[', '(', 'max', 'heart', 'rate', '-', 'resting', 'heart', 'rate', ')', 'x', 'the', 'intensity', '(', '.', '75', 'to', '.', '85', ')', ']', '+', 'resting', 'heart', 'rate', '}', 'well', ',', 'i', \"'\", 've', 'actually', 'tried', 'and', 'it', '(', 'albeit', ',', 'not', 'regularly', 'cause', 'i', \"'\", 'm', 'a', 'bum', ')', 'it', 'does', 'seem', 'to', 'help', ',', 'especially', 'with', 'the', 'mental', 'exhaustion', 'and', 'fatigue', 'after', 'a', 'long', 'day', 'on', 'add', '##eral', '##l', 'x', '##r', '.', 'i', 'don', \"'\", 't', 'recommend', 'doing', 'this', 'while', 'on', 'the', 'effects', 'of', 'st', '##im', '##ula', '##nt', 'medications', 'such', 'as', 'add', '##eral', '##l', ',', 'v', '##y', '##van', '##se', ',', 'etc', '.', 'hope', 'this', 'helps', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'have', 'ad', '##hd', 'and', 'doesn', \"'\", 't', 'drink', 'water', 'much', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'have', 'ad', '##hd', 'and', 'doesn', \"'\", 't', 'drink', 'water', 'much', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['epilogue', ':', 'sh', '##hh', '##hh', '##h', '!', 'i', 'have', 'ad', '##hd', '!', '|', 'ad', '##hd', 'man', 'of', 'distraction']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['epilogue', ':', 'sh', '##hh', '##hh', '##h', '!', 'i', 'have', 'ad', '##hd', '!', '|', 'ad', '##hd', 'man', 'of', 'distraction']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['u', '##w', 'to', 'present', 'a', 'dark', 'comedy', 'about', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['u', '##w', 'to', 'present', 'a', 'dark', 'comedy', 'about', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['driven', 'to', 'vs', 'delivered', 'from', 'distraction', 'i', \"'\", 've', 'heard', '[', 'driven', 'to', 'distraction', ']', '(', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'driven', '-', 'distraction', '-', 'revised', '-', 'recognizing', '-', 'ebook', '/', 'd', '##p', '/', 'b', '##00', '##5', '##gf', '##ii', '##6', '##2', '/', ')', '(', 'this', 'edition', 'is', 'revised', ')', 'suggested', 'a', 'lot', ',', 'but', 'the', 'authors', 'have', 'another', 'book', 'out', 'called', '[', 'delivered', 'from', 'distraction', ']', '(', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'delivered', '-', 'from', '-', 'distraction', '-', 'ebook', '/', 'd', '##p', '/', 'b', '##00', '##0', '##fc', '##k', '##l', '##wk', ')', '.', 'is', 'there', 'a', 'difference', '?', 'if', 'so', ',', 'which', 'is', 'better', '?', '(', 'i', \"'\", 'm', 'not', 'diagnosed', 'yet', ',', 'so', 'i', 'have', 'little', 'idea', 'whether', 'i', 'actually', 'have', 'it', '.', ')', 'also', ',', 'the', 'revision', 'of', 'driven', 'to', 'distraction', 'appears', 'to', 'have', 'been', 'done', 'in', '2011', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'what', 'revisions', 'were', 'made', '.']\n",
      "INFO:__main__:Number of tokens: 161\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['driven', 'to', 'vs', 'delivered', 'from', 'distraction', 'i', \"'\", 've', 'heard', '[', 'driven', 'to', 'distraction', ']', '(', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'driven', '-', 'distraction', '-', 'revised', '-', 'recognizing', '-', 'ebook', '/', 'd', '##p', '/', 'b', '##00', '##5', '##gf', '##ii', '##6', '##2', '/', ')', '(', 'this', 'edition', 'is', 'revised', ')', 'suggested', 'a', 'lot', ',', 'but', 'the', 'authors', 'have', 'another', 'book', 'out', 'called', '[', 'delivered', 'from', 'distraction', ']', '(', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'delivered', '-', 'from', '-', 'distraction', '-', 'ebook', '/', 'd', '##p', '/', 'b', '##00', '##0', '##fc', '##k', '##l', '##wk', ')', '.', 'is', 'there', 'a', 'difference', '?', 'if', 'so', ',', 'which', 'is', 'better', '?', '(', 'i', \"'\", 'm', 'not', 'diagnosed', 'yet', ',', 'so', 'i', 'have', 'little', 'idea', 'whether', 'i', 'actually', 'have', 'it', '.', ')', 'also', ',', 'the', 'revision', 'of', 'driven', 'to', 'distraction', 'appears', 'to', 'have', 'been', 'done', 'in', '2011', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'what', 'revisions', 'were', 'made', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['edge', 'ad', '##hd', 'coaching', 'on', 'cnn']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['edge', 'ad', '##hd', 'coaching', 'on', 'cnn']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'sleeping', 'i', \"'\", 'm', 'on', '36', '##mg', 'concert', '##a', 'tablets', 'for', 'my', 'ad', '##hd', ',', 'they', \"'\", 're', 'really', 'working', 'well', 'and', 'helping', 'big', 'time', 'day', 'to', 'day', 'the', 'only', 'problem', 'i', 'have', 'is', 'that', 'it', 'wears', 'off', 'in', 'the', 'evening', 'and', 'by', 'the', 'time', 'i', 'go', 'to', 'bed', 'my', 'mind', 'is', 'racing', '.', 'i', 'have', 'real', 'difficulty', 'getting', 'to', 'sleep', 'and', 'i', 'was', 'wondering', 'if', 'anyone', 'had', 'any', 'techniques', 'for', 'si', '##len', '##cing', 'the', 'constant', 'barrage', 'of', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 83\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'sleeping', 'i', \"'\", 'm', 'on', '36', '##mg', 'concert', '##a', 'tablets', 'for', 'my', 'ad', '##hd', ',', 'they', \"'\", 're', 'really', 'working', 'well', 'and', 'helping', 'big', 'time', 'day', 'to', 'day', 'the', 'only', 'problem', 'i', 'have', 'is', 'that', 'it', 'wears', 'off', 'in', 'the', 'evening', 'and', 'by', 'the', 'time', 'i', 'go', 'to', 'bed', 'my', 'mind', 'is', 'racing', '.', 'i', 'have', 'real', 'difficulty', 'getting', 'to', 'sleep', 'and', 'i', 'was', 'wondering', 'if', 'anyone', 'had', 'any', 'techniques', 'for', 'si', '##len', '##cing', 'the', 'constant', 'barrage', 'of', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##y', 'and', 'lam', '##ic', '##tal', '?', 'do', 'any', 'of', 'you', 'take', 'this', 'combination', '?', 'i', 'take', '10', 'mg', 'of', 'add', '##eral', '##l', 'and', '200', 'mg', 'of', 'lam', '##ic', '##tal', '.', 'i', 'was', 'wondering', 'if', 'anyone', 'can', 'share', 'their', 'experiences', 'with', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 43\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##y', 'and', 'lam', '##ic', '##tal', '?', 'do', 'any', 'of', 'you', 'take', 'this', 'combination', '?', 'i', 'take', '10', 'mg', 'of', 'add', '##eral', '##l', 'and', '200', 'mg', 'of', 'lam', '##ic', '##tal', '.', 'i', 'was', 'wondering', 'if', 'anyone', 'can', 'share', 'their', 'experiences', 'with', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['in', 'a', 'hole', 'i', 'went', 'to', 'an', 'ad', '##hd', 'evaluation', 'appointment', 'with', 'a', 'psychiatrist', ',', 'who', ',', 'after', 'one', '45', 'minute', 'session', ',', 'prescribed', 'me', 'add', '##er', '##ral', 'for', 'it', '.', 'my', 'cousin', 'thinks', 'i', 'should', 'throw', 'the', 'prescription', 'slip', 'away', 'and', 'go', 'to', 'a', 'phd', 'psychologist', 'to', 'get', 'diagnosed', 'and', 'use', 'therapy', 'instead', 'of', 'med', '##s', '.', 'i', 'feel', 'like', 'i', 'know', 'what', 'to', 'do', 'anyway', '(', 'take', 'med', '##s', 'while', 'improving', 'lifestyle', 'choices', '[', 'health', 'exercise', ']', ')', '.', 'anyway', 'i', 'don', \"'\", 't', 'have', 'time', 'or', 'money', 'to', 'throw', 'away', 'right', 'now', '.', '.', '.', 'what', 'do', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 103\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['in', 'a', 'hole', 'i', 'went', 'to', 'an', 'ad', '##hd', 'evaluation', 'appointment', 'with', 'a', 'psychiatrist', ',', 'who', ',', 'after', 'one', '45', 'minute', 'session', ',', 'prescribed', 'me', 'add', '##er', '##ral', 'for', 'it', '.', 'my', 'cousin', 'thinks', 'i', 'should', 'throw', 'the', 'prescription', 'slip', 'away', 'and', 'go', 'to', 'a', 'phd', 'psychologist', 'to', 'get', 'diagnosed', 'and', 'use', 'therapy', 'instead', 'of', 'med', '##s', '.', 'i', 'feel', 'like', 'i', 'know', 'what', 'to', 'do', 'anyway', '(', 'take', 'med', '##s', 'while', 'improving', 'lifestyle', 'choices', '[', 'health', 'exercise', ']', ')', '.', 'anyway', 'i', 'don', \"'\", 't', 'have', 'time', 'or', 'money', 'to', 'throw', 'away', 'right', 'now', '.', '.', '.', 'what', 'do', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['shire', 'expands', 'scholarship', 'program', 'for', 'individuals', 'with', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['shire', 'expands', 'scholarship', 'program', 'for', 'individuals', 'with', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', 'to', 'vent', 'my', 'brother', 'is', 'pretending', 'he', 'has', 'ad', '##hd', '.', 'what', 'the', 'fuck', '.', 'we', 'all', ',', 'in', 'my', 'house', ',', 'know', 'he', 'routinely', 'lies', 'to', 'get', 'sympathy', 'or', 'get', 'excuses', 'to', 'be', 'lazy', '.', 'he', 'has', 'been', 'doing', 'poorly', 'in', 'college', 'so', 'far', 'and', ',', 'instead', 'of', 'admitting', 'to', 'my', 'parents', 'that', 'he', 'is', 'just', 'slack', '##ing', 'off', 'and', 'party', '##ing', 'with', 'friends', 'as', 'usual', ',', 'he', 'is', 'passing', 'it', 'off', 'as', ',', \"'\", 'oh', 'poor', 'poor', 'me', 'i', 'have', 'a', 'disorder', 'it', \"'\", 's', 'not', 'my', 'fault', \"'\", '.', 'having', 'ad', '##hd', 'myself', 'i', 'can', 'easily', 'tell', 'he', 'doesn', \"'\", 't', 'have', 'it', '.', 'the', 'difference', 'is', 'night', 'and', 'fucking', 'day', '.', 'he', 'doesn', \"'\", 't', 'even', 'have', 'hyper', '##att', '##ent', '##ion', 'for', 'chris', '##sa', '##ke', '.', 'but', 'guess', 'what', '?', 'after', 'a', '30', 'minute', 'doctor', 'visit', 'he', 'got', 'prescribed', 'the', 'medication', '.', 'he', 'refuses', 'to', 'tell', 'anyone', 'what', 'went', 'on', 'at', 'the', 'doctor', \"'\", 's', '-', 'exercising', 'his', 'shiny', 'new', \"'\", 'right', \"'\", 'as', 'an', '18', 'year', 'old', 'not', 'to', 'disclose', 'i', 'guess', '-', 'and', 'gives', 'short', ',', '2', 'word', 'vague', 'answers', 'whenever', 'anyone', 'asks', 'him', 'if', 'the', 'med', '##s', 'are', 'doing', 'anything', '.', 'words', 'cannot', 'describe', 'how', 'pissed', 'off', 'i', 'am', '.', 'so', 'much', 'of', 'society', 'has', 'no', 'goddamn', '*', 'clue', '*', 'what', 'ad', '##hd', 'actually', 'is', '.', 'it', 'is', 'not', 'oh', 'look', 'a', 'fucking', 'squirrel', '.', 'ar', '##gh', '##hh', '##h', '.']\n",
      "INFO:__main__:Number of tokens: 238\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', 'to', 'vent', 'my', 'brother', 'is', 'pretending', 'he', 'has', 'ad', '##hd', '.', 'what', 'the', 'fuck', '.', 'we', 'all', ',', 'in', 'my', 'house', ',', 'know', 'he', 'routinely', 'lies', 'to', 'get', 'sympathy', 'or', 'get', 'excuses', 'to', 'be', 'lazy', '.', 'he', 'has', 'been', 'doing', 'poorly', 'in', 'college', 'so', 'far', 'and', ',', 'instead', 'of', 'admitting', 'to', 'my', 'parents', 'that', 'he', 'is', 'just', 'slack', '##ing', 'off', 'and', 'party', '##ing', 'with', 'friends', 'as', 'usual', ',', 'he', 'is', 'passing', 'it', 'off', 'as', ',', \"'\", 'oh', 'poor', 'poor', 'me', 'i', 'have', 'a', 'disorder', 'it', \"'\", 's', 'not', 'my', 'fault', \"'\", '.', 'having', 'ad', '##hd', 'myself', 'i', 'can', 'easily', 'tell', 'he', 'doesn', \"'\", 't', 'have', 'it', '.', 'the', 'difference', 'is', 'night', 'and', 'fucking', 'day', '.', 'he', 'doesn', \"'\", 't', 'even', 'have', 'hyper', '##att', '##ent', '##ion', 'for', 'chris', '##sa', '##ke', '.', 'but', 'guess', 'what', '?', 'after', 'a', '30', 'minute', 'doctor', 'visit', 'he', 'got', 'prescribed', 'the', 'medication', '.', 'he', 'refuses', 'to', 'tell', 'anyone', 'what', 'went', 'on', 'at', 'the', 'doctor', \"'\", 's', '-', 'exercising', 'his', 'shiny', 'new', \"'\", 'right', \"'\", 'as', 'an', '18', 'year', 'old', 'not', 'to', 'disclose', 'i', 'guess', '-', 'and', 'gives', 'short', ',', '2', 'word', 'vague', 'answers', 'whenever', 'anyone', 'asks', 'him', 'if', 'the', 'med', '##s', 'are', 'doing', 'anything', '.', 'words', 'cannot', 'describe', 'how', 'pissed', 'off', 'i', 'am', '.', 'so', 'much', 'of', 'society', 'has', 'no', 'goddamn', '*', 'clue', '*', 'what', 'ad', '##hd', 'actually', 'is', '.', 'it', 'is', 'not', 'oh', 'look', 'a', 'fucking', 'squirrel', '.', 'ar', '##gh', '##hh', '##h', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'fall', 'asleep', 'when', 'they', 'read', '?', 'it', 'doesn', \"'\", 't', 'always', 'happen', ',', 'but', 'it', 'is', 'consistent', 'enough', 'to', 'be', 'annoying', '.', 'even', 'on', 'add', '##eral', '##l', ',', 'i', 'will', 'just', 'start', 'nodding', 'off', 'as', 'i', 'try', 'to', 'read', ',', 'which', 'means', 'my', 'retention', 'drops', 'to', 'zero', '.', 'anyone', 'else', 'get', 'this', '?', 'how', 'do', 'you', 'fight', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 61\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'fall', 'asleep', 'when', 'they', 'read', '?', 'it', 'doesn', \"'\", 't', 'always', 'happen', ',', 'but', 'it', 'is', 'consistent', 'enough', 'to', 'be', 'annoying', '.', 'even', 'on', 'add', '##eral', '##l', ',', 'i', 'will', 'just', 'start', 'nodding', 'off', 'as', 'i', 'try', 'to', 'read', ',', 'which', 'means', 'my', 'retention', 'drops', 'to', 'zero', '.', 'anyone', 'else', 'get', 'this', '?', 'how', 'do', 'you', 'fight', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['relaxation', 'is', 'one', 'of', 'the', 'main', 'things', 'needed', 'to', 'overcome', 'your', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['relaxation', 'is', 'one', 'of', 'the', 'main', 'things', 'needed', 'to', 'overcome', 'your', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'diagnosed', 'with', 'ad', '##hd', '.', 'question', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'diagnosed', 'with', 'ad', '##hd', '.', 'question', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['doctor', 'keeps', 'wanting', 'to', 'try', 'for', 'depression', 'without', 'screening', 'for', 'ad', '##hd', 't', '##l', ';', 'dr', 'for', 'the', 'ad', '##hd', 'crowd', '.', 'my', 'psychiatrist', 'keeps', 'wanting', 'to', 'med', '##icate', 'for', 'depression', 'even', 'though', 'i', 'told', 'him', 'i', 'think', 'i', 'have', 'ad', '##hd', '.', 'the', 'latest', 'med', 'is', 'k', '##lon', '##op', '##in', 'and', 'i', 'feel', 'like', 'he', \"'\", 's', 'just', 'trying', 'not', 'to', 'be', 'wrong', 'now', 'and', 'i', 'haven', \"'\", 't', 'taken', 'the', 'k', '##lon', '##op', '##in', 'yet', 'for', 'fear', 'of', 'addiction', '.', 'hi', 'all', '.', 'i', 'work', 'in', 'the', 'medical', 'industry', 'in', 'graphic', 'design', ',', 'video', ',', 'and', 'animation', 'and', 'at', 'some', 'point', 'i', 'was', 'doing', 'a', 'video', 'and', 'flash', 'program', 'for', 'concert', '##a', '.', 'you', 'may', 'have', 'seen', 'it', 'with', 'ethan', ',', 'jack', ',', 'and', 'some', 'adult', 'woman', '.', 'at', 'about', 'this', 'time', 'i', 'had', 'to', 'an', '##imate', 'the', 'as', '##rs', 'and', 'i', \"'\", 've', 'check', '##mark', '##ed', 'almost', 'all', 'of', 'the', 'boxes', 'into', 'the', 'gray', 'spots', 'on', 'the', 'as', '##rs', 'in', 'section', 'a', 'and', 'all', 'but', '2', 'in', 'section', 'b', '.', 'i', \"'\", 've', 'been', 'trying', 'to', 'be', 'treated', 'for', 'depression', 'for', '9', '-', '10', 'years', 'with', 'about', '15', 'med', '##s', 'that', 'haven', \"'\", 't', 'worked', '.', 'e', '##ffe', '##x', '##or', 'slightly', 'worked', 'and', 'i', 'hear', 'that', 'it', 'can', 'work', 'with', 'ad', '##hd', 'and', 'nor', '##ep', '##ine', '##ph', '##rine', 'levels', '.', 'i', 'just', 'wanted', 'to', 'list', 'some', 'things', 'that', 'i', \"'\", 'm', 'not', 'sure', 'about', 'and', 'see', 'if', 'the', 'ad', '##hd', 'community', 'thinks', 'it', 'could', 'be', 'a', 'possibility', '.', 'the', 'more', 'fire', '##power', 'i', 'have', 'going', 'into', 'this', 'might', 'make', 'him', 'change', 'his', 'mind', '.', 'even', 'great', 'doctors', 'aren', \"'\", 't', 'quite', 'as', 'informed', 'as', 'they', 'should', 'be', 'about', 'ad', '##hd', 'or', 'think', 'it', \"'\", 's', 'much', 'less', 'common', 'than', 'depression', 'or', 'bipolar', 'or', 'just', 'anxiety', 'disorders', '.', 'edit', ':', 'also', ',', 'i', 'know', 'not', 'everyone', 'reads', 'the', 'whole', 'thing', 'but', 'like', 'i', 'said', 'in', 'grade', 'school', 'i', 'was', 'super', 'hyper', ',', 'had', 'crazy', 'energy', 'and', 'stuff', '.', 'back', 'then', 'you', 'barely', 'got', 'diagnosed', 'for', 'add', 'and', 'i', 'don', \"'\", 't', 'even', 'think', 'ad', '##hd', 'was', 'a', 'thing', 'yet', '.', 'i', \"'\", 've', 'always', ',', 'before', 'drinking', 'or', 'smoking', 'ci', '##gs', ',', 'was', 'super', 'forget', '##ful', '.', 'baseball', 'glove', ',', 'left', 'it', 'at', 'home', ',', 'keys', ',', 'locked', 'them', 'in', 'my', 'car', ',', 'wallet', ',', 'gone', ',', 'football', 'helmet', 'left', 'at', 'home', ',', 'homework', 'left', 'on', 'table', ',', 'etc', '.', 'sometimes', 'i', 'would', 'rush', 'in', 'to', 'get', 'my', 'book', 'before', 'the', 'bus', 'came', ',', 'leave', 'my', 'bag', ',', 'grab', 'my', 'book', 'and', 'then', 'forget', ',', 'rush', 'back', 'in', 'grab', 'my', 'bag', '.', 'my', 'lunch', 'was', 'still', 'on', 'the', 'counter', '*', 'smack', '##s', 'for', '##head', '*', '.', '1', '.', 'had', 'to', 'use', 'a', 'timer', 'to', 'get', 'my', 'school', 'work', 'done', 'in', 'elementary', 'and', 'i', 'still', 'couldn', \"'\", 't', 'get', 'it', 'done', 'fast', 'enough', '.', '1', '.', 'had', 'so', 'much', 'energy', 'i', \"'\", 'd', 'put', 'my', 'fingers', 'of', 'both', 'hands', 'together', 'and', 'shake', 'when', 'i', 'was', 'younger', '1', '.', 'pissed', 'my', 'bed', 'til', 'i', 'was', 'in', '6th', 'grade', '(', 'en', '##ures', '##is', 'has', 'been', 'linked', 'to', 'ad', '##hd', ')', '1', '.', 'would', 'always', 'shut', 'down', 'when', 'i', 'was', 'criticized', '.', 'i', 'still', 'do', '.', 'i', 'try', 'harder', 'now', 'to', 'accept', 'res', '##pon', '##si', '##bl', '##ity', 'though', 'but', 'i', \"'\", 'll', 'shut', 'down', 'hard', 'core', 'if', 'i', 'didn', \"'\", 't', 'do', 'something', 'right', '.', '1', '.', 'i', 'take', 'a', 'very', 'long', 'time', 'in', 'doing', 'things', 'so', 'that', 'i', 'can', 'double', 'check', 'everything', 'and', 'then', 'i', 'still', 'forgot', 'to', 'do', 'something', 'or', 'something', 'is', 'still', 'wrong', 'or', 'out', 'of', 'place', '.', '1', '.', 'had', 'an', 'excited', 'tick', ',', 'felt', 'like', 'so', 'much', 'energy', 'in', 'my', 'body', '.', '1', '.', 'in', 'high', 'school', ',', 'i', 'always', 'got', 'a', \"'\", 's', 'and', 'b', \"'\", 's', 'but', 'would', 'take', 'forever', 'to', 'finish', 'tests', '.', 'sometimes', 'i', \"'\", 'd', 'ask', 'to', 'come', 'back', 'and', 'finish', 'it', 'or', 'stay', 'late', 'to', 'finish', 'it', '.', '1', '.', 'always', 'steal', 'the', 'show', 'with', 'some', 'super', 'hyper', 'anti', '##c', '.', '1', '.', 'con', '##tan', '##tly', 'distracted', 'at', 'work', '.', '1', '.', 'get', 's', '##ni', '##ppy', 'when', 'too', 'many', 'people', 'talk', 'at', 'once', '.', '1', '.', 'get', 's', '##ni', '##ppy', 'when', 'nobody', 'takes', 'any', 'action', 'and', 'sort', '##a', 'just', 'mean', '##ders', 'around', '.', '1', '.', 'messy', '1', '.', 'get', 'really', 'interested', 'in', 'sports', 'or', 'activities', 'or', 'art', '/', 'music', 'projects', 'and', 'can', 'never', 'finish', 'them', 'or', 'never', 'keep', 'with', 'the', 'activity', '.', '1', '.', 'drink', '3', '-', '4', 'days', 'a', 'week', 'never', 'weekends', '.', '4', '-', '8', 'beers', '.', 'no', 'other', 'drugs', '.', 'a', 'lot', 'of', 'ad', '##hd', 'people', 'abuse', 'al', '##cho', '##hol', 'and', 'ci', '##gs', '.', '1', '.', 'i', 'chain', 'smoke', 'but', 'only', 'after', '7', 'or', '8', 'pm', '.', '1', '.', 'chu', '##g', 'any', 'liquid', 'or', 'food', 'instead', 'of', 'sipping', 'or', 'enjoying', 'it', '.', 'could', 'be', 'water', ',', 'could', 'be', 'liquor', '.', 'wolf', 'down', 'dinner', 'before', 'everyone', 'has', 'eaten', '1', '/', '4', '(', 'and', 'i', \"'\", 'm', 'not', 'a', 'food', 'addict', 'or', 'obe', '##se', '.', ')', '1', '.', 'constantly', 'forgetting', 'stuff', '1', '.', 'look', 'like', 'an', 'asshole', 'when', 'talking', 'to', 'someone', 'because', 'i', \"'\", 'm', 'not', 'entirely', 'paying', 'attention', 'to', 'them', 'but', 'fake', 'smiling', ',', 'nodding', 'and', 'saying', ',', 'uh', '-', 'huh', '.', 'sometimes', 'i', \"'\", 'll', 'say', ',', '\"', 'that', \"'\", 's', 'cool', '.', '\"', 'when', 'it', \"'\", 's', 'supposed', 'to', 'suck', '.', '1', '.', 'huge', 'pro', '##cr', '##ast', '##inator', '.', '1', '.', 'did', 'i', 'mention', 'huge', 'pro', '##cr', '##ast', '##inator', '?', '1', '.', 'get', 'depressed', 'but', 'only', 'because', 'i', 'never', 'finish', 'anything', 'or', 'lose', 'interest', 'in', 'something', 'that', 'i', 'know', 'would', 'be', 'bad', '##ass', 'if', 'i', 'finished', 'it', '(', 'i', \"'\", 'm', 'an', 'extremely', 'good', 'musician', 'and', 'artist', 'im', '##o', '.', ')', '1', '.', 'tired', 'a', 'lot', '(', 'mostly', 'from', 'drinking', 'but', 'tired', 'from', 'stimulation', '.', ')', '1', '.', 'can', \"'\", 't', 'sleep', 'at', 'night', 'without', 'a', 'ben', '##od', '##ryl', 'or', 'some', 'drinks', '.', 'will', 'even', 'get', 'hammered', 'and', 'still', 'wake', 'up', 'only', 'after', '3', 'hours', 'of', 'sleep', '.', '1', '.', 'love', 'being', 'around', 'people', 'but', 'super', 'anti', '-', 'social', '.', 'i', 'feel', 'alien', '##ated', 'for', 'some', 'reason', '.', 'like', 'i', 'don', \"'\", 't', 'belong', '.', 'like', 'i', \"'\", 'm', 'weird', '.', '1', '.', 'women', '?', 'forget', 'about', 'it', '.', 'hard', 'time', 'with', 'dating', '.', 'barely', 'have', 'dated', 'in', 'the', 'last', '10', 'years', '.', 'all', 'part', 'of', 'the', 'anti', '-', 'social', 'thing', '.', '1', '.', 'i', 'never', 'feel', 'good', 'enough', '.', '1', '.', 'feel', 'like', 'not', 'enough', 'oxygen', 'going', 'to', 'my', 'brain', '/', 'body', '.', '1', '.', 'want', 'to', 'give', 'up', 'exercise', 'goal', 'instead', 'of', 'go', 'the', 'full', 'way', 'cause', 'i', 'hate', 'physical', 'pain', '.', 'sound', 'like', 'ad', '##hd', 'to', 'you', 'guys', 'or', 'depression', '?', 'i', 'figure', 'if', 'these', 'sound', 'similar', 'to', 'add', 'then', 'i', 'will', 'approach', 'my', 'doctor', 'one', 'more', 'time', 'and', 'possibly', 'point', 'to', 'this', 'red', '##dit', '.', 'i', \"'\", 've', 'already', 'taken', 'the', 'ham', 'd', 'as', 'well', 'like', 'i', 'already', 'said', 'and', 'scored', 'high', '.']\n",
      "INFO:__main__:Number of tokens: 1160\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['doctor', 'keeps', 'wanting', 'to', 'try', 'for', 'depression', 'without', 'screening', 'for', 'ad', '##hd', 't', '##l', ';', 'dr', 'for', 'the', 'ad', '##hd', 'crowd', '.', 'my', 'psychiatrist', 'keeps', 'wanting', 'to', 'med', '##icate', 'for', 'depression', 'even', 'though', 'i', 'told', 'him', 'i', 'think', 'i', 'have', 'ad', '##hd', '.', 'the', 'latest', 'med', 'is', 'k', '##lon', '##op', '##in', 'and', 'i', 'feel', 'like', 'he', \"'\", 's', 'just', 'trying', 'not', 'to', 'be', 'wrong', 'now', 'and', 'i', 'haven', \"'\", 't', 'taken', 'the', 'k', '##lon', '##op', '##in', 'yet', 'for', 'fear', 'of', 'addiction', '.', 'hi', 'all', '.', 'i', 'work', 'in', 'the', 'medical', 'industry', 'in', 'graphic', 'design', ',', 'video', ',', 'and', 'animation', 'and', 'at', 'some', 'point', 'i', 'was', 'doing', 'a', 'video', 'and', 'flash', 'program', 'for', 'concert', '##a', '.', 'you', 'may', 'have', 'seen', 'it', 'with', 'ethan', ',', 'jack', ',', 'and', 'some', 'adult', 'woman', '.', 'at', 'about', 'this', 'time', 'i', 'had', 'to', 'an', '##imate', 'the', 'as', '##rs', 'and', 'i', \"'\", 've', 'check', '##mark', '##ed', 'almost', 'all', 'of', 'the', 'boxes', 'into', 'the', 'gray', 'spots', 'on', 'the', 'as', '##rs', 'in', 'section', 'a', 'and', 'all', 'but', '2', 'in', 'section', 'b', '.', 'i', \"'\", 've', 'been', 'trying', 'to', 'be', 'treated', 'for', 'depression', 'for', '9', '-', '10', 'years', 'with', 'about', '15', 'med', '##s', 'that', 'haven', \"'\", 't', 'worked', '.', 'e', '##ffe', '##x', '##or', 'slightly', 'worked', 'and', 'i', 'hear', 'that', 'it', 'can', 'work', 'with', 'ad', '##hd', 'and', 'nor', '##ep', '##ine', '##ph', '##rine', 'levels', '.', 'i', 'just', 'wanted', 'to', 'list', 'some', 'things', 'that', 'i', \"'\", 'm', 'not', 'sure', 'about', 'and', 'see', 'if', 'the', 'ad', '##hd', 'community', 'thinks', 'it', 'could', 'be', 'a', 'possibility', '.', 'the', 'more', 'fire', '##power', 'i', 'have', 'going', 'into', 'this', 'might', 'make', 'him', 'change', 'his', 'mind', '.', 'even', 'great', 'doctors', 'aren', \"'\", 't', 'quite', 'as', 'informed', 'as', 'they', 'should', 'be', 'about', 'ad', '##hd', 'or', 'think', 'it', \"'\", 's', 'much', 'less', 'common', 'than', 'depression', 'or', 'bipolar', 'or', 'just', 'anxiety', 'disorders', '.', 'edit', ':', 'also', ',', 'i', 'know', 'not', 'everyone', 'reads', 'the', 'whole', 'thing', 'but', 'like', 'i', 'said', 'in', 'grade', 'school', 'i', 'was', 'super', 'hyper', ',', 'had', 'crazy', 'energy', 'and', 'stuff', '.', 'back', 'then', 'you', 'barely', 'got', 'diagnosed', 'for', 'add', 'and', 'i', 'don', \"'\", 't', 'even', 'think', 'ad', '##hd', 'was', 'a', 'thing', 'yet', '.', 'i', \"'\", 've', 'always', ',', 'before', 'drinking', 'or', 'smoking', 'ci', '##gs', ',', 'was', 'super', 'forget', '##ful', '.', 'baseball', 'glove', ',', 'left', 'it', 'at', 'home', ',', 'keys', ',', 'locked', 'them', 'in', 'my', 'car', ',', 'wallet', ',', 'gone', ',', 'football', 'helmet', 'left', 'at', 'home', ',', 'homework', 'left', 'on', 'table', ',', 'etc', '.', 'sometimes', 'i', 'would', 'rush', 'in', 'to', 'get', 'my', 'book', 'before', 'the', 'bus', 'came', ',', 'leave', 'my', 'bag', ',', 'grab', 'my', 'book', 'and', 'then', 'forget', ',', 'rush', 'back', 'in', 'grab', 'my', 'bag', '.', 'my', 'lunch', 'was', 'still', 'on', 'the', 'counter', '*', 'smack', '##s', 'for', '##head', '*', '.', '1', '.', 'had', 'to', 'use', 'a', 'timer', 'to', 'get', 'my', 'school', 'work', 'done', 'in', 'elementary', 'and', 'i', 'still', 'couldn', \"'\", 't', 'get', 'it', 'done', 'fast', 'enough', '.', '1', '.', 'had', 'so', 'much', 'energy', 'i', \"'\", 'd', 'put', 'my', 'fingers', 'of', 'both', 'hands', 'together', 'and', 'shake', 'when', 'i', 'was', 'younger', '1', '.', 'pissed', 'my', 'bed', 'til', 'i', 'was', 'in', '6th', 'grade', '(', 'en', '##ures', '##is'], ['has', 'been', 'linked', 'to', 'ad', '##hd', ')', '1', '.', 'would', 'always', 'shut', 'down', 'when', 'i', 'was', 'criticized', '.', 'i', 'still', 'do', '.', 'i', 'try', 'harder', 'now', 'to', 'accept', 'res', '##pon', '##si', '##bl', '##ity', 'though', 'but', 'i', \"'\", 'll', 'shut', 'down', 'hard', 'core', 'if', 'i', 'didn', \"'\", 't', 'do', 'something', 'right', '.', '1', '.', 'i', 'take', 'a', 'very', 'long', 'time', 'in', 'doing', 'things', 'so', 'that', 'i', 'can', 'double', 'check', 'everything', 'and', 'then', 'i', 'still', 'forgot', 'to', 'do', 'something', 'or', 'something', 'is', 'still', 'wrong', 'or', 'out', 'of', 'place', '.', '1', '.', 'had', 'an', 'excited', 'tick', ',', 'felt', 'like', 'so', 'much', 'energy', 'in', 'my', 'body', '.', '1', '.', 'in', 'high', 'school', ',', 'i', 'always', 'got', 'a', \"'\", 's', 'and', 'b', \"'\", 's', 'but', 'would', 'take', 'forever', 'to', 'finish', 'tests', '.', 'sometimes', 'i', \"'\", 'd', 'ask', 'to', 'come', 'back', 'and', 'finish', 'it', 'or', 'stay', 'late', 'to', 'finish', 'it', '.', '1', '.', 'always', 'steal', 'the', 'show', 'with', 'some', 'super', 'hyper', 'anti', '##c', '.', '1', '.', 'con', '##tan', '##tly', 'distracted', 'at', 'work', '.', '1', '.', 'get', 's', '##ni', '##ppy', 'when', 'too', 'many', 'people', 'talk', 'at', 'once', '.', '1', '.', 'get', 's', '##ni', '##ppy', 'when', 'nobody', 'takes', 'any', 'action', 'and', 'sort', '##a', 'just', 'mean', '##ders', 'around', '.', '1', '.', 'messy', '1', '.', 'get', 'really', 'interested', 'in', 'sports', 'or', 'activities', 'or', 'art', '/', 'music', 'projects', 'and', 'can', 'never', 'finish', 'them', 'or', 'never', 'keep', 'with', 'the', 'activity', '.', '1', '.', 'drink', '3', '-', '4', 'days', 'a', 'week', 'never', 'weekends', '.', '4', '-', '8', 'beers', '.', 'no', 'other', 'drugs', '.', 'a', 'lot', 'of', 'ad', '##hd', 'people', 'abuse', 'al', '##cho', '##hol', 'and', 'ci', '##gs', '.', '1', '.', 'i', 'chain', 'smoke', 'but', 'only', 'after', '7', 'or', '8', 'pm', '.', '1', '.', 'chu', '##g', 'any', 'liquid', 'or', 'food', 'instead', 'of', 'sipping', 'or', 'enjoying', 'it', '.', 'could', 'be', 'water', ',', 'could', 'be', 'liquor', '.', 'wolf', 'down', 'dinner', 'before', 'everyone', 'has', 'eaten', '1', '/', '4', '(', 'and', 'i', \"'\", 'm', 'not', 'a', 'food', 'addict', 'or', 'obe', '##se', '.', ')', '1', '.', 'constantly', 'forgetting', 'stuff', '1', '.', 'look', 'like', 'an', 'asshole', 'when', 'talking', 'to', 'someone', 'because', 'i', \"'\", 'm', 'not', 'entirely', 'paying', 'attention', 'to', 'them', 'but', 'fake', 'smiling', ',', 'nodding', 'and', 'saying', ',', 'uh', '-', 'huh', '.', 'sometimes', 'i', \"'\", 'll', 'say', ',', '\"', 'that', \"'\", 's', 'cool', '.', '\"', 'when', 'it', \"'\", 's', 'supposed', 'to', 'suck', '.', '1', '.', 'huge', 'pro', '##cr', '##ast', '##inator', '.', '1', '.', 'did', 'i', 'mention', 'huge', 'pro', '##cr', '##ast', '##inator', '?', '1', '.', 'get', 'depressed', 'but', 'only', 'because', 'i', 'never', 'finish', 'anything', 'or', 'lose', 'interest', 'in', 'something', 'that', 'i', 'know', 'would', 'be', 'bad', '##ass', 'if', 'i', 'finished', 'it', '(', 'i', \"'\", 'm', 'an', 'extremely', 'good', 'musician', 'and', 'artist', 'im', '##o', '.', ')', '1', '.', 'tired', 'a', 'lot', '(', 'mostly', 'from', 'drinking', 'but', 'tired', 'from', 'stimulation', '.', ')', '1', '.', 'can', \"'\", 't', 'sleep', 'at', 'night', 'without', 'a', 'ben', '##od', '##ryl', 'or', 'some', 'drinks', '.', 'will', 'even', 'get', 'hammered', 'and', 'still', 'wake', 'up', 'only', 'after', '3', 'hours', 'of', 'sleep', '.', '1', '.', 'love', 'being', 'around', 'people', 'but', 'super', 'anti', '-', 'social', '.', 'i', 'feel', 'alien', '##ated', 'for', 'some', 'reason', '.', 'like', 'i', 'don'], [\"'\", 't', 'belong', '.', 'like', 'i', \"'\", 'm', 'weird', '.', '1', '.', 'women', '?', 'forget', 'about', 'it', '.', 'hard', 'time', 'with', 'dating', '.', 'barely', 'have', 'dated', 'in', 'the', 'last', '10', 'years', '.', 'all', 'part', 'of', 'the', 'anti', '-', 'social', 'thing', '.', '1', '.', 'i', 'never', 'feel', 'good', 'enough', '.', '1', '.', 'feel', 'like', 'not', 'enough', 'oxygen', 'going', 'to', 'my', 'brain', '/', 'body', '.', '1', '.', 'want', 'to', 'give', 'up', 'exercise', 'goal', 'instead', 'of', 'go', 'the', 'full', 'way', 'cause', 'i', 'hate', 'physical', 'pain', '.', 'sound', 'like', 'ad', '##hd', 'to', 'you', 'guys', 'or', 'depression', '?', 'i', 'figure', 'if', 'these', 'sound', 'similar', 'to', 'add', 'then', 'i', 'will', 'approach', 'my', 'doctor', 'one', 'more', 'time', 'and', 'possibly', 'point', 'to', 'this', 'red', '##dit', '.', 'i', \"'\", 've', 'already', 'taken', 'the', 'ham', 'd', 'as', 'well', 'like', 'i', 'already', 'said', 'and', 'scored', 'high', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['students', 'with', 'add', 'and', 'ad', '##hd', '(', 'cross', '-', 'post', 'from', 'r', '/', 'teaching', ')', 'i', 'posted', 'this', 'a', 'couple', 'of', 'weeks', 'ago', 'in', 'r', '/', 'teaching', 'and', 'did', 'not', 'get', 'any', 'comments', '.', 'i', 'teach', 'elementary', 'school', '-', '4th', 'grade', '.', 'many', 'of', 'my', 'students', 'have', 'been', 'diagnosed', '(', 'and', 'many', 'appear', 'to', 'have', 'the', 'symptoms', 'but', 'aren', \"'\", 't', 'diagnosed', ')', 'with', 'add', 'and', '/', 'or', 'ad', '##hd', '.', 'to', 'accommodate', 'them', ',', 'i', 'give', 'these', 'students', 'numerous', 'breaks', '(', 'they', 'will', 'often', 'run', 'er', '##rand', '##s', 'for', 'me', ')', 'are', 'allowed', 'to', 'work', 'standing', 'up', ',', 'and', 'can', 'choose', 'to', 'complete', 'their', 'independent', 'work', 'in', 'a', '\"', 'study', 'spot', '\"', 'to', 'help', 'them', 'from', 'being', 'distracted', '.', 'however', ',', 'i', 'struggle', 'with', 'helping', 'students', 'deal', 'with', 'imp', '##ulsive', 'behavior', '.', 'i', 'would', 'like', 'to', 'create', 'a', 'classroom', 'environment', 'that', 'will', 'stimulate', 'instead', 'of', 'hind', '##er', 'my', 'students', 'with', 'add', 'and', 'ad', '##hd', '.', 'what', 'advice', 'would', 'you', 'give', 'to', 'teachers', 'to', 'help', 'these', 'students', '?', 'what', 'advice', 'can', 'i', 'give', 'to', 'parents', 'who', 'want', 'to', 'help', 'their', 'child', 'develop', 'concentration', 'and', 'focus', '?']\n",
      "INFO:__main__:Number of tokens: 186\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['students', 'with', 'add', 'and', 'ad', '##hd', '(', 'cross', '-', 'post', 'from', 'r', '/', 'teaching', ')', 'i', 'posted', 'this', 'a', 'couple', 'of', 'weeks', 'ago', 'in', 'r', '/', 'teaching', 'and', 'did', 'not', 'get', 'any', 'comments', '.', 'i', 'teach', 'elementary', 'school', '-', '4th', 'grade', '.', 'many', 'of', 'my', 'students', 'have', 'been', 'diagnosed', '(', 'and', 'many', 'appear', 'to', 'have', 'the', 'symptoms', 'but', 'aren', \"'\", 't', 'diagnosed', ')', 'with', 'add', 'and', '/', 'or', 'ad', '##hd', '.', 'to', 'accommodate', 'them', ',', 'i', 'give', 'these', 'students', 'numerous', 'breaks', '(', 'they', 'will', 'often', 'run', 'er', '##rand', '##s', 'for', 'me', ')', 'are', 'allowed', 'to', 'work', 'standing', 'up', ',', 'and', 'can', 'choose', 'to', 'complete', 'their', 'independent', 'work', 'in', 'a', '\"', 'study', 'spot', '\"', 'to', 'help', 'them', 'from', 'being', 'distracted', '.', 'however', ',', 'i', 'struggle', 'with', 'helping', 'students', 'deal', 'with', 'imp', '##ulsive', 'behavior', '.', 'i', 'would', 'like', 'to', 'create', 'a', 'classroom', 'environment', 'that', 'will', 'stimulate', 'instead', 'of', 'hind', '##er', 'my', 'students', 'with', 'add', 'and', 'ad', '##hd', '.', 'what', 'advice', 'would', 'you', 'give', 'to', 'teachers', 'to', 'help', 'these', 'students', '?', 'what', 'advice', 'can', 'i', 'give', 'to', 'parents', 'who', 'want', 'to', 'help', 'their', 'child', 'develop', 'concentration', 'and', 'focus', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['today', 'is', 'the', 'second', 'day', 'that', 'i', 'am', 'drinking', 'coffee', 'and', 'it', 'seems', 'to', 'be', 'working', '!', '!', '!', '*', '*', 'i', 'know', 'this', 'is', 'too', 'long', 'to', 'read', 'for', 'us', 'ad', '##hd', 'pee', '##ps', '*', '*', 'i', \"'\", 've', 'hated', 'coffee', 'all', 'my', 'life', '.', 'it', 'makes', 'me', 'sleepy', 'and', 'slow', '.', 'before', 'yesterday', ',', 'i', 'drank', 'coffee', 'three', 'times', 'in', 'my', 'life', 'and', 'hated', 'it', ',', 'not', 'for', 'the', 'taste', 'but', 'for', 'the', 'fa', '##b', '.', 'the', 'other', 'day', 'i', 'was', 'watching', 'dr', '.', 'oz', 'and', 'there', 'was', 'a', 'husband', '/', 'wife', 'team', 'talking', 'about', 'ad', '##hd', 'and', 'he', 'stated', 'medicine', 'never', 'worked', 'for', 'him', ',', 'yet', 'coffee', 'did', '.', 'i', 'bought', 'coffee', 'the', 'other', 'day', 'and', 'yesterday', 'i', 'drank', 'it', 'and', 'noticed', 'i', 'was', 'calm', ',', 'mel', '##low', '.', '.', '.', '.', 'started', 'ya', '##wn', '##ing', 'after', 'about', '3', '-', '4', 'hr', '##s', 'yet', 'was', 'able', 'to', 'read', 'through', 'a', 'mutual', 'fund', 'prospect', '##us', 'without', 'any', 'distraction', '##s', 'or', 'jumping', 'around', '.', 'i', \"'\", 've', 'used', 'concert', '##a', 'in', 'college', 'yet', 'it', 'didn', \"'\", 't', 'work', ';', 'not', 'even', 'at', 'a', 'high', 'dos', '##age', '.', 'now', 'out', 'of', 'college', ',', 'unemployed', 'and', 'no', 'insurance', '.', '.', '.', '.', 'not', 'getting', 'anything', 'done', 'i', 'needed', 'to', 'figure', 'out', 'another', 'solution', 'for', 'myself', '.', 'so', 'far', 'it', \"'\", 's', 'working', '.']\n",
      "INFO:__main__:Number of tokens: 222\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['today', 'is', 'the', 'second', 'day', 'that', 'i', 'am', 'drinking', 'coffee', 'and', 'it', 'seems', 'to', 'be', 'working', '!', '!', '!', '*', '*', 'i', 'know', 'this', 'is', 'too', 'long', 'to', 'read', 'for', 'us', 'ad', '##hd', 'pee', '##ps', '*', '*', 'i', \"'\", 've', 'hated', 'coffee', 'all', 'my', 'life', '.', 'it', 'makes', 'me', 'sleepy', 'and', 'slow', '.', 'before', 'yesterday', ',', 'i', 'drank', 'coffee', 'three', 'times', 'in', 'my', 'life', 'and', 'hated', 'it', ',', 'not', 'for', 'the', 'taste', 'but', 'for', 'the', 'fa', '##b', '.', 'the', 'other', 'day', 'i', 'was', 'watching', 'dr', '.', 'oz', 'and', 'there', 'was', 'a', 'husband', '/', 'wife', 'team', 'talking', 'about', 'ad', '##hd', 'and', 'he', 'stated', 'medicine', 'never', 'worked', 'for', 'him', ',', 'yet', 'coffee', 'did', '.', 'i', 'bought', 'coffee', 'the', 'other', 'day', 'and', 'yesterday', 'i', 'drank', 'it', 'and', 'noticed', 'i', 'was', 'calm', ',', 'mel', '##low', '.', '.', '.', '.', 'started', 'ya', '##wn', '##ing', 'after', 'about', '3', '-', '4', 'hr', '##s', 'yet', 'was', 'able', 'to', 'read', 'through', 'a', 'mutual', 'fund', 'prospect', '##us', 'without', 'any', 'distraction', '##s', 'or', 'jumping', 'around', '.', 'i', \"'\", 've', 'used', 'concert', '##a', 'in', 'college', 'yet', 'it', 'didn', \"'\", 't', 'work', ';', 'not', 'even', 'at', 'a', 'high', 'dos', '##age', '.', 'now', 'out', 'of', 'college', ',', 'unemployed', 'and', 'no', 'insurance', '.', '.', '.', '.', 'not', 'getting', 'anything', 'done', 'i', 'needed', 'to', 'figure', 'out', 'another', 'solution', 'for', 'myself', '.', 'so', 'far', 'it', \"'\", 's', 'working', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['unusual', 'i', 'was', 'recently', 'started', 'on', '20', 'mg', 'add', '##eral', '##l', 'x', '##r', '(', 'two', 'days', 'ago', ')', '.', 'i', 'remember', 'when', 'i', 'first', 'tried', 'add', '##eral', '##l', 'my', 'entire', 'perception', 'became', 'clearer', 'and', 'colors', 'became', 'more', 'vibrant', '.', 'right', 'now', 'my', 'thoughts', 'are', 'more', 'organized', 'in', 'my', 'head', 'and', 'i', \"'\", 'm', 'internally', 'calm', '##er', ',', 'yet', 'my', 'perception', 'isn', \"'\", 't', 'miraculous', '##ly', 'improved', '(', 'only', 'slightly', ')', '.', 'is', 'this', 'the', 'proper', 'effect', 'of', 'this', 'medication', '?', 'it', 'just', 'doesn', \"'\", 't', 'feel', 'the', 'same', 'from', 'that', 'time', 'i', 'tried', 'it', 'a', 'year', 'ago', '.', 'should', 'i', 'change', 'my', 'dos', '##age', '?', 'change', 'the', 'medication', 'itself', '?', 'could', 'it', 'be', 'that', 'i', 'haven', \"'\", 't', 'worked', 'out', 'in', 'two', 'weeks', '?', 'despite', 'all', 'this', ',', 'i', 'am', 'getting', 'my', 'work', 'done', '.']\n",
      "INFO:__main__:Number of tokens: 135\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['unusual', 'i', 'was', 'recently', 'started', 'on', '20', 'mg', 'add', '##eral', '##l', 'x', '##r', '(', 'two', 'days', 'ago', ')', '.', 'i', 'remember', 'when', 'i', 'first', 'tried', 'add', '##eral', '##l', 'my', 'entire', 'perception', 'became', 'clearer', 'and', 'colors', 'became', 'more', 'vibrant', '.', 'right', 'now', 'my', 'thoughts', 'are', 'more', 'organized', 'in', 'my', 'head', 'and', 'i', \"'\", 'm', 'internally', 'calm', '##er', ',', 'yet', 'my', 'perception', 'isn', \"'\", 't', 'miraculous', '##ly', 'improved', '(', 'only', 'slightly', ')', '.', 'is', 'this', 'the', 'proper', 'effect', 'of', 'this', 'medication', '?', 'it', 'just', 'doesn', \"'\", 't', 'feel', 'the', 'same', 'from', 'that', 'time', 'i', 'tried', 'it', 'a', 'year', 'ago', '.', 'should', 'i', 'change', 'my', 'dos', '##age', '?', 'change', 'the', 'medication', 'itself', '?', 'could', 'it', 'be', 'that', 'i', 'haven', \"'\", 't', 'worked', 'out', 'in', 'two', 'weeks', '?', 'despite', 'all', 'this', ',', 'i', 'am', 'getting', 'my', 'work', 'done', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['complex', 'attention', 'and', 'simple', 'attention', 'in', 'ad', '##hd', 'testing', '?', 'what', 'exactly', 'is', 'complex', 'attention', 'and', 'simple', 'attention', ',', 'in', 'regards', 'to', 'ad', '##hd', 'testing', '?', 'what', 'is', 'the', 'difference', 'between', 'the', 'two', '?', 'if', 'a', 'person', 'has', 'problems', 'with', 'complex', 'attention', 'or', 'simple', 'attention', 'only', ',', 'rather', 'than', 'both', ',', 'what', 'does', 'that', 'typically', 'mean', 'in', 'regards', 'to', 'an', 'ad', '##hd', 'diagnosis', '?']\n",
      "INFO:__main__:Number of tokens: 64\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['complex', 'attention', 'and', 'simple', 'attention', 'in', 'ad', '##hd', 'testing', '?', 'what', 'exactly', 'is', 'complex', 'attention', 'and', 'simple', 'attention', ',', 'in', 'regards', 'to', 'ad', '##hd', 'testing', '?', 'what', 'is', 'the', 'difference', 'between', 'the', 'two', '?', 'if', 'a', 'person', 'has', 'problems', 'with', 'complex', 'attention', 'or', 'simple', 'attention', 'only', ',', 'rather', 'than', 'both', ',', 'what', 'does', 'that', 'typically', 'mean', 'in', 'regards', 'to', 'an', 'ad', '##hd', 'diagnosis', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', 'useful', 'ad', '##hd', 'non', '-', 'profit', '.']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', 'useful', 'ad', '##hd', 'non', '-', 'profit', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 've', 'been', 'pretty', 'certain', 'i', 'have', 'ad', '##hd', 'for', 'a', 'while', 'now', 'and', 'want', 'to', 'become', 'clinical', '##ly', 'diagnosed', 'so', 'i', 'can', 'get', 'med', '##s', 'to', 'help', 'me', 'cope', '.', 'i', \"'\", 'm', 'totally', 'new', 'to', 'this', 'so', 'any', 'advice', 'is', 'appreciated', '!', 'since', 'i', 'can', 'remember', ',', 'i', 'have', 'had', 'a', 'hard', 'time', 'completing', 'things', ',', 'from', 'short', '10', 'minute', 'tasks', 'to', 'long', 'term', 'goals', '.', 'i', 'used', 'to', 'think', 'i', 'was', 'just', 'lazy', 'and', 'ir', '##res', '##pon', '##sible', 'which', 'put', 'me', 'into', 'depression', 'most', 'of', 'my', 'life', ',', 'sometimes', 'in', 'a', 'pretty', 'extreme', 'way', 'where', 'i', 'would', 'feel', 'completely', 'useless', 'and', 'help', '##ess', '.', 'earlier', 'this', 'year', 'i', 'met', 'someone', 'who', 'has', 'ad', '##hd', '.', 'after', 'getting', 'to', 'know', 'him', 'better', 'and', 'sharing', 'how', 'i', 'sometimes', 'feel', 'so', 'useless', 'because', 'i', 'have', 'never', 'completed', 'things', 'i', 'began', ',', 'he', 'suggested', 'i', 'read', 'up', 'on', 'ad', '##hd', '.', 'well', 'i', 'did', ',', 'and', 'everything', 'i', 'read', 'i', 'nodded', 'to', 'as', 'i', 'realized', 'i', \"'\", 've', 'experienced', 'it', 'all', '.', 'i', 'was', 'kinda', 'shocked', ',', 'had', 'no', 'idea', 'i', 'had', 'a', \"'\", 'condition', '.', \"'\", 'my', 'friend', 'with', 'ad', '##hd', 'let', 'my', 'try', 'some', '20', '##mg', 'add', '##eral', '##l', 'which', 'i', 'used', 'for', 'a', 'few', 'days', 'and', 'it', 'helped', 'immensely', '!', 'i', 'was', 'so', 'relieved', 'there', 'was', 'medication', 'for', 'this', '!', 'now', 'i', 'want', 'to', 'see', 'a', 'psychiatrist', 'so', 'i', 'can', 'get', 'a', 'prescription', 'for', '20', '##mg', 'add', '##eral', '##l', 'but', 'i', \"'\", 'm', 'concerned', 'about', 'how', 'to', 'explain', 'my', 'experience', '.', 'with', 'the', 'use', 'of', 'add', '##eral', '##l', 'being', 'under', 'such', 'controversy', 'with', 'college', 'students', ',', 'im', 'concerned', 'that', 'the', 'psychiatrist', 'may', 'think', 'i', \"'\", 'm', 'fa', '##king', 'in', 'order', 'to', 'get', 'the', 'med', '##s', '.', 'i', \"'\", 'm', 'a', '22', 'year', 'old', 'college', 'student', 'studying', 'architecture', ',', 'and', 'obviously', 'my', 'studies', 'are', 'a', 'big', 'reason', 'why', 'i', 'want', 'add', '##eral', '##l', '.', 'i', 'dropped', 'my', 'major', 'and', 'picked', 'it', 'up', 'again', 'twice', 'in', '2', 'years', 'and', 'i', 'don', \"'\", 't', 'know', 'why', 'i', 'ever', 'did', '!', 'obviously', 'it', 'was', 'from', 'my', 'ad', '##hd', '.', 'i', \"'\", 've', 'never', 'seen', 'a', 'psychiatrist', 'or', 'even', 'expressed', 'my', 'thoughts', 'on', 'self', '-', 'dia', '##gno', '##sing', 'myself', 'with', 'ad', '##hd', '.', 'do', 'i', 'tell', 'him', '/', 'her', 'the', 'absolute', 'truth', '?', 'everything', '?', 'should', 'i', 'mention', 'i', 'tried', 'add', '##eral', '##l', 'from', 'a', 'friend', 'and', 'it', 'helped', 'and', 'that', \"'\", 's', 'why', 'i', 'want', 'it', '?', 'if', 'i', 'do', 'tell', ',', 'will', 'he', 'think', 'i', \"'\", 'm', 'just', 'some', 'sc', '##umb', '##ag', 'trying', 'to', 'get', 'my', 'hands', 'on', 'some', 'drugs', 'for', '\"', 'super', 'studying', '\"', '?', '?', 'please', 'help', ',', 'as', 'you', 'can', 'see', 'i', 'have', 'so', 'many', 'questions', 'and', 'just', 'need', 'some', 'direction', 'on', 'how', 'to', 'get', 'the', 'right', 'help', '.', 'any', 'rec', '##ome', '##nda', '##tions', 'on', 'where', 'to', 'go', 'for', 'a', 'diagnosis', '?', 'what', 'to', 'say', '?', 'thanks', '!', '(', 'sorry', 'for', 'the', 'long', 'story', 'and', 'thanks', 'if', 'you', 'read', 'it', 'up', 'to', 'here', '!', ')', 'edit', ':', 'gram', '##mer']\n",
      "INFO:__main__:Number of tokens: 502\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 've', 'been', 'pretty', 'certain', 'i', 'have', 'ad', '##hd', 'for', 'a', 'while', 'now', 'and', 'want', 'to', 'become', 'clinical', '##ly', 'diagnosed', 'so', 'i', 'can', 'get', 'med', '##s', 'to', 'help', 'me', 'cope', '.', 'i', \"'\", 'm', 'totally', 'new', 'to', 'this', 'so', 'any', 'advice', 'is', 'appreciated', '!', 'since', 'i', 'can', 'remember', ',', 'i', 'have', 'had', 'a', 'hard', 'time', 'completing', 'things', ',', 'from', 'short', '10', 'minute', 'tasks', 'to', 'long', 'term', 'goals', '.', 'i', 'used', 'to', 'think', 'i', 'was', 'just', 'lazy', 'and', 'ir', '##res', '##pon', '##sible', 'which', 'put', 'me', 'into', 'depression', 'most', 'of', 'my', 'life', ',', 'sometimes', 'in', 'a', 'pretty', 'extreme', 'way', 'where', 'i', 'would', 'feel', 'completely', 'useless', 'and', 'help', '##ess', '.', 'earlier', 'this', 'year', 'i', 'met', 'someone', 'who', 'has', 'ad', '##hd', '.', 'after', 'getting', 'to', 'know', 'him', 'better', 'and', 'sharing', 'how', 'i', 'sometimes', 'feel', 'so', 'useless', 'because', 'i', 'have', 'never', 'completed', 'things', 'i', 'began', ',', 'he', 'suggested', 'i', 'read', 'up', 'on', 'ad', '##hd', '.', 'well', 'i', 'did', ',', 'and', 'everything', 'i', 'read', 'i', 'nodded', 'to', 'as', 'i', 'realized', 'i', \"'\", 've', 'experienced', 'it', 'all', '.', 'i', 'was', 'kinda', 'shocked', ',', 'had', 'no', 'idea', 'i', 'had', 'a', \"'\", 'condition', '.', \"'\", 'my', 'friend', 'with', 'ad', '##hd', 'let', 'my', 'try', 'some', '20', '##mg', 'add', '##eral', '##l', 'which', 'i', 'used', 'for', 'a', 'few', 'days', 'and', 'it', 'helped', 'immensely', '!', 'i', 'was', 'so', 'relieved', 'there', 'was', 'medication', 'for', 'this', '!', 'now', 'i', 'want', 'to', 'see', 'a', 'psychiatrist', 'so', 'i', 'can', 'get', 'a', 'prescription', 'for', '20', '##mg', 'add', '##eral', '##l', 'but', 'i', \"'\", 'm', 'concerned', 'about', 'how', 'to', 'explain', 'my', 'experience', '.', 'with', 'the', 'use', 'of', 'add', '##eral', '##l', 'being', 'under', 'such', 'controversy', 'with', 'college', 'students', ',', 'im', 'concerned', 'that', 'the', 'psychiatrist', 'may', 'think', 'i', \"'\", 'm', 'fa', '##king', 'in', 'order', 'to', 'get', 'the', 'med', '##s', '.', 'i', \"'\", 'm', 'a', '22', 'year', 'old', 'college', 'student', 'studying', 'architecture', ',', 'and', 'obviously', 'my', 'studies', 'are', 'a', 'big', 'reason', 'why', 'i', 'want', 'add', '##eral', '##l', '.', 'i', 'dropped', 'my', 'major', 'and', 'picked', 'it', 'up', 'again', 'twice', 'in', '2', 'years', 'and', 'i', 'don', \"'\", 't', 'know', 'why', 'i', 'ever', 'did', '!', 'obviously', 'it', 'was', 'from', 'my', 'ad', '##hd', '.', 'i', \"'\", 've', 'never', 'seen', 'a', 'psychiatrist', 'or', 'even', 'expressed', 'my', 'thoughts', 'on', 'self', '-', 'dia', '##gno', '##sing', 'myself', 'with', 'ad', '##hd', '.', 'do', 'i', 'tell', 'him', '/', 'her', 'the', 'absolute', 'truth', '?', 'everything', '?', 'should', 'i', 'mention', 'i', 'tried', 'add', '##eral', '##l', 'from', 'a', 'friend', 'and', 'it', 'helped', 'and', 'that', \"'\", 's', 'why', 'i', 'want', 'it', '?', 'if', 'i', 'do', 'tell', ',', 'will', 'he', 'think', 'i', \"'\", 'm', 'just', 'some', 'sc', '##umb', '##ag', 'trying', 'to', 'get', 'my', 'hands', 'on', 'some', 'drugs', 'for', '\"', 'super', 'studying', '\"', '?', '?', 'please', 'help', ',', 'as', 'you', 'can', 'see', 'i', 'have', 'so', 'many', 'questions', 'and', 'just', 'need', 'some', 'direction', 'on', 'how', 'to', 'get', 'the', 'right', 'help', '.', 'any', 'rec', '##ome', '##nda', '##tions', 'on', 'where', 'to', 'go', 'for', 'a', 'diagnosis', '?', 'what', 'to', 'say', '?', 'thanks', '!', '(', 'sorry', 'for', 'the', 'long', 'story', 'and', 'thanks', 'if', 'you', 'read', 'it', 'up', 'to', 'here', '!', ')', 'edit', ':', 'gram', '##mer']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['went', 'in', 'to', 'be', 'tested', 'for', 'ad', '##hd', '.', 'diagnosed', 'with', 'de', '##pressive', 'disorder', '-', 'not', 'otherwise', 'specified', '.', 'what', '?', 'i', 'went', 'in', 'to', 'be', 'tested', 'for', 'ad', '##hd', '.', 'there', 'were', 'a', 'lot', 'of', 'tests', ',', 'such', 'as', 'having', 'to', 'click', 'on', '1', '##s', 'but', 'not', '2', '##s', ',', 'having', 'to', 'remember', 'a', 'list', 'of', 'words', ',', 'and', 'having', 'to', 'draw', 'a', 'complicated', 'picture', 'from', 'memory', '.', 'i', 'got', 'the', 'results', 'back', '.', 'it', 'appears', 'i', 'have', 'problems', 'with', 'complex', 'attention', ',', 'but', 'not', 'simple', 'attention', ',', 'and', 'in', 'their', 'view', ',', 'if', 'i', 'had', 'ad', '##hd', ',', 'i', 'would', 'have', 'both', '.', 'instead', ',', 'i', 'was', 'diagnosed', 'with', 'depression', ',', 'and', 'mentioned', 'possible', 'anxiety', 'issues', '.', 'they', 'recommended', 'psychiatry', 'and', 'therapy', '.', 'the', 'thing', 'is', ',', 'i', 'fully', 'and', 'entirely', 'admit', 'i', 'had', 'depression', 'five', 'years', 'ago', ',', 'and', 'my', 'state', 'of', 'being', 'now', 'is', 'nothing', 'like', 'that', '.', 'i', 'do', 'get', 'down', 'on', 'myself', 'easily', ',', 'but', 'i', 'wouldn', \"'\", 't', 'say', 'i', \"'\", 'm', 'honestly', 'diagnosis', 'worthy', '.', 'what', 'do', 'i', 'get', 'down', 'on', 'myself', 'for', '?', 'not', 'being', 'able', 'to', 'get', 'myself', 'to', 'do', 'homework', 'without', 'taking', 'cop', '##ious', 'amounts', 'of', 'caf', '##fe', '##ine', ',', 'for', 'one', '.', 'not', 'being', 'able', 'to', 'focus', 'on', 'a', 'game', 'of', 'm', ':', 't', '##g', ',', 'and', 'making', 'mistakes', 'because', 'i', \"'\", 'm', 'not', 'paying', 'attention', '.', 'not', 'getting', 'stupid', 'household', 'chores', 'done', '.', 'how', 'accurate', 'are', 'tests', 'like', 'that', '?', 'i', 'mean', ',', 'fuck', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'treated', 'for', 'depression', 'i', 'don', \"'\", 't', 'fucking', 'have', '.', 'i', \"'\", 've', 'tried', 'accepting', 'it', '.', 'it', \"'\", 's', 'been', 'weeks', 'since', 'the', 'meeting', ',', 'and', 'i', 'keep', 'going', 'over', 'it', '.', 'maybe', 'i', 'am', 'depressed', '.', 'but', 'no', '.', 'logical', '##ly', ',', 'it', 'just', 'doesn', \"'\", 't', 'take', 'in', 'my', 'mind', '.', 'i', \"'\", 'm', 'also', 'female', ';', 'i', 'know', 'it', 'tends', 'to', 'be', 'under', '##dia', '##gno', '##sed', 'in', 'females', ',', 'but', 'i', \"'\", 'm', 'not', 'sure', 'if', 'that', 'has', 'any', 'weight', 'in', 'my', 'particular', 'situation', '.', 'edit', ':', 'spoke', 'to', 'the', 'doctor', 'today', '.', 'she', 'said', 'that', 'she', \"'\", 'd', 'taken', 'the', 'differences', 'between', 'adult', 'and', 'child', 'ad', '##hd', 'into', 'account', '.', 'my', 'scores', 'are', 'as', 'follows', ':', 'verbal', 'comprehension', ':', '112', 'working', 'memory', ':', '122', 'per', '##ce', '##pt', '##ual', 'reasoning', ':', '121', 'processing', 'speed', ':', '105', 'she', 'stated', 'that', 'because', 'it', 'was', 'only', 'processing', 'speed', ',', 'and', 'not', 'anything', 'else', ',', 'that', 'she', 'didn', \"'\", 't', 'feel', 'i', 'have', 'ad', '##hd', '.', 'i', 'glanced', 'at', 'my', 'working', 'memory', 'scores', '.', 'i', 'did', 'well', 'on', 'the', 'simple', 'auditory', 'task', 'and', 'the', 'mental', 'calculation', 'task', ',', 'but', 'did', 'average', 'on', 'a', 'task', 'of', 'sustained', 'attention', 'and', 'sequencing', ',', 'and', 'rec', '##iting', 'words', 'from', 'a', 'random', 'list', 'was', 'also', 'average', '.', 'she', 'also', 'mentioned', 'that', 'she', 'felt', 'i', 'have', 'a', 'more', 'cognitive', 'depression', ',', 'where', 'i', 'don', \"'\", 't', 'think', 'i', 'can', 'handle', 'things', ',', 'amongst', 'other', 'negative', 'thoughts', '.']\n",
      "INFO:__main__:Number of tokens: 496\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['went', 'in', 'to', 'be', 'tested', 'for', 'ad', '##hd', '.', 'diagnosed', 'with', 'de', '##pressive', 'disorder', '-', 'not', 'otherwise', 'specified', '.', 'what', '?', 'i', 'went', 'in', 'to', 'be', 'tested', 'for', 'ad', '##hd', '.', 'there', 'were', 'a', 'lot', 'of', 'tests', ',', 'such', 'as', 'having', 'to', 'click', 'on', '1', '##s', 'but', 'not', '2', '##s', ',', 'having', 'to', 'remember', 'a', 'list', 'of', 'words', ',', 'and', 'having', 'to', 'draw', 'a', 'complicated', 'picture', 'from', 'memory', '.', 'i', 'got', 'the', 'results', 'back', '.', 'it', 'appears', 'i', 'have', 'problems', 'with', 'complex', 'attention', ',', 'but', 'not', 'simple', 'attention', ',', 'and', 'in', 'their', 'view', ',', 'if', 'i', 'had', 'ad', '##hd', ',', 'i', 'would', 'have', 'both', '.', 'instead', ',', 'i', 'was', 'diagnosed', 'with', 'depression', ',', 'and', 'mentioned', 'possible', 'anxiety', 'issues', '.', 'they', 'recommended', 'psychiatry', 'and', 'therapy', '.', 'the', 'thing', 'is', ',', 'i', 'fully', 'and', 'entirely', 'admit', 'i', 'had', 'depression', 'five', 'years', 'ago', ',', 'and', 'my', 'state', 'of', 'being', 'now', 'is', 'nothing', 'like', 'that', '.', 'i', 'do', 'get', 'down', 'on', 'myself', 'easily', ',', 'but', 'i', 'wouldn', \"'\", 't', 'say', 'i', \"'\", 'm', 'honestly', 'diagnosis', 'worthy', '.', 'what', 'do', 'i', 'get', 'down', 'on', 'myself', 'for', '?', 'not', 'being', 'able', 'to', 'get', 'myself', 'to', 'do', 'homework', 'without', 'taking', 'cop', '##ious', 'amounts', 'of', 'caf', '##fe', '##ine', ',', 'for', 'one', '.', 'not', 'being', 'able', 'to', 'focus', 'on', 'a', 'game', 'of', 'm', ':', 't', '##g', ',', 'and', 'making', 'mistakes', 'because', 'i', \"'\", 'm', 'not', 'paying', 'attention', '.', 'not', 'getting', 'stupid', 'household', 'chores', 'done', '.', 'how', 'accurate', 'are', 'tests', 'like', 'that', '?', 'i', 'mean', ',', 'fuck', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'treated', 'for', 'depression', 'i', 'don', \"'\", 't', 'fucking', 'have', '.', 'i', \"'\", 've', 'tried', 'accepting', 'it', '.', 'it', \"'\", 's', 'been', 'weeks', 'since', 'the', 'meeting', ',', 'and', 'i', 'keep', 'going', 'over', 'it', '.', 'maybe', 'i', 'am', 'depressed', '.', 'but', 'no', '.', 'logical', '##ly', ',', 'it', 'just', 'doesn', \"'\", 't', 'take', 'in', 'my', 'mind', '.', 'i', \"'\", 'm', 'also', 'female', ';', 'i', 'know', 'it', 'tends', 'to', 'be', 'under', '##dia', '##gno', '##sed', 'in', 'females', ',', 'but', 'i', \"'\", 'm', 'not', 'sure', 'if', 'that', 'has', 'any', 'weight', 'in', 'my', 'particular', 'situation', '.', 'edit', ':', 'spoke', 'to', 'the', 'doctor', 'today', '.', 'she', 'said', 'that', 'she', \"'\", 'd', 'taken', 'the', 'differences', 'between', 'adult', 'and', 'child', 'ad', '##hd', 'into', 'account', '.', 'my', 'scores', 'are', 'as', 'follows', ':', 'verbal', 'comprehension', ':', '112', 'working', 'memory', ':', '122', 'per', '##ce', '##pt', '##ual', 'reasoning', ':', '121', 'processing', 'speed', ':', '105', 'she', 'stated', 'that', 'because', 'it', 'was', 'only', 'processing', 'speed', ',', 'and', 'not', 'anything', 'else', ',', 'that', 'she', 'didn', \"'\", 't', 'feel', 'i', 'have', 'ad', '##hd', '.', 'i', 'glanced', 'at', 'my', 'working', 'memory', 'scores', '.', 'i', 'did', 'well', 'on', 'the', 'simple', 'auditory', 'task', 'and', 'the', 'mental', 'calculation', 'task', ',', 'but', 'did', 'average', 'on', 'a', 'task', 'of', 'sustained', 'attention', 'and', 'sequencing', ',', 'and', 'rec', '##iting', 'words', 'from', 'a', 'random', 'list', 'was', 'also', 'average', '.', 'she', 'also', 'mentioned', 'that', 'she', 'felt', 'i', 'have', 'a', 'more', 'cognitive', 'depression', ',', 'where', 'i', 'don', \"'\", 't', 'think', 'i', 'can', 'handle', 'things', ',', 'amongst', 'other', 'negative', 'thoughts', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['talking', 'to', 'my', 'new', 'doctor', 'tomorrow', 'about', 'ad', '##hd', '/', 'social', 'anxiety', 'i', \"'\", 'm', '23', 'and', 'i', 'just', 'got', 'my', 'own', 'health', 'insurance', 'and', 'so', 'i', \"'\", 'm', 'finally', 'going', 'to', 'get', 'a', 'physical', 'tomorrow', 'and', 'i', \"'\", 'm', 'going', 'to', 'bring', 'up', 'some', 'of', 'the', 'symptoms', 'i', \"'\", 've', 'been', 'having', 'relating', 'to', 'ad', '##hd', 'and', 'social', 'anxiety', '.', 'this', 'is', 'the', 'list', 'i', 'came', 'up', 'with', 'with', 'what', 'i', \"'\", 've', 'been', 'feeling', ':', '*', '*', 'ad', '##hd', '*', '*', '•', 'endless', 'random', 'thoughts', 'while', 'trying', 'to', 'sleep', '.', '•', 'rarely', 'remember', 'dreams', '(', 'not', 'sure', 'if', 'sy', '##mpt', '##om', ',', 'though', ')', '•', 'takes', 'months', 'to', 'finish', 'a', 'project', 'that', 'should', 'only', 'take', 'a', 'few', 'weeks', '.', '•', 'many', 'projects', 'still', 'unfinished', '.', 'and', 'un', '##star', '##ted', '.', '•', 'pro', '##cr', '##ast', '##ination', '•', 'internet', 'addiction', '•', 'tune', 'out', 'of', 'conversations', '•', 'partner', 'feels', 'like', 'i', 'don', '’', 't', 'care', ',', 'sometimes', 'emotion', '##less', '.', '•', 'cannot', 'concentrate', 'in', 'class', 'during', 'work', '##time', 'if', 'teacher', 'is', 'talking', 'to', 'another', 'student', 'near', 'me', '.', '•', 'unconscious', '##ly', 'tapping', 'foot', ',', 'always', 't', '##wi', '##rling', 'pens', 'around', 'fingers', 'all', 'the', 'time', '.', '•', 'zone', 'out', 'while', 'reading', 'and', 'have', 'to', 're', '##rea', '##d', 'the', 'page', '.', '•', 'difficulty', 'prior', '##iti', '##zing', 'tasks', 'by', 'importance', ';', 'would', 'rather', 'play', 'games', 'or', 'surf', 'internet', '.', '“', 'i', '’', 'll', 'have', 'time', 'tomorrow', 'to', 'do', 'it', '”', '•', 'easily', 'irritated', '•', 'hate', 'waiting', 'my', 'turn', 'to', 'do', 'something', '(', 'ex', '.', ',', 'give', 'presentation', ')', '.', 'get', 'anxious', 'while', 'waiting', 'and', 'don', '’', 't', 'pay', 'attention', 'to', 'others', '.', '•', 'difficult', 'to', 'read', 'long', 'articles', ',', 'books', '.', '*', '*', 'social', 'anxiety', '*', '*', '•', 'very', 'difficult', 'to', 'make', 'eye', 'contact', 'with', 'others', '.', '•', 'feel', 'like', 'people', 'are', 'judging', 'me', '.', '•', 'instead', 'of', 'concentrating', 'on', 'driving', ',', 'i', 'concentrate', 'on', 'what', 'other', 'drivers', 'think', 'of', 'me', '.', '•', 'cannot', 'think', 'during', 'confrontation', '##s', '.', 'freeze', 'up', '.', '•', 'stu', '##ttering', 'voice', ',', 'increased', 'heartbeat', 'during', 'confrontation', '##s', '.', '•', 'dislike', 'going', 'to', 'parties', ',', 'bars', ',', 'clubs', ',', 'or', 'even', 'bonfire', '##s', ',', 'dinner', 'with', 'friends', '.', '•', 'difficult', 'to', 'strike', 'up', 'conversations', 'with', 'customers', 'at', 'work', '.', '•', 'can', '’', 't', 'do', 'work', 'if', 'someone', 'is', 'watching', 'over', 'my', 'shoulder', '.', '•', 'relationship', 'difficulties', 'with', 'communicating', ',', 'sex', '.', '•', 'very', 'intro', '##verted', '.', 'live', 'in', 'my', 'bedroom', '.', '•', 'nervous', 'to', 'talk', 'to', 'people', 'in', 'authority', '.', '•', 'blush', 'easily', '•', 'difficult', 'to', 'express', 'feelings', 'to', 'others', '.', '•', 'prefer', 'not', 'to', 'use', 'public', 'restroom', 'when', 'others', 'are', 'present', '.', '•', 'generally', 'feel', 'awkward', 'while', 'talking', 'to', 'others', '.', '•', 'hard', 'to', 'compete', 'with', 'others', 'i', \"'\", 've', 'been', 'really', 'nervous', 'about', 'asking', 'all', 'week', '.', 'figuring', 'out', 'how', 'to', 'tell', 'him', '.', 'i', 'don', \"'\", 't', 'want', 'him', 'to', 'think', 'i', \"'\", 'm', 'just', 'another', 'student', 'looking', 'for', 'add', '##eral', '##l', '.', 'i', \"'\", 'd', 'prefer', 'not', 'to', 'have', 'it', ',', 'but', 'if', 'it', \"'\", 'll', 'help', 'me', 'focus', 'and', 'get', 'my', 'things', 'done', ',', 'it', 'may', 'be', 'necessary', '.', 'but', 'i', 'also', 'don', \"'\", 't', 'want', 'to', 'take', 'a', 'bunch', 'of', 'different', 'medications', '.', 'can', 'add', '##eral', '##l', 'help', 'with', 'both', 'ad', '##hd', 'and', 'social', 'anxiety', '?', 'i', \"'\", 've', 'read', 'forum', 'posts', 'by', 'some', 'people', 'who', 'say', 'it', 'works', 'for', 'both', 'and', 'some', 'who', 'say', 'it', 'doesn', \"'\", 't', '.', 'does', 'anyone', 'else', 'have', 'experiences', '?', '*', '*', 'edit', ':', '*', '*', 'well', ',', 'i', 'saw', 'the', 'doctor', 'and', 'he', 'said', 'he', 'didn', \"'\", 't', 'want', 'to', 'just', 'throw', 'medication', 'at', 'anything', 'until', 'we', \"'\", 're', 'sure', '.', 'which', 'is', 'understand', '##able', '.', 'so', 'he', 'referred', 'me', 'to', 'the', 'clinic', \"'\", 's', 'psychologist', ',', 'however', 'my', 'appointment', 'is', 'november', '30th', '.', 'soo', '##oo', 'far', 'away', '.', 'i', \"'\", 'm', 'gonna', 'try', 'calling', 'periodically', 'to', 'see', 'if', 'anyone', 'cancelled', 'so', 'i', 'can', 'get', 'bumped', 'up', '.']\n",
      "INFO:__main__:Number of tokens: 651\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['talking', 'to', 'my', 'new', 'doctor', 'tomorrow', 'about', 'ad', '##hd', '/', 'social', 'anxiety', 'i', \"'\", 'm', '23', 'and', 'i', 'just', 'got', 'my', 'own', 'health', 'insurance', 'and', 'so', 'i', \"'\", 'm', 'finally', 'going', 'to', 'get', 'a', 'physical', 'tomorrow', 'and', 'i', \"'\", 'm', 'going', 'to', 'bring', 'up', 'some', 'of', 'the', 'symptoms', 'i', \"'\", 've', 'been', 'having', 'relating', 'to', 'ad', '##hd', 'and', 'social', 'anxiety', '.', 'this', 'is', 'the', 'list', 'i', 'came', 'up', 'with', 'with', 'what', 'i', \"'\", 've', 'been', 'feeling', ':', '*', '*', 'ad', '##hd', '*', '*', '•', 'endless', 'random', 'thoughts', 'while', 'trying', 'to', 'sleep', '.', '•', 'rarely', 'remember', 'dreams', '(', 'not', 'sure', 'if', 'sy', '##mpt', '##om', ',', 'though', ')', '•', 'takes', 'months', 'to', 'finish', 'a', 'project', 'that', 'should', 'only', 'take', 'a', 'few', 'weeks', '.', '•', 'many', 'projects', 'still', 'unfinished', '.', 'and', 'un', '##star', '##ted', '.', '•', 'pro', '##cr', '##ast', '##ination', '•', 'internet', 'addiction', '•', 'tune', 'out', 'of', 'conversations', '•', 'partner', 'feels', 'like', 'i', 'don', '’', 't', 'care', ',', 'sometimes', 'emotion', '##less', '.', '•', 'cannot', 'concentrate', 'in', 'class', 'during', 'work', '##time', 'if', 'teacher', 'is', 'talking', 'to', 'another', 'student', 'near', 'me', '.', '•', 'unconscious', '##ly', 'tapping', 'foot', ',', 'always', 't', '##wi', '##rling', 'pens', 'around', 'fingers', 'all', 'the', 'time', '.', '•', 'zone', 'out', 'while', 'reading', 'and', 'have', 'to', 're', '##rea', '##d', 'the', 'page', '.', '•', 'difficulty', 'prior', '##iti', '##zing', 'tasks', 'by', 'importance', ';', 'would', 'rather', 'play', 'games', 'or', 'surf', 'internet', '.', '“', 'i', '’', 'll', 'have', 'time', 'tomorrow', 'to', 'do', 'it', '”', '•', 'easily', 'irritated', '•', 'hate', 'waiting', 'my', 'turn', 'to', 'do', 'something', '(', 'ex', '.', ',', 'give', 'presentation', ')', '.', 'get', 'anxious', 'while', 'waiting', 'and', 'don', '’', 't', 'pay', 'attention', 'to', 'others', '.', '•', 'difficult', 'to', 'read', 'long', 'articles', ',', 'books', '.', '*', '*', 'social', 'anxiety', '*', '*', '•', 'very', 'difficult', 'to', 'make', 'eye', 'contact', 'with', 'others', '.', '•', 'feel', 'like', 'people', 'are', 'judging', 'me', '.', '•', 'instead', 'of', 'concentrating', 'on', 'driving', ',', 'i', 'concentrate', 'on', 'what', 'other', 'drivers', 'think', 'of', 'me', '.', '•', 'cannot', 'think', 'during', 'confrontation', '##s', '.', 'freeze', 'up', '.', '•', 'stu', '##ttering', 'voice', ',', 'increased', 'heartbeat', 'during', 'confrontation', '##s', '.', '•', 'dislike', 'going', 'to', 'parties', ',', 'bars', ',', 'clubs', ',', 'or', 'even', 'bonfire', '##s', ',', 'dinner', 'with', 'friends', '.', '•', 'difficult', 'to', 'strike', 'up', 'conversations', 'with', 'customers', 'at', 'work', '.', '•', 'can', '’', 't', 'do', 'work', 'if', 'someone', 'is', 'watching', 'over', 'my', 'shoulder', '.', '•', 'relationship', 'difficulties', 'with', 'communicating', ',', 'sex', '.', '•', 'very', 'intro', '##verted', '.', 'live', 'in', 'my', 'bedroom', '.', '•', 'nervous', 'to', 'talk', 'to', 'people', 'in', 'authority', '.', '•', 'blush', 'easily', '•', 'difficult', 'to', 'express', 'feelings', 'to', 'others', '.', '•', 'prefer', 'not', 'to', 'use', 'public', 'restroom', 'when', 'others', 'are', 'present', '.', '•', 'generally', 'feel', 'awkward', 'while', 'talking', 'to', 'others', '.', '•', 'hard', 'to', 'compete', 'with', 'others', 'i', \"'\", 've', 'been', 'really', 'nervous', 'about', 'asking', 'all', 'week', '.', 'figuring', 'out', 'how', 'to', 'tell', 'him', '.', 'i', 'don', \"'\", 't', 'want', 'him', 'to', 'think', 'i', \"'\", 'm', 'just', 'another', 'student', 'looking', 'for', 'add', '##eral', '##l', '.', 'i', \"'\", 'd', 'prefer', 'not', 'to', 'have', 'it', ',', 'but', 'if', 'it', \"'\", 'll', 'help', 'me', 'focus', 'and', 'get', 'my', 'things', 'done', ',', 'it', 'may', 'be'], ['necessary', '.', 'but', 'i', 'also', 'don', \"'\", 't', 'want', 'to', 'take', 'a', 'bunch', 'of', 'different', 'medications', '.', 'can', 'add', '##eral', '##l', 'help', 'with', 'both', 'ad', '##hd', 'and', 'social', 'anxiety', '?', 'i', \"'\", 've', 'read', 'forum', 'posts', 'by', 'some', 'people', 'who', 'say', 'it', 'works', 'for', 'both', 'and', 'some', 'who', 'say', 'it', 'doesn', \"'\", 't', '.', 'does', 'anyone', 'else', 'have', 'experiences', '?', '*', '*', 'edit', ':', '*', '*', 'well', ',', 'i', 'saw', 'the', 'doctor', 'and', 'he', 'said', 'he', 'didn', \"'\", 't', 'want', 'to', 'just', 'throw', 'medication', 'at', 'anything', 'until', 'we', \"'\", 're', 'sure', '.', 'which', 'is', 'understand', '##able', '.', 'so', 'he', 'referred', 'me', 'to', 'the', 'clinic', \"'\", 's', 'psychologist', ',', 'however', 'my', 'appointment', 'is', 'november', '30th', '.', 'soo', '##oo', 'far', 'away', '.', 'i', \"'\", 'm', 'gonna', 'try', 'calling', 'periodically', 'to', 'see', 'if', 'anyone', 'cancelled', 'so', 'i', 'can', 'get', 'bumped', 'up', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'habits', ':', 'how', 'to', 'follow', 'thor', '##ugh', 'on', 'habits']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'habits', ':', 'how', 'to', 'follow', 'thor', '##ugh', 'on', 'habits']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['online', 'version', 'of', 'the', 'as', '##rs', 'v', '##1', '.', '1', '(', 'adult', 'ad', '##hd', 'self', '-', 'report', 'scale', ')']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['online', 'version', 'of', 'the', 'as', '##rs', 'v', '##1', '.', '1', '(', 'adult', 'ad', '##hd', 'self', '-', 'report', 'scale', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'received', 'good', 'responses', 'when', 'i', 'commented', 'this', ',', 'so', 'i', \"'\", 'll', 'just', 'post', 'it', '.', 'it', \"'\", 's', 'a', 'nice', 'way', 'of', 'explaining', 'add', '/', 'ad', '##hd', 'and', 'treatment', 'to', 'a', 'parent', 'or', 'anyone', 'else', 'wondering', '/', 'worried', 'about', 'it', '.', 'imagine', 'you', \"'\", 're', 'driving', 'on', 'an', 'unfamiliar', 'road', ',', 'blazing', 'your', 'own', 'trail', 'to', 'a', 'destination', 'specific', 'to', 'you', '.', 'there', 'will', 'be', 'rocks', ',', 'trees', ',', 'animals', ',', 'weather', ',', 'other', 'drivers', ',', 'and', 'all', 'sorts', 'of', 'obstacles', 'in', 'your', 'way', 'that', 'you', 'must', 'avoid', ',', 'conquer', ',', 'and', 'plan', 'for', '.', 'you', \"'\", 're', 'in', 'a', 'car', 'that', 'has', 'some', 'serious', 'power', '.', 'imagine', 'driving', 'a', 'ferrari', ';', 'you', 'definitely', 'need', 'good', 'driving', 'technique', 'to', 'handle', 'all', 'that', 'power', 'and', 'keep', 'from', 'crashing', '.', 'unfortunately', ',', 'its', 'already', 'hard', 'to', 'control', 'because', 'the', 'steering', 'wheel', 'is', 'covered', 'with', 'dried', 'cola', ',', 'the', 'radio', 'is', 'too', 'loud', 'and', 'stuck', 'on', 'political', 'talk', 'radio', '(', 'which', 'is', 'scar', '##ing', 'the', 'shit', 'out', 'of', 'you', ',', 'naturally', ')', ',', 'the', 'windows', 'are', 'dirty', ',', 'the', 'a', '/', 'c', 'isn', \"'\", 't', 'working', ',', 'etc', '.', 'because', 'of', 'these', 'distraction', '##s', ',', 'its', 'hard', 'to', 'control', 'your', 'car', ',', 'and', 'you', \"'\", 're', 'likely', 'to', 'get', 'caught', 'up', ',', 'or', 'even', 'crash', 'and', 'burn', 'when', 'obstacles', 'cross', 'your', 'path', '.', 'on', 'top', 'of', 'the', 'distraction', '##s', ',', 'you', \"'\", 've', 'been', 'driving', 'the', 'same', 'ferrari', 'for', 'all', 'your', 'life', ',', 'but', 'you', \"'\", 've', 'never', 'really', 'learned', 'how', 'to', 'control', 'all', 'that', 'power', '.', 'if', 'you', 'were', 'in', 'a', 'co', '##roll', '##a', ',', 'it', 'would', 'be', 'inherently', 'easier', 'to', 'drive', 'and', 'master', 'without', 'much', 'training', '.', 'the', 'terrain', 'you', 'are', 'travelling', 'on', ',', 'with', 'all', 'its', 'obstacles', ',', 'is', 'your', 'life', '.', 'your', 'destination', 'is', 'success', '(', 'whatever', 'your', 'idea', 'of', 'success', 'is', ')', '.', 'the', 'car', 'is', 'your', 'brain', ',', 'and', 'the', 'driver', 'is', 'you', '(', 'your', 'decision', 'making', 'processes', ')', '.', 'it', \"'\", 's', 'hard', 'for', 'you', 'to', 'drive', 'because', 'you', 'are', '1', ')', 'di', '##sta', '##cted', 'and', '2', ')', 'you', 'barely', 'know', 'how', 'to', 'drive', 'the', 'car', 'safely', 'through', 'the', 'terrain', 'towards', 'your', 'destination', '(', 'if', 'you', 'did', ',', 'you', 'wouldn', \"'\", 't', 'have', 'sought', 'psychiatric', '/', 'psycho', '##logic', 'help', ')', '.', 'from', 'my', 'ex', '##pi', '##rien', '##ce', 'with', 'add', '##eral', '##l', 'and', 'rita', '##lin', ',', 'i', 'can', 'say', 'that', 'the', 'medications', 'clear', 'out', 'the', 'distraction', '##s', ',', 'and', 'slow', 'down', 'your', 'car', ',', 'making', 'it', 'way', 'easier', 'to', 'drive', '.', 'no', 'scary', 'radio', ',', 'no', 'dirty', 'windows', ',', 'no', 'sticky', 'steering', 'wheel', ',', 'just', 'you', 'and', 'the', 'terrain', '.', 'the', 'other', 'reason', 'you', 'are', 'having', 'problems', 'driving', 'is', 'the', 'fact', 'that', 'you', 'are', 'not', 'too', 'good', 'of', 'a', 'driver', '.', 'the', 'solution', 'to', 'this', 'problem', 'is', 'very', 'simple', ',', 'learn', 'how', 'to', 'drive', '.', 'you', 'do', 'that', 'by', 'knowing', 'how', 'to', 'control', 'your', 'car', 'in', 'different', 'situations', '.', 'the', 'more', 'you', 'know', 'about', 'the', 'road', ',', 'and', 'what', 'your', 'car', 'can', 'handle', ',', 'the', 'easier', 'it', 'is', 'to', 'make', 'driving', 'decisions', '.', 'basically', ',', 'the', 'more', 'you', 'know', 'about', 'life', ',', 'and', 'the', 'more', 'you', 'can', 'control', 'your', 'brain', '(', 'emotions', ',', 'reasoning', ',', 'motivation', ',', 'planning', ',', 'etc', '.', ')', ',', 'the', 'easier', 'it', 'will', 'be', 'to', 'progress', 'toward', 'success', '.', 'this', 'is', 'why', 'some', 'psychiatrist', 'won', \"'\", 't', 'pre', '##scribe', 'med', '##s', 'without', 'concurrent', 'psycho', '##therapy', 'treatment', '.', 'even', 'though', 'it', 'sounds', 'lame', ',', 'therapy', 'teaches', 'you', 'what', 'you', \"'\", 're', 'doing', 'wrong', 'with', 'your', 'car', '.', 'there', 'are', 'two', 'treatment', 'options', ':', 'medication', ',', 'and', 'therapy', '.', 'both', 'make', 'it', 'easier', 'to', 'drive', ',', 'but', 'only', 'one', 'is', ',', 'for', 'the', 'most', 'part', ',', 'permanent', '.', 'don', \"'\", 't', 'just', 'rely', 'on', 'the', 'temporary', 'solution', 'of', 'medication', ',', 'use', 'the', 'medication', 'to', 'make', 'it', 'easier', 'to', 'drive', ',', 'and', 'to', 'also', 'make', 'it', 'easier', 'to', 'learn', 'how', 'to', 'drive', '.', 'after', 'all', ',', 'its', 'easier', 'to', 'learn', 'how', 'to', 'drive', 'in', 'a', 'clean', 'co', '##roll', '##a', 'with', 'a', 'a', 'driving', 'instructor', ',', 'than', 'an', 'uncomfortable', 'ferrari', 'by', 'your', 'lone', '##some', '.']\n",
      "INFO:__main__:Number of tokens: 680\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['i', 'received', 'good', 'responses', 'when', 'i', 'commented', 'this', ',', 'so', 'i', \"'\", 'll', 'just', 'post', 'it', '.', 'it', \"'\", 's', 'a', 'nice', 'way', 'of', 'explaining', 'add', '/', 'ad', '##hd', 'and', 'treatment', 'to', 'a', 'parent', 'or', 'anyone', 'else', 'wondering', '/', 'worried', 'about', 'it', '.', 'imagine', 'you', \"'\", 're', 'driving', 'on', 'an', 'unfamiliar', 'road', ',', 'blazing', 'your', 'own', 'trail', 'to', 'a', 'destination', 'specific', 'to', 'you', '.', 'there', 'will', 'be', 'rocks', ',', 'trees', ',', 'animals', ',', 'weather', ',', 'other', 'drivers', ',', 'and', 'all', 'sorts', 'of', 'obstacles', 'in', 'your', 'way', 'that', 'you', 'must', 'avoid', ',', 'conquer', ',', 'and', 'plan', 'for', '.', 'you', \"'\", 're', 'in', 'a', 'car', 'that', 'has', 'some', 'serious', 'power', '.', 'imagine', 'driving', 'a', 'ferrari', ';', 'you', 'definitely', 'need', 'good', 'driving', 'technique', 'to', 'handle', 'all', 'that', 'power', 'and', 'keep', 'from', 'crashing', '.', 'unfortunately', ',', 'its', 'already', 'hard', 'to', 'control', 'because', 'the', 'steering', 'wheel', 'is', 'covered', 'with', 'dried', 'cola', ',', 'the', 'radio', 'is', 'too', 'loud', 'and', 'stuck', 'on', 'political', 'talk', 'radio', '(', 'which', 'is', 'scar', '##ing', 'the', 'shit', 'out', 'of', 'you', ',', 'naturally', ')', ',', 'the', 'windows', 'are', 'dirty', ',', 'the', 'a', '/', 'c', 'isn', \"'\", 't', 'working', ',', 'etc', '.', 'because', 'of', 'these', 'distraction', '##s', ',', 'its', 'hard', 'to', 'control', 'your', 'car', ',', 'and', 'you', \"'\", 're', 'likely', 'to', 'get', 'caught', 'up', ',', 'or', 'even', 'crash', 'and', 'burn', 'when', 'obstacles', 'cross', 'your', 'path', '.', 'on', 'top', 'of', 'the', 'distraction', '##s', ',', 'you', \"'\", 've', 'been', 'driving', 'the', 'same', 'ferrari', 'for', 'all', 'your', 'life', ',', 'but', 'you', \"'\", 've', 'never', 'really', 'learned', 'how', 'to', 'control', 'all', 'that', 'power', '.', 'if', 'you', 'were', 'in', 'a', 'co', '##roll', '##a', ',', 'it', 'would', 'be', 'inherently', 'easier', 'to', 'drive', 'and', 'master', 'without', 'much', 'training', '.', 'the', 'terrain', 'you', 'are', 'travelling', 'on', ',', 'with', 'all', 'its', 'obstacles', ',', 'is', 'your', 'life', '.', 'your', 'destination', 'is', 'success', '(', 'whatever', 'your', 'idea', 'of', 'success', 'is', ')', '.', 'the', 'car', 'is', 'your', 'brain', ',', 'and', 'the', 'driver', 'is', 'you', '(', 'your', 'decision', 'making', 'processes', ')', '.', 'it', \"'\", 's', 'hard', 'for', 'you', 'to', 'drive', 'because', 'you', 'are', '1', ')', 'di', '##sta', '##cted', 'and', '2', ')', 'you', 'barely', 'know', 'how', 'to', 'drive', 'the', 'car', 'safely', 'through', 'the', 'terrain', 'towards', 'your', 'destination', '(', 'if', 'you', 'did', ',', 'you', 'wouldn', \"'\", 't', 'have', 'sought', 'psychiatric', '/', 'psycho', '##logic', 'help', ')', '.', 'from', 'my', 'ex', '##pi', '##rien', '##ce', 'with', 'add', '##eral', '##l', 'and', 'rita', '##lin', ',', 'i', 'can', 'say', 'that', 'the', 'medications', 'clear', 'out', 'the', 'distraction', '##s', ',', 'and', 'slow', 'down', 'your', 'car', ',', 'making', 'it', 'way', 'easier', 'to', 'drive', '.', 'no', 'scary', 'radio', ',', 'no', 'dirty', 'windows', ',', 'no', 'sticky', 'steering', 'wheel', ',', 'just', 'you', 'and', 'the', 'terrain', '.', 'the', 'other', 'reason', 'you', 'are', 'having', 'problems', 'driving', 'is', 'the', 'fact', 'that', 'you', 'are', 'not', 'too', 'good', 'of', 'a', 'driver', '.', 'the', 'solution', 'to', 'this', 'problem', 'is', 'very', 'simple', ',', 'learn', 'how', 'to', 'drive', '.', 'you', 'do', 'that', 'by', 'knowing', 'how', 'to', 'control', 'your', 'car', 'in', 'different', 'situations', '.', 'the', 'more', 'you', 'know', 'about', 'the', 'road', ',', 'and', 'what', 'your', 'car', 'can', 'handle', ',', 'the', 'easier', 'it', 'is', 'to', 'make', 'driving', 'decisions', '.', 'basically', ',', 'the', 'more'], ['you', 'know', 'about', 'life', ',', 'and', 'the', 'more', 'you', 'can', 'control', 'your', 'brain', '(', 'emotions', ',', 'reasoning', ',', 'motivation', ',', 'planning', ',', 'etc', '.', ')', ',', 'the', 'easier', 'it', 'will', 'be', 'to', 'progress', 'toward', 'success', '.', 'this', 'is', 'why', 'some', 'psychiatrist', 'won', \"'\", 't', 'pre', '##scribe', 'med', '##s', 'without', 'concurrent', 'psycho', '##therapy', 'treatment', '.', 'even', 'though', 'it', 'sounds', 'lame', ',', 'therapy', 'teaches', 'you', 'what', 'you', \"'\", 're', 'doing', 'wrong', 'with', 'your', 'car', '.', 'there', 'are', 'two', 'treatment', 'options', ':', 'medication', ',', 'and', 'therapy', '.', 'both', 'make', 'it', 'easier', 'to', 'drive', ',', 'but', 'only', 'one', 'is', ',', 'for', 'the', 'most', 'part', ',', 'permanent', '.', 'don', \"'\", 't', 'just', 'rely', 'on', 'the', 'temporary', 'solution', 'of', 'medication', ',', 'use', 'the', 'medication', 'to', 'make', 'it', 'easier', 'to', 'drive', ',', 'and', 'to', 'also', 'make', 'it', 'easier', 'to', 'learn', 'how', 'to', 'drive', '.', 'after', 'all', ',', 'its', 'easier', 'to', 'learn', 'how', 'to', 'drive', 'in', 'a', 'clean', 'co', '##roll', '##a', 'with', 'a', 'a', 'driving', 'instructor', ',', 'than', 'an', 'uncomfortable', 'ferrari', 'by', 'your', 'lone', '##some', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['doctor', 'wants', 'to', 'pre', '##scribe', 'me', 'lam', '##ic', '##tal', '.', 'does', 'anyone', 'have', 'experience', 'with', 'this', 'drug', '?', 'it', \"'\", 's', 'listed', 'as', 'an', 'anti', '##con', '##vu', '##ls', '##ant', ',', 'but', 'apparently', 'it', \"'\", 's', 'used', 'for', 'depression', 'and', 'ad', '##hd', ',', 'as', 'well', '.', 'i', \"'\", 'm', 'completely', 'unfamiliar', 'with', 'lam', '##ic', '##tal', 'and', 'was', 'wondering', 'if', 'any', 'red', '##dit', '##ors', 'have', 'tried', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 66\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['doctor', 'wants', 'to', 'pre', '##scribe', 'me', 'lam', '##ic', '##tal', '.', 'does', 'anyone', 'have', 'experience', 'with', 'this', 'drug', '?', 'it', \"'\", 's', 'listed', 'as', 'an', 'anti', '##con', '##vu', '##ls', '##ant', ',', 'but', 'apparently', 'it', \"'\", 's', 'used', 'for', 'depression', 'and', 'ad', '##hd', ',', 'as', 'well', '.', 'i', \"'\", 'm', 'completely', 'unfamiliar', 'with', 'lam', '##ic', '##tal', 'and', 'was', 'wondering', 'if', 'any', 'red', '##dit', '##ors', 'have', 'tried', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tips', 'on', 'developing', 'time', 'management', 'skills', '?', 'i', 'started', 'my', 'current', 'medication', '(', 'st', '##rat', '##tera', 'and', 'we', '##lb', '##ut', '##rin', ')', 'about', 'a', 'month', 'ago', '.', 'since', 'then', ',', 'i', \"'\", 'm', 'doing', 'a', 'lot', 'more', 'than', 'i', 'have', '.', '.', '.', 'practically', 'ever', '!', 'but', 'it', 'seems', 'like', 'i', 'still', 'have', 'to', 'work', 'on', 'managing', 'time', 'properly', '.', 'i', 'have', 'set', 'up', 'a', 'calendar', 'with', 'reminder', '##s', 'to', 'do', 'things', 'like', 'laundry', ',', 'getting', 'groceries', ',', 'studying', ',', 'etc', '.', 'it', 'helps', ',', 'but', 'i', 'seem', 'to', 'still', 'fall', 'behind', 'on', 'random', 'points', 'in', 'the', 'schedule', '.', 'i', 'still', '\"', 'wander', '\"', 'a', 'little', '(', 'it', \"'\", 's', 'dramatically', 'reduced', 'im', '##o', ')', 'but', 'i', 'think', 'i', 'might', 'be', 'over', 'doing', 'it', '.', 'have', 'any', 'of', 'you', 'successfully', 'gone', 'from', 'running', 'in', 'circles', ',', 'to', 'actually', 'being', 'productive', '?']\n",
      "INFO:__main__:Number of tokens: 141\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tips', 'on', 'developing', 'time', 'management', 'skills', '?', 'i', 'started', 'my', 'current', 'medication', '(', 'st', '##rat', '##tera', 'and', 'we', '##lb', '##ut', '##rin', ')', 'about', 'a', 'month', 'ago', '.', 'since', 'then', ',', 'i', \"'\", 'm', 'doing', 'a', 'lot', 'more', 'than', 'i', 'have', '.', '.', '.', 'practically', 'ever', '!', 'but', 'it', 'seems', 'like', 'i', 'still', 'have', 'to', 'work', 'on', 'managing', 'time', 'properly', '.', 'i', 'have', 'set', 'up', 'a', 'calendar', 'with', 'reminder', '##s', 'to', 'do', 'things', 'like', 'laundry', ',', 'getting', 'groceries', ',', 'studying', ',', 'etc', '.', 'it', 'helps', ',', 'but', 'i', 'seem', 'to', 'still', 'fall', 'behind', 'on', 'random', 'points', 'in', 'the', 'schedule', '.', 'i', 'still', '\"', 'wander', '\"', 'a', 'little', '(', 'it', \"'\", 's', 'dramatically', 'reduced', 'im', '##o', ')', 'but', 'i', 'think', 'i', 'might', 'be', 'over', 'doing', 'it', '.', 'have', 'any', 'of', 'you', 'successfully', 'gone', 'from', 'running', 'in', 'circles', ',', 'to', 'actually', 'being', 'productive', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '+', 'tv', ':', 'what', \"'\", 's', 'the', 'relationship', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '+', 'tv', ':', 'what', \"'\", 's', 'the', 'relationship', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '+', 'high', 'iq', '-', 'totally', 'add', \"'\", 's', 'dr', '.', 'jain', 'considers', 'research', 'by', 'steve', 'far', '##ao', '##ne', 'in', 'which', 'ad', '##hd', 'individuals', 'with', 'high', 'iq', 'were', 'compared', 'to', 'individuals', 'who', 'were', 'not', 'ad', '##hd', 'but', 'had', 'high', 'iq', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 44\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '+', 'high', 'iq', '-', 'totally', 'add', \"'\", 's', 'dr', '.', 'jain', 'considers', 'research', 'by', 'steve', 'far', '##ao', '##ne', 'in', 'which', 'ad', '##hd', 'individuals', 'with', 'high', 'iq', 'were', 'compared', 'to', 'individuals', 'who', 'were', 'not', 'ad', '##hd', 'but', 'had', 'high', 'iq', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'science', '-', 'dr', 'j', 'considers', 'recent', 'research', 'into', 'factors', 'that', 'cause', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'science', '-', 'dr', 'j', 'considers', 'recent', 'research', 'into', 'factors', 'that', 'cause', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'shortage', 'i', 'knew', 'it', 'was', 'bad', '.', 'however', ',', 'there', 'is', 'not', 'one', 'pharmacy', 'in', 'raleigh', 'that', 'has', 'any', 'left', 'at', 'least', 'in', 'my', 'area', '.', 'i', 'got', 'the', 'last', '20', 'mg', 'prescription', '.', 'wal', '##mart', 'says', 'they', 'have', 'called', 'everywhere', '?', 'anyone', 'else', 'having', 'this', 'problem', '?']\n",
      "INFO:__main__:Number of tokens: 51\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'shortage', 'i', 'knew', 'it', 'was', 'bad', '.', 'however', ',', 'there', 'is', 'not', 'one', 'pharmacy', 'in', 'raleigh', 'that', 'has', 'any', 'left', 'at', 'least', 'in', 'my', 'area', '.', 'i', 'got', 'the', 'last', '20', 'mg', 'prescription', '.', 'wal', '##mart', 'says', 'they', 'have', 'called', 'everywhere', '?', 'anyone', 'else', 'having', 'this', 'problem', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['different', 'kinds', 'of', 'ad', '##hd', ',', 'different', 'kinds', 'of', 'coaches', '|', 'psychology', 'today']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['different', 'kinds', 'of', 'ad', '##hd', ',', 'different', 'kinds', 'of', 'coaches', '|', 'psychology', 'today']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'can', 'you', 'do', 'when', 'you', 'just', 'feel', 'stupid', '?', 'it', \"'\", 's', 'been', 'one', 'of', 'those', 'days', 'for', 'me', '.', '.', '.', 'when', 'i', 'screw', 'everything', 'up', 'yet', 'again', 'and', 'feel', 'as', 'though', 'i', \"'\", 'm', 'an', 'idiot', 'with', 'no', 'purpose', '.', 'how', 'can', 'i', 'cope', '?']\n",
      "INFO:__main__:Number of tokens: 48\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'can', 'you', 'do', 'when', 'you', 'just', 'feel', 'stupid', '?', 'it', \"'\", 's', 'been', 'one', 'of', 'those', 'days', 'for', 'me', '.', '.', '.', 'when', 'i', 'screw', 'everything', 'up', 'yet', 'again', 'and', 'feel', 'as', 'though', 'i', \"'\", 'm', 'an', 'idiot', 'with', 'no', 'purpose', '.', 'how', 'can', 'i', 'cope', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'without', 'hyper', '##act', '##ivity', 'i', 'realized', 'a', 'few', 'weeks', 'ago', 'that', 'it', 'was', 'possible', 'to', 'have', 'ad', '##hd', 'without', 'hyper', '##act', '##ivity', 'and', 'it', 'got', 'me', 'thinking', '.', 'i', 'started', 'looking', 'at', 'symptoms', 'and', 'a', 'majority', '(', 'i', \"'\", 'd', 'say', '90', '%', ')', ',', 'fit', 'me', 'to', '-', 'a', '-', 'tee', '.', 'throughout', 'elementary', 'school', 'i', 'always', 'got', 'straight', 'a', \"'\", 's', 'with', 'a', 'few', 'occasional', 'b', \"'\", 's', '.', 'i', 'was', 'always', 'one', 'of', 'the', 'quiet', 'kids', '-', 'i', 'really', 'failed', 'at', 'making', 'small', 'talk', 'and', ',', 'while', 'i', 'had', 'friends', ',', 'it', 'was', 'difficult', 'to', 'make', 'new', 'ones', '.', 'once', 'middle', 'school', 'hit', 'and', 'the', 'material', 'got', 'harder', '-', 'my', 'grades', 'in', 'math', 'went', 'from', 'a', \"'\", 's', 'and', 'tank', '##ed', 'to', 'c', \"'\", 's', 'once', 'algebra', 'hit', 'in', '7th', 'grade', '.', 'i', \"'\", 'm', 'a', 'freshman', 'in', 'college', 'now', 'and', 'i', 'am', 'still', 'failing', 'relatively', 'basic', 'algebra', 'courses', '.', 'more', 'to', 'my', 'point', ':', 'every', 'day', 'when', 'i', 'wake', 'up', ',', 'my', 'mind', 'always', 'feels', 'cl', '##uttered', 'with', 'random', 'thoughts', 'about', 'everything', '.', 'it', \"'\", 's', 'racing', '.', 'sometimes', 'i', \"'\", 'll', 'open', 'up', 'note', '##pad', 'on', 'my', 'computer', 'and', 'just', 'type', 'up', '5', 'paragraph', '##s', 'of', 'thoughts', 'in', 'a', 'few', 'minutes', 'randomly', ',', 'but', 'it', \"'\", 'll', 'take', 'me', 'two', 'weeks', 'to', 'write', 'a', 'two', 'page', 'paper', '.', 'my', 'grades', 'in', 'college', 'are', '.', '.', '.', '.', 'adequate', '.', '.', '.', 'low', 'b', \"'\", 's', 'and', 'a', 'c', '.', 'when', 'i', 'try', 'to', 'study', ',', 'i', 'get', 'extremely', 'frustrated', 'and', 'impatient', 'trying', 'to', 'look', 'at', 'notes', '.', 'i', 'wind', 'up', 'pro', '##cr', '##ast', '##inating', 'everything', '.', 'my', 'homework', 'score', 'in', 'math', 'is', 'a', '31', '%', 'because', 'i', 'start', 'the', 'material', 'but', 'the', 'second', 'it', 'gets', 'difficult', 'i', 'give', 'up', 'and', 'i', 'cannot', 'focus', 'on', 'it', 'long', 'enough', '.', 'if', 'something', 'doesn', \"'\", 't', 'interest', 'me', ',', 'i', 'feel', 'like', 'my', 'brain', 'is', 'working', 'at', 'its', 'full', 'capacity', 'to', 'stop', 'me', 'from', 'learning', 'it', 'and', 'i', 'wind', 'up', 'getting', 'physically', 'tired', '.', 'i', 'just', 'walked', 'into', 'algebra', 'this', 'morning', 'telling', 'myself', 'i', 'will', 'focus', 'but', 'my', 'mind', 'drift', '##s', 'and', 'i', 'don', \"'\", 't', 'pick', 'up', 'on', 'it', 'until', 'it', \"'\", 's', 'too', 'late', '.', 'at', 'night', 'i', 'usually', 'get', 'depressed', 'because', 'it', \"'\", 's', 'still', 'hard', 'for', 'me', 'to', 'make', 'friends', 'and', 'i', 'have', 'a', 'really', 'low', '-', 'self', 'confidence', 'because', 'of', 'it', '.', 'i', \"'\", 'm', 'not', 'learning', 'anything', 'in', 'college', 'and', 'i', 'want', 'to', 'leave', ',', 'only', 'because', 'i', 'wait', 'until', 'the', 'last', 'second', 'to', 'do', 'things', 'and', 'wind', 'up', 'not', 'completing', 'them', '.', 'i', 'have', '3', 'projects', 'ahead', 'of', 'me', 'and', 'none', 'of', 'them', 'are', 'finished', '.', 'i', 'feel', 'extremely', 'sc', '##atter', '-', 'brain', '##ed', '.', 'this', 'goes', 'beyond', 'average', 'pro', '##cr', '##ast', '##ination', 'because', 'i', 'cannot', 'bring', 'myself', 'to', 'finish', 'things', 'consistently', '.', 'it', \"'\", 's', 'really', 'terrible', '.', 'can', 'anyone', 'offer', 'advice', 'or', 'ask', 'me', 'anything', 'else', '.', 'i', \"'\", 'm', 'really', 'never', 'happy', 'with', 'myself', '.']\n",
      "INFO:__main__:Number of tokens: 499\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'without', 'hyper', '##act', '##ivity', 'i', 'realized', 'a', 'few', 'weeks', 'ago', 'that', 'it', 'was', 'possible', 'to', 'have', 'ad', '##hd', 'without', 'hyper', '##act', '##ivity', 'and', 'it', 'got', 'me', 'thinking', '.', 'i', 'started', 'looking', 'at', 'symptoms', 'and', 'a', 'majority', '(', 'i', \"'\", 'd', 'say', '90', '%', ')', ',', 'fit', 'me', 'to', '-', 'a', '-', 'tee', '.', 'throughout', 'elementary', 'school', 'i', 'always', 'got', 'straight', 'a', \"'\", 's', 'with', 'a', 'few', 'occasional', 'b', \"'\", 's', '.', 'i', 'was', 'always', 'one', 'of', 'the', 'quiet', 'kids', '-', 'i', 'really', 'failed', 'at', 'making', 'small', 'talk', 'and', ',', 'while', 'i', 'had', 'friends', ',', 'it', 'was', 'difficult', 'to', 'make', 'new', 'ones', '.', 'once', 'middle', 'school', 'hit', 'and', 'the', 'material', 'got', 'harder', '-', 'my', 'grades', 'in', 'math', 'went', 'from', 'a', \"'\", 's', 'and', 'tank', '##ed', 'to', 'c', \"'\", 's', 'once', 'algebra', 'hit', 'in', '7th', 'grade', '.', 'i', \"'\", 'm', 'a', 'freshman', 'in', 'college', 'now', 'and', 'i', 'am', 'still', 'failing', 'relatively', 'basic', 'algebra', 'courses', '.', 'more', 'to', 'my', 'point', ':', 'every', 'day', 'when', 'i', 'wake', 'up', ',', 'my', 'mind', 'always', 'feels', 'cl', '##uttered', 'with', 'random', 'thoughts', 'about', 'everything', '.', 'it', \"'\", 's', 'racing', '.', 'sometimes', 'i', \"'\", 'll', 'open', 'up', 'note', '##pad', 'on', 'my', 'computer', 'and', 'just', 'type', 'up', '5', 'paragraph', '##s', 'of', 'thoughts', 'in', 'a', 'few', 'minutes', 'randomly', ',', 'but', 'it', \"'\", 'll', 'take', 'me', 'two', 'weeks', 'to', 'write', 'a', 'two', 'page', 'paper', '.', 'my', 'grades', 'in', 'college', 'are', '.', '.', '.', '.', 'adequate', '.', '.', '.', 'low', 'b', \"'\", 's', 'and', 'a', 'c', '.', 'when', 'i', 'try', 'to', 'study', ',', 'i', 'get', 'extremely', 'frustrated', 'and', 'impatient', 'trying', 'to', 'look', 'at', 'notes', '.', 'i', 'wind', 'up', 'pro', '##cr', '##ast', '##inating', 'everything', '.', 'my', 'homework', 'score', 'in', 'math', 'is', 'a', '31', '%', 'because', 'i', 'start', 'the', 'material', 'but', 'the', 'second', 'it', 'gets', 'difficult', 'i', 'give', 'up', 'and', 'i', 'cannot', 'focus', 'on', 'it', 'long', 'enough', '.', 'if', 'something', 'doesn', \"'\", 't', 'interest', 'me', ',', 'i', 'feel', 'like', 'my', 'brain', 'is', 'working', 'at', 'its', 'full', 'capacity', 'to', 'stop', 'me', 'from', 'learning', 'it', 'and', 'i', 'wind', 'up', 'getting', 'physically', 'tired', '.', 'i', 'just', 'walked', 'into', 'algebra', 'this', 'morning', 'telling', 'myself', 'i', 'will', 'focus', 'but', 'my', 'mind', 'drift', '##s', 'and', 'i', 'don', \"'\", 't', 'pick', 'up', 'on', 'it', 'until', 'it', \"'\", 's', 'too', 'late', '.', 'at', 'night', 'i', 'usually', 'get', 'depressed', 'because', 'it', \"'\", 's', 'still', 'hard', 'for', 'me', 'to', 'make', 'friends', 'and', 'i', 'have', 'a', 'really', 'low', '-', 'self', 'confidence', 'because', 'of', 'it', '.', 'i', \"'\", 'm', 'not', 'learning', 'anything', 'in', 'college', 'and', 'i', 'want', 'to', 'leave', ',', 'only', 'because', 'i', 'wait', 'until', 'the', 'last', 'second', 'to', 'do', 'things', 'and', 'wind', 'up', 'not', 'completing', 'them', '.', 'i', 'have', '3', 'projects', 'ahead', 'of', 'me', 'and', 'none', 'of', 'them', 'are', 'finished', '.', 'i', 'feel', 'extremely', 'sc', '##atter', '-', 'brain', '##ed', '.', 'this', 'goes', 'beyond', 'average', 'pro', '##cr', '##ast', '##ination', 'because', 'i', 'cannot', 'bring', 'myself', 'to', 'finish', 'things', 'consistently', '.', 'it', \"'\", 's', 'really', 'terrible', '.', 'can', 'anyone', 'offer', 'advice', 'or', 'ask', 'me', 'anything', 'else', '.', 'i', \"'\", 'm', 'really', 'never', 'happy', 'with', 'myself', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'ran', '##t', '(', 'will', '##power', ',', 'some', 'other', 'stuff', ')']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'ran', '##t', '(', 'will', '##power', ',', 'some', 'other', 'stuff', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['doctor', 'doesn', \"'\", 't', 'think', 'i', 'have', 'ad', '##hd', '.', 'prescribed', 'an', 'ssr', '##i', '.', 't', '##l', ';', 'dr', ':', 'might', 'have', 'ad', '##hd', '.', 'a', 'physician', 'thought', 'i', 'had', 'anxiety', '/', 'depression', 'and', 'prescribed', 'ci', '##tal', '##op', '##ram', '(', 'an', 'ssr', '##i', ')', '.', 'booked', 'a', 'proper', 'diagnosis', 'session', 'with', 'a', 'psychologist', '.', 'should', 'i', 'keep', 'taking', 'the', 'ssr', '##i', '?', 'hi', 'red', '##dit', '.', 'for', 'the', 'past', 'seven', 'or', 'eight', 'years', '(', 'i', \"'\", 'm', '23', ')', 'i', \"'\", 've', 'struggled', 'with', 'keeping', 'up', 'with', 'school', '-', '-', 'safely', 'passing', 'two', 'or', 'three', 'subjects', 'each', 'year', ',', 'barely', 'passing', 'the', 'other', 'five', 'or', 'six', '.', 'my', 'inability', 'to', 'keep', 'schedule', 'has', 'become', 'progressively', 'less', 'manage', '##able', 'with', 'every', 'year', 'passing', 'and', 'for', 'the', 'past', 'couple', 'of', 'years', 'i', \"'\", 've', 'been', 'semi', '-', 'actively', 'trying', 'to', 'find', 'something', 'in', 'psychology', 'or', 'psychiatry', 'to', 'help', 'me', '.', 'for', 'i', 'don', \"'\", 't', 'know', 'how', 'many', 'years', 'i', \"'\", 've', 'occasionally', 'had', 'people', 'mentioning', 'that', 'i', 'might', 'have', 'a', 'touch', 'of', 'add', ',', 'including', 'my', 'father', 'and', 'my', 'now', 'ex', '-', 'g', '##f', '.', 'however', ',', 'i', 'never', 'took', 'it', 'seriously', '.', 'if', 'i', 'had', 'it', 'in', 'the', 'first', 'place', ',', 'i', \"'\", 'd', 'have', 'passed', 'by', 'the', 'most', 'difficult', 'part', 'anyway', ',', 'right', '?', 'i', \"'\", 'm', 'up', 'to', 'college', 'and', 'it', \"'\", 's', 'only', 'kids', 'who', 'deal', 'with', 'add', ',', 'right', '?', 'well', ',', 'trying', 'to', 'cut', 'the', 'chase', ',', 'i', 'finally', 'put', 'two', 'or', 'two', 'together', 'and', 'realized', 'ad', '##hd', 'might', 'actually', 'be', 'something', 'i', 'have', 'and', 'something', 'that', \"'\", 's', 'di', '##sa', '##bling', 'me', '.', 'after', 'looking', 'up', 'and', 'recognizing', 'the', 'symptoms', ',', 'i', 'went', 'to', 'a', 'general', 'physician', 'to', 'see', 'if', 'he', 'could', 'help', 'me', 'find', 'a', 'psychiatrist', 'or', 'a', 'psychologist', 'to', 'deal', 'with', 'this', '.', 'well', ',', 'he', 'turned', 'out', 'pretty', 'inc', '##red', '##ulous', 'about', 'my', 'conjecture', ',', 'said', 'he', \"'\", 'd', 'drop', 'an', 'ad', '##hd', 'specialist', 'a', 'line', 'but', 'in', 'the', 'meanwhile', ',', 'i', 'should', 'try', 'ci', '##tal', '##op', '##ram', 'and', 'told', 'me', 'to', 'come', 'back', 'three', 'weeks', 'later', '.', 'even', 'though', 'i', \"'\", 've', 'been', 'tested', 'for', 'those', '-', '-', 'including', 'once', 'during', 'that', 'session', ',', 'with', 'nothing', 'more', 'than', 'ambiguous', 'results', '-', '-', 'he', 'believed', 'i', 'probably', 'had', 'depression', 'or', 'anxiety', 'disorders', '.', 'since', 'no', 'psychiatrist', '##s', 'were', 'immediately', 'available', ',', 'i', 'contacted', 'a', 'psychologist', 'who', 'does', 'ad', '##hd', 'diagnosis', 'and', 'she', \"'\", 'll', 'see', 'me', 'monday', 'after', 'a', 'week', 'now', '.', 'well', ',', 'i', \"'\", 've', 'been', 'taking', 'the', 'ci', '##tal', '##op', '##ram', 'for', 'three', 'days', 'now', ',', 'half', 'dos', '##age', 'every', 'day', ',', 'and', 'my', 'first', 'full', 'dose', 'should', 'be', 'tomorrow', '.', 'i', 'didn', \"'\", 't', 'feel', 'anything', 'until', 'tonight', '.', 'it', \"'\", 's', 'the', 'weird', '##est', 'thing', ',', 'i', \"'\", 've', 'never', 'felt', 'this', 'way', 'except', 'for', 'the', 'few', 'times', 'i', \"'\", 've', 'smoked', 'pot', '(', 'which', 'i', 'don', \"'\", 't', 'like', 'very', 'much', 'at', 'all', ')', '.', 'i', \"'\", 'm', 'fairly', 'pl', '##ac', '##id', ',', 'my', 'heart', 'seems', 'to', 'beat', 'slightly', 'heavier', 'than', 'usual', '(', 'he', 'warned', 'me', 'about', 'anxiety', '-', 'like', 'symptoms', 'for', 'the', 'first', 'week', 'or', 'so', ')', 'but', 'my', 'at', '##ten', '##tive', '##ness', 'has', 'plum', '##met', '##ed', '.', 'it', 'feels', 'a', 'little', 'like', 'i', \"'\", 'm', 'completely', 'exhausted', ',', 'only', 'i', \"'\", 'm', 'physically', 'well', '-', 'awake', ';', 'i', 'just', 'can', \"'\", 't', 'focus', 'on', 'anything', '.', 'if', 'this', 'is', 'how', 'i', \"'\", 'll', 'be', 'feeling', 'for', 'the', 'whole', 'time', 'i', \"'\", 'm', 'on', 'this', ',', 'i', 'cannot', 'afford', 'it', '.', 'multiple', 'days', 'of', 'school', 'and', 'homework', 'with', 'even', 'lower', 'at', '##ten', '##tive', '##ness', 'than', 'usual', 'would', 'be', 'a', 'dear', 'price', 'just', 'to', 'ind', '##ul', '##ge', 'a', 'physician', '.', 'so', 'what', 'do', 'you', 'recommend', ',', 'r', '/', 'ad', '##hd', '?', 'am', 'i', 'likely', 'to', 'keep', 'experiencing', 'this', 'feeling', 'or', 'is', 'it', 'just', 'temporary', 'like', 'the', 'anxiety', 'symptoms', '?', 'could', 'it', 'be', 'some', 'sort', 'of', 'place', '##bo', 'effect', '?', 'is', 'there', 'good', 'reason', 'for', 'me', 'to', 'put', 'this', 'drug', 'aside', 'until', 'i', 'get', 'results', 'from', 'a', 'proper', 'diagnosis', '?']\n",
      "INFO:__main__:Number of tokens: 674\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['doctor', 'doesn', \"'\", 't', 'think', 'i', 'have', 'ad', '##hd', '.', 'prescribed', 'an', 'ssr', '##i', '.', 't', '##l', ';', 'dr', ':', 'might', 'have', 'ad', '##hd', '.', 'a', 'physician', 'thought', 'i', 'had', 'anxiety', '/', 'depression', 'and', 'prescribed', 'ci', '##tal', '##op', '##ram', '(', 'an', 'ssr', '##i', ')', '.', 'booked', 'a', 'proper', 'diagnosis', 'session', 'with', 'a', 'psychologist', '.', 'should', 'i', 'keep', 'taking', 'the', 'ssr', '##i', '?', 'hi', 'red', '##dit', '.', 'for', 'the', 'past', 'seven', 'or', 'eight', 'years', '(', 'i', \"'\", 'm', '23', ')', 'i', \"'\", 've', 'struggled', 'with', 'keeping', 'up', 'with', 'school', '-', '-', 'safely', 'passing', 'two', 'or', 'three', 'subjects', 'each', 'year', ',', 'barely', 'passing', 'the', 'other', 'five', 'or', 'six', '.', 'my', 'inability', 'to', 'keep', 'schedule', 'has', 'become', 'progressively', 'less', 'manage', '##able', 'with', 'every', 'year', 'passing', 'and', 'for', 'the', 'past', 'couple', 'of', 'years', 'i', \"'\", 've', 'been', 'semi', '-', 'actively', 'trying', 'to', 'find', 'something', 'in', 'psychology', 'or', 'psychiatry', 'to', 'help', 'me', '.', 'for', 'i', 'don', \"'\", 't', 'know', 'how', 'many', 'years', 'i', \"'\", 've', 'occasionally', 'had', 'people', 'mentioning', 'that', 'i', 'might', 'have', 'a', 'touch', 'of', 'add', ',', 'including', 'my', 'father', 'and', 'my', 'now', 'ex', '-', 'g', '##f', '.', 'however', ',', 'i', 'never', 'took', 'it', 'seriously', '.', 'if', 'i', 'had', 'it', 'in', 'the', 'first', 'place', ',', 'i', \"'\", 'd', 'have', 'passed', 'by', 'the', 'most', 'difficult', 'part', 'anyway', ',', 'right', '?', 'i', \"'\", 'm', 'up', 'to', 'college', 'and', 'it', \"'\", 's', 'only', 'kids', 'who', 'deal', 'with', 'add', ',', 'right', '?', 'well', ',', 'trying', 'to', 'cut', 'the', 'chase', ',', 'i', 'finally', 'put', 'two', 'or', 'two', 'together', 'and', 'realized', 'ad', '##hd', 'might', 'actually', 'be', 'something', 'i', 'have', 'and', 'something', 'that', \"'\", 's', 'di', '##sa', '##bling', 'me', '.', 'after', 'looking', 'up', 'and', 'recognizing', 'the', 'symptoms', ',', 'i', 'went', 'to', 'a', 'general', 'physician', 'to', 'see', 'if', 'he', 'could', 'help', 'me', 'find', 'a', 'psychiatrist', 'or', 'a', 'psychologist', 'to', 'deal', 'with', 'this', '.', 'well', ',', 'he', 'turned', 'out', 'pretty', 'inc', '##red', '##ulous', 'about', 'my', 'conjecture', ',', 'said', 'he', \"'\", 'd', 'drop', 'an', 'ad', '##hd', 'specialist', 'a', 'line', 'but', 'in', 'the', 'meanwhile', ',', 'i', 'should', 'try', 'ci', '##tal', '##op', '##ram', 'and', 'told', 'me', 'to', 'come', 'back', 'three', 'weeks', 'later', '.', 'even', 'though', 'i', \"'\", 've', 'been', 'tested', 'for', 'those', '-', '-', 'including', 'once', 'during', 'that', 'session', ',', 'with', 'nothing', 'more', 'than', 'ambiguous', 'results', '-', '-', 'he', 'believed', 'i', 'probably', 'had', 'depression', 'or', 'anxiety', 'disorders', '.', 'since', 'no', 'psychiatrist', '##s', 'were', 'immediately', 'available', ',', 'i', 'contacted', 'a', 'psychologist', 'who', 'does', 'ad', '##hd', 'diagnosis', 'and', 'she', \"'\", 'll', 'see', 'me', 'monday', 'after', 'a', 'week', 'now', '.', 'well', ',', 'i', \"'\", 've', 'been', 'taking', 'the', 'ci', '##tal', '##op', '##ram', 'for', 'three', 'days', 'now', ',', 'half', 'dos', '##age', 'every', 'day', ',', 'and', 'my', 'first', 'full', 'dose', 'should', 'be', 'tomorrow', '.', 'i', 'didn', \"'\", 't', 'feel', 'anything', 'until', 'tonight', '.', 'it', \"'\", 's', 'the', 'weird', '##est', 'thing', ',', 'i', \"'\", 've', 'never', 'felt', 'this', 'way', 'except', 'for', 'the', 'few', 'times', 'i', \"'\", 've', 'smoked', 'pot', '(', 'which', 'i', 'don', \"'\", 't', 'like', 'very', 'much', 'at', 'all', ')', '.', 'i', \"'\", 'm', 'fairly', 'pl', '##ac', '##id', ',', 'my', 'heart', 'seems', 'to', 'beat', 'slightly', 'heavier', 'than', 'usual', '(', 'he', 'warned', 'me', 'about'], ['anxiety', '-', 'like', 'symptoms', 'for', 'the', 'first', 'week', 'or', 'so', ')', 'but', 'my', 'at', '##ten', '##tive', '##ness', 'has', 'plum', '##met', '##ed', '.', 'it', 'feels', 'a', 'little', 'like', 'i', \"'\", 'm', 'completely', 'exhausted', ',', 'only', 'i', \"'\", 'm', 'physically', 'well', '-', 'awake', ';', 'i', 'just', 'can', \"'\", 't', 'focus', 'on', 'anything', '.', 'if', 'this', 'is', 'how', 'i', \"'\", 'll', 'be', 'feeling', 'for', 'the', 'whole', 'time', 'i', \"'\", 'm', 'on', 'this', ',', 'i', 'cannot', 'afford', 'it', '.', 'multiple', 'days', 'of', 'school', 'and', 'homework', 'with', 'even', 'lower', 'at', '##ten', '##tive', '##ness', 'than', 'usual', 'would', 'be', 'a', 'dear', 'price', 'just', 'to', 'ind', '##ul', '##ge', 'a', 'physician', '.', 'so', 'what', 'do', 'you', 'recommend', ',', 'r', '/', 'ad', '##hd', '?', 'am', 'i', 'likely', 'to', 'keep', 'experiencing', 'this', 'feeling', 'or', 'is', 'it', 'just', 'temporary', 'like', 'the', 'anxiety', 'symptoms', '?', 'could', 'it', 'be', 'some', 'sort', 'of', 'place', '##bo', 'effect', '?', 'is', 'there', 'good', 'reason', 'for', 'me', 'to', 'put', 'this', 'drug', 'aside', 'until', 'i', 'get', 'results', 'from', 'a', 'proper', 'diagnosis', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dr', 'russell', 'bark', '##ley', '-', 'ad', '##hd', 'motivation', 'deficit', 'disorder']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dr', 'russell', 'bark', '##ley', '-', 'ad', '##hd', 'motivation', 'deficit', 'disorder']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', \"'\", 's', 'your', 'resting', 'heart', 'rate', '?', 'mine', 'is', '80', '-', '90', 'without', 'concert', '##a', ',', 'would', 'you', 'say', 'that', \"'\", 's', 'ok', '/', 'bad', '?', 'my', 'resting', 'heart', 'rate', 'when', 'waking', 'up', 'is', 'somewhere', 'between', '80', '-', '90', '.', 'after', 'concert', '##a', 'and', 'some', 'coffee', 'i', 'hove', '##r', 'around', 'a', 'hundred', 'bp', '##m', '.', 'what', 'are', 'your', 'experience', 'with', 'resting', 'heart', 'rate', 'and', 'concert', '##a', '?']\n",
      "INFO:__main__:Number of tokens: 68\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', \"'\", 's', 'your', 'resting', 'heart', 'rate', '?', 'mine', 'is', '80', '-', '90', 'without', 'concert', '##a', ',', 'would', 'you', 'say', 'that', \"'\", 's', 'ok', '/', 'bad', '?', 'my', 'resting', 'heart', 'rate', 'when', 'waking', 'up', 'is', 'somewhere', 'between', '80', '-', '90', '.', 'after', 'concert', '##a', 'and', 'some', 'coffee', 'i', 'hove', '##r', 'around', 'a', 'hundred', 'bp', '##m', '.', 'what', 'are', 'your', 'experience', 'with', 'resting', 'heart', 'rate', 'and', 'concert', '##a', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['urgent', ':', 'need', 'tips', 'on', 'focusing', '.']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['urgent', ':', 'need', 'tips', 'on', 'focusing', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['medication', 'often', 'doesn', \"'\", 't', 'seem', 'enough', '.', 'does', 'anyone', 'have', 'experience', 'with', \"'\", 'ad', '##hd', 'coaching', \"'\", 'or', 'similar', 'methods', '?', 'does', 'it', 'help', '?']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['medication', 'often', 'doesn', \"'\", 't', 'seem', 'enough', '.', 'does', 'anyone', 'have', 'experience', 'with', \"'\", 'ad', '##hd', 'coaching', \"'\", 'or', 'similar', 'methods', '?', 'does', 'it', 'help', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['attention', 'disorder', 'drug', 'shortage', 'prompt', '##s', 'finger', '-', 'pointing']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['attention', 'disorder', 'drug', 'shortage', 'prompt', '##s', 'finger', '-', 'pointing']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['should', 'i', 'see', 'a', 'psychologist', 'or', 'ne', '##uro', '##logist', 'for', 'ad', '##hd', '-', 'pi', 'testing', '?', 'i', \"'\", 'd', 'like', 'to', 'get', 'myself', 'checked', 'out', 'for', 'ad', '##hd', '-', 'pi', '.', 'should', 'i', 'see', 'a', 'ne', '##uro', '##logist', 'or', 'psychiatrist', 'for', 'testing', '?', 'any', 'tips', 'on', 'finding', 'a', 'really', 'good', 'doc', '?']\n",
      "INFO:__main__:Number of tokens: 52\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['should', 'i', 'see', 'a', 'psychologist', 'or', 'ne', '##uro', '##logist', 'for', 'ad', '##hd', '-', 'pi', 'testing', '?', 'i', \"'\", 'd', 'like', 'to', 'get', 'myself', 'checked', 'out', 'for', 'ad', '##hd', '-', 'pi', '.', 'should', 'i', 'see', 'a', 'ne', '##uro', '##logist', 'or', 'psychiatrist', 'for', 'testing', '?', 'any', 'tips', 'on', 'finding', 'a', 'really', 'good', 'doc', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'semester', 'in', 'college', ',', 'and', 'i', 'can', \"'\", 't', 'stand', 'to', 'be', 'here', 'any', 'longer', '.', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', 'semester', 'in', 'college', ',', 'and', 'i', 'can', \"'\", 't', 'stand', 'to', 'be', 'here', 'any', 'longer', '.', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['when', 'to', 'quit', '?', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'two', 'months', 'ago', ',', 'i', 'was', 'given', 'a', 'script', 'for', '30', 'tab', '##s', '.', '10', '##mg', 'a', 'day', '.', 'i', 'did', 'that', 'for', '3', 'days', ',', 'nothing', ',', 'i', 'then', 'up', '##ed', 'it', 'to', '20', '##mg', 'a', 'day', ',', 'nothing', '.', 'i', 'then', 'tried', '30', '##mg', 'again', 'nothing', '.', 'i', 'told', 'this', 'to', 'my', 'doctor', ',', 'and', 'he', 'prescribed', 'me', 'rita', '##lin', 'again', 'but', 'this', 'time', 'told', 'me', 'to', 'try', '40', '##mg', '.', 'should', 'i', 'just', 'try', '60', 'mg', 'and', 'see', 'if', 'it', 'has', 'any', 'effect', '?', 'how', 'often', 'does', 'rita', '##lin', 'just', 'not', 'work', '?', 'i', 'ha', '##vn', \"'\", 't', 'really', 'been', 'very', 'consistent', 'with', 'my', 'consumption', 'but', 'from', 'what', 'i', 'can', 'tell', 'it', \"'\", 's', 'a', 'st', '##im', '##ula', '##nt', 'and', 'should', 'have', 'an', 'effect', 'immediately', ',', 'some', 'people', 'suggest', 'waiting', 'several', 'weeks', 'or', 'months', '.']\n",
      "INFO:__main__:Number of tokens: 148\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['when', 'to', 'quit', '?', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'two', 'months', 'ago', ',', 'i', 'was', 'given', 'a', 'script', 'for', '30', 'tab', '##s', '.', '10', '##mg', 'a', 'day', '.', 'i', 'did', 'that', 'for', '3', 'days', ',', 'nothing', ',', 'i', 'then', 'up', '##ed', 'it', 'to', '20', '##mg', 'a', 'day', ',', 'nothing', '.', 'i', 'then', 'tried', '30', '##mg', 'again', 'nothing', '.', 'i', 'told', 'this', 'to', 'my', 'doctor', ',', 'and', 'he', 'prescribed', 'me', 'rita', '##lin', 'again', 'but', 'this', 'time', 'told', 'me', 'to', 'try', '40', '##mg', '.', 'should', 'i', 'just', 'try', '60', 'mg', 'and', 'see', 'if', 'it', 'has', 'any', 'effect', '?', 'how', 'often', 'does', 'rita', '##lin', 'just', 'not', 'work', '?', 'i', 'ha', '##vn', \"'\", 't', 'really', 'been', 'very', 'consistent', 'with', 'my', 'consumption', 'but', 'from', 'what', 'i', 'can', 'tell', 'it', \"'\", 's', 'a', 'st', '##im', '##ula', '##nt', 'and', 'should', 'have', 'an', 'effect', 'immediately', ',', 'some', 'people', 'suggest', 'waiting', 'several', 'weeks', 'or', 'months', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['cigarette', '.', 'yeah', ',', 'i', 'said', 'it', ',', 'don', \"'\", 't', 'act', 'like', 'you', 'weren', \"'\", 't', 'already', 'thinking', 'about', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['cigarette', '.', 'yeah', ',', 'i', 'said', 'it', ',', 'don', \"'\", 't', 'act', 'like', 'you', 'weren', \"'\", 't', 'already', 'thinking', 'about', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'drugs', 'showed', 'no', 'risk', 'of', 'cardiovascular', 'events']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'drugs', 'showed', 'no', 'risk', 'of', 'cardiovascular', 'events']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'prescribed', 'rita', '##lin', '.', '.', '.', 'afraid', 'to', 'start', 'taking', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'prescribed', 'rita', '##lin', '.', '.', '.', 'afraid', 'to', 'start', 'taking', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['beyond', 'medications', '-', 'chad', '##d', 'leadership', 'blog']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['beyond', 'medications', '-', 'chad', '##d', 'leadership', 'blog']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'long', 'did', 'it', 'take', 'you', 'to', 'officially', 'become', 'diagnosed', '?', 'was', 'it', 'a', 'long', 'process', '?', 'short', '?', 'enjoyable', '?', 'awful', '?', 'share', 'your', 'experiences', '.', '.', 'since', 'i', 'have', 'insurance', 'now', ',', 'i', 'will', 'be', 'visiting', 'a', 'doctor', 'or', 'going', 'strait', 'to', 'a', 'ps', '##y', '##cia', '##tri', '##st', 'to', 'explain', 'that', 'i', 'may', 'have', 'ad', '##hd', 'after', 'researching', 'about', 'symptoms', 'and', 'comparing', 'to', 'my', 'own', 'experiences', '.', 'how', 'will', 'they', 'dia', '##gno', '##se', 'me', '?', 'questions', '?', 'physical', 'tests', '?', 'will', 'it', 'take', 'more', 'than', 'one', 'visit', '?', 'if', 'so', ',', 'how', 'many', '?', 'i', 'have', 'read', 'many', 'other', 'posts', 'where', 'people', 'go', 'to', 'get', 'diagnosed', 'for', 'ad', '##hd', 'and', 'leave', 'with', 'a', 'diagnosis', 'for', 'depression', ',', 'or', 'other', 'conditions', '.', 'how', 'often', 'does', 'this', 'happen', '?', 'did', 'it', 'happen', 'to', 'you', '?', 'if', 'you', \"'\", 're', 'comfortable', 'sharing', ',', 'what', 'was', 'your', 'experience', 'with', 'becoming', 'diagnosed', '?', 'thanks', '.', '.']\n",
      "INFO:__main__:Number of tokens: 153\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'long', 'did', 'it', 'take', 'you', 'to', 'officially', 'become', 'diagnosed', '?', 'was', 'it', 'a', 'long', 'process', '?', 'short', '?', 'enjoyable', '?', 'awful', '?', 'share', 'your', 'experiences', '.', '.', 'since', 'i', 'have', 'insurance', 'now', ',', 'i', 'will', 'be', 'visiting', 'a', 'doctor', 'or', 'going', 'strait', 'to', 'a', 'ps', '##y', '##cia', '##tri', '##st', 'to', 'explain', 'that', 'i', 'may', 'have', 'ad', '##hd', 'after', 'researching', 'about', 'symptoms', 'and', 'comparing', 'to', 'my', 'own', 'experiences', '.', 'how', 'will', 'they', 'dia', '##gno', '##se', 'me', '?', 'questions', '?', 'physical', 'tests', '?', 'will', 'it', 'take', 'more', 'than', 'one', 'visit', '?', 'if', 'so', ',', 'how', 'many', '?', 'i', 'have', 'read', 'many', 'other', 'posts', 'where', 'people', 'go', 'to', 'get', 'diagnosed', 'for', 'ad', '##hd', 'and', 'leave', 'with', 'a', 'diagnosis', 'for', 'depression', ',', 'or', 'other', 'conditions', '.', 'how', 'often', 'does', 'this', 'happen', '?', 'did', 'it', 'happen', 'to', 'you', '?', 'if', 'you', \"'\", 're', 'comfortable', 'sharing', ',', 'what', 'was', 'your', 'experience', 'with', 'becoming', 'diagnosed', '?', 'thanks', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['symptoms', 'in', 'gifted', 'children', 'often', 'go', 'un', '##tre', '##ated', 'well', 'into', 'or', 'after', 'college', '.']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['symptoms', 'in', 'gifted', 'children', 'often', 'go', 'un', '##tre', '##ated', 'well', 'into', 'or', 'after', 'college', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['r', '/', 'ad', '##hd', ',', 'do', 'you', 'have', 'any', 'other', 'developmental', '/', 'learning', 'disorders', ',', 'and', 'how', 'have', 'you', 'learned', 'to', 'cope', 'with', 'it', 'all', '?', 'just', 'wondering', '.', 'i', 'got', 'diagnosed', 'a', 'few', 'months', 'ago', 'with', 'ad', '##hd', 'predominantly', 'ina', '##tten', '##tive', 'and', 'an', 'nl', '##d', ',', 'or', 'non', '##ver', '##bal', 'learning', 'disorder', '.', 'i', 'was', 'doing', 'some', 'research', 'yesterday', 'after', 'taking', 'my', 'first', 'dose', 'of', 'concert', '##a', 'when', 'i', 'realized', 'how', 'many', 'of', 'these', 'behavioral', 'qui', '##rks', 'i', \"'\", 've', 'developed', 'to', 'deal', 'with', 'my', 'problems', '.', 'for', 'example', ':', '-', 'i', 'talk', 'to', 'myself', 'whenever', 'i', \"'\", 'm', 'under', 'du', '##ress', '.', 'the', 'nl', '##d', 'means', 'i', 'process', 'everything', 'through', 'words', ',', 'and', 'since', 'kids', 'with', 'ad', '##hd', 'tend', 'to', 'be', 'bad', 'at', \"'\", 'self', '-', 'talk', \"'\", ',', 'i', \"'\", 've', 'learned', 'to', 'literally', 'stop', 'and', 'force', 'myself', 'to', 'explain', 'to', 'myself', 'what', \"'\", 's', 'happening', '.', '-', 'i', 'fi', '##dget', 'all', 'the', 'fucking', 'time', '(', 'air', 'guitar', '/', 'piano', '/', 'drums', ',', 'picking', 'at', 'my', 'hair', ',', 'pacing', ',', 'organizing', 'things', ',', 'etc', ')', '.', 'it', \"'\", 's', 'not', 'because', 'i', 'have', 'the', 'hyper', '##act', '##ivity', 'thing', 'going', 'on', ';', 'it', \"'\", 's', 'because', 'doing', 'something', 'else', 'that', 'doesn', \"'\", 't', 'require', 'a', 'lot', 'of', 'brain', '##power', 'stops', 'my', 'brain', 'from', 'running', 'off', 'on', 'me', '.', 'this', 'also', 'means', 'people', 'often', 'think', 'i', \"'\", 'm', 'ignoring', 'them', 'when', 'i', \"'\", 'm', 'really', 'trying', 'my', 'best', 'to', 'focus', 'by', 'doing', 'something', 'else', 'while', 'they', 'talk', '.', 'do', 'you', 'have', 'any', 'qui', '##rks', 'that', 'are', 'explained', 'by', 'your', 'ad', '##hd', 'or', 'any', 'other', 'learning', 'disorder', '?']\n",
      "INFO:__main__:Number of tokens: 269\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['r', '/', 'ad', '##hd', ',', 'do', 'you', 'have', 'any', 'other', 'developmental', '/', 'learning', 'disorders', ',', 'and', 'how', 'have', 'you', 'learned', 'to', 'cope', 'with', 'it', 'all', '?', 'just', 'wondering', '.', 'i', 'got', 'diagnosed', 'a', 'few', 'months', 'ago', 'with', 'ad', '##hd', 'predominantly', 'ina', '##tten', '##tive', 'and', 'an', 'nl', '##d', ',', 'or', 'non', '##ver', '##bal', 'learning', 'disorder', '.', 'i', 'was', 'doing', 'some', 'research', 'yesterday', 'after', 'taking', 'my', 'first', 'dose', 'of', 'concert', '##a', 'when', 'i', 'realized', 'how', 'many', 'of', 'these', 'behavioral', 'qui', '##rks', 'i', \"'\", 've', 'developed', 'to', 'deal', 'with', 'my', 'problems', '.', 'for', 'example', ':', '-', 'i', 'talk', 'to', 'myself', 'whenever', 'i', \"'\", 'm', 'under', 'du', '##ress', '.', 'the', 'nl', '##d', 'means', 'i', 'process', 'everything', 'through', 'words', ',', 'and', 'since', 'kids', 'with', 'ad', '##hd', 'tend', 'to', 'be', 'bad', 'at', \"'\", 'self', '-', 'talk', \"'\", ',', 'i', \"'\", 've', 'learned', 'to', 'literally', 'stop', 'and', 'force', 'myself', 'to', 'explain', 'to', 'myself', 'what', \"'\", 's', 'happening', '.', '-', 'i', 'fi', '##dget', 'all', 'the', 'fucking', 'time', '(', 'air', 'guitar', '/', 'piano', '/', 'drums', ',', 'picking', 'at', 'my', 'hair', ',', 'pacing', ',', 'organizing', 'things', ',', 'etc', ')', '.', 'it', \"'\", 's', 'not', 'because', 'i', 'have', 'the', 'hyper', '##act', '##ivity', 'thing', 'going', 'on', ';', 'it', \"'\", 's', 'because', 'doing', 'something', 'else', 'that', 'doesn', \"'\", 't', 'require', 'a', 'lot', 'of', 'brain', '##power', 'stops', 'my', 'brain', 'from', 'running', 'off', 'on', 'me', '.', 'this', 'also', 'means', 'people', 'often', 'think', 'i', \"'\", 'm', 'ignoring', 'them', 'when', 'i', \"'\", 'm', 'really', 'trying', 'my', 'best', 'to', 'focus', 'by', 'doing', 'something', 'else', 'while', 'they', 'talk', '.', 'do', 'you', 'have', 'any', 'qui', '##rks', 'that', 'are', 'explained', 'by', 'your', 'ad', '##hd', 'or', 'any', 'other', 'learning', 'disorder', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'know', 'when', 'dos', '##age', '/', 'medication', 'is', 'about', 'right', '?', 'what', 'are', 'reasonable', 'expectations', '?', 'i', \"'\", 'm', 'just', 'starting', 'to', 'work', 'with', 'a', 'psychiatrist', 'again', 'for', 'ad', '##hd', '.', 'this', 'time', ',', 'the', 'medication', 'we', 'started', 'with', 'is', 'a', 'lot', 'better', 'than', 'what', 'i', 'took', 'when', 'i', 'tried', 'this', 'years', 'ago', ',', 'so', 'i', 'think', 'i', \"'\", 'm', 'on', 'the', 'right', 'track', '.', 'at', 'the', 'same', 'time', ',', 'i', \"'\", 'm', 'not', 'all', 'the', 'way', 'there', 'i', 'think', '.', 'i', 'can', 'focus', 'on', '75', '%', 'of', 'the', 'lecture', 'instead', 'of', '40', '%', '.', 'i', 'used', 'to', 'average', '30', 'minutes', 'of', 'work', 'for', 'every', '1', 'hour', 'brows', '##ing', 'the', 'internet', '.', 'now', 'it', \"'\", 's', 'more', 'like', '2', 'hours', 'work', 'to', '45', 'minutes', 'of', 'brows', '##ing', '.', 'i', \"'\", 'm', 'hoping', 'i', 'can', 'catch', 'all', 'of', 'a', 'lecture', 'and', 'work', 'pretty', 'continuously', 'without', 'getting', 'distracted', '.', 'is', 'that', 'too', 'high', 'of', 'expectation', '?']\n",
      "INFO:__main__:Number of tokens: 156\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'know', 'when', 'dos', '##age', '/', 'medication', 'is', 'about', 'right', '?', 'what', 'are', 'reasonable', 'expectations', '?', 'i', \"'\", 'm', 'just', 'starting', 'to', 'work', 'with', 'a', 'psychiatrist', 'again', 'for', 'ad', '##hd', '.', 'this', 'time', ',', 'the', 'medication', 'we', 'started', 'with', 'is', 'a', 'lot', 'better', 'than', 'what', 'i', 'took', 'when', 'i', 'tried', 'this', 'years', 'ago', ',', 'so', 'i', 'think', 'i', \"'\", 'm', 'on', 'the', 'right', 'track', '.', 'at', 'the', 'same', 'time', ',', 'i', \"'\", 'm', 'not', 'all', 'the', 'way', 'there', 'i', 'think', '.', 'i', 'can', 'focus', 'on', '75', '%', 'of', 'the', 'lecture', 'instead', 'of', '40', '%', '.', 'i', 'used', 'to', 'average', '30', 'minutes', 'of', 'work', 'for', 'every', '1', 'hour', 'brows', '##ing', 'the', 'internet', '.', 'now', 'it', \"'\", 's', 'more', 'like', '2', 'hours', 'work', 'to', '45', 'minutes', 'of', 'brows', '##ing', '.', 'i', \"'\", 'm', 'hoping', 'i', 'can', 'catch', 'all', 'of', 'a', 'lecture', 'and', 'work', 'pretty', 'continuously', 'without', 'getting', 'distracted', '.', 'is', 'that', 'too', 'high', 'of', 'expectation', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['problems', 'with', 'eating', 'while', 'on', 'concert', '##a', '.', 'any', 'suggestions', '?', 'red', '##dit', ',', 'i', 'have', 'been', 'taking', 'cone', '##rta', '(', '54', '##mg', ')', 'for', 'a', 'few', 'months', 'now', ',', 'works', 'great', 'for', 'me', '.', 'but', 'there', 'is', '1', 'downs', '##ide', ':', 'i', 'really', 'have', 'problems', 'with', 'eating', 'during', 'the', 'day', '.', 'i', 'have', 'to', 'force', 'myself', 'to', 'cr', '##am', 'something', 'in', 'my', 'mouth', 'during', 'lunch', '.', 'which', 'makes', 'me', 'feel', 'almost', 'sick', 'from', 'hunger', 'during', 'the', 'day', ',', 'i', 'know', 'i', 'have', 'to', 'eat', ',', 'but', 'i', 'just', 'can', \"'\", 't', '.', 'it', \"'\", 's', 'like', 'my', 'body', 'won', \"'\", 't', 'let', 'me', '.', 'does', 'anyone', 'have', 'the', 'same', 'problem', ',', 'and', '/', 'or', 'a', 'suggestion', 'on', 'what', 'might', 'help', 'me', '?']\n",
      "INFO:__main__:Number of tokens: 123\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['problems', 'with', 'eating', 'while', 'on', 'concert', '##a', '.', 'any', 'suggestions', '?', 'red', '##dit', ',', 'i', 'have', 'been', 'taking', 'cone', '##rta', '(', '54', '##mg', ')', 'for', 'a', 'few', 'months', 'now', ',', 'works', 'great', 'for', 'me', '.', 'but', 'there', 'is', '1', 'downs', '##ide', ':', 'i', 'really', 'have', 'problems', 'with', 'eating', 'during', 'the', 'day', '.', 'i', 'have', 'to', 'force', 'myself', 'to', 'cr', '##am', 'something', 'in', 'my', 'mouth', 'during', 'lunch', '.', 'which', 'makes', 'me', 'feel', 'almost', 'sick', 'from', 'hunger', 'during', 'the', 'day', ',', 'i', 'know', 'i', 'have', 'to', 'eat', ',', 'but', 'i', 'just', 'can', \"'\", 't', '.', 'it', \"'\", 's', 'like', 'my', 'body', 'won', \"'\", 't', 'let', 'me', '.', 'does', 'anyone', 'have', 'the', 'same', 'problem', ',', 'and', '/', 'or', 'a', 'suggestion', 'on', 'what', 'might', 'help', 'me', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['started', 'on', 'st', '##rat', '##tera', '/', 'atom', '##ox', '##eti', '##ne', 'and', 'i', 'feel', 'weird']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['started', 'on', 'st', '##rat', '##tera', '/', 'atom', '##ox', '##eti', '##ne', 'and', 'i', 'feel', 'weird']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 'medicine', 'in', 'london', '?']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 'medicine', 'in', 'london', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'should', 'i', 'expect', 'from', 'med', '##s', '?', 'i', 'was', 'red', '##ia', '##gno', '##sed', 'with', 'ad', '##hd', '(', 'along', 'with', 'moderate', 'depression', ')', 'last', 'week', 'by', 'a', 'psychologist', ',', 'and', 'i', \"'\", 'm', 'seeing', 'my', 'doctor', 'again', 'this', 'week', 'to', 'hopefully', 'get', 'a', 'prescription', 'of', 'some', 'sort', '.', 'the', 'doctor', 'doesn', \"'\", 't', 'want', 'to', 'give', 'me', 'add', '##eral', '##l', 'or', 'rid', '##dl', '##in', 'because', 'i', \"'\", 've', 'had', 'them', 'as', 'a', 'young', 'child', 'and', 'didn', \"'\", 't', 'react', 'to', 'them', 'well', '.', 'i', \"'\", 'm', 'wondering', 'what', 'a', 'non', '-', 'st', '##im', '##ula', '##nt', 'drug', 'will', 'do', 'for', 'me', '.', 'i', \"'\", 'm', 'a', 'full', 'time', 'college', 'student', ',', 'with', 'several', 'jobs', '.', 'this', 'semester', 'has', 'kicked', 'me', 'around', ',', 'a', 'lot', '.', 'i', \"'\", 've', 'come', 'to', 'the', 'point', 'that', 'i', 'really', 'don', \"'\", 't', 'care', 'about', 'my', 'schooling', 'at', 'this', 'point', '.', 'i', 'pretty', 'much', 'failed', 'one', 'course', '(', 'haven', \"'\", 't', 'received', 'my', 'final', 'marks', 'yet', ')', ',', 'and', 'i', \"'\", 've', 'been', 'doing', 'poorly', 'because', 'i', 'just', 'haven', \"'\", 't', 'been', 'able', 'to', 'pay', 'attention', '.', 'throughout', 'hs', 'and', 'generals', 'in', 'college', ',', 'i', 'never', 'had', 'to', 'study', '.', 'engineering', 'courses', 'seem', 'to', 'require', 'a', 'lot', 'of', 'study', ',', 'and', 'i', 'just', 'can', \"'\", 't', 'do', 'it', '(', 'not', 'because', 'i', \"'\", 'm', 'not', 'intelligent', 'or', 'don', \"'\", 't', 'have', 'interest', 'generally', ')', '.', 'will', 'the', 'drugs', '\"', 'magical', '##ly', '\"', 'help', 'me', 'to', 'focus', 'better', ',', 'along', 'with', 'some', 'motivation', '?', 'i', \"'\", 'm', 'just', 'so', 'burnt', 'out', 'trying', 'so', 'hard', ',', 'but', 'one', 'can', 'always', 'try', 'harder', ',', 'and', 'i', 'am', 'all', 'the', 'more', 'frustrated', 'because', 'of', 'this', '.']\n",
      "INFO:__main__:Number of tokens: 276\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'should', 'i', 'expect', 'from', 'med', '##s', '?', 'i', 'was', 'red', '##ia', '##gno', '##sed', 'with', 'ad', '##hd', '(', 'along', 'with', 'moderate', 'depression', ')', 'last', 'week', 'by', 'a', 'psychologist', ',', 'and', 'i', \"'\", 'm', 'seeing', 'my', 'doctor', 'again', 'this', 'week', 'to', 'hopefully', 'get', 'a', 'prescription', 'of', 'some', 'sort', '.', 'the', 'doctor', 'doesn', \"'\", 't', 'want', 'to', 'give', 'me', 'add', '##eral', '##l', 'or', 'rid', '##dl', '##in', 'because', 'i', \"'\", 've', 'had', 'them', 'as', 'a', 'young', 'child', 'and', 'didn', \"'\", 't', 'react', 'to', 'them', 'well', '.', 'i', \"'\", 'm', 'wondering', 'what', 'a', 'non', '-', 'st', '##im', '##ula', '##nt', 'drug', 'will', 'do', 'for', 'me', '.', 'i', \"'\", 'm', 'a', 'full', 'time', 'college', 'student', ',', 'with', 'several', 'jobs', '.', 'this', 'semester', 'has', 'kicked', 'me', 'around', ',', 'a', 'lot', '.', 'i', \"'\", 've', 'come', 'to', 'the', 'point', 'that', 'i', 'really', 'don', \"'\", 't', 'care', 'about', 'my', 'schooling', 'at', 'this', 'point', '.', 'i', 'pretty', 'much', 'failed', 'one', 'course', '(', 'haven', \"'\", 't', 'received', 'my', 'final', 'marks', 'yet', ')', ',', 'and', 'i', \"'\", 've', 'been', 'doing', 'poorly', 'because', 'i', 'just', 'haven', \"'\", 't', 'been', 'able', 'to', 'pay', 'attention', '.', 'throughout', 'hs', 'and', 'generals', 'in', 'college', ',', 'i', 'never', 'had', 'to', 'study', '.', 'engineering', 'courses', 'seem', 'to', 'require', 'a', 'lot', 'of', 'study', ',', 'and', 'i', 'just', 'can', \"'\", 't', 'do', 'it', '(', 'not', 'because', 'i', \"'\", 'm', 'not', 'intelligent', 'or', 'don', \"'\", 't', 'have', 'interest', 'generally', ')', '.', 'will', 'the', 'drugs', '\"', 'magical', '##ly', '\"', 'help', 'me', 'to', 'focus', 'better', ',', 'along', 'with', 'some', 'motivation', '?', 'i', \"'\", 'm', 'just', 'so', 'burnt', 'out', 'trying', 'so', 'hard', ',', 'but', 'one', 'can', 'always', 'try', 'harder', ',', 'and', 'i', 'am', 'all', 'the', 'more', 'frustrated', 'because', 'of', 'this', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dia', '##gni', '##osed', 'with', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dia', '##gni', '##osed', 'with', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['asthma', 'and', 'st', '##im', '##ula', '##nts']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['asthma', 'and', 'st', '##im', '##ula', '##nts']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['starting', 'a', 'new', 'ad', '##hd', 'forum', ',', 'looking', 'for', 'moderator', '##s', '.']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['starting', 'a', 'new', 'ad', '##hd', 'forum', ',', 'looking', 'for', 'moderator', '##s', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['discount', '##ed', 'and', 'free', 'v', '##y', '##van', '##se', 'programs', 'due', 'to', 'the', 'shortage', 'of', 'my', 'previous', 'r', '##x', ',', 'my', 'doctor', 'prescribed', 'me', 'v', '##y', '##van', '##se', 'last', 'week', '.', 'we', 'had', 'decided', 'not', 'to', 'go', 'the', 'v', '##y', '##van', '##se', 'route', 'previously', 'due', 'to', 'cost', 'and', 'my', 'lou', '##sy', 'self', 'employed', 'insurance', 'coverage', ',', 'but', 'it', 'seemed', 'to', 'be', 'the', 'best', 'option', '.', 'he', 'gave', 'me', 'a', '[', '50', '%', 'off', 'discount', 'card', ']', '(', 'http', ':', '/', '/', 'www', '.', 'v', '##y', '##van', '##ses', '##avi', '##ng', '##s', '.', 'com', '/', ')', 'from', 'the', 'manufacturer', ',', 'which', 'filled', 'a', '30', 'day', '50', '##mg', 'r', '##x', 'for', 'about', '$', '75', '-', 'that', \"'\", 's', 'based', 'on', '$', '180', 'retail', 'at', 'wal', '##gree', '##ns', ',', 'less', '$', '30', 'insurance', ',', 'and', 'then', '50', '%', 'off', 'my', 'out', 'of', 'pocket', 'cost', '.', 'but', 'what', 'he', 'didn', \"'\", 't', 'mention', 'was', 'the', '[', 'shire', 'patient', 'assistance', 'program', ']', '(', 'http', ':', '/', '/', 'www', '.', 'shire', '.', 'com', '/', 'shire', '##pl', '##c', '/', 'en', '/', 'contact', '##us', '/', 'patient', '##ass', '##istan', '##ce', ')', '.', 'when', 'i', 'called', 'shire', 'couple', 'days', 'ago', ',', 'they', 'said', 'they', 'generally', 'make', 'exceptions', 'to', 'the', '\"', 'no', 'insurance', '\"', 'rule', 'and', 'could', 'cover', 'the', 'difference', 'between', 'what', 'my', 'insurance', 'doesn', \"'\", 't', 'pay', '.', 'does', 'anyone', 'with', 'modest', 'or', 'no', 'insurance', 'have', 'other', 'methods', 'of', 'afford', '##ing', 'their', 'med', '##s', '?', 'any', 'advice', 're', ':', 'dealing', 'with', 'the', 'shire', 'program', '?', 'i', 'don', \"'\", 't', 'feel', 'so', 'bad', 'taking', 'them', 'up', 'on', 'their', 'offer', 'and', 'spreading', 'the', 'word', 'when', 'they', 'appear', 'to', 'be', 'artificial', '##ly', 'limiting', 'the', 'supply', 'of', 'cheaper', 'ad', '##hd', 'generic', '##s', '.']\n",
      "INFO:__main__:Number of tokens: 277\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['discount', '##ed', 'and', 'free', 'v', '##y', '##van', '##se', 'programs', 'due', 'to', 'the', 'shortage', 'of', 'my', 'previous', 'r', '##x', ',', 'my', 'doctor', 'prescribed', 'me', 'v', '##y', '##van', '##se', 'last', 'week', '.', 'we', 'had', 'decided', 'not', 'to', 'go', 'the', 'v', '##y', '##van', '##se', 'route', 'previously', 'due', 'to', 'cost', 'and', 'my', 'lou', '##sy', 'self', 'employed', 'insurance', 'coverage', ',', 'but', 'it', 'seemed', 'to', 'be', 'the', 'best', 'option', '.', 'he', 'gave', 'me', 'a', '[', '50', '%', 'off', 'discount', 'card', ']', '(', 'http', ':', '/', '/', 'www', '.', 'v', '##y', '##van', '##ses', '##avi', '##ng', '##s', '.', 'com', '/', ')', 'from', 'the', 'manufacturer', ',', 'which', 'filled', 'a', '30', 'day', '50', '##mg', 'r', '##x', 'for', 'about', '$', '75', '-', 'that', \"'\", 's', 'based', 'on', '$', '180', 'retail', 'at', 'wal', '##gree', '##ns', ',', 'less', '$', '30', 'insurance', ',', 'and', 'then', '50', '%', 'off', 'my', 'out', 'of', 'pocket', 'cost', '.', 'but', 'what', 'he', 'didn', \"'\", 't', 'mention', 'was', 'the', '[', 'shire', 'patient', 'assistance', 'program', ']', '(', 'http', ':', '/', '/', 'www', '.', 'shire', '.', 'com', '/', 'shire', '##pl', '##c', '/', 'en', '/', 'contact', '##us', '/', 'patient', '##ass', '##istan', '##ce', ')', '.', 'when', 'i', 'called', 'shire', 'couple', 'days', 'ago', ',', 'they', 'said', 'they', 'generally', 'make', 'exceptions', 'to', 'the', '\"', 'no', 'insurance', '\"', 'rule', 'and', 'could', 'cover', 'the', 'difference', 'between', 'what', 'my', 'insurance', 'doesn', \"'\", 't', 'pay', '.', 'does', 'anyone', 'with', 'modest', 'or', 'no', 'insurance', 'have', 'other', 'methods', 'of', 'afford', '##ing', 'their', 'med', '##s', '?', 'any', 'advice', 're', ':', 'dealing', 'with', 'the', 'shire', 'program', '?', 'i', 'don', \"'\", 't', 'feel', 'so', 'bad', 'taking', 'them', 'up', 'on', 'their', 'offer', 'and', 'spreading', 'the', 'word', 'when', 'they', 'appear', 'to', 'be', 'artificial', '##ly', 'limiting', 'the', 'supply', 'of', 'cheaper', 'ad', '##hd', 'generic', '##s', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['pill', 'recommendation', '?']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['pill', 'recommendation', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['med', '##s', 'wearing', 'off', ',', 'can', 'you', 'tell', '?', 'or', 'is', 'it', 'all', 'in', 'my', 'head', '?']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['med', '##s', 'wearing', 'off', ',', 'can', 'you', 'tell', '?', 'or', 'is', 'it', 'all', 'in', 'my', 'head', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'did', 'add', '##eral', '##l', 'affected', 'you', 'at', 'first', '?', 'what', 'did', 'add', '##eral', '##l', 'do', 'to', 'you', 'upon', 'first', 'taking', 'it', '?', 'did', 'the', 'effects', 'change', 'after', 'a', 'few', 'days', '/', 'weeks', '/', 'months', '?', 'my', 'doctor', 'gave', 'me', 'a', 'prescription', 'for', 'add', '##eral', '##l', '(', 'he', 'would', 'have', 'preferred', 'more', 'expensive', 'alternatives', ',', 'but', 'i', \"'\", 'm', 'not', 'budget', '##ed', 'for', 'those', ')', '.', 'i', 'took', 'my', 'first', '10', '##mg', 'this', 'morning', 'an', 'hour', 'before', 'class', '.', '~', '30', '##min', 'in', 'i', 'feel', 'it', 'kicking', 'in', '.', 'i', 'felt', 'restless', ',', 'wanting', 'to', 'move', 'my', 'hands', 'rapidly', 'and', 'shaking', 'my', 'leg', '.', '~', '45', '-', '90', '##min', 'in', 'heartbeat', 'is', 'harder', ',', 'as', 'if', 'i', \"'\", 've', 'been', 'for', 'a', 'run', '.', 'it', \"'\", 's', 'not', 'necessarily', 'faster', ',', 'though', '.', '~', '50', '-', '55', '##min', 'in', 'i', 'feel', 'very', 'alert', ',', 'and', 'el', '##ated', '.', '~', '60', '##min', 'in', 'i', \"'\", 'm', 'in', 'class', ',', 'but', 'i', 'feel', 'more', 'focused', 'on', 'the', 'fact', 'that', 'i', \"'\", 'm', 'on', 'add', '##eral', '##l', 'than', 'on', 'what', 'the', 'professor', 'is', 'saying', '.', 'throughout', 'the', 'class', ',', 'i', 'feel', 'tired', 'mentally', ',', 'though', 'my', 'body', 'seems', 'to', 'be', 'racing', '.', 'very', 'anxious', ',', 'but', 'i', \"'\", 'm', 'not', 'having', 'the', 'physical', 'reactions', 'that', 'i', 'normally', 'have', 'when', 'anxious', '.', '~', '110', '-', '125', '##min', 'in', 'i', 'speak', 'with', 'a', 'classmate', 'about', 'an', 'assignment', '.', 'i', 'can', \"'\", 't', 'follow', 'much', 'of', 'what', 'he', \"'\", 's', 'saying', '(', 'never', '##mind', 'his', 'thick', 'accent', 'and', 'rough', 'gram', '##mer', ')', '.', 'i', 'have', 'trouble', 'focusing', 'on', 'the', 'assignment', 'he', \"'\", 's', 'working', 'on', 'me', 'with', ',', 'i', 'feel', 'distant', 'and', 'i', \"'\", 'm', 'unable', 'to', 'connect', 'things', 'like', 'i', \"'\", 'd', 'like', 'to', '.', 'should', 'i', 'continue', 'trying', 'this', 'drug', ',', 'or', 'should', 'i', 'be', 'concerned', '?', 'i', 'don', \"'\", 't', 'have', 'any', 'other', 'med', '##s', 'i', \"'\", 'm', 'on', '.', 'i', 'was', 'thinking', 'about', 'trying', 'again', 'tomorrow', 'morning', 'with', '5', '##mg', ',', 'and', 'maybe', 'on', 'saturday', 'to', 'so', 'i', 'can', 'observe', 'myself', 'without', 'other', 'factors', 'interfering', '.', '*', '*', 'edit', '*', '*', 'i', 'had', 'other', 'interesting', 'side', 'effects', 'throughout', 'the', 'first', 'day', ',', 'including', 'dry', 'mouth', '.', 'next', 'day', 'i', 'didn', \"'\", 't', 'take', 'them', 'right', 'away', 'in', 'the', 'morning', ',', 'and', 'felt', 'a', 'pressure', 'on', 'my', 'chest', '-', 'almost', 'like', 'i', \"'\", 'm', 'short', 'of', 'breath', '.', 'things', 'have', 'gotten', 'easier', ',', 'though', '.', 'when', 'i', 'take', 'add', '##eral', '##l', 'now', 'i', 'don', \"'\", 't', 'really', 'get', 'fig', '##ity', ',', 'and', 'i', 'don', \"'\", 't', 'get', 'super', 'anxious', '-', 'it', \"'\", 's', 'consistently', 'mild', '.', 'study', 'seems', 'to', 'be', 'easier', '-', 'still', 'get', 'distraction', '##s', ',', 'but', 'it', \"'\", 's', 'much', 'easier', 'to', 'dismiss', 'them', 'and', 'continue', 'with', 'the', 'task', 'at', 'hand', ',', 'where', 'as', 'before', 'it', 'was', 'like', 'i', 'had', 'no', 'choice', '.']\n",
      "INFO:__main__:Number of tokens: 472\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'did', 'add', '##eral', '##l', 'affected', 'you', 'at', 'first', '?', 'what', 'did', 'add', '##eral', '##l', 'do', 'to', 'you', 'upon', 'first', 'taking', 'it', '?', 'did', 'the', 'effects', 'change', 'after', 'a', 'few', 'days', '/', 'weeks', '/', 'months', '?', 'my', 'doctor', 'gave', 'me', 'a', 'prescription', 'for', 'add', '##eral', '##l', '(', 'he', 'would', 'have', 'preferred', 'more', 'expensive', 'alternatives', ',', 'but', 'i', \"'\", 'm', 'not', 'budget', '##ed', 'for', 'those', ')', '.', 'i', 'took', 'my', 'first', '10', '##mg', 'this', 'morning', 'an', 'hour', 'before', 'class', '.', '~', '30', '##min', 'in', 'i', 'feel', 'it', 'kicking', 'in', '.', 'i', 'felt', 'restless', ',', 'wanting', 'to', 'move', 'my', 'hands', 'rapidly', 'and', 'shaking', 'my', 'leg', '.', '~', '45', '-', '90', '##min', 'in', 'heartbeat', 'is', 'harder', ',', 'as', 'if', 'i', \"'\", 've', 'been', 'for', 'a', 'run', '.', 'it', \"'\", 's', 'not', 'necessarily', 'faster', ',', 'though', '.', '~', '50', '-', '55', '##min', 'in', 'i', 'feel', 'very', 'alert', ',', 'and', 'el', '##ated', '.', '~', '60', '##min', 'in', 'i', \"'\", 'm', 'in', 'class', ',', 'but', 'i', 'feel', 'more', 'focused', 'on', 'the', 'fact', 'that', 'i', \"'\", 'm', 'on', 'add', '##eral', '##l', 'than', 'on', 'what', 'the', 'professor', 'is', 'saying', '.', 'throughout', 'the', 'class', ',', 'i', 'feel', 'tired', 'mentally', ',', 'though', 'my', 'body', 'seems', 'to', 'be', 'racing', '.', 'very', 'anxious', ',', 'but', 'i', \"'\", 'm', 'not', 'having', 'the', 'physical', 'reactions', 'that', 'i', 'normally', 'have', 'when', 'anxious', '.', '~', '110', '-', '125', '##min', 'in', 'i', 'speak', 'with', 'a', 'classmate', 'about', 'an', 'assignment', '.', 'i', 'can', \"'\", 't', 'follow', 'much', 'of', 'what', 'he', \"'\", 's', 'saying', '(', 'never', '##mind', 'his', 'thick', 'accent', 'and', 'rough', 'gram', '##mer', ')', '.', 'i', 'have', 'trouble', 'focusing', 'on', 'the', 'assignment', 'he', \"'\", 's', 'working', 'on', 'me', 'with', ',', 'i', 'feel', 'distant', 'and', 'i', \"'\", 'm', 'unable', 'to', 'connect', 'things', 'like', 'i', \"'\", 'd', 'like', 'to', '.', 'should', 'i', 'continue', 'trying', 'this', 'drug', ',', 'or', 'should', 'i', 'be', 'concerned', '?', 'i', 'don', \"'\", 't', 'have', 'any', 'other', 'med', '##s', 'i', \"'\", 'm', 'on', '.', 'i', 'was', 'thinking', 'about', 'trying', 'again', 'tomorrow', 'morning', 'with', '5', '##mg', ',', 'and', 'maybe', 'on', 'saturday', 'to', 'so', 'i', 'can', 'observe', 'myself', 'without', 'other', 'factors', 'interfering', '.', '*', '*', 'edit', '*', '*', 'i', 'had', 'other', 'interesting', 'side', 'effects', 'throughout', 'the', 'first', 'day', ',', 'including', 'dry', 'mouth', '.', 'next', 'day', 'i', 'didn', \"'\", 't', 'take', 'them', 'right', 'away', 'in', 'the', 'morning', ',', 'and', 'felt', 'a', 'pressure', 'on', 'my', 'chest', '-', 'almost', 'like', 'i', \"'\", 'm', 'short', 'of', 'breath', '.', 'things', 'have', 'gotten', 'easier', ',', 'though', '.', 'when', 'i', 'take', 'add', '##eral', '##l', 'now', 'i', 'don', \"'\", 't', 'really', 'get', 'fig', '##ity', ',', 'and', 'i', 'don', \"'\", 't', 'get', 'super', 'anxious', '-', 'it', \"'\", 's', 'consistently', 'mild', '.', 'study', 'seems', 'to', 'be', 'easier', '-', 'still', 'get', 'distraction', '##s', ',', 'but', 'it', \"'\", 's', 'much', 'easier', 'to', 'dismiss', 'them', 'and', 'continue', 'with', 'the', 'task', 'at', 'hand', ',', 'where', 'as', 'before', 'it', 'was', 'like', 'i', 'had', 'no', 'choice', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['shouldn', \"'\", 't', 'we', 'be', 'discussing', 'more', 'about', 'drug', 'free', 'treatment', 'of', 'ad', '##hd', '?', 'drugs', 'do', 'not', '\"', 'cure', '\"', 'or', '\"', 'help', '\"', 'without', 'dependence', '.']\n",
      "INFO:__main__:Number of tokens: 28\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['shouldn', \"'\", 't', 'we', 'be', 'discussing', 'more', 'about', 'drug', 'free', 'treatment', 'of', 'ad', '##hd', '?', 'drugs', 'do', 'not', '\"', 'cure', '\"', 'or', '\"', 'help', '\"', 'without', 'dependence', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['time', 'management', 'skills', '.', 'more', 'specifically', ',', 'where', 'do', 'i', 'start', '?']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['time', 'management', 'skills', '.', 'more', 'specifically', ',', 'where', 'do', 'i', 'start', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['well', ',', 'is', 'it', '?', 'and', 'if', 'so', ',', 'what', 'next', '?', '*', '*', 't', '##l', ';', 'dr', ':', '*', '*', 'pretty', 'sure', 'i', 'have', 'ad', '##hd', ',', 'wondering', 'what', 'diagnostic', 'steps', 'would', 'confirm', 'it', '.', 'also', ',', 'would', 'prefer', 'to', 'avoid', 'most', 'of', 'the', 'first', '-', 'line', 'treatments', ',', 'wondering', 'if', 'people', 'have', 'experience', 'with', 'other', 'treatments', 'like', 'bus', '##par', '.', 'trying', 'to', 'figure', 'it', 'out', '.', 'i', \"'\", 've', 'had', 'a', 'lot', 'of', 'problems', 'with', 'focus', ',', 'pro', '##cr', '##ast', '##ination', ',', 'etc', ',', 'over', 'the', 'last', '3', 'years', '(', 'which', 'is', 'when', 'i', 'finally', 'completed', 'school', 'and', 'lived', 'completely', 'on', 'my', 'own', 'for', 'the', 'first', 'time', ')', '.', 'they', \"'\", 've', 'been', 'compound', '##ed', 'by', 'crap', '##py', 'work', '/', 'life', 'situations', 'which', 'have', 'led', 'to', 'mood', 'issues', '.', 'i', 'started', 'seeing', 'a', 'therapist', 'earlier', 'this', 'year', 'for', 'depression', '/', 'stress', ',', 'and', 'we', \"'\", 've', 'made', 'quite', 'a', 'bit', 'of', 'progress', 'on', 'the', 'situation', '##al', 'stuff', '(', 'my', 'mood', 'is', 'way', 'better', ')', 'but', 'i', \"'\", 'm', 'still', 'having', 'trouble', 'with', 'focus', 'and', 'getting', 'stuff', 'done', 'on', 'time', '.', 'i', \"'\", 'm', 'high', '-', 'functioning', '(', 'graduated', 'from', 'college', ',', 'did', 'a', 'masters', ',', 'got', 'a', 'doctorate', ')', ',', 'so', 'i', 'assumed', 'that', 'i', 'couldn', \"'\", 't', 'have', 'done', 'all', 'of', 'that', 'with', 'und', '##ia', '##gno', '##sed', 'ad', '##hd', '.', 'my', 'brother', 'has', 'a', 'textbook', 'case', 'of', 'ad', '##hd', ',', 'and', 'compared', 'to', 'him', ',', 'i', \"'\", 've', 'always', 'been', '\"', 'the', 'good', 'student', '\"', '.', 'but', 'on', 'the', 'other', 'hand', ',', 'i', \"'\", 've', 'often', 'felt', 'slightly', 'scattered', '(', 'at', 'best', ')', ',', 'had', 'issues', 'with', 'pro', '##cr', '##ast', '##ination', '/', 'time', 'management', ',', 'had', 'difficulty', 'waking', 'up', ',', 'and', 'generally', 'felt', 'like', 'somewhat', 'of', 'an', 'under', '##achi', '##ever', '.', 'looking', 'back', ',', 'i', 'think', 'i', 'just', 'had', 'coping', 'mechanisms', 'that', 'worked', 'pretty', 'well', 'when', 'i', 'had', 'external', 'structure', '.', 'other', 'than', 'sy', '##mpt', '##om', 'review', 'what', 'would', 'be', 'next', 'steps', 'to', 'confirm', 'the', 'diagnosis', '?', 'i', 'think', 'if', 'that', \"'\", 's', 'it', ',', 'it', 'might', 'be', 'time', 'for', 'med', '##s', ',', 'since', 'the', 'childhood', 'coping', 'skills', 'aren', \"'\", 't', 'really', 'haven', \"'\", 't', 'been', 'helping', 'much', 'any', 'more', '.', 'i', \"'\", 'd', 'like', 'to', 'avoid', 'st', '##im', '##ula', '##nts', 'because', 'of', 'the', 'cardiac', 'effects', '.', 'i', 'already', 'take', 'a', 'blood', 'pressure', 'med', ',', 'so', 'i', 'can', \"'\", 't', 'take', 'cl', '##oni', '##dine', '.', 'there', \"'\", 's', 'a', 'risk', 'of', 'mania', 'with', 'well', '##bu', '##tri', '##n', ',', 'and', 'that', \"'\", 's', 'not', 'good', 'with', 'my', 'family', 'history', ',', 'plus', 'it', 'can', 'ex', '##ace', '##rba', '##te', 'anxiety', ',', 'which', 'i', \"'\", 'm', 'prone', 'to', '.', 'that', 'leaves', 'st', '##rat', '##tera', ',', 'tc', '##as', ',', 'or', 'bus', '##par', '.', 'my', 'brother', 'takes', 'st', '##rat', '##tera', 'and', 'says', 'it', 'gives', 'him', 'occasional', 'bad', 'headache', '##s', '-', 'something', 'i', 'can', \"'\", 't', 'risk', 'because', 'i', 'have', 'mig', '##raine', '##s', 'that', 'are', 'barely', 'kept', 'under', 'control', 'on', 'multiple', 'med', '##s', '.', 'of', 'all', 'the', 'med', '##s', ',', 'i', 'think', 'the', 'side', 'effect', 'profile', 'of', 'bus', '##par', 'seems', 'the', 'most', 'attractive', 'to', 'me', '.', 'anyone', 'have', 'experience', 'with', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 518\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['well', ',', 'is', 'it', '?', 'and', 'if', 'so', ',', 'what', 'next', '?', '*', '*', 't', '##l', ';', 'dr', ':', '*', '*', 'pretty', 'sure', 'i', 'have', 'ad', '##hd', ',', 'wondering', 'what', 'diagnostic', 'steps', 'would', 'confirm', 'it', '.', 'also', ',', 'would', 'prefer', 'to', 'avoid', 'most', 'of', 'the', 'first', '-', 'line', 'treatments', ',', 'wondering', 'if', 'people', 'have', 'experience', 'with', 'other', 'treatments', 'like', 'bus', '##par', '.', 'trying', 'to', 'figure', 'it', 'out', '.', 'i', \"'\", 've', 'had', 'a', 'lot', 'of', 'problems', 'with', 'focus', ',', 'pro', '##cr', '##ast', '##ination', ',', 'etc', ',', 'over', 'the', 'last', '3', 'years', '(', 'which', 'is', 'when', 'i', 'finally', 'completed', 'school', 'and', 'lived', 'completely', 'on', 'my', 'own', 'for', 'the', 'first', 'time', ')', '.', 'they', \"'\", 've', 'been', 'compound', '##ed', 'by', 'crap', '##py', 'work', '/', 'life', 'situations', 'which', 'have', 'led', 'to', 'mood', 'issues', '.', 'i', 'started', 'seeing', 'a', 'therapist', 'earlier', 'this', 'year', 'for', 'depression', '/', 'stress', ',', 'and', 'we', \"'\", 've', 'made', 'quite', 'a', 'bit', 'of', 'progress', 'on', 'the', 'situation', '##al', 'stuff', '(', 'my', 'mood', 'is', 'way', 'better', ')', 'but', 'i', \"'\", 'm', 'still', 'having', 'trouble', 'with', 'focus', 'and', 'getting', 'stuff', 'done', 'on', 'time', '.', 'i', \"'\", 'm', 'high', '-', 'functioning', '(', 'graduated', 'from', 'college', ',', 'did', 'a', 'masters', ',', 'got', 'a', 'doctorate', ')', ',', 'so', 'i', 'assumed', 'that', 'i', 'couldn', \"'\", 't', 'have', 'done', 'all', 'of', 'that', 'with', 'und', '##ia', '##gno', '##sed', 'ad', '##hd', '.', 'my', 'brother', 'has', 'a', 'textbook', 'case', 'of', 'ad', '##hd', ',', 'and', 'compared', 'to', 'him', ',', 'i', \"'\", 've', 'always', 'been', '\"', 'the', 'good', 'student', '\"', '.', 'but', 'on', 'the', 'other', 'hand', ',', 'i', \"'\", 've', 'often', 'felt', 'slightly', 'scattered', '(', 'at', 'best', ')', ',', 'had', 'issues', 'with', 'pro', '##cr', '##ast', '##ination', '/', 'time', 'management', ',', 'had', 'difficulty', 'waking', 'up', ',', 'and', 'generally', 'felt', 'like', 'somewhat', 'of', 'an', 'under', '##achi', '##ever', '.', 'looking', 'back', ',', 'i', 'think', 'i', 'just', 'had', 'coping', 'mechanisms', 'that', 'worked', 'pretty', 'well', 'when', 'i', 'had', 'external', 'structure', '.', 'other', 'than', 'sy', '##mpt', '##om', 'review', 'what', 'would', 'be', 'next', 'steps', 'to', 'confirm', 'the', 'diagnosis', '?', 'i', 'think', 'if', 'that', \"'\", 's', 'it', ',', 'it', 'might', 'be', 'time', 'for', 'med', '##s', ',', 'since', 'the', 'childhood', 'coping', 'skills', 'aren', \"'\", 't', 'really', 'haven', \"'\", 't', 'been', 'helping', 'much', 'any', 'more', '.', 'i', \"'\", 'd', 'like', 'to', 'avoid', 'st', '##im', '##ula', '##nts', 'because', 'of', 'the', 'cardiac', 'effects', '.', 'i', 'already', 'take', 'a', 'blood', 'pressure', 'med', ',', 'so', 'i', 'can', \"'\", 't', 'take', 'cl', '##oni', '##dine', '.', 'there', \"'\", 's', 'a', 'risk', 'of', 'mania', 'with', 'well', '##bu', '##tri', '##n', ',', 'and', 'that', \"'\", 's', 'not', 'good', 'with', 'my', 'family', 'history', ',', 'plus', 'it', 'can', 'ex', '##ace', '##rba', '##te', 'anxiety', ',', 'which', 'i', \"'\", 'm', 'prone', 'to', '.', 'that', 'leaves', 'st', '##rat', '##tera', ',', 'tc', '##as', ',', 'or', 'bus', '##par', '.', 'my', 'brother', 'takes', 'st', '##rat', '##tera', 'and', 'says', 'it', 'gives', 'him', 'occasional', 'bad', 'headache', '##s', '-', 'something', 'i', 'can', \"'\", 't', 'risk', 'because', 'i', 'have', 'mig', '##raine', '##s', 'that', 'are', 'barely', 'kept', 'under', 'control', 'on', 'multiple', 'med', '##s', '.', 'of', 'all', 'the', 'med', '##s', ',', 'i', 'think', 'the', 'side', 'effect', 'profile', 'of', 'bus', '##par', 'seems', 'the', 'most', 'attractive', 'to', 'me', '.'], ['anyone', 'have', 'experience', 'with', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hyper', '##vent', '##ilation', 'as', 'a', 'sy', '##mpt', '##om', 'of', '(', 'not', 'using', ')', 'medication', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hyper', '##vent', '##ilation', 'as', 'a', 'sy', '##mpt', '##om', 'of', '(', 'not', 'using', ')', 'medication', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'psycho', '##sti', '##mu', '##lan', '##ts', 'like', 'rita', '##lin', 'and', 'add', '##eral', '##l', 'calm', 'people', 'with', 'ad', '##hd', 'down', '?', '[', 'via', '/', 'r', '/', 'asks', '##cie', '##nce', ']']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'psycho', '##sti', '##mu', '##lan', '##ts', 'like', 'rita', '##lin', 'and', 'add', '##eral', '##l', 'calm', 'people', 'with', 'ad', '##hd', 'down', '?', '[', 'via', '/', 'r', '/', 'asks', '##cie', '##nce', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'you', 'guys', 'think', 'of', 'taking', 'well', '##bu', '##tri', '##n', 'for', 'ad', '##hd', '?', 'i', 'went', 'to', 'my', 'doc', 'a', 'couple', 'days', 'ago', 'and', 'among', 'other', 'things', 'we', 'discussed', 'social', 'anxiety', ',', 'my', 'lack', 'of', 'coping', 'skills', ',', 'and', 'a', 'change', 'in', 'medication', '.', 'the', 'list', 'of', 'med', '##s', 'she', 'gave', 'me', 'to', 'choose', 'from', 'consists', 'of', ':', 'add', '##eral', '##l', ',', 'concert', '##a', ',', 'focal', '##in', ',', 'int', '##uni', '##v', ',', 'ka', '##p', '##va', '##y', ',', 'st', '##rat', '##tera', ',', 'and', 'well', '##bu', '##tri', '##n', 'as', 'an', 'alternative', 'option', '.', 'i', \"'\", 've', 'been', 'pre', '##sc', '##ri', '##ved', 'to', 'st', '##rat', '##tera', 'before', ',', 'but', 'don', \"'\", 't', 'remember', 'whether', 'i', 'liked', 'it', 'or', 'not', '.', 'the', 'common', 'side', 'effects', 'for', 'the', 'rest', 'of', 'them', 'make', 'them', 'una', '##ppe', '##aling', ',', 'except', 'for', 'well', '##bu', '##tri', '##n', '.', 'i', 'read', 'a', 'little', 'on', 'it', 'and', 'the', 'studies', 'on', 'its', 'use', 'in', 'ad', '##hd', 'is', 'pretty', 'mixed', ',', 'with', 'the', 'most', 'recent', 'one', '(', 'that', 'i', 'know', 'of', ')', 'saying', 'that', 'it', 'was', 'about', 'as', 'effective', 'as', 'place', '##bo', '.', 'has', 'anyone', 'else', 'been', 'prescribed', 'well', '##bu', '##tri', '##n', 'for', 'ad', '##hd', '?', 'if', 'so', ',', 'what', 'do', 'you', 'think', 'of', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 204\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'you', 'guys', 'think', 'of', 'taking', 'well', '##bu', '##tri', '##n', 'for', 'ad', '##hd', '?', 'i', 'went', 'to', 'my', 'doc', 'a', 'couple', 'days', 'ago', 'and', 'among', 'other', 'things', 'we', 'discussed', 'social', 'anxiety', ',', 'my', 'lack', 'of', 'coping', 'skills', ',', 'and', 'a', 'change', 'in', 'medication', '.', 'the', 'list', 'of', 'med', '##s', 'she', 'gave', 'me', 'to', 'choose', 'from', 'consists', 'of', ':', 'add', '##eral', '##l', ',', 'concert', '##a', ',', 'focal', '##in', ',', 'int', '##uni', '##v', ',', 'ka', '##p', '##va', '##y', ',', 'st', '##rat', '##tera', ',', 'and', 'well', '##bu', '##tri', '##n', 'as', 'an', 'alternative', 'option', '.', 'i', \"'\", 've', 'been', 'pre', '##sc', '##ri', '##ved', 'to', 'st', '##rat', '##tera', 'before', ',', 'but', 'don', \"'\", 't', 'remember', 'whether', 'i', 'liked', 'it', 'or', 'not', '.', 'the', 'common', 'side', 'effects', 'for', 'the', 'rest', 'of', 'them', 'make', 'them', 'una', '##ppe', '##aling', ',', 'except', 'for', 'well', '##bu', '##tri', '##n', '.', 'i', 'read', 'a', 'little', 'on', 'it', 'and', 'the', 'studies', 'on', 'its', 'use', 'in', 'ad', '##hd', 'is', 'pretty', 'mixed', ',', 'with', 'the', 'most', 'recent', 'one', '(', 'that', 'i', 'know', 'of', ')', 'saying', 'that', 'it', 'was', 'about', 'as', 'effective', 'as', 'place', '##bo', '.', 'has', 'anyone', 'else', 'been', 'prescribed', 'well', '##bu', '##tri', '##n', 'for', 'ad', '##hd', '?', 'if', 'so', ',', 'what', 'do', 'you', 'think', 'of', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'many', 'ad', '##hd', 'kids', 'does', 'it', 'take', 'to', 'change', 'a', 'light', 'bulb', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'many', 'ad', '##hd', 'kids', 'does', 'it', 'take', 'to', 'change', 'a', 'light', 'bulb', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'i', 'actually', 'have', 'ad', '##hd', '?', 'when', 'i', 'was', 'little', ',', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'because', 'of', 'my', 'tangent', '##ial', 'and', 'hyper', 'personality', 'and', 'was', 'put', 'on', 'rita', '##lin', '(', 'which', 'i', 'haven', \"'\", 't', 'taken', 'in', '10', 'years', 'now', ')', 'however', ':', 'i', 'do', 'not', 'have', '(', 'and', 'have', 'never', 'had', ')', 'hyper', '##act', '##ivity', ',', 'fi', '##d', '##git', '##ing', ',', 'or', 'ina', '##tten', '##tive', '##ness', 'of', 'any', 'kind', '.', 'i', 'am', 'a', 'relatively', 'calm', 'person', 'and', 'i', 'can', 'focus', 'just', 'fine', '.', 'it', \"'\", 's', 'just', 'that', 'my', 'mind', 'is', 'over', '##flow', '##ing', 'with', 'ideas', 'and', 'i', 'feel', 'like', 'i', 'have', 'to', 'share', 'them', 'with', 'people', '.', 'my', 'mind', 'races', '100', '%', 'of', 'the', 'time', ',', 'and', 'i', 'have', 'great', 'difficulty', 'sleeping', 'at', 'night', '.', 'a', 'friend', 'of', 'mine', 'said', 'that', 'i', 'was', '\"', 'literally', 'a', 'genius', '\"', 'and', 'that', 'they', 'had', 'mis', '##dia', '##gno', '##sed', 'me', 'as', 'someone', 'with', 'ad', '##hd', 'because', 'of', '\"', 'the', 'sheer', 'amount', 'of', 'goddamn', '##ed', 'creative', 'content', 'you', 'put', 'out', 'all', 'the', 'fucking', 'time', '.', '\"', 'i', \"'\", 'm', 'open', 'to', 'this', 'possibility', ',', 'but', 'i', 'don', \"'\", 't', 'honestly', 'think', 'i', \"'\", 'm', 'a', 'genius', '.', 'do', 'you', 'think', 'i', 'have', 'ad', '##hd', ',', 'red', '##dit', '?']\n",
      "INFO:__main__:Number of tokens: 209\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'i', 'actually', 'have', 'ad', '##hd', '?', 'when', 'i', 'was', 'little', ',', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'because', 'of', 'my', 'tangent', '##ial', 'and', 'hyper', 'personality', 'and', 'was', 'put', 'on', 'rita', '##lin', '(', 'which', 'i', 'haven', \"'\", 't', 'taken', 'in', '10', 'years', 'now', ')', 'however', ':', 'i', 'do', 'not', 'have', '(', 'and', 'have', 'never', 'had', ')', 'hyper', '##act', '##ivity', ',', 'fi', '##d', '##git', '##ing', ',', 'or', 'ina', '##tten', '##tive', '##ness', 'of', 'any', 'kind', '.', 'i', 'am', 'a', 'relatively', 'calm', 'person', 'and', 'i', 'can', 'focus', 'just', 'fine', '.', 'it', \"'\", 's', 'just', 'that', 'my', 'mind', 'is', 'over', '##flow', '##ing', 'with', 'ideas', 'and', 'i', 'feel', 'like', 'i', 'have', 'to', 'share', 'them', 'with', 'people', '.', 'my', 'mind', 'races', '100', '%', 'of', 'the', 'time', ',', 'and', 'i', 'have', 'great', 'difficulty', 'sleeping', 'at', 'night', '.', 'a', 'friend', 'of', 'mine', 'said', 'that', 'i', 'was', '\"', 'literally', 'a', 'genius', '\"', 'and', 'that', 'they', 'had', 'mis', '##dia', '##gno', '##sed', 'me', 'as', 'someone', 'with', 'ad', '##hd', 'because', 'of', '\"', 'the', 'sheer', 'amount', 'of', 'goddamn', '##ed', 'creative', 'content', 'you', 'put', 'out', 'all', 'the', 'fucking', 'time', '.', '\"', 'i', \"'\", 'm', 'open', 'to', 'this', 'possibility', ',', 'but', 'i', 'don', \"'\", 't', 'honestly', 'think', 'i', \"'\", 'm', 'a', 'genius', '.', 'do', 'you', 'think', 'i', 'have', 'ad', '##hd', ',', 'red', '##dit', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['md', '##ma', 'fixed', 'my', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['md', '##ma', 'fixed', 'my', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['md', '##ma']\n",
      "INFO:__main__:Number of tokens: 2\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['md', '##ma']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hyper', '##fo', '##cus', 'vs', '.', 'repetitive', 'thoughts']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hyper', '##fo', '##cus', 'vs', '.', 'repetitive', 'thoughts']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'gift', 'of', 'hyper', '-', 'focus', 'is', 'yours', '!', 'here', '’', 's', 'the', 'secret', '!']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'gift', 'of', 'hyper', '-', 'focus', 'is', 'yours', '!', 'here', '’', 's', 'the', 'secret', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['depressed', 'with', 'ad', '##hd', '?', 'i', \"'\", 'd', 'like', 'to', 'know', 'about', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['depressed', 'with', 'ad', '##hd', '?', 'i', \"'\", 'd', 'like', 'to', 'know', 'about', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'doctor', 'wants', 'to', 'pre', '##scribe', 'me', 'v', '##y', '##van', '##se', ',', 'but', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'doctor', 'wants', 'to', 'pre', '##scribe', 'me', 'v', '##y', '##van', '##se', ',', 'but', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['recently', 'diagnosed', 'but', 'unable', 'to', 'get', 'prescription', 'for', 'med', '##s', '.', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['recently', 'diagnosed', 'but', 'unable', 'to', 'get', 'prescription', 'for', 'med', '##s', '.', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['specialist', 'never', 'heard', 'of', 'des', '##ox', '##yn', ',', 'should', 'i', 'be', 'worried', '?', 'wanting', 'to', 'switch', 'from', 'add', '##eral', '##l', 'and', 'st', '##rate', '##rra']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['specialist', 'never', 'heard', 'of', 'des', '##ox', '##yn', ',', 'should', 'i', 'be', 'worried', '?', 'wanting', 'to', 'switch', 'from', 'add', '##eral', '##l', 'and', 'st', '##rate', '##rra']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['experienced', 'ta', '##chy', '##card', '##ia', 'on', 'add', '##eral', '##l', '15', '##mg', 'ir', 'x', '##2', 'day', '.', 'seems', 'like', 'a', 'low', 'dose', '?', 'am', 'i', 'wrong', '?', 'tried', 'other', 'things', ',', 'went', 'back', 'to', 'cutting', 'up', 'the', 'add', '##eral', '##l', '.', 'just', 'looking', 'for', 'some', 'advice', '.', 'so', '(', 'throw', '##away', ')', 'experiencing', 'what', 'i', 'would', 'assume', 'to', 'be', 'focus', 'problems', '(', 'childhood', 'on', 'add', 'med', '##s', ')', 'in', 'my', 'adult', 'life', 'i', 'decided', 'see', 'if', 'add', '##eral', '##l', 'would', 'help', 'me', 'out', '.', 'i', 'was', 'prescribed', '15', '##mg', 'instant', 'pills', 'to', 'be', 'taken', '2', '##x', 'a', 'day', '.', 'on', 'the', 'first', 'day', 'i', 'felt', 'what', 'i', 'would', 'describe', 'as', 'a', 'high', '.', 'it', 'just', 'felt', 'good', ',', 'i', 'seemed', 'hyper', 'focused', '(', 'that', 'is', 'i', 'sat', 'and', 'repeated', 'one', 'activity', 'for', 'about', '5', 'hours', ')', '.', 'after', 'the', 'second', 'day', 'my', 'heart', 'began', 'to', 'race', 'and', 'the', 'fastest', 'i', 'noticed', 'it', 'at', 'was', 'around', '170', '##b', '##pm', '.', 'i', 'feel', 'like', 'my', 'rapid', 'pulse', '(', 'while', 'certainly', 'st', '##im', '##ula', '##nt', 'related', ')', 'may', 'have', 'also', 'been', 'anxiety', 'related', '.', 'i', 'went', 'back', 'to', 'the', 'doctor', 'and', 'tried', 'a', 'few', 'different', 'medications', 'but', 'st', '##rate', '##rra', 'had', 'terrible', 'side', 'effects', '(', 'deco', '##up', '##led', 'e', '##ja', '##cula', '##tion', 'from', 'orgasm', '!', ')', 'and', 'i', 'couldn', \"'\", 't', 'deal', 'with', 'the', 'anxiety', 'i', 'felt', 'taking', 'bu', '##pro', '##pin', 'as', 'i', 'couldn', \"'\", 't', 'stop', 'reading', 'about', 'seizure', 'threshold', 'lowering', '.', 'so', ',', 'despite', 'not', 'wanting', 'to', 'have', 'a', 'heart', 'attack', 'i', 'broke', 'my', 'remaining', '15', '##mg', 'add', '##eral', '##l', 'pills', 'into', 'halves', 'and', 'then', 'halves', 'again', 'leaving', 'me', 'with', 'either', '7', '.', '5', 'in', 'half', 'pill', 'or', '3', '.', '75', 'in', 'cr', '##umb', '##s', '.', 'so', 'i', 'took', 'the', '<', '4', '##mg', 'and', 'noticed', 'immediately', 'a', 'similar', 'good', 'sensation', 'to', 'the', 'first', 'time', 'i', 'took', 'the', '15', '##mg', 'but', 'more', 'noticeably', 'i', 'was', 'able', 'to', 'focus', 'on', 'the', 'tasks', 'in', 'front', 'of', 'me', '.', 'it', \"'\", 's', 'hard', 'for', 'me', 'to', 'explain', 'the', 'calm', 'i', 'experience', 'on', 'add', '##eral', '##l', '.', 'it', \"'\", 's', 'like', 'the', '\"', 'chatter', '\"', 'that', 'is', 'in', 'my', 'brain', 'stops', '.', 'i', 'constantly', 'have', 'problems', 'with', 'what', 'i', 'would', 'consider', 'inappropriate', 'thoughts', 'popping', 'into', 'my', 'head', 'at', 'the', 'worst', 'times', 'but', 'on', 'add', '##eral', '##l', 'suddenly', 'there', 'is', 'just', 'serene', 'silence', 'and', 'i', 'can', 'sit', 'down', 'and', 'work', 'or', 'hold', 'a', 'conversation', 'with', 'my', 'girlfriend', 'without', 'stopping', 'her', 'halfway', 'through', 'and', 'saying', '\"', 'what', 'did', 'you', 'just', 'say', '?', '\"', 'i', 'tried', '7', '.', '5', '##mg', 'today', 'and', 'i', 'believe', 'i', 'feel', 'a', 'little', 'hyper', '##fo', '##cus', '##ed', 'and', 'i', 'feel', 'warm', 'with', 'a', 'heart', '##rate', '(', 'sitting', ')', 'around', '100', '##b', '##pm', '.', 'so', 'i', 'guess', 'i', \"'\", 'm', 'wondering', '.', 'is', '<', '4', '##mg', 'even', 'enough', 'to', 'do', 'anything', 'or', 'am', 'i', 'place', '##bo', 'concentrating', '?', 'i', 'am', 'obviously', 'doing', 'this', 'without', 'the', 'consent', 'of', 'my', 'doctor', 'who', 'decided', 'i', 'should', 'be', 'off', 'the', '30', '##mg', 'a', 'day', 'due', 'to', 'the', 'ta', '##chy', '##card', '##ia', '.', 'hence', 'my', 'initial', 'experimentation', 'with', 'low', 'dos', '##ages', 'and', 'seems', 'to', 'have', 'only', 'mild', 'side', 'effects', 'com', '##para', '##bility', '.', 'any', 'advice', 'on', 'what', 'do', '?', 'how', 'i', 'might', 'discuss', 'this', 'with', 'my', 'doctor', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'seem', 'like', 'i', \"'\", 'm', 'begging', 'for', 'add', '##eral', '##l', 'but', 'it', \"'\", 's', 'the', 'only', 'thing', 'that', 'seems', 'like', 'it', 'works', 'at', 'all', '.', 'i', 'feel', 'like', 'bringing', 'up', 'the', 'topic', '\"', 'so', ',', 'i', 'saved', 'those', 'pills', 'that', 'are', 'highly', 'regulated', 'to', 'try', 'my', 'own', 'experiments', 'with', 'myself', 'on', '\"', 'might', 'not', 'be', 'the', 'best', 'start', 'to', 'a', 'conversation', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'take', 'blood', 'pressure', 'lowering', 'med', '##s', 'to', 'reduce', 'my', 'heart', 'rate', 'because', 'i', 'feel', 'like', 'i', 'would', 'be', 'worse', 'off', 'cr', '##am', '##ming', 'a', 'bunch', 'of', 'medication', 'into', 'my', 'body', 'versus', 'just', 'suffering', 'with', 'my', 'problems', '.', 'anything', '?', 'anybody', '?', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 654\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['experienced', 'ta', '##chy', '##card', '##ia', 'on', 'add', '##eral', '##l', '15', '##mg', 'ir', 'x', '##2', 'day', '.', 'seems', 'like', 'a', 'low', 'dose', '?', 'am', 'i', 'wrong', '?', 'tried', 'other', 'things', ',', 'went', 'back', 'to', 'cutting', 'up', 'the', 'add', '##eral', '##l', '.', 'just', 'looking', 'for', 'some', 'advice', '.', 'so', '(', 'throw', '##away', ')', 'experiencing', 'what', 'i', 'would', 'assume', 'to', 'be', 'focus', 'problems', '(', 'childhood', 'on', 'add', 'med', '##s', ')', 'in', 'my', 'adult', 'life', 'i', 'decided', 'see', 'if', 'add', '##eral', '##l', 'would', 'help', 'me', 'out', '.', 'i', 'was', 'prescribed', '15', '##mg', 'instant', 'pills', 'to', 'be', 'taken', '2', '##x', 'a', 'day', '.', 'on', 'the', 'first', 'day', 'i', 'felt', 'what', 'i', 'would', 'describe', 'as', 'a', 'high', '.', 'it', 'just', 'felt', 'good', ',', 'i', 'seemed', 'hyper', 'focused', '(', 'that', 'is', 'i', 'sat', 'and', 'repeated', 'one', 'activity', 'for', 'about', '5', 'hours', ')', '.', 'after', 'the', 'second', 'day', 'my', 'heart', 'began', 'to', 'race', 'and', 'the', 'fastest', 'i', 'noticed', 'it', 'at', 'was', 'around', '170', '##b', '##pm', '.', 'i', 'feel', 'like', 'my', 'rapid', 'pulse', '(', 'while', 'certainly', 'st', '##im', '##ula', '##nt', 'related', ')', 'may', 'have', 'also', 'been', 'anxiety', 'related', '.', 'i', 'went', 'back', 'to', 'the', 'doctor', 'and', 'tried', 'a', 'few', 'different', 'medications', 'but', 'st', '##rate', '##rra', 'had', 'terrible', 'side', 'effects', '(', 'deco', '##up', '##led', 'e', '##ja', '##cula', '##tion', 'from', 'orgasm', '!', ')', 'and', 'i', 'couldn', \"'\", 't', 'deal', 'with', 'the', 'anxiety', 'i', 'felt', 'taking', 'bu', '##pro', '##pin', 'as', 'i', 'couldn', \"'\", 't', 'stop', 'reading', 'about', 'seizure', 'threshold', 'lowering', '.', 'so', ',', 'despite', 'not', 'wanting', 'to', 'have', 'a', 'heart', 'attack', 'i', 'broke', 'my', 'remaining', '15', '##mg', 'add', '##eral', '##l', 'pills', 'into', 'halves', 'and', 'then', 'halves', 'again', 'leaving', 'me', 'with', 'either', '7', '.', '5', 'in', 'half', 'pill', 'or', '3', '.', '75', 'in', 'cr', '##umb', '##s', '.', 'so', 'i', 'took', 'the', '<', '4', '##mg', 'and', 'noticed', 'immediately', 'a', 'similar', 'good', 'sensation', 'to', 'the', 'first', 'time', 'i', 'took', 'the', '15', '##mg', 'but', 'more', 'noticeably', 'i', 'was', 'able', 'to', 'focus', 'on', 'the', 'tasks', 'in', 'front', 'of', 'me', '.', 'it', \"'\", 's', 'hard', 'for', 'me', 'to', 'explain', 'the', 'calm', 'i', 'experience', 'on', 'add', '##eral', '##l', '.', 'it', \"'\", 's', 'like', 'the', '\"', 'chatter', '\"', 'that', 'is', 'in', 'my', 'brain', 'stops', '.', 'i', 'constantly', 'have', 'problems', 'with', 'what', 'i', 'would', 'consider', 'inappropriate', 'thoughts', 'popping', 'into', 'my', 'head', 'at', 'the', 'worst', 'times', 'but', 'on', 'add', '##eral', '##l', 'suddenly', 'there', 'is', 'just', 'serene', 'silence', 'and', 'i', 'can', 'sit', 'down', 'and', 'work', 'or', 'hold', 'a', 'conversation', 'with', 'my', 'girlfriend', 'without', 'stopping', 'her', 'halfway', 'through', 'and', 'saying', '\"', 'what', 'did', 'you', 'just', 'say', '?', '\"', 'i', 'tried', '7', '.', '5', '##mg', 'today', 'and', 'i', 'believe', 'i', 'feel', 'a', 'little', 'hyper', '##fo', '##cus', '##ed', 'and', 'i', 'feel', 'warm', 'with', 'a', 'heart', '##rate', '(', 'sitting', ')', 'around', '100', '##b', '##pm', '.', 'so', 'i', 'guess', 'i', \"'\", 'm', 'wondering', '.', 'is', '<', '4', '##mg', 'even', 'enough', 'to', 'do', 'anything', 'or', 'am', 'i', 'place', '##bo', 'concentrating', '?', 'i', 'am', 'obviously', 'doing', 'this', 'without', 'the', 'consent', 'of', 'my', 'doctor', 'who', 'decided', 'i', 'should', 'be', 'off', 'the', '30', '##mg', 'a', 'day', 'due', 'to', 'the', 'ta', '##chy', '##card', '##ia', '.', 'hence', 'my', 'initial', 'experimentation', 'with', 'low', 'dos', '##ages', 'and', 'seems'], ['to', 'have', 'only', 'mild', 'side', 'effects', 'com', '##para', '##bility', '.', 'any', 'advice', 'on', 'what', 'do', '?', 'how', 'i', 'might', 'discuss', 'this', 'with', 'my', 'doctor', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'seem', 'like', 'i', \"'\", 'm', 'begging', 'for', 'add', '##eral', '##l', 'but', 'it', \"'\", 's', 'the', 'only', 'thing', 'that', 'seems', 'like', 'it', 'works', 'at', 'all', '.', 'i', 'feel', 'like', 'bringing', 'up', 'the', 'topic', '\"', 'so', ',', 'i', 'saved', 'those', 'pills', 'that', 'are', 'highly', 'regulated', 'to', 'try', 'my', 'own', 'experiments', 'with', 'myself', 'on', '\"', 'might', 'not', 'be', 'the', 'best', 'start', 'to', 'a', 'conversation', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'take', 'blood', 'pressure', 'lowering', 'med', '##s', 'to', 'reduce', 'my', 'heart', 'rate', 'because', 'i', 'feel', 'like', 'i', 'would', 'be', 'worse', 'off', 'cr', '##am', '##ming', 'a', 'bunch', 'of', 'medication', 'into', 'my', 'body', 'versus', 'just', 'suffering', 'with', 'my', 'problems', '.', 'anything', '?', 'anybody', '?', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['decided', 'to', 'stop', 'taking', 'st', '##im', '##ula', '##nts', '.', 'any', 'advice', '?', 'title', 'pretty', 'much', 'says', 'it', 'all', '.', 'i', \"'\", 've', 'been', 'on', 'v', '##y', '##van', '##se', 'for', 'a', 'little', 'over', 'a', 'year', 'now', ',', 'and', 'i', 'want', 'to', 'stop', 'taking', 'it', '.', 'i', 'feel', 'like', 'the', 'side', 'effects', 'are', 'not', 'worth', 'it', 'anymore', 'and', 'i', \"'\", 'm', 'sick', 'of', 'it', '.', 'at', 'the', 'same', 'time', ',', 'i', 'can', 'see', 'that', 'it', 'helps', 'me', 'and', 'i', \"'\", 'm', 'scared', 'of', 'what', 'will', 'happen', 'if', 'i', 'don', \"'\", 't', 'take', 'it', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'fuck', 'up', 'my', 'life', 'ir', '##re', '##vers', '##ibly', '.', 'i', 'just', 'want', 'to', 'be', 'able', 'to', 'be', '\"', 'normal', '\"', 'without', 'needing', 'to', 'take', 'a', 'pill', 'every', 'morning', ',', 'and', 'i', 'think', 'i', 'can', 'do', 'it', 'if', 'i', 'work', 'hard', 'to', 'develop', 'other', 'coping', 'mechanisms', '(', 'caf', '##fe', '##ine', ',', 'exercise', ',', 'time', 'management', ',', 'etc', '.', ')', '.', 'anybody', 'else', 'been', 'in', 'this', 'situation', 'before', '?', 'any', 'advice', 'about', 'how', 'to', 'ease', 'the', 'transition', '?', '*', '*', 'edit', '(', '11', '/', '15', ')', ':', 'after', 'reading', 'some', 'of', 'the', 'comments', 'here', 'and', 'researching', 'a', 'little', 'about', 'dos', '##age', ',', 'i', 'am', 'going', 'to', 'try', 'hal', '##ving', 'my', 'dose', 'for', 'a', 'week', '.', 'i', 'really', 'think', 'the', 'dose', 'i', \"'\", 'm', 'on', 'now', '(', '50', '##mg', ')', 'is', 'too', 'high', 'for', 'me', 'and', 'want', 'to', 'see', 'if', 'a', 'smaller', 'dose', 'will', 'help', 'cut', 'down', 'on', 'some', 'of', 'the', 'worst', 'side', 'effects', 'i', \"'\", 'm', 'experiencing', '(', 'alternating', 'hyper', '##fo', '##cus', 'and', 'brain', 'fog', ',', 'loss', 'of', 'appetite', ',', 'dry', 'mouth', ',', 'etc', '.', ')', '.', '*', '*', '*', '*', 'edit', '(', '11', '/', '16', ')', ':', 'stupid', 'decision', 'on', 'my', 'part', '!', 'lower', 'dose', 'helped', 'reduce', 'some', 'side', 'effects', ',', 'but', 'my', 'sleep', 'was', 'horrible', 'last', 'night', 'and', 'i', 'feel', 'so', 'restless', 'right', 'now', 'that', 'it', \"'\", 's', 'actually', 'painful', 'to', 'be', 'sitting', 'here', '.', 'going', 'back', 'to', 'my', 'normal', 'dose', 'asa', '##p', '.', '*', '*']\n",
      "INFO:__main__:Number of tokens: 334\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['decided', 'to', 'stop', 'taking', 'st', '##im', '##ula', '##nts', '.', 'any', 'advice', '?', 'title', 'pretty', 'much', 'says', 'it', 'all', '.', 'i', \"'\", 've', 'been', 'on', 'v', '##y', '##van', '##se', 'for', 'a', 'little', 'over', 'a', 'year', 'now', ',', 'and', 'i', 'want', 'to', 'stop', 'taking', 'it', '.', 'i', 'feel', 'like', 'the', 'side', 'effects', 'are', 'not', 'worth', 'it', 'anymore', 'and', 'i', \"'\", 'm', 'sick', 'of', 'it', '.', 'at', 'the', 'same', 'time', ',', 'i', 'can', 'see', 'that', 'it', 'helps', 'me', 'and', 'i', \"'\", 'm', 'scared', 'of', 'what', 'will', 'happen', 'if', 'i', 'don', \"'\", 't', 'take', 'it', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'fuck', 'up', 'my', 'life', 'ir', '##re', '##vers', '##ibly', '.', 'i', 'just', 'want', 'to', 'be', 'able', 'to', 'be', '\"', 'normal', '\"', 'without', 'needing', 'to', 'take', 'a', 'pill', 'every', 'morning', ',', 'and', 'i', 'think', 'i', 'can', 'do', 'it', 'if', 'i', 'work', 'hard', 'to', 'develop', 'other', 'coping', 'mechanisms', '(', 'caf', '##fe', '##ine', ',', 'exercise', ',', 'time', 'management', ',', 'etc', '.', ')', '.', 'anybody', 'else', 'been', 'in', 'this', 'situation', 'before', '?', 'any', 'advice', 'about', 'how', 'to', 'ease', 'the', 'transition', '?', '*', '*', 'edit', '(', '11', '/', '15', ')', ':', 'after', 'reading', 'some', 'of', 'the', 'comments', 'here', 'and', 'researching', 'a', 'little', 'about', 'dos', '##age', ',', 'i', 'am', 'going', 'to', 'try', 'hal', '##ving', 'my', 'dose', 'for', 'a', 'week', '.', 'i', 'really', 'think', 'the', 'dose', 'i', \"'\", 'm', 'on', 'now', '(', '50', '##mg', ')', 'is', 'too', 'high', 'for', 'me', 'and', 'want', 'to', 'see', 'if', 'a', 'smaller', 'dose', 'will', 'help', 'cut', 'down', 'on', 'some', 'of', 'the', 'worst', 'side', 'effects', 'i', \"'\", 'm', 'experiencing', '(', 'alternating', 'hyper', '##fo', '##cus', 'and', 'brain', 'fog', ',', 'loss', 'of', 'appetite', ',', 'dry', 'mouth', ',', 'etc', '.', ')', '.', '*', '*', '*', '*', 'edit', '(', '11', '/', '16', ')', ':', 'stupid', 'decision', 'on', 'my', 'part', '!', 'lower', 'dose', 'helped', 'reduce', 'some', 'side', 'effects', ',', 'but', 'my', 'sleep', 'was', 'horrible', 'last', 'night', 'and', 'i', 'feel', 'so', 'restless', 'right', 'now', 'that', 'it', \"'\", 's', 'actually', 'painful', 'to', 'be', 'sitting', 'here', '.', 'going', 'back', 'to', 'my', 'normal', 'dose', 'asa', '##p', '.', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['help', 'i', \"'\", 've', 'been', 'sitting', 'in', '##front', 'of', 'my', 'computer', 'for', 'the', 'second', 'night', 'in', 'a', 'row', 'without', 'getting', 'a', 'single', 'bit', 'of', 'work', 'done', '.', 'mainly', 'i', 'end', 'up', 'on', 'facebook', ',', 'red', '##dit', ',', 'and', 'news', 'sites', '.', 'tonight', ',', 'i', \"'\", 've', 'literally', 'been', 'at', 'my', 'computer', 'for', 'the', 'last', '6', 'hours', 'and', 'gotten', 'nothing', 'done', '.', 'i', 'have', 'a', '10', 'page', 'paper', 'that', \"'\", 's', 'past', 'due', 'and', 'i', 'haven', \"'\", 't', 'gotten', 'past', 'a', 'half', 'a', 'page', '.', 'i', \"'\", 've', 'already', 'outlined', 'everything', '.', 'and', 'i', \"'\", 've', 'got', 'a', 'good', 'idea', 'of', 'where', 'to', 'get', 'all', 'the', 'info', 'i', 'need', '.', 'but', 'i', \"'\", 'm', 'having', 'a', 'shit', 'hard', 'time', 'actually', 'doing', 'it', '.', 'this', 'is', 'really', 'rough', '.', 'i', 'haven', \"'\", 't', 'been', 'this', 'bad', 'in', 'a', 'long', 'time', '.', 'also', ',', 'i', \"'\", 'm', 'sick', 'with', 'a', 'cold', 'i', 'think', ',', 'which', 'may', 'have', 'something', 'to', 'do', 'with', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 160\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['help', 'i', \"'\", 've', 'been', 'sitting', 'in', '##front', 'of', 'my', 'computer', 'for', 'the', 'second', 'night', 'in', 'a', 'row', 'without', 'getting', 'a', 'single', 'bit', 'of', 'work', 'done', '.', 'mainly', 'i', 'end', 'up', 'on', 'facebook', ',', 'red', '##dit', ',', 'and', 'news', 'sites', '.', 'tonight', ',', 'i', \"'\", 've', 'literally', 'been', 'at', 'my', 'computer', 'for', 'the', 'last', '6', 'hours', 'and', 'gotten', 'nothing', 'done', '.', 'i', 'have', 'a', '10', 'page', 'paper', 'that', \"'\", 's', 'past', 'due', 'and', 'i', 'haven', \"'\", 't', 'gotten', 'past', 'a', 'half', 'a', 'page', '.', 'i', \"'\", 've', 'already', 'outlined', 'everything', '.', 'and', 'i', \"'\", 've', 'got', 'a', 'good', 'idea', 'of', 'where', 'to', 'get', 'all', 'the', 'info', 'i', 'need', '.', 'but', 'i', \"'\", 'm', 'having', 'a', 'shit', 'hard', 'time', 'actually', 'doing', 'it', '.', 'this', 'is', 'really', 'rough', '.', 'i', 'haven', \"'\", 't', 'been', 'this', 'bad', 'in', 'a', 'long', 'time', '.', 'also', ',', 'i', \"'\", 'm', 'sick', 'with', 'a', 'cold', 'i', 'think', ',', 'which', 'may', 'have', 'something', 'to', 'do', 'with', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['exercising', 'on', 'add', '##eral', '##l', 'and', 'resting', 'heart', 'rate', 'do', 'you', 'know', 'if', 'this', 'would', 'be', 'bad', '?', 'i', \"'\", 'm', 'really', 'anxious', 'about', 'taking', 'it', 'and', 'i', 'want', 'to', 'start', 'exercising', 'but', 'i', 'feel', 'like', 'it', 'would', 'kick', 'my', 'heart', 'into', 'like', 'super', 'over', '##drive', '.', 'and', 'what', \"'\", 's', 'the', 'highest', 'resting', 'bp', '##m', 'that', 'ever', 'had', '?', 'i', \"'\", 'm', 'just', 'sitting', 'here', 'and', 'am', 'at', 'like', 'over', '100', 'bp', '##m', 'so', 'i', 'want', 'to', 'start', 'running', ',', 'will', 'this', 'be', 'bad', 'coupled', 'with', 'add', '##eral', '##l', '?']\n",
      "INFO:__main__:Number of tokens: 91\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['exercising', 'on', 'add', '##eral', '##l', 'and', 'resting', 'heart', 'rate', 'do', 'you', 'know', 'if', 'this', 'would', 'be', 'bad', '?', 'i', \"'\", 'm', 'really', 'anxious', 'about', 'taking', 'it', 'and', 'i', 'want', 'to', 'start', 'exercising', 'but', 'i', 'feel', 'like', 'it', 'would', 'kick', 'my', 'heart', 'into', 'like', 'super', 'over', '##drive', '.', 'and', 'what', \"'\", 's', 'the', 'highest', 'resting', 'bp', '##m', 'that', 'ever', 'had', '?', 'i', \"'\", 'm', 'just', 'sitting', 'here', 'and', 'am', 'at', 'like', 'over', '100', 'bp', '##m', 'so', 'i', 'want', 'to', 'start', 'running', ',', 'will', 'this', 'be', 'bad', 'coupled', 'with', 'add', '##eral', '##l', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['questions', 'on', 'add', '/', 'ad', '##hd', 'and', 'anxiety', 't', '##ld', '##r', ';', 'i', \"'\", 'm', 'wondering', 'how', 'other', 'people', 'with', 'ad', '##hd', 'deal', 'with', 'their', 'anxiety', 'after', 'talking', 'with', 'my', 'wife', 'and', 'going', 'to', 'the', 'doctor', 'i', \"'\", 'm', 'noticing', 'that', 'even', 'when', 'i', 'can', 'focus', 'i', 'still', 'blow', 'things', 'out', 'of', 'proportion', '.', 'my', 'wife', 'mentioned', 'at', 'the', 'doctor', 'yesterday', 'that', 'i', 'seem', 'to', 'have', 'anxiety', 'issues', 'but', 'i', 'did', 'not', 'connect', 'the', 'dots', 'to', 'realize', 'it', 'is', 'a', 'separate', 'issue', 'that', 'com', '##pl', '##icate', '##s', 'things', 'just', 'a', 'bit', '.', 'also', 'after', 'talking', 'with', 'my', 'sister', 'we', 'realized', 'that', 'anxiety', 'issues', 'run', 'in', 'the', 'family', '.', 'so', 'i', \"'\", 'm', 'trying', 'to', 'get', 'a', 'feel', 'for', 'how', 'other', 'people', 'deal', '.', 'i', \"'\", 'm', 'not', 'expecting', 'a', 'magic', 'bullet', 'or', 'a', 'medication', 'fix', '.', 'i', 'do', 'not', 'think', 'my', 'anxiety', 'issues', 'are', 'not', 'something', 'other', 'people', 'have', 'had', 'to', 'deal', 'with', '.', 'i', 'plan', 'on', 'going', 'to', 'my', 'doctor', 'to', 'address', 'this', 'but', 'i', \"'\", 've', 'found', 'that', 'if', 'i', 'go', 'in', 'with', 'an', 'idea', 'of', 'what', 'my', 'issues', 'are', 'the', 'conversation', 'is', 'more', 'of', 'a', 'discussion', 'and', 'less', 'a', 'lecture', '.', 'edit', ':', 'i', \"'\", 'm', 'going', 'to', 'go', 'see', 'my', 'doctor', 'regarding', 'this', '.', 'but', 'it', 'is', 'helpful', 'to', 'talk', 'to', 'other', 'people', '.']\n",
      "INFO:__main__:Number of tokens: 218\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['questions', 'on', 'add', '/', 'ad', '##hd', 'and', 'anxiety', 't', '##ld', '##r', ';', 'i', \"'\", 'm', 'wondering', 'how', 'other', 'people', 'with', 'ad', '##hd', 'deal', 'with', 'their', 'anxiety', 'after', 'talking', 'with', 'my', 'wife', 'and', 'going', 'to', 'the', 'doctor', 'i', \"'\", 'm', 'noticing', 'that', 'even', 'when', 'i', 'can', 'focus', 'i', 'still', 'blow', 'things', 'out', 'of', 'proportion', '.', 'my', 'wife', 'mentioned', 'at', 'the', 'doctor', 'yesterday', 'that', 'i', 'seem', 'to', 'have', 'anxiety', 'issues', 'but', 'i', 'did', 'not', 'connect', 'the', 'dots', 'to', 'realize', 'it', 'is', 'a', 'separate', 'issue', 'that', 'com', '##pl', '##icate', '##s', 'things', 'just', 'a', 'bit', '.', 'also', 'after', 'talking', 'with', 'my', 'sister', 'we', 'realized', 'that', 'anxiety', 'issues', 'run', 'in', 'the', 'family', '.', 'so', 'i', \"'\", 'm', 'trying', 'to', 'get', 'a', 'feel', 'for', 'how', 'other', 'people', 'deal', '.', 'i', \"'\", 'm', 'not', 'expecting', 'a', 'magic', 'bullet', 'or', 'a', 'medication', 'fix', '.', 'i', 'do', 'not', 'think', 'my', 'anxiety', 'issues', 'are', 'not', 'something', 'other', 'people', 'have', 'had', 'to', 'deal', 'with', '.', 'i', 'plan', 'on', 'going', 'to', 'my', 'doctor', 'to', 'address', 'this', 'but', 'i', \"'\", 've', 'found', 'that', 'if', 'i', 'go', 'in', 'with', 'an', 'idea', 'of', 'what', 'my', 'issues', 'are', 'the', 'conversation', 'is', 'more', 'of', 'a', 'discussion', 'and', 'less', 'a', 'lecture', '.', 'edit', ':', 'i', \"'\", 'm', 'going', 'to', 'go', 'see', 'my', 'doctor', 'regarding', 'this', '.', 'but', 'it', 'is', 'helpful', 'to', 'talk', 'to', 'other', 'people', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['pediatric', 'ad', '##hd', '?', '-', 'the', 'quick', 'fix', 'strategy', ':', 'are', 'pediatric', '##ians', 'too', 'quick', 'on', 'the', 'draw', 'when', 'it', 'comes', 'to', 'prescription', '?']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['pediatric', 'ad', '##hd', '?', '-', 'the', 'quick', 'fix', 'strategy', ':', 'are', 'pediatric', '##ians', 'too', 'quick', 'on', 'the', 'draw', 'when', 'it', 'comes', 'to', 'prescription', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'music', 'affect', 'your', 'mental', 'state', 'at', 'all', 'while', 'under', 'the', 'influence', 'of', 'rita', '##lin', '(', 'and', 'other', 'amp', '##het', '##amine', '-', 'based', 'drugs', ')', '?', '(', 'diagnosed', 'with', 'ad', '##hd', 'since', 'el', '##em', '.', 'school', ')', 'edit', ':', 'age', '20', 'i', 'was', 'just', 'curious', 'about', 'this', 'because', 'i', 'always', 'find', 'myself', 'studying', 'or', 'doing', 'work', 'while', 'listening', 'to', 'music', ',', 'and', 'i', 'don', \"'\", 't', 'ever', 'actually', 'notice', 'the', 'music', '.', '.', '.', 'i', 'usually', 'put', 'something', 'nice', 'on', 'like', 'john', 'mayer', ',', 'maroon', '5', ',', 'classical', 'music', ',', 'etc', '.', 'and', 'once', 'i', 'start', 'on', 'my', 'assignment', ',', 'i', 'completely', 'block', 'out', 'the', 'music', 'and', 'it', 'just', 'becomes', 'a', 'blur', 'in', 'the', 'back', 'of', 'my', 'head', '.', 'i', 'guess', 'my', 'question', 'would', 'be', '.', '.', '.', 'am', 'i', 'just', 'simply', 'ignoring', 'the', 'music', 'that', 'is', 'distracting', 'the', 'thoughts', 'that', 'keep', 'me', 'off', '-', 'track', '?', 'or', 'is', 'the', 'music', 'actually', 'helping', 'my', 'focus', 'by', 'giving', 'me', 'that', \"'\", 'white', '-', 'noise', \"'\", '?', 'sorry', 'if', 'this', 'doesn', \"'\", 't', 'make', 'much', 'sense', '.', '.', '.', 'trying', 'my', 'best', 'here', '.', ':', '(', '[', 'currently', 'listening', 'to', \"'\", 'we', 'are', 'young', 'ft', '.', 'jane', '##lle', 'mona', '##e', \"'\", 'by', 'fun', '.', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'f', '##q', '##l', '##gh', '##ph', '##z', '##x', '##j', '##c', '&', 'feature', '=', 'share', ')', 'edit', ':', '[', 'simply', 'noise', ']', '(', 'www', '.', 'simply', '##no', '##ise', '.', 'com', ')', ':', 'a', 'fantastic', 'background', 'noise', 'site', 'brought', 'up', 'by', 'mach', '##u', '##u', 'edit', ':', 'explosions', 'in', 'the', 'sky', ':', 'a', 'good', 'post', '-', 'rock', 'band', ',', 'nice', 'for', 'background', 'music', 'and', 'for', 'concentration', '.']\n",
      "INFO:__main__:Number of tokens: 281\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'music', 'affect', 'your', 'mental', 'state', 'at', 'all', 'while', 'under', 'the', 'influence', 'of', 'rita', '##lin', '(', 'and', 'other', 'amp', '##het', '##amine', '-', 'based', 'drugs', ')', '?', '(', 'diagnosed', 'with', 'ad', '##hd', 'since', 'el', '##em', '.', 'school', ')', 'edit', ':', 'age', '20', 'i', 'was', 'just', 'curious', 'about', 'this', 'because', 'i', 'always', 'find', 'myself', 'studying', 'or', 'doing', 'work', 'while', 'listening', 'to', 'music', ',', 'and', 'i', 'don', \"'\", 't', 'ever', 'actually', 'notice', 'the', 'music', '.', '.', '.', 'i', 'usually', 'put', 'something', 'nice', 'on', 'like', 'john', 'mayer', ',', 'maroon', '5', ',', 'classical', 'music', ',', 'etc', '.', 'and', 'once', 'i', 'start', 'on', 'my', 'assignment', ',', 'i', 'completely', 'block', 'out', 'the', 'music', 'and', 'it', 'just', 'becomes', 'a', 'blur', 'in', 'the', 'back', 'of', 'my', 'head', '.', 'i', 'guess', 'my', 'question', 'would', 'be', '.', '.', '.', 'am', 'i', 'just', 'simply', 'ignoring', 'the', 'music', 'that', 'is', 'distracting', 'the', 'thoughts', 'that', 'keep', 'me', 'off', '-', 'track', '?', 'or', 'is', 'the', 'music', 'actually', 'helping', 'my', 'focus', 'by', 'giving', 'me', 'that', \"'\", 'white', '-', 'noise', \"'\", '?', 'sorry', 'if', 'this', 'doesn', \"'\", 't', 'make', 'much', 'sense', '.', '.', '.', 'trying', 'my', 'best', 'here', '.', ':', '(', '[', 'currently', 'listening', 'to', \"'\", 'we', 'are', 'young', 'ft', '.', 'jane', '##lle', 'mona', '##e', \"'\", 'by', 'fun', '.', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'f', '##q', '##l', '##gh', '##ph', '##z', '##x', '##j', '##c', '&', 'feature', '=', 'share', ')', 'edit', ':', '[', 'simply', 'noise', ']', '(', 'www', '.', 'simply', '##no', '##ise', '.', 'com', ')', ':', 'a', 'fantastic', 'background', 'noise', 'site', 'brought', 'up', 'by', 'mach', '##u', '##u', 'edit', ':', 'explosions', 'in', 'the', 'sky', ':', 'a', 'good', 'post', '-', 'rock', 'band', ',', 'nice', 'for', 'background', 'music', 'and', 'for', 'concentration', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'shop', '##ped', 'and', 'dropped', '-', 'an', 'overheard', 'conversation', 'raises', 'rick', \"'\", 's', 'blood', 'pressure']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'shop', '##ped', 'and', 'dropped', '-', 'an', 'overheard', 'conversation', 'raises', 'rick', \"'\", 's', 'blood', 'pressure']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['drug', 'money', '-', 'forever', 'scarred', '-', 'understanding', 'the', 'rules', 'that', 'di', '##cta', '##te', 'disclosure', 'and', 'the', 'perceptions', 'of', 'good', 'science']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['drug', 'money', '-', 'forever', 'scarred', '-', 'understanding', 'the', 'rules', 'that', 'di', '##cta', '##te', 'disclosure', 'and', 'the', 'perceptions', 'of', 'good', 'science']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['lack', 'of', 'sleep', 'and', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['lack', 'of', 'sleep', 'and', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'attention', 'deficit', 'is', 'gone', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'attention', 'deficit', 'is', 'gone', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['useful', 'tip', ',', 'don', \"'\", 't', 'try', ',', 'just', 'slow', 'the', 'speed', 'of', 'whatever', 'you', 'do', ',', 'talking', ',', 'moving', '.', 'don', \"'\", 't', 'think', 'too', 'much', 'about', 'what', 'you', 'are', 'about', 'to', 'say', 'just', 'let', 'it', 'out', '.']\n",
      "INFO:__main__:Number of tokens: 39\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['useful', 'tip', ',', 'don', \"'\", 't', 'try', ',', 'just', 'slow', 'the', 'speed', 'of', 'whatever', 'you', 'do', ',', 'talking', ',', 'moving', '.', 'don', \"'\", 't', 'think', 'too', 'much', 'about', 'what', 'you', 'are', 'about', 'to', 'say', 'just', 'let', 'it', 'out', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'shortage', 'what', 'is', 'the', 'deal', 'with', 'this', '?', 'how', 'does', 'one', 'find', 'it', '?', 'shotgun', 'call', 'all', 'the', 'local', 'ph', '##arm', '##acies', '?']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'shortage', 'what', 'is', 'the', 'deal', 'with', 'this', '?', 'how', 'does', 'one', 'find', 'it', '?', 'shotgun', 'call', 'all', 'the', 'local', 'ph', '##arm', '##acies', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['day', '##tra', '##na', 'patch', 'for', 'adults', '?', 'i', 'posted', 'a', 'few', 'days', 'ago', 'about', 'wanting', 'to', 'get', 'off', 'v', '##y', '##van', '##se', '.', 'it', 'worked', 'wonderful', '##ly', 'for', 'a', 'year', 'or', 'so', ',', 'but', 'after', 'taking', 'the', 'summer', 'off', ',', 'i', 'find', 'the', '50', '##mg', 'dose', 'way', 'too', 'se', '##dating', '.', 'i', 'love', 'not', 'having', 'anxiety', ',', 'emotional', 'outburst', '##s', ',', 'and', 'improved', 'social', 'skills', ',', 'but', 'even', 'those', 'positive', 'effects', 'don', \"'\", 't', 'seem', 'worth', 'it', 'to', 'me', 'anymore', 'because', 'i', 'feel', 'so', 'z', '##om', '##bi', '##fied', 'most', 'of', 'the', 'day', '.', 'it', \"'\", 's', 'like', 'my', 'brain', 'is', 'fog', '##gy', ',', 'i', 'frequently', 'fall', 'asleep', 'an', 'hour', 'or', 'two', 'after', 'taking', 'it', ',', 'and', 'it', 'takes', 'away', 'my', 'appetite', '.', 'i', \"'\", 've', 'tried', 'hal', '##ving', 'the', 'dose', '(', 'today', 'and', 'yesterday', ')', 'and', 'it', 'seemed', 'to', 'help', 'a', 'little', ',', 'but', 'it', 'doesn', \"'\", 't', 'last', 'more', 'than', '3', 'or', '4', 'hours', '.', 'getting', 'to', 'sleep', 'last', 'night', 'was', 'an', 'absolute', 'nightmare', 'and', 'i', 'woke', 'up', 'in', 'the', 'middle', 'of', 'night', 'feeling', 'like', 'i', 'was', 'was', 'ready', 'to', 'jump', 'out', 'of', 'my', 'skin', '.', 'i', 'feel', 'like', 'i', 'need', 'something', 'long', '-', 'lasting', ',', 'but', 'not', 'super', '-', 'powerful', 'and', 'am', 'thinking', 'the', 'day', '##tra', '##na', 'patch', 'could', 'be', 'helpful', '.', 'i', 'like', 'being', 'able', 'to', 'control', 'the', 'dose', ',', 'and', 'have', 'read', 'some', 'people', 'even', 'leave', 'it', 'on', 'during', 'sleep', '(', 'i', 'think', 'that', 'would', 'help', 'me', ')', '.', '*', '*', 'have', 'any', 'adults', 'tried', 'the', 'day', '##tra', '##na', 'patch', '?', 'what', 'was', 'your', 'experience', 'on', 'it', '?', '*', '*']\n",
      "INFO:__main__:Number of tokens: 264\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['day', '##tra', '##na', 'patch', 'for', 'adults', '?', 'i', 'posted', 'a', 'few', 'days', 'ago', 'about', 'wanting', 'to', 'get', 'off', 'v', '##y', '##van', '##se', '.', 'it', 'worked', 'wonderful', '##ly', 'for', 'a', 'year', 'or', 'so', ',', 'but', 'after', 'taking', 'the', 'summer', 'off', ',', 'i', 'find', 'the', '50', '##mg', 'dose', 'way', 'too', 'se', '##dating', '.', 'i', 'love', 'not', 'having', 'anxiety', ',', 'emotional', 'outburst', '##s', ',', 'and', 'improved', 'social', 'skills', ',', 'but', 'even', 'those', 'positive', 'effects', 'don', \"'\", 't', 'seem', 'worth', 'it', 'to', 'me', 'anymore', 'because', 'i', 'feel', 'so', 'z', '##om', '##bi', '##fied', 'most', 'of', 'the', 'day', '.', 'it', \"'\", 's', 'like', 'my', 'brain', 'is', 'fog', '##gy', ',', 'i', 'frequently', 'fall', 'asleep', 'an', 'hour', 'or', 'two', 'after', 'taking', 'it', ',', 'and', 'it', 'takes', 'away', 'my', 'appetite', '.', 'i', \"'\", 've', 'tried', 'hal', '##ving', 'the', 'dose', '(', 'today', 'and', 'yesterday', ')', 'and', 'it', 'seemed', 'to', 'help', 'a', 'little', ',', 'but', 'it', 'doesn', \"'\", 't', 'last', 'more', 'than', '3', 'or', '4', 'hours', '.', 'getting', 'to', 'sleep', 'last', 'night', 'was', 'an', 'absolute', 'nightmare', 'and', 'i', 'woke', 'up', 'in', 'the', 'middle', 'of', 'night', 'feeling', 'like', 'i', 'was', 'was', 'ready', 'to', 'jump', 'out', 'of', 'my', 'skin', '.', 'i', 'feel', 'like', 'i', 'need', 'something', 'long', '-', 'lasting', ',', 'but', 'not', 'super', '-', 'powerful', 'and', 'am', 'thinking', 'the', 'day', '##tra', '##na', 'patch', 'could', 'be', 'helpful', '.', 'i', 'like', 'being', 'able', 'to', 'control', 'the', 'dose', ',', 'and', 'have', 'read', 'some', 'people', 'even', 'leave', 'it', 'on', 'during', 'sleep', '(', 'i', 'think', 'that', 'would', 'help', 'me', ')', '.', '*', '*', 'have', 'any', 'adults', 'tried', 'the', 'day', '##tra', '##na', 'patch', '?', 'what', 'was', 'your', 'experience', 'on', 'it', '?', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['pro', '##cr', '##ast', '##inating', 'again', '.', '.', '.', 'i', 'haven', \"'\", 't', 'been', 'diagnosed', ',', 'but', 'i', 'swear', 'if', 'i', 'don', \"'\", 't', 'have', 'an', 'attention', 'disorder', 'i', 'must', 'be', 'the', 'la', '##zie', '##st', 'person', 'ever', '.', 'i', \"'\", 've', 'written', 'half', 'of', 'the', 'intro', 'to', 'my', 'three', 'page', 'paper', 'and', 'can', \"'\", 't', 'seem', 'to', 'keep', 'going', '.', 'it', 'took', 'me', 'about', '3', 'hours', 'to', 'get', 'this', 'far', '.', 'i', 'want', 'to', 'do', 'something', 'else', '.', 'anything', 'else', ',', 'right', 'now', '.', 'math', 'homework', 'sounds', 'fucking', 'great', 'but', 'as', 'soon', 'as', 'i', 'try', 'to', 'start', 'doing', 'it', 'instead', 'i', \"'\", 'll', 'fl', '##ake', 'and', 'have', 'the', 'same', 'issue', 'three', 'problems', 'in', '.', 'just', 'w', '##hini', '##ng', ':', '(']\n",
      "INFO:__main__:Number of tokens: 119\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['pro', '##cr', '##ast', '##inating', 'again', '.', '.', '.', 'i', 'haven', \"'\", 't', 'been', 'diagnosed', ',', 'but', 'i', 'swear', 'if', 'i', 'don', \"'\", 't', 'have', 'an', 'attention', 'disorder', 'i', 'must', 'be', 'the', 'la', '##zie', '##st', 'person', 'ever', '.', 'i', \"'\", 've', 'written', 'half', 'of', 'the', 'intro', 'to', 'my', 'three', 'page', 'paper', 'and', 'can', \"'\", 't', 'seem', 'to', 'keep', 'going', '.', 'it', 'took', 'me', 'about', '3', 'hours', 'to', 'get', 'this', 'far', '.', 'i', 'want', 'to', 'do', 'something', 'else', '.', 'anything', 'else', ',', 'right', 'now', '.', 'math', 'homework', 'sounds', 'fucking', 'great', 'but', 'as', 'soon', 'as', 'i', 'try', 'to', 'start', 'doing', 'it', 'instead', 'i', \"'\", 'll', 'fl', '##ake', 'and', 'have', 'the', 'same', 'issue', 'three', 'problems', 'in', '.', 'just', 'w', '##hini', '##ng', ':', '(']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'med', '##s', 'long', '-', 'term', 'effects', 'ok', '.', 'bit', 'of', 'a', 'long', 'story', 'here', ',', 'but', 'the', 'background', 'is', 'important', '.', 'i', 'was', 'diagnosed', 'with', 'add', '/', 'ad', '##hd', '(', 'which', 'may', 'or', 'may', 'not', 'have', 'actually', 'been', 'true', ')', 'when', 'i', 'was', 'really', 'young', '.', 'so', ',', 'like', 'probably', 'many', 'of', 'you', 'guys', 'on', 'here', ',', 'i', 'spent', 'my', 'childhood', 'on', 'add', 'drugs', '-', 'first', 'rita', '##lin', ',', 'then', 'add', '##eral', '##l', '.', 'there', 'were', 'a', 'couple', 'weeks', 'on', 'dex', '##ed', '##rine', 'too', ',', 'but', 'i', 'don', \"'\", 't', 'talk', 'about', 'those', 'because', 'i', 'have', 'no', 'memory', 'of', 'them', 'whatsoever', '.', '(', 'kind', 'of', 'why', 'i', 'went', 'off', 'that', 'one', '.', 'anyway', '.', ')', 'i', 'finally', 'dropped', 'them', 'when', 'i', 'got', 'into', 'college', ',', 'and', 'in', 'that', 'one', 'year', 'i', 'discovered', 'an', 'almost', 'infinite', 'appetite', '.', 'in', 'nine', 'months', 'i', 'gained', 'thirty', 'pounds', ',', 'and', 'i', 'haven', \"'\", 't', 'been', 'skinny', 'since', '.', 'now', ',', 'i', 'know', 'st', '##im', '##ula', '##nts', 'like', 'rita', '##lin', 'and', 'add', '##eral', '##l', 'can', 'act', 'as', 'appetite', 'suppress', '##ants', '.', 'my', 'question', 'is', 'this', '-', 'da', '##e', 'have', 'a', 'similar', 'story', '?', 'spend', 'your', 'youth', 'marina', '##ted', 'in', 'amp', '##het', '##amine', '##s', 'and', 'get', 'fat', 'when', 'you', 'stop', '?', 'information', 'on', 'the', 'subject', 'is', 'sore', '##ly', 'lacking', 'thus', 'far', '.', 'if', 'you', 'did', 'get', 'fat', ',', 'what', 'other', 'circumstances', 'surrounded', 'your', 'stopping', 'the', 'medicine', '?', 'for', 'me', ',', 'it', 'was', 'something', 'like', 'depression', '-', 'which', 'makes', 'my', 'own', 'data', 'at', 'least', 'somewhat', 'suspect', '.', 'if', 'you', 'didn', \"'\", 't', ',', 'what', 'circumstances', 'surrounded', 'that', '?', 'i', 'need', 'both', 'sides', 'of', 'the', 'story', 'here', '.', 'i', 'know', 'it', \"'\", 's', 'crude', 'methodology', 'at', 'best', ',', 'but', 'i', \"'\", 'm', 'curious', 'to', 'see', 'if', 'my', 'hypothesis', 'even', 'has', 'a', 'leg', 'to', 'stand', 'on', 'or', 'if', 'i', \"'\", 'm', 'just', 'tilting', 'at', 'windmill', '##s', '.']\n",
      "INFO:__main__:Number of tokens: 310\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'med', '##s', 'long', '-', 'term', 'effects', 'ok', '.', 'bit', 'of', 'a', 'long', 'story', 'here', ',', 'but', 'the', 'background', 'is', 'important', '.', 'i', 'was', 'diagnosed', 'with', 'add', '/', 'ad', '##hd', '(', 'which', 'may', 'or', 'may', 'not', 'have', 'actually', 'been', 'true', ')', 'when', 'i', 'was', 'really', 'young', '.', 'so', ',', 'like', 'probably', 'many', 'of', 'you', 'guys', 'on', 'here', ',', 'i', 'spent', 'my', 'childhood', 'on', 'add', 'drugs', '-', 'first', 'rita', '##lin', ',', 'then', 'add', '##eral', '##l', '.', 'there', 'were', 'a', 'couple', 'weeks', 'on', 'dex', '##ed', '##rine', 'too', ',', 'but', 'i', 'don', \"'\", 't', 'talk', 'about', 'those', 'because', 'i', 'have', 'no', 'memory', 'of', 'them', 'whatsoever', '.', '(', 'kind', 'of', 'why', 'i', 'went', 'off', 'that', 'one', '.', 'anyway', '.', ')', 'i', 'finally', 'dropped', 'them', 'when', 'i', 'got', 'into', 'college', ',', 'and', 'in', 'that', 'one', 'year', 'i', 'discovered', 'an', 'almost', 'infinite', 'appetite', '.', 'in', 'nine', 'months', 'i', 'gained', 'thirty', 'pounds', ',', 'and', 'i', 'haven', \"'\", 't', 'been', 'skinny', 'since', '.', 'now', ',', 'i', 'know', 'st', '##im', '##ula', '##nts', 'like', 'rita', '##lin', 'and', 'add', '##eral', '##l', 'can', 'act', 'as', 'appetite', 'suppress', '##ants', '.', 'my', 'question', 'is', 'this', '-', 'da', '##e', 'have', 'a', 'similar', 'story', '?', 'spend', 'your', 'youth', 'marina', '##ted', 'in', 'amp', '##het', '##amine', '##s', 'and', 'get', 'fat', 'when', 'you', 'stop', '?', 'information', 'on', 'the', 'subject', 'is', 'sore', '##ly', 'lacking', 'thus', 'far', '.', 'if', 'you', 'did', 'get', 'fat', ',', 'what', 'other', 'circumstances', 'surrounded', 'your', 'stopping', 'the', 'medicine', '?', 'for', 'me', ',', 'it', 'was', 'something', 'like', 'depression', '-', 'which', 'makes', 'my', 'own', 'data', 'at', 'least', 'somewhat', 'suspect', '.', 'if', 'you', 'didn', \"'\", 't', ',', 'what', 'circumstances', 'surrounded', 'that', '?', 'i', 'need', 'both', 'sides', 'of', 'the', 'story', 'here', '.', 'i', 'know', 'it', \"'\", 's', 'crude', 'methodology', 'at', 'best', ',', 'but', 'i', \"'\", 'm', 'curious', 'to', 'see', 'if', 'my', 'hypothesis', 'even', 'has', 'a', 'leg', 'to', 'stand', 'on', 'or', 'if', 'i', \"'\", 'm', 'just', 'tilting', 'at', 'windmill', '##s', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['five', 'online', 'resources', 'recommended', 'by', 'the', 'centre', 'for', 'ad', '##hd', 'awareness']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['five', 'online', 'resources', 'recommended', 'by', 'the', 'centre', 'for', 'ad', '##hd', 'awareness']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['fuck', 'you', 'for', 'making', 'me', 'accept', 'that', 'i', 'had', 'ad', '##hd', 'and', 'had', 'to', 'live', 'with', 'it', '.', 'my', 'symptoms', 'are', 'under', 'my', 'own', 'control', 'now', '.']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['fuck', 'you', 'for', 'making', 'me', 'accept', 'that', 'i', 'had', 'ad', '##hd', 'and', 'had', 'to', 'live', 'with', 'it', '.', 'my', 'symptoms', 'are', 'under', 'my', 'own', 'control', 'now', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['thinking', 'of', 'going', 'for', 'diagnosis', '.', 'if', 'i', 'get', 'put', 'on', 'med', '##s', ',', 'would', 'i', 'stop', 'fi', '##dget', '##ing', '/', 'playing', 'with', 'things', '?', 'for', 'the', 'last', 'couple', 'of', 'years', 'i', \"'\", 've', 'considered', 'that', 'i', 'likely', 'have', 'ad', '##hd', ',', 'and', 'after', 'describing', 'some', 'things', 'to', 'my', 'doctor', '(', 'lack', 'of', 'focus', ',', 'forget', '##fulness', ')', 'while', 'talking', 'about', 'something', 'else', ',', 'they', 'asked', 'if', 'i', \"'\", 've', 'ever', 'had', 'a', 'formal', 'diagnosis', 'of', 'add', '/', 'ad', '##hd', ',', 'which', 'i', 'haven', \"'\", 't', '.', 'i', \"'\", 'm', 'scheduled', 'to', 'see', 'them', 'again', 'next', 'month', ',', 'and', 'after', 'reading', 'some', 'things', 'here', 'and', 'on', '/', 'r', '/', 'add', ',', 'i', \"'\", 'm', 'going', 'to', 'try', 'bringing', 'it', 'up', '.', 'however', ',', 'i', \"'\", 've', 'always', 'been', 'a', 'fi', '##dget', '##er', '.', 'i', \"'\", 'm', 'almost', 'always', 'playing', 'with', 'my', 'toes', 'or', 'fingers', 'while', 'sitting', '(', 'which', 'unfortunately', 'includes', 'biting', 'nails', ')', ',', 'and', 'if', 'i', \"'\", 'm', 'standing', 'around', 'or', 'talking', 'to', 'somebody', ',', 'i', 'always', 'seem', 'to', 'pick', 'up', 'something', 'and', 'start', 'playing', 'with', 'that', 'while', 'talking', '.', 'while', 'it', 'does', 'have', 'its', 'downs', '##ides', '(', 'apparently', 'as', 'a', 'kid', 'i', 'used', 'to', 'break', 'a', 'lot', 'of', 'things', 'in', 'my', 'dad', \"'\", 's', 'office', 'doing', 'it', ')', 'it', 'does', 'increase', 'the', 'number', 'of', 'cal', '##ories', 'burnt', 'in', 'a', 'day', '.', 'one', 'article', 'i', 'read', 'awhile', 'ago', 'quoted', 'about', '300', 'cal', '##ories', 'a', 'day', '.', 'added', 'on', 'to', 'the', 'fact', 'that', 'i', 'generally', 'enjoy', 'it', ',', 'it', \"'\", 's', 'something', 'i', \"'\", 'd', 'actually', 'fear', 'losing', 'if', 'i', 'started', 'taking', 'medication', '.', 'has', 'anybody', 'around', 'here', 'been', 'a', 'fi', '##dget', '##er', 'before', 'starting', 'on', 'medication', ',', 'then', 'stopped', 'fi', '##dget', '##ing', 'once', 'on', 'med', '##s', '?']\n",
      "INFO:__main__:Number of tokens: 288\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['thinking', 'of', 'going', 'for', 'diagnosis', '.', 'if', 'i', 'get', 'put', 'on', 'med', '##s', ',', 'would', 'i', 'stop', 'fi', '##dget', '##ing', '/', 'playing', 'with', 'things', '?', 'for', 'the', 'last', 'couple', 'of', 'years', 'i', \"'\", 've', 'considered', 'that', 'i', 'likely', 'have', 'ad', '##hd', ',', 'and', 'after', 'describing', 'some', 'things', 'to', 'my', 'doctor', '(', 'lack', 'of', 'focus', ',', 'forget', '##fulness', ')', 'while', 'talking', 'about', 'something', 'else', ',', 'they', 'asked', 'if', 'i', \"'\", 've', 'ever', 'had', 'a', 'formal', 'diagnosis', 'of', 'add', '/', 'ad', '##hd', ',', 'which', 'i', 'haven', \"'\", 't', '.', 'i', \"'\", 'm', 'scheduled', 'to', 'see', 'them', 'again', 'next', 'month', ',', 'and', 'after', 'reading', 'some', 'things', 'here', 'and', 'on', '/', 'r', '/', 'add', ',', 'i', \"'\", 'm', 'going', 'to', 'try', 'bringing', 'it', 'up', '.', 'however', ',', 'i', \"'\", 've', 'always', 'been', 'a', 'fi', '##dget', '##er', '.', 'i', \"'\", 'm', 'almost', 'always', 'playing', 'with', 'my', 'toes', 'or', 'fingers', 'while', 'sitting', '(', 'which', 'unfortunately', 'includes', 'biting', 'nails', ')', ',', 'and', 'if', 'i', \"'\", 'm', 'standing', 'around', 'or', 'talking', 'to', 'somebody', ',', 'i', 'always', 'seem', 'to', 'pick', 'up', 'something', 'and', 'start', 'playing', 'with', 'that', 'while', 'talking', '.', 'while', 'it', 'does', 'have', 'its', 'downs', '##ides', '(', 'apparently', 'as', 'a', 'kid', 'i', 'used', 'to', 'break', 'a', 'lot', 'of', 'things', 'in', 'my', 'dad', \"'\", 's', 'office', 'doing', 'it', ')', 'it', 'does', 'increase', 'the', 'number', 'of', 'cal', '##ories', 'burnt', 'in', 'a', 'day', '.', 'one', 'article', 'i', 'read', 'awhile', 'ago', 'quoted', 'about', '300', 'cal', '##ories', 'a', 'day', '.', 'added', 'on', 'to', 'the', 'fact', 'that', 'i', 'generally', 'enjoy', 'it', ',', 'it', \"'\", 's', 'something', 'i', \"'\", 'd', 'actually', 'fear', 'losing', 'if', 'i', 'started', 'taking', 'medication', '.', 'has', 'anybody', 'around', 'here', 'been', 'a', 'fi', '##dget', '##er', 'before', 'starting', 'on', 'medication', ',', 'then', 'stopped', 'fi', '##dget', '##ing', 'once', 'on', 'med', '##s', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'fact', 'check', '?', 'add', '##eral', '##l', 'shortage', 'due', 'to', 'a', 'generic', 'producer', 'going', 'out', 'of', 'business', 'and', 'fda', 'won', \"'\", 't', 'allow', 'companies', 'to', 'make', 'more', '?']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'fact', 'check', '?', 'add', '##eral', '##l', 'shortage', 'due', 'to', 'a', 'generic', 'producer', 'going', 'out', 'of', 'business', 'and', 'fda', 'won', \"'\", 't', 'allow', 'companies', 'to', 'make', 'more', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'a', 'book', 'to', 'read', 'by', 'monday', '.', 'it', 'is', 'my', 'worst', 'nightmare', '.', 'help', 'me', 'with', 'strategies', ',', 'red', '##dit', '.', 'college', 'has', 'basically', 'collapsed', 'around', 'me', 'this', 'week', '.', 'the', 'assignment', 'was', 'given', 'to', 'me', 'on', 'monday', ',', 'but', 'two', 'art', 'classes', 'both', 'pi', '##ling', 'on', 'projects', 'and', 'daily', 'hour', 'requirements', 'and', 'english', 'do', '##ling', 'out', 'essays', ',', 'and', 'classes', 'going', 'until', '5', 'most', 'of', 'the', 'week', 'i', 'just', 'have', 'not', 'had', 'time', '.', 'i', 'need', 'to', 'read', '[', 'napoleon', 'by', 'felix', 'markham', ']', '(', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'napoleon', '-', 'mentor', '-', 'felix', '-', 'markham', '/', 'd', '##p', '/', '04', '##51', '##6', '##27', '##9', '##8', '##9', '/', 'ref', '=', 'sr', '_', '1', '_', '2', '?', 's', '=', 'books', '&', 'ie', '=', 'ut', '##f', '##8', '&', 'qi', '##d', '=', '132', '##16', '##9', '##9', '##16', '##1', '&', 'sr', '=', '1', '-', '2', ')', '.', 'normally', ',', 'i', 'like', 'history', '.', 'but', 'this', 'book', 'is', 'brutal', '.', 'it', 'is', 'not', 'interesting', 'in', 'the', 'slightest', '.', 'the', 'manner', 'in', 'which', 'it', 'is', 'written', 'destroys', 'any', 'chance', 'of', 'this', '.', 'every', 'paragraph', 'is', 'so', 'cr', '##ammed', 'with', 'different', 'facts', 'and', 'an', '##ec', '##dote', '##s', 'that', 'nothing', 'is', 'expanded', 'on', '.', 'when', 'something', 'seems', 'remotely', 'interesting', 'it', 'doesn', \"'\", 't', 'matter', 'because', 'it', \"'\", 's', 'moved', 'on', 'from', 'in', 'the', 'blink', 'of', 'an', 'eye', '.', 'it', \"'\", 's', 'so', 'fast', 'and', 'so', 'slow', 'at', 'the', 'same', 'time', 'it', 'is', 'jar', '##ring', '.', 'it', 'somehow', 'has', '300', 'whole', 'pages', 'of', 'these', 'super', 'condensed', 'paragraph', '##s', '.', '.', '.', '.', 'what', '.', '300', 'pages', 'of', 'this', 'shit', '.', 'i', 'can', 'knock', 'out', 'hp', '##7', 'in', 'one', 'day', 'but', 'not', 'this', '.', 'i', 'need', 'to', 'answer', 'essay', 'questions', 'afterwards', '.', 'the', 'professor', 'is', 'adamant', 'about', 'making', 'sure', 'we', 'have', 'read', 'the', 'book', '.', 'he', 'has', 'said', 'that', 'if', 'he', 'can', 'tell', 'we', \"'\", 've', 'gotten', 'the', 'information', 'from', 'another', 'source', 'he', 'will', 'fail', 'the', 'paper', '.', 'all', 'of', 'these', 'factors', 'make', 'the', 'book', 'un', '##ski', '##mma', '##ble', '.', 'and', 'the', 'way', 'it', \"'\", 's', 'written', ',', 'i', 'feel', 'like', 'it', \"'\", 's', 'written', 'for', 'people', 'that', 'already', 'are', 'studying', 'napoleon', 'in', 'length', 'and', 'we', 'haven', \"'\", 't', ',', 'so', 'i', 'am', 'slightly', 'lost', 'even', '.', 'every', 'time', 'i', 'try', 'to', 'read', 'it', 'i', 'just', 'die', 'a', 'little', 'inside', '.', 'i', \"'\", 've', 'been', 'trying', 'to', 'read', 'it', 'this', 'week', 'but', 'it', 'has', 'been', 'disastrous', ',', 'i', 'am', 'only', 'like', '30', 'pages', 'in', '.', 'help', '.']\n",
      "INFO:__main__:Number of tokens: 416\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'a', 'book', 'to', 'read', 'by', 'monday', '.', 'it', 'is', 'my', 'worst', 'nightmare', '.', 'help', 'me', 'with', 'strategies', ',', 'red', '##dit', '.', 'college', 'has', 'basically', 'collapsed', 'around', 'me', 'this', 'week', '.', 'the', 'assignment', 'was', 'given', 'to', 'me', 'on', 'monday', ',', 'but', 'two', 'art', 'classes', 'both', 'pi', '##ling', 'on', 'projects', 'and', 'daily', 'hour', 'requirements', 'and', 'english', 'do', '##ling', 'out', 'essays', ',', 'and', 'classes', 'going', 'until', '5', 'most', 'of', 'the', 'week', 'i', 'just', 'have', 'not', 'had', 'time', '.', 'i', 'need', 'to', 'read', '[', 'napoleon', 'by', 'felix', 'markham', ']', '(', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'napoleon', '-', 'mentor', '-', 'felix', '-', 'markham', '/', 'd', '##p', '/', '04', '##51', '##6', '##27', '##9', '##8', '##9', '/', 'ref', '=', 'sr', '_', '1', '_', '2', '?', 's', '=', 'books', '&', 'ie', '=', 'ut', '##f', '##8', '&', 'qi', '##d', '=', '132', '##16', '##9', '##9', '##16', '##1', '&', 'sr', '=', '1', '-', '2', ')', '.', 'normally', ',', 'i', 'like', 'history', '.', 'but', 'this', 'book', 'is', 'brutal', '.', 'it', 'is', 'not', 'interesting', 'in', 'the', 'slightest', '.', 'the', 'manner', 'in', 'which', 'it', 'is', 'written', 'destroys', 'any', 'chance', 'of', 'this', '.', 'every', 'paragraph', 'is', 'so', 'cr', '##ammed', 'with', 'different', 'facts', 'and', 'an', '##ec', '##dote', '##s', 'that', 'nothing', 'is', 'expanded', 'on', '.', 'when', 'something', 'seems', 'remotely', 'interesting', 'it', 'doesn', \"'\", 't', 'matter', 'because', 'it', \"'\", 's', 'moved', 'on', 'from', 'in', 'the', 'blink', 'of', 'an', 'eye', '.', 'it', \"'\", 's', 'so', 'fast', 'and', 'so', 'slow', 'at', 'the', 'same', 'time', 'it', 'is', 'jar', '##ring', '.', 'it', 'somehow', 'has', '300', 'whole', 'pages', 'of', 'these', 'super', 'condensed', 'paragraph', '##s', '.', '.', '.', '.', 'what', '.', '300', 'pages', 'of', 'this', 'shit', '.', 'i', 'can', 'knock', 'out', 'hp', '##7', 'in', 'one', 'day', 'but', 'not', 'this', '.', 'i', 'need', 'to', 'answer', 'essay', 'questions', 'afterwards', '.', 'the', 'professor', 'is', 'adamant', 'about', 'making', 'sure', 'we', 'have', 'read', 'the', 'book', '.', 'he', 'has', 'said', 'that', 'if', 'he', 'can', 'tell', 'we', \"'\", 've', 'gotten', 'the', 'information', 'from', 'another', 'source', 'he', 'will', 'fail', 'the', 'paper', '.', 'all', 'of', 'these', 'factors', 'make', 'the', 'book', 'un', '##ski', '##mma', '##ble', '.', 'and', 'the', 'way', 'it', \"'\", 's', 'written', ',', 'i', 'feel', 'like', 'it', \"'\", 's', 'written', 'for', 'people', 'that', 'already', 'are', 'studying', 'napoleon', 'in', 'length', 'and', 'we', 'haven', \"'\", 't', ',', 'so', 'i', 'am', 'slightly', 'lost', 'even', '.', 'every', 'time', 'i', 'try', 'to', 'read', 'it', 'i', 'just', 'die', 'a', 'little', 'inside', '.', 'i', \"'\", 've', 'been', 'trying', 'to', 'read', 'it', 'this', 'week', 'but', 'it', 'has', 'been', 'disastrous', ',', 'i', 'am', 'only', 'like', '30', 'pages', 'in', '.', 'help', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['been', 'doing', 'this', 'lately', 'to', 'improve', 'my', 'attention']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['been', 'doing', 'this', 'lately', 'to', 'improve', 'my', 'attention']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['weekend', 'and', 'holidays', 'pro', '##cr', '##ast', '##ination', 'problem', '.']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['weekend', 'and', 'holidays', 'pro', '##cr', '##ast', '##ination', 'problem', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anybody', 'else', 'get', 'chocolate', 'pudding', 'po', '##op', ',', 'and', 'shrink', 'din', '##k', 'on', 'add', '##eral', '##l', '?']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anybody', 'else', 'get', 'chocolate', 'pudding', 'po', '##op', ',', 'and', 'shrink', 'din', '##k', 'on', 'add', '##eral', '##l', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['cotton', 'mouth', ',', 'dry', 'heaving', ',', 'and', 'starving', 'myself', 'when', 'i', 'take', 'my', 'medication', '.', '.', '.', '.', 'when', 'i', 'take', 'my', 'pre', '##sr', '##ibe', '##d', 'medication', 'i', 'get', 'in', 'the', 'zone', 'and', 'do', 'not', 'eat', 'until', 'the', 'next', 'day', '.', 'no', 'matter', 'how', 'much', 'water', 'i', 'drink', 'i', 'still', 'have', 'cotton', 'mouth', 'and', 'i', 'dry', 'he', '##ave', 'cough', 'o', '##cca', '##ssion', '##ally', '.', '.', 'these', 'symptoms', 'go', 'stat', 'until', 'i', 'fall', 'asleep', 'and', 'wake', 'up', '.', '.', 'which', 'is', 'usually', '12', 'hours', 'later', '.', '.', 'i', 'usually', 'lose', '1', '-', '3', 'pounds', 'when', 'i', \"'\", 'm', 'on', 'it', '.', '.', '.', 'i', 'was', 'wondering', 'does', 'anyone', 'else', 'feel', 'this', 'way', 'as', 'well', 'and', 'what', 're', '##med', '##ies', 'have', 'you', 'used', '?', 'i', 'try', 'my', 'hardest', 'to', 'force', 'myself', 'to', 'eat', 'but', 'it', \"'\", 's', 'just', 'too', 'dry', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 141\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['cotton', 'mouth', ',', 'dry', 'heaving', ',', 'and', 'starving', 'myself', 'when', 'i', 'take', 'my', 'medication', '.', '.', '.', '.', 'when', 'i', 'take', 'my', 'pre', '##sr', '##ibe', '##d', 'medication', 'i', 'get', 'in', 'the', 'zone', 'and', 'do', 'not', 'eat', 'until', 'the', 'next', 'day', '.', 'no', 'matter', 'how', 'much', 'water', 'i', 'drink', 'i', 'still', 'have', 'cotton', 'mouth', 'and', 'i', 'dry', 'he', '##ave', 'cough', 'o', '##cca', '##ssion', '##ally', '.', '.', 'these', 'symptoms', 'go', 'stat', 'until', 'i', 'fall', 'asleep', 'and', 'wake', 'up', '.', '.', 'which', 'is', 'usually', '12', 'hours', 'later', '.', '.', 'i', 'usually', 'lose', '1', '-', '3', 'pounds', 'when', 'i', \"'\", 'm', 'on', 'it', '.', '.', '.', 'i', 'was', 'wondering', 'does', 'anyone', 'else', 'feel', 'this', 'way', 'as', 'well', 'and', 'what', 're', '##med', '##ies', 'have', 'you', 'used', '?', 'i', 'try', 'my', 'hardest', 'to', 'force', 'myself', 'to', 'eat', 'but', 'it', \"'\", 's', 'just', 'too', 'dry', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['28', '.', '35', 'grams', 'of', 'prevention', 'is', 'worth', '.', 'o', '##oo', '##5', 'metric', 'tons', 'of', 'cure', '-', 'it', '’', 's', 'amazing', 'how', 'many', 'aspects', 'of', 'modern', 'life', 'are', '‘', 'cure', '##s', '’', 'for', 'problems', 'rather', 'than', 'prevention', '##s', '.']\n",
      "INFO:__main__:Number of tokens: 39\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['28', '.', '35', 'grams', 'of', 'prevention', 'is', 'worth', '.', 'o', '##oo', '##5', 'metric', 'tons', 'of', 'cure', '-', 'it', '’', 's', 'amazing', 'how', 'many', 'aspects', 'of', 'modern', 'life', 'are', '‘', 'cure', '##s', '’', 'for', 'problems', 'rather', 'than', 'prevention', '##s', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'great', 'american', 'add', '##eral', '##l', 'drought']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'great', 'american', 'add', '##eral', '##l', 'drought']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['constantly', 'changing', 'my', 'mind', 'and', 'my', 'interests', '.', 'can', 'anyone', 'relate', '?', 'i', 'have', 'been', 'diagnosed', 'as', 'having', 'ad', '##hd', '.', 'but', ',', 'i', 'was', 'curious', 'if', 'anyone', 'who', 'also', 'suffers', 'from', 'ad', '##hd', 'has', 'issues', 'with', 'changing', 'your', 'mind', 'regarding', 'just', 'about', 'anything', '?', 'in', 'particular', 'it', 'really', 'affects', 'my', 'interests', '.', 'i', 'may', 'be', 'very', 'interested', 'in', '\"', 'hobby', 'a', '\"', ',', 'but', 'might', 'wake', 'up', 'the', 'next', 'morning', 'and', 'be', 'completely', 'di', '##sin', '##ter', '##ested', 'and', 'decided', 'to', 'move', 'on', 'to', '\"', 'hobby', 'b', '\"', '.', 'it', 'seems', 'that', 'i', 'can', \"'\", 't', 'ever', 'get', 'good', 'at', 'anything', 'because', 'for', 'whatever', 'reason', 'as', 'soon', 'as', 'i', 'make', 'a', 'little', 'progress', 'i', 'seem', 'to', 'do', 'a', '180', 'and', 'lose', 'all', 'interest', '.', 'it', \"'\", 's', 'frustrating', 'to', 'no', 'end', '.']\n",
      "INFO:__main__:Number of tokens: 132\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['constantly', 'changing', 'my', 'mind', 'and', 'my', 'interests', '.', 'can', 'anyone', 'relate', '?', 'i', 'have', 'been', 'diagnosed', 'as', 'having', 'ad', '##hd', '.', 'but', ',', 'i', 'was', 'curious', 'if', 'anyone', 'who', 'also', 'suffers', 'from', 'ad', '##hd', 'has', 'issues', 'with', 'changing', 'your', 'mind', 'regarding', 'just', 'about', 'anything', '?', 'in', 'particular', 'it', 'really', 'affects', 'my', 'interests', '.', 'i', 'may', 'be', 'very', 'interested', 'in', '\"', 'hobby', 'a', '\"', ',', 'but', 'might', 'wake', 'up', 'the', 'next', 'morning', 'and', 'be', 'completely', 'di', '##sin', '##ter', '##ested', 'and', 'decided', 'to', 'move', 'on', 'to', '\"', 'hobby', 'b', '\"', '.', 'it', 'seems', 'that', 'i', 'can', \"'\", 't', 'ever', 'get', 'good', 'at', 'anything', 'because', 'for', 'whatever', 'reason', 'as', 'soon', 'as', 'i', 'make', 'a', 'little', 'progress', 'i', 'seem', 'to', 'do', 'a', '180', 'and', 'lose', 'all', 'interest', '.', 'it', \"'\", 's', 'frustrating', 'to', 'no', 'end', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['shitty', 'day', ',', 'need', 'input', ':', '(', '.', 'kind', 'of', 'a', 'bad', 'day', 'today', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', '13', ',', 'however', ',', 'my', 'parents', 'were', 'the', 'kind', 'of', 'people', 'who', 'believed', '\"', 'you', 'don', \"'\", 't', 'need', 'add', '##eral', '##l', ',', 'you', 'need', 'a', 'belt', '.', '\"', 'i', 'went', 'un', '##tre', '##ated', 'for', '8', 'years', '.', 'during', 'the', 'un', '##tre', '##ated', 'years', 'i', 'did', 'horribly', 'in', 'school', ';', 'i', 'was', 'never', 'motivated', 'to', 'do', 'anything', 'expect', 'focus', 'on', 'video', 'games', '.', 'i', 'dropped', 'out', 'in', '10th', 'grade', 'and', 'was', 'in', 'and', 'out', 'of', 'juvenile', 'for', 'stupid', 'shit', '.', 'i', 'was', 'originally', 'put', 'on', 'probation', 'for', 'graffiti', 'at', '15', ',', 'and', 'continuously', 'broke', 'my', 'probation', '.', 'not', 'going', 'to', 'school', ',', 'being', 'a', 'smart', 'ass', '.', 'i', 'never', 'accepted', 'authority', '.', 'when', 'i', 'turned', '18', 'i', 'did', 'bullshit', 'labor', 'jobs', '.', 'i', 'was', 'a', 'electrical', 'apprentice', 'at', 'one', 'time', ',', 'but', 'after', 'a', 'while', 'i', 'realized', 'i', 'didn', \"'\", 't', 'want', 'to', 'come', 'home', 'drenched', 'in', 'sweat', 'every', 'night', '.', 'i', 'enrolled', 'in', 'ge', '##d', 'classes', 'at', 'the', 'community', 'college', ',', 'it', 'was', 'pretty', 'difficult', 'and', 'a', 'alien', 'concept', 'to', 'me', '―', 'i', 'have', 'never', 'applied', 'myself', 'to', 'school', 'work', ',', 'and', 'to', 'do', 'it', 'was', 'strange', '.', 'i', 'got', 'my', 'ge', '##d', 'when', 'i', 'was', '19', 'and', 'enrolled', 'in', 'college', '.', 'the', 'first', 'two', 'semester', '##s', 'of', 'college', 'were', 'tough', '.', 'i', 'took', 'a', 'placement', 'test', ',', 'and', 'was', 'placed', 'in', 're', '##media', '##l', 'math', ',', 'the', 'lowest', 'level', '.', 'i', 'pulled', 'c', \"'\", 's', 'the', 'first', 'two', 'semester', '##s', ',', 'and', 'dropped', 'math', ',', 'i', 'couldn', \"'\", 't', 'do', 'it', '.', 'after', 'two', 'semester', '##s', 'i', 'decided', 'i', 'needed', 'help', '.', 'i', 'was', 'depressed', ',', 'i', 'was', 'un', '##mot', '##ivated', ',', 'i', 'was', 'an', 'asshole', 'to', 'everyone', 'around', 'me', ',', 'and', 'i', 'couldn', \"'\", 't', 'hold', 'a', 'job', ';', 'i', 'would', 'get', 'one', ',', 'be', 'into', 'it', 'at', 'first', ',', 'then', 'slowly', 'over', 'time', 'become', 'bored', 'and', 'quit', '.', 'the', 'doctor', 'placed', 'we', 'on', 'well', '##bu', '##tri', '##n', 'at', 'first', ',', 'and', 'it', 'slightly', 'helped', '.', 'it', 'gave', 'me', 'a', 'slight', 'warm', 'fuzzy', 'feeling', ',', 'and', 'made', 'me', 'not', 'dread', 'studying', 'so', 'much', ',', 'however', ',', 'i', 'was', 'still', 'having', 'major', 'trouble', 'just', 'comprehend', '##ing', 'school', '.', 'during', 'lectures', 'my', 'mind', 'would', 'think', 'about', 'anything', 'and', 'everything', 'expect', 'the', 'lecture', '.', 'the', 'well', '##bu', '##tri', '##n', 'wouldn', \"'\", 't', 'let', 'me', 'get', 'my', 'dick', 'hard', 'either', ',', 'it', 'was', 'back', 'to', 'the', 'doctor', '.', 'the', 'doctor', 'put', 'me', 'on', 'add', '##eral', '##l', ',', 'and', 'my', 'life', 'changed', 'completely', '.', 'the', 'semester', 'was', 'almost', 'over', ',', 'but', 'i', 'managed', 'to', 'pull', 'up', '2', 'of', 'my', 'grades', 'to', 'b', \"'\", 's', '.', 'the', 'next', 'semester', 'was', '2', 'a', \"'\", 's', ',', '2', 'b', \"'\", 's', '.', 'add', '##eral', '##l', 'gave', 'me', 'the', 'motivation', 'to', 'focus', ',', 'to', 'sit', 'down', 'and', 'study', '.', 'i', 'was', 'nice', '##r', ',', 'and', 'overall', 'i', 'was', 'a', 'better', 'person', '.', 'my', 'friends', 'and', 'family', 'started', 'to', 'compliment', 'the', 'change', '.', 'this', 'current', 'semester', 'was', 'going', 'great', ',', 'i', 'have', 'been', 'bust', '##ing', 'my', 'ass', ',', 'and', 'was', 'expecting', 'all', 'a', \"'\", 's', ',', 'this', 'would', 'be', 'a', 'first', 'i', 'ever', 'pulled', 'all', 'a', \"'\", 's', '.', 'the', 'most', 'amazing', 'improvement', 'was', 'the', 'math', '.', 'holy', 'fuck', 'i', 'realized', 'i', 'am', 'good', 'at', 'math', '!', 'this', 'semester', 'i', 'have', 'been', 'using', 'the', 'add', '##eral', '##l', 'everyday', ',', 'and', 'not', 'taking', 'a', 'break', ',', 'as', 'said', 'before', 'i', 'have', 'been', 'bust', '##ing', 'my', 'ass', '.', 'i', 'had', 'a', 'math', 'test', 'today', '―', 'all', 'week', 'i', 'had', 'studied', 'and', 'nailed', 'it', 'down', '.', 'thursday', 'i', 'decided', 'after', 'months', 'of', 'use', ',', 'i', 'should', 'take', 'a', 'break', '.', 'i', 'took', 'a', 'break', 'thursday', ',', 'friday', ',', 'saturday', ',', 'sunday', ',', 'and', 'decided', 'not', 'to', 'take', 'the', 'add', '##eral', '##l', 'today', ',', 'monday', ',', 'the', 'day', 'of', 'the', 'test', '.', 'i', 'stumbled', 'and', 'fumbled', 'and', 'was', 'unable', 'to', 'do', 'simple', 'arithmetic', ',', 'thankfully', 'i', 'had', 'my', 'graph', '##ing', 'cal', '##cula', '##tor', 'and', 'knew', 'how', 'to', 'check', 'my', 'work', ',', 'i', 'did', 'the', 'math', 'problems', 'over', 'and', 'over', 'until', 'i', 'got', 'the', 'answer', 'i', 'should', 'of', '.', 'i', 'turned', 'my', 'test', 'in', 'and', 'walked', 'out', '.', 'my', 'teacher', 'email', '##ed', 'me', '30', 'minutes', 'later', 'to', 'my', 'ed', '##u', 'address', ';', 'i', 'had', 'skipped', 'a', 'entire', 'page', ',', 'and', 'had', 'put', 'several', 'answers', 'in', 'the', 'wrong', 'place', '.', 'she', 'wanted', 'me', 'to', 'come', 'back', 'asa', '##p', '.', 'this', 'was', 'at', '9', '##am', ',', 'and', 'i', 'didn', \"'\", 't', 'read', 'the', 'email', 'until', '8', '##pm', '.', 'lately', 'i', 'have', 'been', 'getting', 'more', 'and', 'more', 'bum', '##med', '.', 'i', 'keep', 'thinking', '\"', 'will', 'i', 'ever', 'be', 'able', 'to', 'function', 'like', 'a', 'normal', 'adult', 'without', 'taking', 'st', '##im', '##ula', '##nts', '?', '\"', 'i', 'want', 'to', 'become', 'a', 'engineering', ',', 'and', 'i', \"'\", 'm', '100', '%', 'sure', 'i', 'can', 'do', 'it', 'if', 'i', 'bust', 'my', 'ass', '.', 'this', 'new', 'goal', 'in', 'my', 'life', 'is', 'very', 'reach', '##able', 'and', 'it', 'mo', '##tiv', '##ates', 'me', 'to', 'do', 'better', '.', 'however', ',', 'i', 'know', 'that', 'without', 'add', '##eral', '##l', ',', 'i', 'will', 'never', 'be', 'able', 'to', 'do', 'the', 'intense', 'school', 'work', ',', 'and', 'this', 'is', 'a', 'bitter', 'bullet', 'to', 'bite', '.', 'sorry', 'for', 'the', 'long', 'ran', '##t', 'guys', '.', 'i', \"'\", 'm', 'pretty', 'depressed', 'about', 'the', 'math', 'class', 'thing', ',', 'i', 'have', 'busted', 'my', 'ass', 'and', 'had', 'a', '98', 'average', '.', 'i', \"'\", 'm', 'not', 'sure', 'what', 'my', 'math', 'teacher', 'will', 'do', ',', 'if', 'she', 'awards', 'me', 'a', 'zero', 'it', 'will', 'bring', 'my', 'average', 'down', 'to', 'a', '78', ';', 'a', 'c', '.']\n",
      "INFO:__main__:Number of tokens: 938\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['shitty', 'day', ',', 'need', 'input', ':', '(', '.', 'kind', 'of', 'a', 'bad', 'day', 'today', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', '13', ',', 'however', ',', 'my', 'parents', 'were', 'the', 'kind', 'of', 'people', 'who', 'believed', '\"', 'you', 'don', \"'\", 't', 'need', 'add', '##eral', '##l', ',', 'you', 'need', 'a', 'belt', '.', '\"', 'i', 'went', 'un', '##tre', '##ated', 'for', '8', 'years', '.', 'during', 'the', 'un', '##tre', '##ated', 'years', 'i', 'did', 'horribly', 'in', 'school', ';', 'i', 'was', 'never', 'motivated', 'to', 'do', 'anything', 'expect', 'focus', 'on', 'video', 'games', '.', 'i', 'dropped', 'out', 'in', '10th', 'grade', 'and', 'was', 'in', 'and', 'out', 'of', 'juvenile', 'for', 'stupid', 'shit', '.', 'i', 'was', 'originally', 'put', 'on', 'probation', 'for', 'graffiti', 'at', '15', ',', 'and', 'continuously', 'broke', 'my', 'probation', '.', 'not', 'going', 'to', 'school', ',', 'being', 'a', 'smart', 'ass', '.', 'i', 'never', 'accepted', 'authority', '.', 'when', 'i', 'turned', '18', 'i', 'did', 'bullshit', 'labor', 'jobs', '.', 'i', 'was', 'a', 'electrical', 'apprentice', 'at', 'one', 'time', ',', 'but', 'after', 'a', 'while', 'i', 'realized', 'i', 'didn', \"'\", 't', 'want', 'to', 'come', 'home', 'drenched', 'in', 'sweat', 'every', 'night', '.', 'i', 'enrolled', 'in', 'ge', '##d', 'classes', 'at', 'the', 'community', 'college', ',', 'it', 'was', 'pretty', 'difficult', 'and', 'a', 'alien', 'concept', 'to', 'me', '―', 'i', 'have', 'never', 'applied', 'myself', 'to', 'school', 'work', ',', 'and', 'to', 'do', 'it', 'was', 'strange', '.', 'i', 'got', 'my', 'ge', '##d', 'when', 'i', 'was', '19', 'and', 'enrolled', 'in', 'college', '.', 'the', 'first', 'two', 'semester', '##s', 'of', 'college', 'were', 'tough', '.', 'i', 'took', 'a', 'placement', 'test', ',', 'and', 'was', 'placed', 'in', 're', '##media', '##l', 'math', ',', 'the', 'lowest', 'level', '.', 'i', 'pulled', 'c', \"'\", 's', 'the', 'first', 'two', 'semester', '##s', ',', 'and', 'dropped', 'math', ',', 'i', 'couldn', \"'\", 't', 'do', 'it', '.', 'after', 'two', 'semester', '##s', 'i', 'decided', 'i', 'needed', 'help', '.', 'i', 'was', 'depressed', ',', 'i', 'was', 'un', '##mot', '##ivated', ',', 'i', 'was', 'an', 'asshole', 'to', 'everyone', 'around', 'me', ',', 'and', 'i', 'couldn', \"'\", 't', 'hold', 'a', 'job', ';', 'i', 'would', 'get', 'one', ',', 'be', 'into', 'it', 'at', 'first', ',', 'then', 'slowly', 'over', 'time', 'become', 'bored', 'and', 'quit', '.', 'the', 'doctor', 'placed', 'we', 'on', 'well', '##bu', '##tri', '##n', 'at', 'first', ',', 'and', 'it', 'slightly', 'helped', '.', 'it', 'gave', 'me', 'a', 'slight', 'warm', 'fuzzy', 'feeling', ',', 'and', 'made', 'me', 'not', 'dread', 'studying', 'so', 'much', ',', 'however', ',', 'i', 'was', 'still', 'having', 'major', 'trouble', 'just', 'comprehend', '##ing', 'school', '.', 'during', 'lectures', 'my', 'mind', 'would', 'think', 'about', 'anything', 'and', 'everything', 'expect', 'the', 'lecture', '.', 'the', 'well', '##bu', '##tri', '##n', 'wouldn', \"'\", 't', 'let', 'me', 'get', 'my', 'dick', 'hard', 'either', ',', 'it', 'was', 'back', 'to', 'the', 'doctor', '.', 'the', 'doctor', 'put', 'me', 'on', 'add', '##eral', '##l', ',', 'and', 'my', 'life', 'changed', 'completely', '.', 'the', 'semester', 'was', 'almost', 'over', ',', 'but', 'i', 'managed', 'to', 'pull', 'up', '2', 'of', 'my', 'grades', 'to', 'b', \"'\", 's', '.', 'the', 'next', 'semester', 'was', '2', 'a', \"'\", 's', ',', '2', 'b', \"'\", 's', '.', 'add', '##eral', '##l', 'gave', 'me', 'the', 'motivation', 'to', 'focus', ',', 'to', 'sit', 'down', 'and', 'study', '.', 'i', 'was', 'nice', '##r', ',', 'and', 'overall', 'i', 'was', 'a', 'better', 'person', '.', 'my', 'friends', 'and', 'family', 'started', 'to', 'compliment', 'the', 'change', '.', 'this', 'current'], ['semester', 'was', 'going', 'great', ',', 'i', 'have', 'been', 'bust', '##ing', 'my', 'ass', ',', 'and', 'was', 'expecting', 'all', 'a', \"'\", 's', ',', 'this', 'would', 'be', 'a', 'first', 'i', 'ever', 'pulled', 'all', 'a', \"'\", 's', '.', 'the', 'most', 'amazing', 'improvement', 'was', 'the', 'math', '.', 'holy', 'fuck', 'i', 'realized', 'i', 'am', 'good', 'at', 'math', '!', 'this', 'semester', 'i', 'have', 'been', 'using', 'the', 'add', '##eral', '##l', 'everyday', ',', 'and', 'not', 'taking', 'a', 'break', ',', 'as', 'said', 'before', 'i', 'have', 'been', 'bust', '##ing', 'my', 'ass', '.', 'i', 'had', 'a', 'math', 'test', 'today', '―', 'all', 'week', 'i', 'had', 'studied', 'and', 'nailed', 'it', 'down', '.', 'thursday', 'i', 'decided', 'after', 'months', 'of', 'use', ',', 'i', 'should', 'take', 'a', 'break', '.', 'i', 'took', 'a', 'break', 'thursday', ',', 'friday', ',', 'saturday', ',', 'sunday', ',', 'and', 'decided', 'not', 'to', 'take', 'the', 'add', '##eral', '##l', 'today', ',', 'monday', ',', 'the', 'day', 'of', 'the', 'test', '.', 'i', 'stumbled', 'and', 'fumbled', 'and', 'was', 'unable', 'to', 'do', 'simple', 'arithmetic', ',', 'thankfully', 'i', 'had', 'my', 'graph', '##ing', 'cal', '##cula', '##tor', 'and', 'knew', 'how', 'to', 'check', 'my', 'work', ',', 'i', 'did', 'the', 'math', 'problems', 'over', 'and', 'over', 'until', 'i', 'got', 'the', 'answer', 'i', 'should', 'of', '.', 'i', 'turned', 'my', 'test', 'in', 'and', 'walked', 'out', '.', 'my', 'teacher', 'email', '##ed', 'me', '30', 'minutes', 'later', 'to', 'my', 'ed', '##u', 'address', ';', 'i', 'had', 'skipped', 'a', 'entire', 'page', ',', 'and', 'had', 'put', 'several', 'answers', 'in', 'the', 'wrong', 'place', '.', 'she', 'wanted', 'me', 'to', 'come', 'back', 'asa', '##p', '.', 'this', 'was', 'at', '9', '##am', ',', 'and', 'i', 'didn', \"'\", 't', 'read', 'the', 'email', 'until', '8', '##pm', '.', 'lately', 'i', 'have', 'been', 'getting', 'more', 'and', 'more', 'bum', '##med', '.', 'i', 'keep', 'thinking', '\"', 'will', 'i', 'ever', 'be', 'able', 'to', 'function', 'like', 'a', 'normal', 'adult', 'without', 'taking', 'st', '##im', '##ula', '##nts', '?', '\"', 'i', 'want', 'to', 'become', 'a', 'engineering', ',', 'and', 'i', \"'\", 'm', '100', '%', 'sure', 'i', 'can', 'do', 'it', 'if', 'i', 'bust', 'my', 'ass', '.', 'this', 'new', 'goal', 'in', 'my', 'life', 'is', 'very', 'reach', '##able', 'and', 'it', 'mo', '##tiv', '##ates', 'me', 'to', 'do', 'better', '.', 'however', ',', 'i', 'know', 'that', 'without', 'add', '##eral', '##l', ',', 'i', 'will', 'never', 'be', 'able', 'to', 'do', 'the', 'intense', 'school', 'work', ',', 'and', 'this', 'is', 'a', 'bitter', 'bullet', 'to', 'bite', '.', 'sorry', 'for', 'the', 'long', 'ran', '##t', 'guys', '.', 'i', \"'\", 'm', 'pretty', 'depressed', 'about', 'the', 'math', 'class', 'thing', ',', 'i', 'have', 'busted', 'my', 'ass', 'and', 'had', 'a', '98', 'average', '.', 'i', \"'\", 'm', 'not', 'sure', 'what', 'my', 'math', 'teacher', 'will', 'do', ',', 'if', 'she', 'awards', 'me', 'a', 'zero', 'it', 'will', 'bring', 'my', 'average', 'down', 'to', 'a', '78', ';', 'a', 'c', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'ina', '##de', '##qua', '##cy', 'of', 'ad', '##hd', 'research', 'methodology', '-', 'dr', 'j', 'has', 'major', 'beef', 'with', 'science']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'ina', '##de', '##qua', '##cy', 'of', 'ad', '##hd', 'research', 'methodology', '-', 'dr', 'j', 'has', 'major', 'beef', 'with', 'science']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['struggling', 'with', 'current', 'prescription', '(', '2', 'x', '20', '##mg', 'add', '##eral', '##l', 'x', '##r', ')', '.', 'ineffective', '##ness', ',', 'depression', ',', 'etc', '.', 'despite', 'watching', 'diet', 'and', 'extremely', 'heavy', 'use', 'of', 'com', '##pen', '##sat', '##ory', 'strategies', '.', 'can', 'any', 'longtime', 'x', '##r', 'users', 'offer', 'advice', '(', 'practical', ',', 'not', 'medical', ')', 'as', 'to', 'where', 'to', 'go', 'from', 'here', '?', 'i', \"'\", 'm', 'currently', 'taking', 'both', '(', 'with', 'water', ')', 'as', 'soon', 'as', 'i', 'wake', 'up', ',', 'and', 'wait', 'at', 'least', 'an', 'hour', 'to', 'eat', 'a', 'high', '-', 'ph', 'breakfast', '.', 'used', 'to', 'last', '8', '-', '10', 'hours', ',', 'then', 'only', '4', '-', '6', ',', 'and', 'now', 'it', \"'\", 's', 'to', 'the', 'point', 'where', 'i', 'don', \"'\", 't', 'notice', 'any', 'difference', 'at', 'all', '.', 'i', 'can', 'no', 'longer', 'focus', 'for', 'more', 'than', '5', '-', '10', 'minutes', 'and', 'the', 'thought', 'of', 'doing', 'even', 'the', 'smallest', 'thing', 'seems', 'incredibly', 'overwhelming', ',', 'stress', '##ful', ',', 'and', 'anxiety', '-', 'pro', '##voking', '.', 'being', 'fully', 'aware', 'of', 'the', 'contrast', 'between', 'how', 'i', 'feel', 'now', 'and', 'how', 'i', 'felt', 'for', 'those', 'couple', 'months', 'when', 'the', 'medication', 'was', 'working', 'has', 'only', 'added', 'to', 'the', 'stress', ',', 'and', 'now', 'i', \"'\", 'm', 'starting', 'to', 'feel', 'the', 'rapid', 'onset', 'of', 'depression', ',', 'which', 'i', 'used', 'to', 'get', 'for', '1', '-', '2', 'weeks', 'at', 'a', 'time', 'every', '1', '-', '2', 'months', ',', 'but', 'haven', \"'\", 't', 'had', 'at', 'all', 'for', 'nearly', '3', 'months', '.', 'i', 'have', 'a', 'master', \"'\", 's', 'in', 'school', 'psychology', 'and', 'am', 'about', 'halfway', 'to', 'my', 'phd', 'so', 'i', 'am', 'well', '-', 'verse', '##d', 'in', 'all', 'of', 'the', 'techniques', 'that', 'can', 'be', 'used', 'to', 'manage', 'the', 'symptoms', '.', 'i', 'have', 'dozens', 'of', 'weekly', 'alarms', ',', 'i', 'carry', 'a', 'note', '##pad', ',', 'i', 'break', 'down', 'large', 'tasks', 'into', 'small', 'chunks', ',', 'i', 'schedule', 'them', 'out', '.', '.', '.', '.', 'it', \"'\", 's', 'all', 'falling', 'apart', 'for', 'me', 'anyway', '.', 'i', 'have', 'an', 'appointment', 'with', 'my', 'psychiatrist', 'in', 'a', 'couple', 'weeks', 'but', 'he', \"'\", 's', 'already', 'stated', 'that', 'i', 'am', 'at', 'the', '\"', 'maximum', '\"', 'dos', '##age', 'so', 'i', 'am', 'not', 'optimistic', 'that', 'i', 'will', 'be', 'able', 'to', 'get', 'my', 'prescription', 'increased', '.', 'if', 'anyone', 'here', 'has', 'gotten', 'through', 'a', 'similar', 'situation', ',', 'i', 'would', 'greatly', 'appreciate', 'some', 'advice', 'or', 'even', 'just', 'a', 'sympathetic', 'ear', '.']\n",
      "INFO:__main__:Number of tokens: 376\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['struggling', 'with', 'current', 'prescription', '(', '2', 'x', '20', '##mg', 'add', '##eral', '##l', 'x', '##r', ')', '.', 'ineffective', '##ness', ',', 'depression', ',', 'etc', '.', 'despite', 'watching', 'diet', 'and', 'extremely', 'heavy', 'use', 'of', 'com', '##pen', '##sat', '##ory', 'strategies', '.', 'can', 'any', 'longtime', 'x', '##r', 'users', 'offer', 'advice', '(', 'practical', ',', 'not', 'medical', ')', 'as', 'to', 'where', 'to', 'go', 'from', 'here', '?', 'i', \"'\", 'm', 'currently', 'taking', 'both', '(', 'with', 'water', ')', 'as', 'soon', 'as', 'i', 'wake', 'up', ',', 'and', 'wait', 'at', 'least', 'an', 'hour', 'to', 'eat', 'a', 'high', '-', 'ph', 'breakfast', '.', 'used', 'to', 'last', '8', '-', '10', 'hours', ',', 'then', 'only', '4', '-', '6', ',', 'and', 'now', 'it', \"'\", 's', 'to', 'the', 'point', 'where', 'i', 'don', \"'\", 't', 'notice', 'any', 'difference', 'at', 'all', '.', 'i', 'can', 'no', 'longer', 'focus', 'for', 'more', 'than', '5', '-', '10', 'minutes', 'and', 'the', 'thought', 'of', 'doing', 'even', 'the', 'smallest', 'thing', 'seems', 'incredibly', 'overwhelming', ',', 'stress', '##ful', ',', 'and', 'anxiety', '-', 'pro', '##voking', '.', 'being', 'fully', 'aware', 'of', 'the', 'contrast', 'between', 'how', 'i', 'feel', 'now', 'and', 'how', 'i', 'felt', 'for', 'those', 'couple', 'months', 'when', 'the', 'medication', 'was', 'working', 'has', 'only', 'added', 'to', 'the', 'stress', ',', 'and', 'now', 'i', \"'\", 'm', 'starting', 'to', 'feel', 'the', 'rapid', 'onset', 'of', 'depression', ',', 'which', 'i', 'used', 'to', 'get', 'for', '1', '-', '2', 'weeks', 'at', 'a', 'time', 'every', '1', '-', '2', 'months', ',', 'but', 'haven', \"'\", 't', 'had', 'at', 'all', 'for', 'nearly', '3', 'months', '.', 'i', 'have', 'a', 'master', \"'\", 's', 'in', 'school', 'psychology', 'and', 'am', 'about', 'halfway', 'to', 'my', 'phd', 'so', 'i', 'am', 'well', '-', 'verse', '##d', 'in', 'all', 'of', 'the', 'techniques', 'that', 'can', 'be', 'used', 'to', 'manage', 'the', 'symptoms', '.', 'i', 'have', 'dozens', 'of', 'weekly', 'alarms', ',', 'i', 'carry', 'a', 'note', '##pad', ',', 'i', 'break', 'down', 'large', 'tasks', 'into', 'small', 'chunks', ',', 'i', 'schedule', 'them', 'out', '.', '.', '.', '.', 'it', \"'\", 's', 'all', 'falling', 'apart', 'for', 'me', 'anyway', '.', 'i', 'have', 'an', 'appointment', 'with', 'my', 'psychiatrist', 'in', 'a', 'couple', 'weeks', 'but', 'he', \"'\", 's', 'already', 'stated', 'that', 'i', 'am', 'at', 'the', '\"', 'maximum', '\"', 'dos', '##age', 'so', 'i', 'am', 'not', 'optimistic', 'that', 'i', 'will', 'be', 'able', 'to', 'get', 'my', 'prescription', 'increased', '.', 'if', 'anyone', 'here', 'has', 'gotten', 'through', 'a', 'similar', 'situation', ',', 'i', 'would', 'greatly', 'appreciate', 'some', 'advice', 'or', 'even', 'just', 'a', 'sympathetic', 'ear', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'you', 'tell', 'people', 'that', 'try', 'to', 'say', 'this', 'about', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'you', 'tell', 'people', 'that', 'try', 'to', 'say', 'this', 'about', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'so', 'frustrated', 'ran', '##t', '/', 'this', 'is', 'fucking', 'frustrating', '.', 'i', 'want', 'to', 'do', 'stuff', 'with', 'my', 'life', '.', 'intellectual', '##ly', 'challenging', 'stuff', '.', 'forget', 'want', '.', 'have', 'to', 'do', '.', 'because', 'if', 'i', \"'\", 'm', 'not', 'intellectual', '##ly', 'challenged', ',', 'i', 'can', \"'\", 't', 'pay', 'attention', '.', 'can', \"'\", 't', 'pay', 'attention', ',', 'lower', 'grades', ',', 'put', 'on', 'the', 'boring', 'class', 'tracks', ',', 'even', 'less', 'attention', ',', 'even', 'lower', 'grades', '.', 'leading', 'to', 'a', 'shitty', 'job', '.', 'leading', 'to', 'no', 'focus', ',', 'leading', 'to', 'no', 'passion', '.', '.', '.', 'and', 'i', 'can', \"'\", 't', 'work', 'without', 'being', 'passionate', ',', 'my', 'mind', 'just', 'won', \"'\", 't', 'do', 'it', '!', 'i', 'work', 'three', 'as', 'hard', 'as', 'everyone', 'i', 'know', '.', 'spend', 'almost', 'all', 'waking', 'hours', 'trying', 'to', 'study', ',', 'trying', 'to', 'focus', '.', 'but', 'no', '.', 'when', 'i', \"'\", 'm', 'trying', 'to', 'do', 'calculus', 'my', 'brain', 'will', 'start', 'composing', 'music', ',', 'if', 'i', 'try', 'to', 'do', 'chemistry', 'i', 'start', 'doing', 'math', 'proof', '##s', 'in', 'my', 'head', ',', 'if', 'i', 'write', 'an', 'essay', 'there', 'is', 'an', 'entirely', 'different', 'essay', 'from', 'the', 'one', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'writing', 'running', 'in', 'my', 'head', '.', '.', 'when', 'i', 'got', 'diagnosed', ',', 'i', 'got', 'my', 'iq', 'tested', '.', 'psychologist', 'was', 'beaming', 'at', 'me', ',', 'both', 'of', 'us', 'were', 'grinning', 'from', 'ear', 'to', 'ear', ',', 'him', 'telling', 'me', 'that', 'i', 'am', 'testing', 'in', 'the', 'top', '1', '%', 'of', 'iq', '##s', ',', 'that', 'he', 'hasn', \"'\", 't', 'ever', 'seen', 'anyone', 'solve', 'those', 'block', 'puzzles', 'so', 'fast', ',', 'that', 'i', 'am', 'definitely', 'over', 'thinking', 'everything', ',', 'there', 'is', 'nothing', 'wrong', 'with', 'me', '.', 'and', 'then', 'he', 'does', 'the', 'to', '##va', ',', 'and', 'the', 'working', 'memory', 'tests', ',', 'and', 'my', 'smile', 'is', 'gone', '.', 'because', 'it', 'doesn', \"'\", 't', 'matter', 'how', 'smart', 'i', 'am', 'or', 'how', 'hard', 'i', 'work', '.', 'because', 'my', 'to', '##va', 'score', 'is', 'so', 'bad', 'that', 'the', 'results', 'can', \"'\", 't', 'even', 'be', 'read', '.', 'i', 'know', 'what', 'every', 'single', 'one', 'of', 'these', 'tests', 'mean', ',', 'and', 'knew', 'exactly', 'what', 'the', 'dia', '##gno', '##ses', 'would', 'be', 'before', 'i', 'stepped', 'in', ',', 'but', 'i', 'never', 'imagined', 'pressing', 'that', 'damn', 'button', 'when', 'the', 'shape', 'appeared', 'would', 'be', 'so', 'difficult', '.', 'all', 'the', 'intelligence', 'and', 'hard', 'work', 'in', 'the', 'world', ',', 'and', 'my', 'achievements', 'are', '.', '.', '.', 'average', '.', 'always', ',', 'just', 'average', '.', 'if', 'only', 'he', 'would', 'apply', 'himself', '.', 'if', 'only', 'he', 'weren', \"'\", 't', 'so', 'lazy', '-', 'that', 'one', 'just', 'makes', 'my', 'head', 'explode', '.', 'so', 'the', 'shrink', 'says', 'the', 'words', 'which', 'i', \"'\", 've', 'been', 'thinking', 'all', 'my', 'life', '.', 'you', \"'\", 've', 'got', 'so', 'much', 'potential', '.', 'so', 'many', 'mental', 'resources', ',', 'that', 'you', 'can', \"'\", 't', 'apply', 'because', 'you', 'can', \"'\", 't', 'focus', '.', 'take', 'the', 'medication', ',', 'you', 'will', 'be', 'able', 'to', 'un', '##lea', '##sh', 'so', 'much', 'but', 'i', 'take', 'that', 'medication', ',', 'and', 'all', 'those', 'thoughts', 'that', 'are', 'rushing', 'through', 'my', 'head', ',', 'stopping', 'me', 'from', 'focusing', ',', 'turn', 'off', '.', 'que', 'amazing', 'focus', '.', '.', '.', 'at', 'first', '.', 'but', 'suddenly', ',', 'all', 'my', 'special', 'abilities', 'that', 'separated', 'me', 'from', 'the', 'crowe', '##d', 'start', 'to', 'disappear', '.', 'i', 'can', \"'\", 't', 'tell', 'children', 'funny', 'stories', 'made', 'up', 'on', 'the', 'spot', 'without', 'struggling', '.', 'i', 'don', \"'\", 't', 'automatically', 'notice', 'the', 'expressions', 'in', 'the', 'eyes', 'of', 'people', 'i', 'love', ',', 'read', 'their', 'emotions', ',', 'unless', 'i', 'am', 'specifically', 'trying', 'to', '.', 'i', 'don', \"'\", 't', 'realize', 'i', \"'\", 'm', 'losing', 'my', 'temper', 'until', 'i', \"'\", 've', 'lost', 'it', '.', 'and', 'i', 'realized', ',', 'putting', 'those', 'rushing', 'thoughts', 'to', 'good', 'use', 'was', 'the', 'whole', 'reason', 'i', 'wanted', 'to', 'succeed', 'academic', '##ally', 'in', 'the', 'first', 'place', '.', 'can', \"'\", 't', 'have', 'it', 'both', 'ways', ',', 'right', '?', 'whatever', ',', 'just', 'keep', 'pushing', 'on', '.', '.', '.', 'one', 'day', ',', 'i', \"'\", 'll', 'be', 'in', 'a', 'position', 'when', 'all', 'the', 'thoughts', 'in', 'my', 'head', 'will', 'be', 'my', 'strength', ',', 'my', 'advantage', '.', 'one', 'day', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 658\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'so', 'frustrated', 'ran', '##t', '/', 'this', 'is', 'fucking', 'frustrating', '.', 'i', 'want', 'to', 'do', 'stuff', 'with', 'my', 'life', '.', 'intellectual', '##ly', 'challenging', 'stuff', '.', 'forget', 'want', '.', 'have', 'to', 'do', '.', 'because', 'if', 'i', \"'\", 'm', 'not', 'intellectual', '##ly', 'challenged', ',', 'i', 'can', \"'\", 't', 'pay', 'attention', '.', 'can', \"'\", 't', 'pay', 'attention', ',', 'lower', 'grades', ',', 'put', 'on', 'the', 'boring', 'class', 'tracks', ',', 'even', 'less', 'attention', ',', 'even', 'lower', 'grades', '.', 'leading', 'to', 'a', 'shitty', 'job', '.', 'leading', 'to', 'no', 'focus', ',', 'leading', 'to', 'no', 'passion', '.', '.', '.', 'and', 'i', 'can', \"'\", 't', 'work', 'without', 'being', 'passionate', ',', 'my', 'mind', 'just', 'won', \"'\", 't', 'do', 'it', '!', 'i', 'work', 'three', 'as', 'hard', 'as', 'everyone', 'i', 'know', '.', 'spend', 'almost', 'all', 'waking', 'hours', 'trying', 'to', 'study', ',', 'trying', 'to', 'focus', '.', 'but', 'no', '.', 'when', 'i', \"'\", 'm', 'trying', 'to', 'do', 'calculus', 'my', 'brain', 'will', 'start', 'composing', 'music', ',', 'if', 'i', 'try', 'to', 'do', 'chemistry', 'i', 'start', 'doing', 'math', 'proof', '##s', 'in', 'my', 'head', ',', 'if', 'i', 'write', 'an', 'essay', 'there', 'is', 'an', 'entirely', 'different', 'essay', 'from', 'the', 'one', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'writing', 'running', 'in', 'my', 'head', '.', '.', 'when', 'i', 'got', 'diagnosed', ',', 'i', 'got', 'my', 'iq', 'tested', '.', 'psychologist', 'was', 'beaming', 'at', 'me', ',', 'both', 'of', 'us', 'were', 'grinning', 'from', 'ear', 'to', 'ear', ',', 'him', 'telling', 'me', 'that', 'i', 'am', 'testing', 'in', 'the', 'top', '1', '%', 'of', 'iq', '##s', ',', 'that', 'he', 'hasn', \"'\", 't', 'ever', 'seen', 'anyone', 'solve', 'those', 'block', 'puzzles', 'so', 'fast', ',', 'that', 'i', 'am', 'definitely', 'over', 'thinking', 'everything', ',', 'there', 'is', 'nothing', 'wrong', 'with', 'me', '.', 'and', 'then', 'he', 'does', 'the', 'to', '##va', ',', 'and', 'the', 'working', 'memory', 'tests', ',', 'and', 'my', 'smile', 'is', 'gone', '.', 'because', 'it', 'doesn', \"'\", 't', 'matter', 'how', 'smart', 'i', 'am', 'or', 'how', 'hard', 'i', 'work', '.', 'because', 'my', 'to', '##va', 'score', 'is', 'so', 'bad', 'that', 'the', 'results', 'can', \"'\", 't', 'even', 'be', 'read', '.', 'i', 'know', 'what', 'every', 'single', 'one', 'of', 'these', 'tests', 'mean', ',', 'and', 'knew', 'exactly', 'what', 'the', 'dia', '##gno', '##ses', 'would', 'be', 'before', 'i', 'stepped', 'in', ',', 'but', 'i', 'never', 'imagined', 'pressing', 'that', 'damn', 'button', 'when', 'the', 'shape', 'appeared', 'would', 'be', 'so', 'difficult', '.', 'all', 'the', 'intelligence', 'and', 'hard', 'work', 'in', 'the', 'world', ',', 'and', 'my', 'achievements', 'are', '.', '.', '.', 'average', '.', 'always', ',', 'just', 'average', '.', 'if', 'only', 'he', 'would', 'apply', 'himself', '.', 'if', 'only', 'he', 'weren', \"'\", 't', 'so', 'lazy', '-', 'that', 'one', 'just', 'makes', 'my', 'head', 'explode', '.', 'so', 'the', 'shrink', 'says', 'the', 'words', 'which', 'i', \"'\", 've', 'been', 'thinking', 'all', 'my', 'life', '.', 'you', \"'\", 've', 'got', 'so', 'much', 'potential', '.', 'so', 'many', 'mental', 'resources', ',', 'that', 'you', 'can', \"'\", 't', 'apply', 'because', 'you', 'can', \"'\", 't', 'focus', '.', 'take', 'the', 'medication', ',', 'you', 'will', 'be', 'able', 'to', 'un', '##lea', '##sh', 'so', 'much', 'but', 'i', 'take', 'that', 'medication', ',', 'and', 'all', 'those', 'thoughts', 'that', 'are', 'rushing', 'through', 'my', 'head', ',', 'stopping', 'me', 'from', 'focusing', ',', 'turn', 'off', '.', 'que', 'amazing', 'focus', '.', '.', '.', 'at', 'first', '.', 'but', 'suddenly', ',', 'all', 'my', 'special', 'abilities', 'that'], ['separated', 'me', 'from', 'the', 'crowe', '##d', 'start', 'to', 'disappear', '.', 'i', 'can', \"'\", 't', 'tell', 'children', 'funny', 'stories', 'made', 'up', 'on', 'the', 'spot', 'without', 'struggling', '.', 'i', 'don', \"'\", 't', 'automatically', 'notice', 'the', 'expressions', 'in', 'the', 'eyes', 'of', 'people', 'i', 'love', ',', 'read', 'their', 'emotions', ',', 'unless', 'i', 'am', 'specifically', 'trying', 'to', '.', 'i', 'don', \"'\", 't', 'realize', 'i', \"'\", 'm', 'losing', 'my', 'temper', 'until', 'i', \"'\", 've', 'lost', 'it', '.', 'and', 'i', 'realized', ',', 'putting', 'those', 'rushing', 'thoughts', 'to', 'good', 'use', 'was', 'the', 'whole', 'reason', 'i', 'wanted', 'to', 'succeed', 'academic', '##ally', 'in', 'the', 'first', 'place', '.', 'can', \"'\", 't', 'have', 'it', 'both', 'ways', ',', 'right', '?', 'whatever', ',', 'just', 'keep', 'pushing', 'on', '.', '.', '.', 'one', 'day', ',', 'i', \"'\", 'll', 'be', 'in', 'a', 'position', 'when', 'all', 'the', 'thoughts', 'in', 'my', 'head', 'will', 'be', 'my', 'strength', ',', 'my', 'advantage', '.', 'one', 'day', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['false', 'negative', 'or', 'should', 'i', 'give', 'up', '?', 'i', \"'\", 'm', 'a', 'high', 'school', 'senior', '.', 'i', 'got', 'tested', 'for', 'ad', '##hd', 'a', 'couple', 'of', 'weeks', 'ago', ',', 'which', 'involved', 'a', 'brief', 'patient', 'history', 'and', 'a', 'battery', 'of', 'tests', '.', 'after', 'weeks', 'of', 'waiting', ',', 'i', 'got', 'the', 'result', 'back', ':', 'not', 'ad', '##hd', ',', 'just', 'some', 'minor', 'depression', 'and', 'anxiety', '(', 'not', 'serious', 'at', 'all', ')', '.', 'but', 'i', 'don', \"'\", 't', 'believe', 'this', 'result', '.', 'there', \"'\", 's', 'a', 'lot', 'of', 'behaviors', 'i', 'do', 'that', 'are', 'just', 'so', 'similar', '.', 'the', 'chronic', 'pro', '##cr', '##ast', '##ination', 'can', 'be', 'explained', 'by', 'other', 'things', ',', 'including', 'la', '##zine', '##ss', 'and', 'depression', '(', '?', ')', 'and', 'anxiety', ',', 'and', 'my', 'constant', 'tar', '##dine', '##ss', 'to', 'my', 'first', 'class', 'due', 'to', 'a', 'complete', 'inability', 'to', 'get', 'up', 'in', 'the', 'morning', 'isn', \"'\", 't', 'technically', 'sy', '##mpt', '##oma', '##tic', 'of', 'ad', '##hd', '.', 'but', 'there', \"'\", 's', 'more', 'than', 'just', 'that', '!', 'i', 'fi', '##dget', 'with', 'my', 'legs', 'or', 'something', 'in', 'my', 'hands', '(', 'a', 'pencil', ',', 'cloth', ',', 'etc', '.', ')', 'all', 'the', 'time', '.', 'i', 'day', '##dre', '##am', 'very', 'often', ',', 'and', 'if', 'i', \"'\", 'm', 'alone', 'when', 'i', 'do', 'so', ',', 'i', 'pace', 'or', 'run', 'around', 'or', 'jump', 'off', 'of', 'stuff', ',', 'usually', 'without', 'realizing', 'i', \"'\", 'm', 'doing', 'so', '.', 'also', ',', 'caf', '##fe', '##ine', 'seems', 'to', 'slow', 'me', 'down', ':', 'i', 'actually', 'feel', 'tired', 'and', 'give', 'up', 'if', 'i', 'get', 'up', 'to', 'pace', '.', 'there', \"'\", 's', 'also', 'reasons', 'i', 'may', 'not', 'have', 'ad', '##hd', '.', 'i', \"'\", 'm', 'a', 'pretty', 'organized', 'person', ',', 'and', 'i', 'don', \"'\", 't', 'lose', 'things', 'that', 'i', 'need', 'for', 'class', ',', 'and', 'i', 'don', \"'\", 't', 'think', 'i', \"'\", 'm', 'forget', '##ful', '.', 'the', 'caf', '##fe', '##ine', 'effect', 'could', 'be', 'place', '##bo', 'effect', '.', '.', 'i', 'think', 'the', 'report', 'said', 'that', 'i', 'did', 'okay', 'on', 'the', 'attention', 'tests', ',', 'thus', 'the', 'throwing', 'out', 'of', 'this', 'diagnosis', ',', 'but', 'i', \"'\", 'm', 'not', 'sure', '.', 'even', 'if', 'that', \"'\", 's', 'true', ',', 'how', 'can', 'i', 'explain', 'all', 'these', 'behaviors', 'i', 'have', '?', 'so', 'how', 'do', 'i', 'continue', '?', 'should', 'i', 'keep', 'pushing', 'that', 'i', 'might', 'have', 'ad', '##hd', ',', 'or', 'do', 'i', 'give', 'up', 'and', 'accept', 'that', 'the', 'problems', 'i', 'have', 'aren', \"'\", 't', 'due', 'to', 'a', 'disorder', '?', 't', '##l', ';', 'dr', ':', 'i', 'tested', 'negative', 'for', 'ad', '##hd', ',', 'but', 'i', 'have', 'lots', 'of', 'ad', '##hd', '-', 'like', 'behavior', '.', 'and', 'some', 'behaviors', 'that', 'aren', \"'\", 't', '.', 'now', 'what', '?']\n",
      "INFO:__main__:Number of tokens: 420\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['false', 'negative', 'or', 'should', 'i', 'give', 'up', '?', 'i', \"'\", 'm', 'a', 'high', 'school', 'senior', '.', 'i', 'got', 'tested', 'for', 'ad', '##hd', 'a', 'couple', 'of', 'weeks', 'ago', ',', 'which', 'involved', 'a', 'brief', 'patient', 'history', 'and', 'a', 'battery', 'of', 'tests', '.', 'after', 'weeks', 'of', 'waiting', ',', 'i', 'got', 'the', 'result', 'back', ':', 'not', 'ad', '##hd', ',', 'just', 'some', 'minor', 'depression', 'and', 'anxiety', '(', 'not', 'serious', 'at', 'all', ')', '.', 'but', 'i', 'don', \"'\", 't', 'believe', 'this', 'result', '.', 'there', \"'\", 's', 'a', 'lot', 'of', 'behaviors', 'i', 'do', 'that', 'are', 'just', 'so', 'similar', '.', 'the', 'chronic', 'pro', '##cr', '##ast', '##ination', 'can', 'be', 'explained', 'by', 'other', 'things', ',', 'including', 'la', '##zine', '##ss', 'and', 'depression', '(', '?', ')', 'and', 'anxiety', ',', 'and', 'my', 'constant', 'tar', '##dine', '##ss', 'to', 'my', 'first', 'class', 'due', 'to', 'a', 'complete', 'inability', 'to', 'get', 'up', 'in', 'the', 'morning', 'isn', \"'\", 't', 'technically', 'sy', '##mpt', '##oma', '##tic', 'of', 'ad', '##hd', '.', 'but', 'there', \"'\", 's', 'more', 'than', 'just', 'that', '!', 'i', 'fi', '##dget', 'with', 'my', 'legs', 'or', 'something', 'in', 'my', 'hands', '(', 'a', 'pencil', ',', 'cloth', ',', 'etc', '.', ')', 'all', 'the', 'time', '.', 'i', 'day', '##dre', '##am', 'very', 'often', ',', 'and', 'if', 'i', \"'\", 'm', 'alone', 'when', 'i', 'do', 'so', ',', 'i', 'pace', 'or', 'run', 'around', 'or', 'jump', 'off', 'of', 'stuff', ',', 'usually', 'without', 'realizing', 'i', \"'\", 'm', 'doing', 'so', '.', 'also', ',', 'caf', '##fe', '##ine', 'seems', 'to', 'slow', 'me', 'down', ':', 'i', 'actually', 'feel', 'tired', 'and', 'give', 'up', 'if', 'i', 'get', 'up', 'to', 'pace', '.', 'there', \"'\", 's', 'also', 'reasons', 'i', 'may', 'not', 'have', 'ad', '##hd', '.', 'i', \"'\", 'm', 'a', 'pretty', 'organized', 'person', ',', 'and', 'i', 'don', \"'\", 't', 'lose', 'things', 'that', 'i', 'need', 'for', 'class', ',', 'and', 'i', 'don', \"'\", 't', 'think', 'i', \"'\", 'm', 'forget', '##ful', '.', 'the', 'caf', '##fe', '##ine', 'effect', 'could', 'be', 'place', '##bo', 'effect', '.', '.', 'i', 'think', 'the', 'report', 'said', 'that', 'i', 'did', 'okay', 'on', 'the', 'attention', 'tests', ',', 'thus', 'the', 'throwing', 'out', 'of', 'this', 'diagnosis', ',', 'but', 'i', \"'\", 'm', 'not', 'sure', '.', 'even', 'if', 'that', \"'\", 's', 'true', ',', 'how', 'can', 'i', 'explain', 'all', 'these', 'behaviors', 'i', 'have', '?', 'so', 'how', 'do', 'i', 'continue', '?', 'should', 'i', 'keep', 'pushing', 'that', 'i', 'might', 'have', 'ad', '##hd', ',', 'or', 'do', 'i', 'give', 'up', 'and', 'accept', 'that', 'the', 'problems', 'i', 'have', 'aren', \"'\", 't', 'due', 'to', 'a', 'disorder', '?', 't', '##l', ';', 'dr', ':', 'i', 'tested', 'negative', 'for', 'ad', '##hd', ',', 'but', 'i', 'have', 'lots', 'of', 'ad', '##hd', '-', 'like', 'behavior', '.', 'and', 'some', 'behaviors', 'that', 'aren', \"'\", 't', '.', 'now', 'what', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['who', 'else', 'does', 'this', '?', 'i', \"'\", 'm', 'traveling', 'for', 'thanksgiving', ',', 'so', 'on', 'the', 'day', 'i', 'booked', 'my', 'ti', '##x', ',', 'i', 'also', 'put', 'that', 'on', 'my', 'calendar', ':', '\"', 'fly', 'to', 'xx', '\"', ',', 'at', '1', '##pm', '.', 'i', 'wake', 'up', 'this', 'morning', ',', 'have', 'a', 'nice', 'relaxed', 'morning', ',', 'pack', ',', 'etc', ',', 'thinking', 'i', 'have', 'plenty', 'of', 'time', '.', '12', ':', '15', '##pm', 'rolls', 'around', ',', 'i', 'go', 'to', 'check', 'my', 'flight', 'info', ',', 'etc', '.', 'i', \"'\", 'm', 'feeling', 'pretty', 'good', 'because', 'hey', ',', 'i', 'don', \"'\", 't', 'have', 'to', 'leave', 'for', 'another', '45', '##mins', 'and', 'i', \"'\", 'm', 'completely', 'ready', 'for', 'once', '.', 'i', 'open', 'up', 'my', 'email', 'and', '*', '*', 'ff', '##ff', '##ff', '##fu', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '*', '*', '.', '1', '##pm', 'isn', \"'\", 't', 'the', 'time', 'i', 'have', 'to', 'leave', 'my', 'house', ',', 'it', \"'\", 's', 'the', 'time', 'that', 'the', 'flight', 'leaves', '.', 'there', \"'\", 's', 'no', 'way', 'i', 'can', 'make', 'it', 'to', 'the', 'airport', 'and', 'through', 'security', 'in', 'time', '.', 'a', 'very', 'panicked', 'phone', 'call', 'to', 'the', 'airline', 'later', ',', 'and', 'i', \"'\", 'm', 'on', 'the', 'next', 'flight', 'for', '$', '25', 'extra', '.', 'ph', '##ew', '.', 'then', 'time', 'for', 'a', 'stress', '/', 'relief', 'cry', '.']\n",
      "INFO:__main__:Number of tokens: 212\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['who', 'else', 'does', 'this', '?', 'i', \"'\", 'm', 'traveling', 'for', 'thanksgiving', ',', 'so', 'on', 'the', 'day', 'i', 'booked', 'my', 'ti', '##x', ',', 'i', 'also', 'put', 'that', 'on', 'my', 'calendar', ':', '\"', 'fly', 'to', 'xx', '\"', ',', 'at', '1', '##pm', '.', 'i', 'wake', 'up', 'this', 'morning', ',', 'have', 'a', 'nice', 'relaxed', 'morning', ',', 'pack', ',', 'etc', ',', 'thinking', 'i', 'have', 'plenty', 'of', 'time', '.', '12', ':', '15', '##pm', 'rolls', 'around', ',', 'i', 'go', 'to', 'check', 'my', 'flight', 'info', ',', 'etc', '.', 'i', \"'\", 'm', 'feeling', 'pretty', 'good', 'because', 'hey', ',', 'i', 'don', \"'\", 't', 'have', 'to', 'leave', 'for', 'another', '45', '##mins', 'and', 'i', \"'\", 'm', 'completely', 'ready', 'for', 'once', '.', 'i', 'open', 'up', 'my', 'email', 'and', '*', '*', 'ff', '##ff', '##ff', '##fu', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '*', '*', '.', '1', '##pm', 'isn', \"'\", 't', 'the', 'time', 'i', 'have', 'to', 'leave', 'my', 'house', ',', 'it', \"'\", 's', 'the', 'time', 'that', 'the', 'flight', 'leaves', '.', 'there', \"'\", 's', 'no', 'way', 'i', 'can', 'make', 'it', 'to', 'the', 'airport', 'and', 'through', 'security', 'in', 'time', '.', 'a', 'very', 'panicked', 'phone', 'call', 'to', 'the', 'airline', 'later', ',', 'and', 'i', \"'\", 'm', 'on', 'the', 'next', 'flight', 'for', '$', '25', 'extra', '.', 'ph', '##ew', '.', 'then', 'time', 'for', 'a', 'stress', '/', 'relief', 'cry', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'can', 'i', 'be', 'better', 'with', 'time', 'management', '/', 'pro', '##cr', '##ast', '##ination', '?', 'i', 'have', 'a', 'high', 'probability', 'of', 'ad', '##hd', '(', 'i', \"'\", 'm', 'in', 'the', 'middle', 'of', 'a', 'diagnosis', 'and', 'this', 'is', 'as', 'far', 'as', 'we', 'are', 'right', 'now', ')', 'so', 'since', 'i', 'am', 'not', 'eligible', 'for', 'medications', 'or', 'behavioral', 'therapy', 'at', 'this', 'time', ',', 'and', 'i', 'really', 'need', 'help', ',', 'i', 'want', 'to', 'know', 'if', 'anyone', 'has', 'good', 'tips', 'of', 'how', 'i', 'can', 'overcome', 'my', 'shitty', 'time', 'management', 'and', 'pro', '##cr', '##ast', '##ination', '.']\n",
      "INFO:__main__:Number of tokens: 88\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'can', 'i', 'be', 'better', 'with', 'time', 'management', '/', 'pro', '##cr', '##ast', '##ination', '?', 'i', 'have', 'a', 'high', 'probability', 'of', 'ad', '##hd', '(', 'i', \"'\", 'm', 'in', 'the', 'middle', 'of', 'a', 'diagnosis', 'and', 'this', 'is', 'as', 'far', 'as', 'we', 'are', 'right', 'now', ')', 'so', 'since', 'i', 'am', 'not', 'eligible', 'for', 'medications', 'or', 'behavioral', 'therapy', 'at', 'this', 'time', ',', 'and', 'i', 'really', 'need', 'help', ',', 'i', 'want', 'to', 'know', 'if', 'anyone', 'has', 'good', 'tips', 'of', 'how', 'i', 'can', 'overcome', 'my', 'shitty', 'time', 'management', 'and', 'pro', '##cr', '##ast', '##ination', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['brother', 'bought', 'me', 'a', 'kind', '##le', 'fire', 'for', 'christmas', '.', 'what', 'are', 'some', 'features', 'i', 'can', 'use', 'with', 'it', 'to', 'help', 'with', 'my', 'ad', '##hd', '?', 'funny', 'thing', 'is', ',', 'i', 'don', \"'\", 't', 'read', 'a', 'lot', 'of', 'books', '.', 'it', 'might', 'come', 'in', 'handy', 'for', 'college', 'textbooks', ',', 'but', 'i', 'have', 'a', 'feeling', '75', '%', 'of', 'my', 'text', 'books', 'won', \"'\", 't', 'be', 'available', 'via', 'kind', '##le', '.']\n",
      "INFO:__main__:Number of tokens: 69\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['brother', 'bought', 'me', 'a', 'kind', '##le', 'fire', 'for', 'christmas', '.', 'what', 'are', 'some', 'features', 'i', 'can', 'use', 'with', 'it', 'to', 'help', 'with', 'my', 'ad', '##hd', '?', 'funny', 'thing', 'is', ',', 'i', 'don', \"'\", 't', 'read', 'a', 'lot', 'of', 'books', '.', 'it', 'might', 'come', 'in', 'handy', 'for', 'college', 'textbooks', ',', 'but', 'i', 'have', 'a', 'feeling', '75', '%', 'of', 'my', 'text', 'books', 'won', \"'\", 't', 'be', 'available', 'via', 'kind', '##le', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['car', '##bs', '.', 'the', 'enemy', 'of', 'concentration', '.', 'i', 'blew', 'my', 'low', 'car', '##b', 'diet', 'with', 'thanksgiving', ',', 'but', 'saw', '[', 'this', 'article', ']', '(', 'http', ':', '/', '/', 'www', '.', 'er', '##go', '-', 'log', '.', 'com', '/', 'car', '##bs', '##me', '##mo', '##ry', '.', 'html', ')', 'today', ',', 'and', 'thought', 'i', 'would', 'post', 'about', 'diet', 'and', 'ad', '##hd', '.', 'i', 'have', 'been', 'working', 'with', 'some', 'au', '##tist', '##ic', 'kids', 'and', 'keep', 'seeing', 'the', 'idea', 'of', 'a', 'no', 'car', '##b', '/', 'no', 'case', '##in', 'diet', 'coming', 'up', ',', 'so', 'i', 'looked', 'around', ',', 'and', 'sure', 'enough', ',', 'articles', '[', 'like', 'this', ']', '(', 'http', ':', '/', '/', 'www', '.', 'adult', '-', 'child', '-', 'add', '-', 'ad', '##hd', '.', 'com', '/', 'categories', '/', 'help', '/', 'add', '_', 'diet', '.', 'php', ')', 'started', 'popping', 'up', '.', 'i', 'didn', \"'\", 't', 'take', 'much', 'stock', ',', 'but', 'thought', 'i', 'would', 'give', 'it', 'a', 'try', '.', 'the', 'results', 'were', 'amazing', '.', 'easily', 'blows', 'add', '##eral', '##l', 'away', ',', 'with', 'no', 'side', 'effects', 'at', 'all', '.', '(', 'other', 'than', 'wanting', 'to', 'eat', 'bread', ')', '.', 'i', 'have', 'a', 'zen', 'like', 'focus', 'now', '.', 'i', 'don', \"'\", 't', 'think', 'it', 'is', 'a', \"'\", 'cure', \"'\", 'for', 'ad', '##hd', 'though', '.', 'the', 'diet', 'dies', 'kind', 'of', 'suck', 'to', 'be', 'on', '.', 'protein', 'doesn', \"'\", 't', 'fill', 'you', 'the', 'way', 'car', '##bs', 'do', ',', 'so', 'you', 'are', 'generally', 'hungry', 'an', 'hour', 'or', 'two', 'before', 'you', 'normally', 'are', '.', 'it', 'is', 'easy', 'to', 'eat', '4', '-', '5', 'times', 'a', 'day', 'to', 'compensate', 'for', 'this', 'though', '.', 'and', 'it', 'is', 'pretty', 'impossible', 'to', 'eat', 'out', 'and', 'avoid', 'car', '##bs', 'and', 'milk', '.', 'it', 'can', 'be', 'done', 'easily', 'if', 'have', 'time', 'and', 'enjoy', 'cooking', 'for', 'yourself', '.', 'a', 'diet', 'as', 'simple', 'as', '[', 'the', '##the', '3', '-', 'minute', '\"', 'slow', '-', 'car', '##b', '\"', 'breakfast', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'f', '##d', '-', '7', '##a', '_', 'w', '##d', '##v', '##z', '##k', ')', ',', 'then', 'chicken', 'breast', 'for', 'lunch', ',', 'snack', ',', 'and', 'dinner', 'is', 'very', 'easy', 'and', 'takes', 'almost', 'no', 'prep', '.', 'you', 'can', 'buy', 'a', '4', 'pound', 'bag', 'of', 'chicken', 'breasts', 'and', 'nu', '##ke', 'them', 'as', 'you', 'eat', ',', 'or', 'fry', 'one', 'up', 'every', 'time', 'you', 'are', 'hungry', '.', 'do', 'i', 'think', 'this', 'will', 'work', 'for', 'children', '?', 'no', ',', 'not', 'really', '.', 'kids', 'and', 'diet', '##s', 'don', \"'\", 't', 'work', 'very', 'well', '.', 'they', 'don', \"'\", 't', 'have', 'the', 'dedication', 'to', 'stick', 'to', 'it', ',', 'or', 'the', 'will', 'power', 'to', 'avoid', 'the', 'candy', 'and', 'snacks', '.', 'that', 'up', '##hill', 'fight', 'is', 'probably', 'not', 'worth', 'the', 'benefits', '.', 'trading', 'focus', 'for', 'quality', 'of', 'life', '(', 'always', 'being', 'yelled', 'at', 'over', 'food', ')', 'is', 'a', 'crap', '##py', 'trade', 'off', '.', 'but', 'if', 'you', 'are', 'out', 'on', 'your', 'own', ',', 'and', 'make', 'your', 'own', 'food', ',', 'doing', 'a', '2', 'month', 'trial', 'is', 'well', 'worth', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 483\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['car', '##bs', '.', 'the', 'enemy', 'of', 'concentration', '.', 'i', 'blew', 'my', 'low', 'car', '##b', 'diet', 'with', 'thanksgiving', ',', 'but', 'saw', '[', 'this', 'article', ']', '(', 'http', ':', '/', '/', 'www', '.', 'er', '##go', '-', 'log', '.', 'com', '/', 'car', '##bs', '##me', '##mo', '##ry', '.', 'html', ')', 'today', ',', 'and', 'thought', 'i', 'would', 'post', 'about', 'diet', 'and', 'ad', '##hd', '.', 'i', 'have', 'been', 'working', 'with', 'some', 'au', '##tist', '##ic', 'kids', 'and', 'keep', 'seeing', 'the', 'idea', 'of', 'a', 'no', 'car', '##b', '/', 'no', 'case', '##in', 'diet', 'coming', 'up', ',', 'so', 'i', 'looked', 'around', ',', 'and', 'sure', 'enough', ',', 'articles', '[', 'like', 'this', ']', '(', 'http', ':', '/', '/', 'www', '.', 'adult', '-', 'child', '-', 'add', '-', 'ad', '##hd', '.', 'com', '/', 'categories', '/', 'help', '/', 'add', '_', 'diet', '.', 'php', ')', 'started', 'popping', 'up', '.', 'i', 'didn', \"'\", 't', 'take', 'much', 'stock', ',', 'but', 'thought', 'i', 'would', 'give', 'it', 'a', 'try', '.', 'the', 'results', 'were', 'amazing', '.', 'easily', 'blows', 'add', '##eral', '##l', 'away', ',', 'with', 'no', 'side', 'effects', 'at', 'all', '.', '(', 'other', 'than', 'wanting', 'to', 'eat', 'bread', ')', '.', 'i', 'have', 'a', 'zen', 'like', 'focus', 'now', '.', 'i', 'don', \"'\", 't', 'think', 'it', 'is', 'a', \"'\", 'cure', \"'\", 'for', 'ad', '##hd', 'though', '.', 'the', 'diet', 'dies', 'kind', 'of', 'suck', 'to', 'be', 'on', '.', 'protein', 'doesn', \"'\", 't', 'fill', 'you', 'the', 'way', 'car', '##bs', 'do', ',', 'so', 'you', 'are', 'generally', 'hungry', 'an', 'hour', 'or', 'two', 'before', 'you', 'normally', 'are', '.', 'it', 'is', 'easy', 'to', 'eat', '4', '-', '5', 'times', 'a', 'day', 'to', 'compensate', 'for', 'this', 'though', '.', 'and', 'it', 'is', 'pretty', 'impossible', 'to', 'eat', 'out', 'and', 'avoid', 'car', '##bs', 'and', 'milk', '.', 'it', 'can', 'be', 'done', 'easily', 'if', 'have', 'time', 'and', 'enjoy', 'cooking', 'for', 'yourself', '.', 'a', 'diet', 'as', 'simple', 'as', '[', 'the', '##the', '3', '-', 'minute', '\"', 'slow', '-', 'car', '##b', '\"', 'breakfast', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'f', '##d', '-', '7', '##a', '_', 'w', '##d', '##v', '##z', '##k', ')', ',', 'then', 'chicken', 'breast', 'for', 'lunch', ',', 'snack', ',', 'and', 'dinner', 'is', 'very', 'easy', 'and', 'takes', 'almost', 'no', 'prep', '.', 'you', 'can', 'buy', 'a', '4', 'pound', 'bag', 'of', 'chicken', 'breasts', 'and', 'nu', '##ke', 'them', 'as', 'you', 'eat', ',', 'or', 'fry', 'one', 'up', 'every', 'time', 'you', 'are', 'hungry', '.', 'do', 'i', 'think', 'this', 'will', 'work', 'for', 'children', '?', 'no', ',', 'not', 'really', '.', 'kids', 'and', 'diet', '##s', 'don', \"'\", 't', 'work', 'very', 'well', '.', 'they', 'don', \"'\", 't', 'have', 'the', 'dedication', 'to', 'stick', 'to', 'it', ',', 'or', 'the', 'will', 'power', 'to', 'avoid', 'the', 'candy', 'and', 'snacks', '.', 'that', 'up', '##hill', 'fight', 'is', 'probably', 'not', 'worth', 'the', 'benefits', '.', 'trading', 'focus', 'for', 'quality', 'of', 'life', '(', 'always', 'being', 'yelled', 'at', 'over', 'food', ')', 'is', 'a', 'crap', '##py', 'trade', 'off', '.', 'but', 'if', 'you', 'are', 'out', 'on', 'your', 'own', ',', 'and', 'make', 'your', 'own', 'food', ',', 'doing', 'a', '2', 'month', 'trial', 'is', 'well', 'worth', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'kind', 'of', 'weird', 'or', 'embarrassing', 'things', 'have', 'your', 'ad', '##hd', 'caused', 'you', 'to', 'do', '?', 'i', 'can', 'be', 'quite', 'the', 'lunatic', 'when', 'not', 'med', '##icated', ',', 'and', 'this', 'sometimes', 'causes', 'me', 'to', 'do', 'embarrassing', 'stuff', '.', 'sometimes', 'funny', ',', 'sometimes', 'potentially', 'tragic', '.', 'is', 'this', 'common', '?', 'before', 'i', 'got', 'diagnosed', 'with', 'ad', '##hd', ',', 'i', 'often', 'thought', 'i', 'had', 'some', 'form', 'of', 'extremely', 'early', 'alzheimer', \"'\", 's', 'disease', ',', 'cause', 'i', 'used', 'to', 'work', 'with', 'alzheimer', \"'\", 's', 'patients', 'and', 'sometimes', ',', 'i', 'felt', 'just', 'about', 'as', 'lost', '.', 'in', 'the', 'last', 'few', 'weeks', 'before', 'i', 'started', 'to', 'take', 'med', '##s', ',', 'it', 'got', 'so', 'bad', 'that', 'i', 'kept', 'ap', '##olo', '##gizing', 'to', 'people', 'for', 'being', 'so', \"'\", 'out', 'of', 'it', \"'\", '.', 'my', 'favourite', 'excuse', 'was', \"'\", 'wow', ',', 'i', 'am', 'not', 'awake', 'yet', ',', 'i', 'need', 'coffee', \"'\", ',', 'no', 'matter', 'what', 'time', 'of', 'day', '.', 'it', 'felt', 'embarrassing', 'because', 'i', 'started', 'feeling', 'like', 'it', 'made', 'me', 'look', 'like', 'i', 'was', 'mentally', 'challenged', 'or', 'something', ',', 'when', 'in', 'fact', 'i', 'am', 'actually', 'gifted', '.', 'if', 'i', 'went', 'for', 'groceries', ',', 'i', \"'\", 'd', 'forget', 'that', 'the', 'cash', '##ier', 'handed', 'me', 'my', 'bags', 'and', 'just', 'blankly', 'stare', 'at', 'her', 'waiting', 'for', 'them', ',', 'when', 'in', 'fact', 'i', \"'\", 've', 'been', 'holding', 'them', 'the', 'whole', 'time', '.', 'ur', '##gh', '!', '*', 'by', 'the', 'way', ',', 'sorry', 'for', 'the', 'mistakes', ',', 'english', 'is', 'not', 'my', 'first', 'language', '.']\n",
      "INFO:__main__:Number of tokens: 239\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'kind', 'of', 'weird', 'or', 'embarrassing', 'things', 'have', 'your', 'ad', '##hd', 'caused', 'you', 'to', 'do', '?', 'i', 'can', 'be', 'quite', 'the', 'lunatic', 'when', 'not', 'med', '##icated', ',', 'and', 'this', 'sometimes', 'causes', 'me', 'to', 'do', 'embarrassing', 'stuff', '.', 'sometimes', 'funny', ',', 'sometimes', 'potentially', 'tragic', '.', 'is', 'this', 'common', '?', 'before', 'i', 'got', 'diagnosed', 'with', 'ad', '##hd', ',', 'i', 'often', 'thought', 'i', 'had', 'some', 'form', 'of', 'extremely', 'early', 'alzheimer', \"'\", 's', 'disease', ',', 'cause', 'i', 'used', 'to', 'work', 'with', 'alzheimer', \"'\", 's', 'patients', 'and', 'sometimes', ',', 'i', 'felt', 'just', 'about', 'as', 'lost', '.', 'in', 'the', 'last', 'few', 'weeks', 'before', 'i', 'started', 'to', 'take', 'med', '##s', ',', 'it', 'got', 'so', 'bad', 'that', 'i', 'kept', 'ap', '##olo', '##gizing', 'to', 'people', 'for', 'being', 'so', \"'\", 'out', 'of', 'it', \"'\", '.', 'my', 'favourite', 'excuse', 'was', \"'\", 'wow', ',', 'i', 'am', 'not', 'awake', 'yet', ',', 'i', 'need', 'coffee', \"'\", ',', 'no', 'matter', 'what', 'time', 'of', 'day', '.', 'it', 'felt', 'embarrassing', 'because', 'i', 'started', 'feeling', 'like', 'it', 'made', 'me', 'look', 'like', 'i', 'was', 'mentally', 'challenged', 'or', 'something', ',', 'when', 'in', 'fact', 'i', 'am', 'actually', 'gifted', '.', 'if', 'i', 'went', 'for', 'groceries', ',', 'i', \"'\", 'd', 'forget', 'that', 'the', 'cash', '##ier', 'handed', 'me', 'my', 'bags', 'and', 'just', 'blankly', 'stare', 'at', 'her', 'waiting', 'for', 'them', ',', 'when', 'in', 'fact', 'i', \"'\", 've', 'been', 'holding', 'them', 'the', 'whole', 'time', '.', 'ur', '##gh', '!', '*', 'by', 'the', 'way', ',', 'sorry', 'for', 'the', 'mistakes', ',', 'english', 'is', 'not', 'my', 'first', 'language', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['effect', 'of', 'st', '##im', '##ula', '##nts', 'on', 'growth', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['effect', 'of', 'st', '##im', '##ula', '##nts', 'on', 'growth', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['motivation', '!', 'why', 'you', 'no', 'stay', 'till', 'i', \"'\", 'm', 'done', '[', 'posted', 'on', 'r', '/', 'advice', 'animals', ']']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['motivation', '!', 'why', 'you', 'no', 'stay', 'till', 'i', \"'\", 'm', 'done', '[', 'posted', 'on', 'r', '/', 'advice', 'animals', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'respond', 'to', 'people', 'who', 'don', \"'\", 't', 'believe', 'that', 'you', 'have', 'ad', '##hd', 'or', 'say', 'that', 'it', 'is', 'merely', 'an', 'excuse', '?']\n",
      "INFO:__main__:Number of tokens: 25\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'respond', 'to', 'people', 'who', 'don', \"'\", 't', 'believe', 'that', 'you', 'have', 'ad', '##hd', 'or', 'say', 'that', 'it', 'is', 'merely', 'an', 'excuse', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['at', 'some', 'point', 'you', 'just', 'start', 'giving', 'up', '.', 'i', 'a', 'few', 'months', 'ago', ',', 'i', 'was', 'tested', 'for', 'ad', '##hd', ',', 'but', 'was', 'diagnosed', 'with', 'de', '##pressive', 'disorder', '-', 'not', 'otherwise', 'specified', 'a', 'couple', 'months', 'ago', '.', 'op', '[', 'here', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'l', '##ns', '##j', '##y', '/', 'went', '_', 'in', '_', 'to', '_', 'be', '_', 'tested', '_', 'for', '_', 'ad', '##hd', '_', 'diagnosed', '_', 'with', '/', ')', '.', 'i', 'was', 'able', 'to', 'see', 'the', 'school', 'counselor', '.', 'great', 'guy', ',', 'and', 'he', \"'\", 's', 'focusing', 'on', 'how', 'we', 'can', 'work', 'on', 'my', 'attention', 'problems', 'outside', 'of', 'medication', '.', 'he', \"'\", 's', 'actually', 'willing', 'to', 'listen', 'to', 'my', 'problems', ',', 'and', 'work', 'on', 'what', 'i', 'actually', 'started', 'this', 'process', 'for', 'in', 'the', 'first', 'place', '.', 'the', 'problem', 'is', ',', 'however', ',', 'the', 'school', \"'\", 's', 'place', 'for', 'refer', '##ral', '##s', 'is', 'the', 'same', 'place', 'i', 'was', 'tested', 'at', 'originally', '.', 'they', 'can', 'refer', 'me', 'to', 'other', 'places', ',', 'but', 'at', 'those', 'places', ',', 'i', \"'\", 'll', 'have', 'to', 'pay', 'out', 'of', 'pocket', '.', 'so', '.', 'nope', '.', 'no', 'second', 'opinion', '.', 'i', \"'\", 'm', 'pretty', 'fucked', ',', 'right', 'about', 'now', '.', 'i', 'failed', 'out', 'of', 'my', 'programming', 'class', ';', 'i', 'can', \"'\", 't', 'get', 'simple', 'chores', 'done', 'around', 'the', 'house', ',', 'which', 'is', 'wreck', '##ing', 'my', 'relationship', '.', 'yep', '.', 'can', \"'\", 't', 'even', 'practice', 'at', 'hon', ',', 'as', 'much', 'as', 'i', 'want', 'to', 'get', 'better', '.', 'a', 'fucking', 'video', 'game', ',', 'and', 'i', 'can', \"'\", 't', 'get', 'myself', 'to', 'press', 'that', 'match', '##making', 'button', 'to', 'start', '.', 'my', 'life', 'is', 'being', 'held', 'hostage', 'by', 'my', 'mind', ',', 'and', 'i', \"'\", 'm', 'powerless', 'to', 'do', 'anything', 'because', 'someone', 'thinks', 'i', \"'\", 'm', 'depressed', ',', 'ignoring', 'the', 'fact', 'that', 'all', 'my', 'symptoms', 'match', 'ad', '##hd', ',', 'i', 'lack', 'the', 'symptoms', 'to', 'dia', '##gno', '##se', 'me', 'with', 'most', 'specific', 'depression', '##s', '(', 'hence', '\"', 'de', '##pressive', 'disorder', '-', 'not', 'otherwise', 'specified', '\"', ')', ',', 'and', 'most', 'fucking', 'importantly', ':', 'i', 'am', 'not', 'depressed', '.', 'i', 'am', 'upset', '.', 'upset', 'at', 'the', 'refusal', 'to', 'listen', ',', 'and', 'the', 'subsequent', 'inability', 'to', 'find', 'someone', 'who', 'will', '.']\n",
      "INFO:__main__:Number of tokens: 371\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['at', 'some', 'point', 'you', 'just', 'start', 'giving', 'up', '.', 'i', 'a', 'few', 'months', 'ago', ',', 'i', 'was', 'tested', 'for', 'ad', '##hd', ',', 'but', 'was', 'diagnosed', 'with', 'de', '##pressive', 'disorder', '-', 'not', 'otherwise', 'specified', 'a', 'couple', 'months', 'ago', '.', 'op', '[', 'here', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'l', '##ns', '##j', '##y', '/', 'went', '_', 'in', '_', 'to', '_', 'be', '_', 'tested', '_', 'for', '_', 'ad', '##hd', '_', 'diagnosed', '_', 'with', '/', ')', '.', 'i', 'was', 'able', 'to', 'see', 'the', 'school', 'counselor', '.', 'great', 'guy', ',', 'and', 'he', \"'\", 's', 'focusing', 'on', 'how', 'we', 'can', 'work', 'on', 'my', 'attention', 'problems', 'outside', 'of', 'medication', '.', 'he', \"'\", 's', 'actually', 'willing', 'to', 'listen', 'to', 'my', 'problems', ',', 'and', 'work', 'on', 'what', 'i', 'actually', 'started', 'this', 'process', 'for', 'in', 'the', 'first', 'place', '.', 'the', 'problem', 'is', ',', 'however', ',', 'the', 'school', \"'\", 's', 'place', 'for', 'refer', '##ral', '##s', 'is', 'the', 'same', 'place', 'i', 'was', 'tested', 'at', 'originally', '.', 'they', 'can', 'refer', 'me', 'to', 'other', 'places', ',', 'but', 'at', 'those', 'places', ',', 'i', \"'\", 'll', 'have', 'to', 'pay', 'out', 'of', 'pocket', '.', 'so', '.', 'nope', '.', 'no', 'second', 'opinion', '.', 'i', \"'\", 'm', 'pretty', 'fucked', ',', 'right', 'about', 'now', '.', 'i', 'failed', 'out', 'of', 'my', 'programming', 'class', ';', 'i', 'can', \"'\", 't', 'get', 'simple', 'chores', 'done', 'around', 'the', 'house', ',', 'which', 'is', 'wreck', '##ing', 'my', 'relationship', '.', 'yep', '.', 'can', \"'\", 't', 'even', 'practice', 'at', 'hon', ',', 'as', 'much', 'as', 'i', 'want', 'to', 'get', 'better', '.', 'a', 'fucking', 'video', 'game', ',', 'and', 'i', 'can', \"'\", 't', 'get', 'myself', 'to', 'press', 'that', 'match', '##making', 'button', 'to', 'start', '.', 'my', 'life', 'is', 'being', 'held', 'hostage', 'by', 'my', 'mind', ',', 'and', 'i', \"'\", 'm', 'powerless', 'to', 'do', 'anything', 'because', 'someone', 'thinks', 'i', \"'\", 'm', 'depressed', ',', 'ignoring', 'the', 'fact', 'that', 'all', 'my', 'symptoms', 'match', 'ad', '##hd', ',', 'i', 'lack', 'the', 'symptoms', 'to', 'dia', '##gno', '##se', 'me', 'with', 'most', 'specific', 'depression', '##s', '(', 'hence', '\"', 'de', '##pressive', 'disorder', '-', 'not', 'otherwise', 'specified', '\"', ')', ',', 'and', 'most', 'fucking', 'importantly', ':', 'i', 'am', 'not', 'depressed', '.', 'i', 'am', 'upset', '.', 'upset', 'at', 'the', 'refusal', 'to', 'listen', ',', 'and', 'the', 'subsequent', 'inability', 'to', 'find', 'someone', 'who', 'will', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['did', 'you', 'know', 'these', 'facts', 'about', 'attention', 'deficit', 'disorder', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['did', 'you', 'know', 'these', 'facts', 'about', 'attention', 'deficit', 'disorder', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', 'many', 'still', 'perceive', 'ad', '##hd', 'as', '‘', 'fake', '?', '’', 'an', 'excellent', 'analysis', 'of', 'what', 'makes', 'diseases', 'socially', 'acceptable', '.']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', 'many', 'still', 'perceive', 'ad', '##hd', 'as', '‘', 'fake', '?', '’', 'an', 'excellent', 'analysis', 'of', 'what', 'makes', 'diseases', 'socially', 'acceptable', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', 'many', 'still', 'perceive', 'ad', '##hd', 'as', '‘', 'made', '-', 'up', '?', '’', 'an', 'in', '-', 'depth', 'analysis', 'of', 'why', 'some', 'diseases', 'are', 'more', 'socially', 'acceptable', 'than', 'others', '.']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', 'many', 'still', 'perceive', 'ad', '##hd', 'as', '‘', 'made', '-', 'up', '?', '’', 'an', 'in', '-', 'depth', 'analysis', 'of', 'why', 'some', 'diseases', 'are', 'more', 'socially', 'acceptable', 'than', 'others', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', 'many', 'still', 'perceive', 'ad', '##hd', 'as', '‘', 'made', '-', 'up', '?', '’', 'an', 'in', '-', 'depth', 'analysis', 'of', 'what', 'makes', 'some', 'diseases', 'more', 'socially', 'acceptable', 'than', 'others', '.']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', 'many', 'still', 'perceive', 'ad', '##hd', 'as', '‘', 'made', '-', 'up', '?', '’', 'an', 'in', '-', 'depth', 'analysis', 'of', 'what', 'makes', 'some', 'diseases', 'more', 'socially', 'acceptable', 'than', 'others', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['30', 'year', 'old', 'woman', '.', '.', '.', 'just', 'figured', 'out', 'i', 'probably', 'have', 'ad', '##hd', '.']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['30', 'year', 'old', 'woman', '.', '.', '.', 'just', 'figured', 'out', 'i', 'probably', 'have', 'ad', '##hd', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'adult', 'ad', '##hd', 'and', 'no', 'insurance', '.', 'i', 'found', 'this', 'medication', 'price', 'guide', 'helpful', 'in', 'looking', 'at', 'options', '.']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'adult', 'ad', '##hd', 'and', 'no', 'insurance', '.', 'i', 'found', 'this', 'medication', 'price', 'guide', 'helpful', 'in', 'looking', 'at', 'options', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'in', 'african', 'american', 'children', '-', 'associated', 'content', 'from', 'yahoo', '!', '-', 'associated', '##con', '##ten', '##t', '.', 'com']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'in', 'african', 'american', 'children', '-', 'associated', 'content', 'from', 'yahoo', '!', '-', 'associated', '##con', '##ten', '##t', '.', 'com']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'i', 'have', \"'\", 'ad', '##hd', \"'\", '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'i', 'have', \"'\", 'ad', '##hd', \"'\", '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['failing', 'everything', 'in', 'high', 'school', '.', 'please', 'help', 'me', ':', '(']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['failing', 'everything', 'in', 'high', 'school', '.', 'please', 'help', 'me', ':', '(']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'got', 'diagnosed', ',', 'now', 'on', 'concert', '##a', '.', 'what', 'should', 'i', 'expect', '?', 'i', 'made', 'a', 'post', 'probably', 'about', 'a', 'month', 'ago', 'about', 'my', 'struggles', 'and', 'how', 'i', 'realized', 'i', 'could', 'have', 'ad', '##hd', '.', 'well', ',', 'over', 'fall', 'break', 'i', 'got', 'tested', 'back', 'home', 'and', 'i', 'was', 'officially', 'diagnosed', '(', 'just', 'the', 'ina', '##tten', '##tive', 'sub', '-', 'type', ')', 'and', 'put', 'on', 'concert', '##a', '.', 'so', 'i', \"'\", 've', 'been', 'on', 'it', 'about', 'a', 'week', '.', 'classes', 'just', 'started', 'today', 'for', 'me', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'feeling', '.', 'i', 'want', 'to', 'think', 'it', \"'\", 's', 'working', 'but', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'expect', '.', 'the', 'only', 'feeling', 'that', 'is', 'different', 'is', 'that', 'i', 'feel', 'like', 'i', \"'\", 'm', 'breathing', 'lighter', ',', 'but', 'it', \"'\", 's', 'very', 'slight', '.', 'i', 'don', \"'\", 't', 'notice', 'a', 'difference', 'with', 'anything', 'else', 'yet', '.']\n",
      "INFO:__main__:Number of tokens: 153\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'got', 'diagnosed', ',', 'now', 'on', 'concert', '##a', '.', 'what', 'should', 'i', 'expect', '?', 'i', 'made', 'a', 'post', 'probably', 'about', 'a', 'month', 'ago', 'about', 'my', 'struggles', 'and', 'how', 'i', 'realized', 'i', 'could', 'have', 'ad', '##hd', '.', 'well', ',', 'over', 'fall', 'break', 'i', 'got', 'tested', 'back', 'home', 'and', 'i', 'was', 'officially', 'diagnosed', '(', 'just', 'the', 'ina', '##tten', '##tive', 'sub', '-', 'type', ')', 'and', 'put', 'on', 'concert', '##a', '.', 'so', 'i', \"'\", 've', 'been', 'on', 'it', 'about', 'a', 'week', '.', 'classes', 'just', 'started', 'today', 'for', 'me', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'feeling', '.', 'i', 'want', 'to', 'think', 'it', \"'\", 's', 'working', 'but', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'expect', '.', 'the', 'only', 'feeling', 'that', 'is', 'different', 'is', 'that', 'i', 'feel', 'like', 'i', \"'\", 'm', 'breathing', 'lighter', ',', 'but', 'it', \"'\", 's', 'very', 'slight', '.', 'i', 'don', \"'\", 't', 'notice', 'a', 'difference', 'with', 'anything', 'else', 'yet', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'think', 'i', \"'\", 'm', 'becoming', 'dependent', '.', '.', '.', '.', '.', '.', 'ok', 'so', 'i', \"'\", 've', 'been', 'on', 'concert', '##a', '36', '##mg', 'for', 'about', '6', 'months', 'now', ',', 'its', 'been', 'going', 'great', ',', 'my', 'overall', 'life', 'has', 'improved', 'whether', 'it', 'be', 'social', 'skills', ',', 'school', ',', 'weightlifting', 'and', 'over', 'all', 'mental', 'health', '.', 'but', 'lately', ',', 'my', 'dad', 'has', 'made', 'some', 'good', 'points', 'telling', 'me', 'that', 'i', 'can', \"'\", 't', 'be', 'on', 'this', 'forever', ',', 'that', 'i', \"'\", 'll', 'eventually', 'have', 'to', 'grow', 'out', '.', 'so', 'lately', 'during', 'the', 'end', 'of', 'the', 'week', ',', 'when', 'i', 'don', \"'\", 't', 'have', 'anything', 'important', 'to', 'do', ',', 'i', \"'\", 'll', 'skip', 'my', 'dose', 'to', 'see', 'how', 'i', \"'\", 'll', 'react', '.', 'sometimes', 'i', \"'\", 'll', 'take', 'one', 'day', 'or', 'two', 'days', 'straight', 'off', ',', 'i', 'hated', 'it', '.', 'and', 'its', 'not', 'even', 'the', 'fact', 'that', 'i', 'sub', '##con', '##cious', '##ly', 'knew', 'that', 'i', 'wasn', \"'\", 't', 'on', 'it', 'which', 'was', 'driving', 'my', 'mind', 'into', 'bel', '##ei', '##ving', 'that', 'i', 'needed', 'it', ',', 'sometimes', 'i', \"'\", 'd', 'forget', 'to', 'take', 'it', 'till', 'about', 'half', 'way', 'throughout', 'the', 'day', 'realizing', 'how', 'shitty', 'i', 'am', 'feeling', ',', 'how', 'depressed', 'i', 'am', '.', 'i', 'realized', 'that', 'without', 'my', 'daily', 'dose', 'of', 'concert', '##a', 'and', 'caf', '##fe', '##ine', ',', 'i', \"'\", 'm', 'a', 'completely', 'depressed', 'and', 'have', 'no', 'drive', 'for', 'anything', '.', 'what', 'do', 'i', 'do', '?', 'edit', ':', 'sorry', 'i', 'should', 'give', 'some', 'background', 'info', 'on', 'myself', 'i', \"'\", 'm', '20', ',', 'single', '(', 'never', 'been', 'in', 'a', 'relationship', ')', 'and', 'a', 'college', 'student', 'with', 'over', '##pro', '##tec', '##tive', 'parents', '.']\n",
      "INFO:__main__:Number of tokens: 267\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'think', 'i', \"'\", 'm', 'becoming', 'dependent', '.', '.', '.', '.', '.', '.', 'ok', 'so', 'i', \"'\", 've', 'been', 'on', 'concert', '##a', '36', '##mg', 'for', 'about', '6', 'months', 'now', ',', 'its', 'been', 'going', 'great', ',', 'my', 'overall', 'life', 'has', 'improved', 'whether', 'it', 'be', 'social', 'skills', ',', 'school', ',', 'weightlifting', 'and', 'over', 'all', 'mental', 'health', '.', 'but', 'lately', ',', 'my', 'dad', 'has', 'made', 'some', 'good', 'points', 'telling', 'me', 'that', 'i', 'can', \"'\", 't', 'be', 'on', 'this', 'forever', ',', 'that', 'i', \"'\", 'll', 'eventually', 'have', 'to', 'grow', 'out', '.', 'so', 'lately', 'during', 'the', 'end', 'of', 'the', 'week', ',', 'when', 'i', 'don', \"'\", 't', 'have', 'anything', 'important', 'to', 'do', ',', 'i', \"'\", 'll', 'skip', 'my', 'dose', 'to', 'see', 'how', 'i', \"'\", 'll', 'react', '.', 'sometimes', 'i', \"'\", 'll', 'take', 'one', 'day', 'or', 'two', 'days', 'straight', 'off', ',', 'i', 'hated', 'it', '.', 'and', 'its', 'not', 'even', 'the', 'fact', 'that', 'i', 'sub', '##con', '##cious', '##ly', 'knew', 'that', 'i', 'wasn', \"'\", 't', 'on', 'it', 'which', 'was', 'driving', 'my', 'mind', 'into', 'bel', '##ei', '##ving', 'that', 'i', 'needed', 'it', ',', 'sometimes', 'i', \"'\", 'd', 'forget', 'to', 'take', 'it', 'till', 'about', 'half', 'way', 'throughout', 'the', 'day', 'realizing', 'how', 'shitty', 'i', 'am', 'feeling', ',', 'how', 'depressed', 'i', 'am', '.', 'i', 'realized', 'that', 'without', 'my', 'daily', 'dose', 'of', 'concert', '##a', 'and', 'caf', '##fe', '##ine', ',', 'i', \"'\", 'm', 'a', 'completely', 'depressed', 'and', 'have', 'no', 'drive', 'for', 'anything', '.', 'what', 'do', 'i', 'do', '?', 'edit', ':', 'sorry', 'i', 'should', 'give', 'some', 'background', 'info', 'on', 'myself', 'i', \"'\", 'm', '20', ',', 'single', '(', 'never', 'been', 'in', 'a', 'relationship', ')', 'and', 'a', 'college', 'student', 'with', 'over', '##pro', '##tec', '##tive', 'parents', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'love', 'hyper', '##fo', '##cus', '##ing', '?', 'i', \"'\", 've', 'heard', 'people', 'talk', 'about', 'how', 'they', 'hate', 'it', ',', 'but', 'i', 'don', \"'\", 't', '.', 'it', 'feels', 'awesome', ',', 'and', 'it', \"'\", 's', 'tons', 'of', 'fun', '.', 'am', 'i', 'the', 'only', 'one', '?']\n",
      "INFO:__main__:Number of tokens: 45\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'love', 'hyper', '##fo', '##cus', '##ing', '?', 'i', \"'\", 've', 'heard', 'people', 'talk', 'about', 'how', 'they', 'hate', 'it', ',', 'but', 'i', 'don', \"'\", 't', '.', 'it', 'feels', 'awesome', ',', 'and', 'it', \"'\", 's', 'tons', 'of', 'fun', '.', 'am', 'i', 'the', 'only', 'one', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['when', 'your', 'brain', 'makes', 'a', 'connection', 'or', 'you', 'ask', 'yourself', 'a', 'question', ',', 'you', 'must', 'look', 'it', 'up', 'immediately', '.', 'i', 'find', 'this', 'is', 'one', 'of', 'my', 'most', 'distracting', 'events', '.', 'when', 'i', 'think', 'of', 'something', 'like', \"'\", 'i', 'wonder', 'how', 'reliable', 'my', 'car', 'is', '?', \"'\", 'or', \"'\", 'i', 'wonder', 'what', 'parts', 'of', 'my', 'diet', 'could', 'be', 'effect', '##ing', '_', '_', '_', '_', \"'\", 'or', \"'\", 'i', 'should', 'remember', 'to', 'pay', 'that', 'bill', '.', '.', '.', \"'\", 'i', 'tend', 'to', 'go', 'straight', 'to', 'the', 'internet', 'for', 'a', 'search', 'or', 'to', 'hit', 'up', 'the', 'online', 'banking', '.', 'i', 'will', 'be', 'working', 'on', 'something', 'at', 'work', ',', 'for', 'maybe', '10', '-', '15', 'minutes', ',', 'when', 'suddenly', 'bam', ',', 'thought', 'of', 'something', ',', 'brain', 'made', 'a', 'connection', ',', 'must', 'google', 'now', '.', 'anyone', 'else', 'have', 'this', 'issue', '?', 'i', 'am', 'trying', 'to', 'think', 'of', 'good', 'ways', 'to', 'combat', 'this', ',', 'but', 'it', \"'\", 's', 'one', 'of', 'those', 'things', ',', 'that', 'once', 'it', 'overcome', '##s', 'you', ',', 'there', \"'\", 's', 'no', 'turning', 'back', '.', 'it', \"'\", 's', 'rather', 'impossible', 'to', 'control', 'because', 'of', 'the', 'fact', 'that', 'when', 'it', 'happens', 'it', 'means', 'i', 'have', 'lost', 'control', '.', 'i', 'thought', 'of', 'maybe', 'having', 'a', 'place', 'where', 'i', 'can', 'write', 'down', 'the', 'things', 'i', 'want', 'to', 'look', 'up', 'later', ',', 'but', 'i', 'always', 'tend', 'to', 'neglect', 'to', 'use', 'this', 'method', 'even', 'if', 'i', 'tell', 'myself', 'that', 'i', 'will', '.', 'thoughts', 'or', 'ideas', '?']\n",
      "INFO:__main__:Number of tokens: 237\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['when', 'your', 'brain', 'makes', 'a', 'connection', 'or', 'you', 'ask', 'yourself', 'a', 'question', ',', 'you', 'must', 'look', 'it', 'up', 'immediately', '.', 'i', 'find', 'this', 'is', 'one', 'of', 'my', 'most', 'distracting', 'events', '.', 'when', 'i', 'think', 'of', 'something', 'like', \"'\", 'i', 'wonder', 'how', 'reliable', 'my', 'car', 'is', '?', \"'\", 'or', \"'\", 'i', 'wonder', 'what', 'parts', 'of', 'my', 'diet', 'could', 'be', 'effect', '##ing', '_', '_', '_', '_', \"'\", 'or', \"'\", 'i', 'should', 'remember', 'to', 'pay', 'that', 'bill', '.', '.', '.', \"'\", 'i', 'tend', 'to', 'go', 'straight', 'to', 'the', 'internet', 'for', 'a', 'search', 'or', 'to', 'hit', 'up', 'the', 'online', 'banking', '.', 'i', 'will', 'be', 'working', 'on', 'something', 'at', 'work', ',', 'for', 'maybe', '10', '-', '15', 'minutes', ',', 'when', 'suddenly', 'bam', ',', 'thought', 'of', 'something', ',', 'brain', 'made', 'a', 'connection', ',', 'must', 'google', 'now', '.', 'anyone', 'else', 'have', 'this', 'issue', '?', 'i', 'am', 'trying', 'to', 'think', 'of', 'good', 'ways', 'to', 'combat', 'this', ',', 'but', 'it', \"'\", 's', 'one', 'of', 'those', 'things', ',', 'that', 'once', 'it', 'overcome', '##s', 'you', ',', 'there', \"'\", 's', 'no', 'turning', 'back', '.', 'it', \"'\", 's', 'rather', 'impossible', 'to', 'control', 'because', 'of', 'the', 'fact', 'that', 'when', 'it', 'happens', 'it', 'means', 'i', 'have', 'lost', 'control', '.', 'i', 'thought', 'of', 'maybe', 'having', 'a', 'place', 'where', 'i', 'can', 'write', 'down', 'the', 'things', 'i', 'want', 'to', 'look', 'up', 'later', ',', 'but', 'i', 'always', 'tend', 'to', 'neglect', 'to', 'use', 'this', 'method', 'even', 'if', 'i', 'tell', 'myself', 'that', 'i', 'will', '.', 'thoughts', 'or', 'ideas', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['opposite', 'of', 'ad', '##hd', '?', 'i', \"'\", 'm', 'not', 'sure', 'what', 'to', 'call', 'it', '.', 'i', \"'\", 'm', 'not', 'sure', 'how', 'to', 'describe', 'how', 'my', 'brain', 'functions', '.', 'i', \"'\", 'm', 'always', 'thinking', 'about', 'more', 'things', 'at', 'once', 'than', 'i', 'can', 'control', ',', 'especially', 'during', 'conversation', '.', 'the', 'only', 'time', 'i', 'feel', 'like', 'i', 'can', 'actually', 'in', 'control', 'of', 'my', 'brain', 'is', 'when', 'i', \"'\", 'm', 'trying', 'to', 'solve', 'a', 'puzzle', 'or', 'learn', 'about', 'something', 'new', 'and', 'complex', ',', 'the', 'rest', 'of', 'the', 'time', 'i', \"'\", 'm', 'analyzing', 'and', 'thinking', 'about', 'things', 'unrelated', 'to', 'anything', 'going', 'on', 'around', 'me', '.', 'i', 'previously', 'e', '##qua', '##ted', 'this', 'to', 'having', 'social', 'anxiety', '.', 'i', 'actually', 'prefer', 'being', 'around', 'people', 'and', 'talking', ',', 'and', 'am', 'rather', 'good', 'at', 'reading', 'them', 'though', '.', 'the', 'anxiety', 'part', 'comes', 'from', 'the', 'fact', 'that', 'while', 'i', \"'\", 'm', 'speaking', 'with', 'them', ',', 'in', 'the', 'moment', ',', 'i', 'feel', 'that', 'i', \"'\", 'm', 'just', 'acting', 'on', 'instinct', 'while', 'my', 'thoughts', 'are', 'elsewhere', '.', 'depending', 'on', 'what', 'i', \"'\", 'm', 'doing', ',', 'i', 'either', 'lose', 'interest', 'close', 'to', 'immediately', 'or', 'become', 'en', '##th', '##ral', '##led', 'in', 'whatever', 'it', 'is', 'for', 'an', 'ex', '##or', '##bit', '##ant', 'amount', 'of', 'time', 'and', 'neglect', 'other', 'things', 'i', 'should', 'be', 'doing', ',', 'such', 'as', 'iron', '##ing', 'the', 'other', 'half', 'of', 'my', 'shirt', 'tomorrow', 'instead', 'of', 'writing', 'this', '.', 'i', \"'\", 've', 'searched', 'around', 'a', 'little', 'bit', 'online', ',', 'but', 'it', 'seems', 'to', 'me', 'like', 'i', 'meet', 'half', 'the', 'requirements', 'for', 'ad', '##hd', 'and', 'am', 'the', 'polar', 'opposite', 'for', 'the', 'other', 'half', '.', 'any', 'clues', '?', '?', '?', '?']\n",
      "INFO:__main__:Number of tokens: 264\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['opposite', 'of', 'ad', '##hd', '?', 'i', \"'\", 'm', 'not', 'sure', 'what', 'to', 'call', 'it', '.', 'i', \"'\", 'm', 'not', 'sure', 'how', 'to', 'describe', 'how', 'my', 'brain', 'functions', '.', 'i', \"'\", 'm', 'always', 'thinking', 'about', 'more', 'things', 'at', 'once', 'than', 'i', 'can', 'control', ',', 'especially', 'during', 'conversation', '.', 'the', 'only', 'time', 'i', 'feel', 'like', 'i', 'can', 'actually', 'in', 'control', 'of', 'my', 'brain', 'is', 'when', 'i', \"'\", 'm', 'trying', 'to', 'solve', 'a', 'puzzle', 'or', 'learn', 'about', 'something', 'new', 'and', 'complex', ',', 'the', 'rest', 'of', 'the', 'time', 'i', \"'\", 'm', 'analyzing', 'and', 'thinking', 'about', 'things', 'unrelated', 'to', 'anything', 'going', 'on', 'around', 'me', '.', 'i', 'previously', 'e', '##qua', '##ted', 'this', 'to', 'having', 'social', 'anxiety', '.', 'i', 'actually', 'prefer', 'being', 'around', 'people', 'and', 'talking', ',', 'and', 'am', 'rather', 'good', 'at', 'reading', 'them', 'though', '.', 'the', 'anxiety', 'part', 'comes', 'from', 'the', 'fact', 'that', 'while', 'i', \"'\", 'm', 'speaking', 'with', 'them', ',', 'in', 'the', 'moment', ',', 'i', 'feel', 'that', 'i', \"'\", 'm', 'just', 'acting', 'on', 'instinct', 'while', 'my', 'thoughts', 'are', 'elsewhere', '.', 'depending', 'on', 'what', 'i', \"'\", 'm', 'doing', ',', 'i', 'either', 'lose', 'interest', 'close', 'to', 'immediately', 'or', 'become', 'en', '##th', '##ral', '##led', 'in', 'whatever', 'it', 'is', 'for', 'an', 'ex', '##or', '##bit', '##ant', 'amount', 'of', 'time', 'and', 'neglect', 'other', 'things', 'i', 'should', 'be', 'doing', ',', 'such', 'as', 'iron', '##ing', 'the', 'other', 'half', 'of', 'my', 'shirt', 'tomorrow', 'instead', 'of', 'writing', 'this', '.', 'i', \"'\", 've', 'searched', 'around', 'a', 'little', 'bit', 'online', ',', 'but', 'it', 'seems', 'to', 'me', 'like', 'i', 'meet', 'half', 'the', 'requirements', 'for', 'ad', '##hd', 'and', 'am', 'the', 'polar', 'opposite', 'for', 'the', 'other', 'half', '.', 'any', 'clues', '?', '?', '?', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['seriously', ',', 'why', 'don', \"'\", 't', 'we', 'just', 'marry', 'the', 'two', 'communities', '&', 'have', '/', 'r', '/', 'add', '-', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['seriously', ',', 'why', 'don', \"'\", 't', 'we', 'just', 'marry', 'the', 'two', 'communities', '&', 'have', '/', 'r', '/', 'add', '-', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['lindsay', 'lo', '##han', 'latest', 'victim', 'of', 'ad', '##hd', 'mis', '##dia', '##gno', '##sis', 'and', 'add', '##eral', '##l', 'abuse', '-', 'quite', 'the', 'alarm', '##ist', 'article', '.']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['lindsay', 'lo', '##han', 'latest', 'victim', 'of', 'ad', '##hd', 'mis', '##dia', '##gno', '##sis', 'and', 'add', '##eral', '##l', 'abuse', '-', 'quite', 'the', 'alarm', '##ist', 'article', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', 'thought', 'on', 'down', '##vot', '##es', 'stolen', 'from', 'the', 'side', '##bar', 'of', 'r', '/', 'ama', ':', 'about', 'down', '##vot', '##es', ':', 'in', 'order', 'to', 'protect', 'minority', 'opinions', ',', 'comment', 'down', '##vot', '##es', 'have', 'been', 'disabled', 'via', 'cs', '##s', '.', 'without', 'accountability', ',', 'red', '##dit', '##quette', 'guidelines', 'mean', 'nothing', 'and', 'will', 'only', 'result', 'in', 'a', 'combination', 'of', 'unconscious', 'group', '##thi', '##nk', '(', 'the', 'human', 'mind', 'naturally', 'will', 'try', 'to', 'agree', 'with', 'the', 'majority', 'opinion', 'of', 'the', 'social', 'group', 'he', 'identifies', 'with', ')', 'and', 'de', 'facto', 'censorship', '(', 'down', '##vot', '##es', 'literally', 'hide', 'comments', 'from', 'view', ')', '.', 'combined', ',', 'this', 'will', 'lead', 'to', 'a', 'cycle', 'where', 'only', 'the', 'opinions', 'held', 'by', 'the', 'majority', 'will', 'considered', 'without', 'bias', '.']\n",
      "INFO:__main__:Number of tokens: 117\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', 'thought', 'on', 'down', '##vot', '##es', 'stolen', 'from', 'the', 'side', '##bar', 'of', 'r', '/', 'ama', ':', 'about', 'down', '##vot', '##es', ':', 'in', 'order', 'to', 'protect', 'minority', 'opinions', ',', 'comment', 'down', '##vot', '##es', 'have', 'been', 'disabled', 'via', 'cs', '##s', '.', 'without', 'accountability', ',', 'red', '##dit', '##quette', 'guidelines', 'mean', 'nothing', 'and', 'will', 'only', 'result', 'in', 'a', 'combination', 'of', 'unconscious', 'group', '##thi', '##nk', '(', 'the', 'human', 'mind', 'naturally', 'will', 'try', 'to', 'agree', 'with', 'the', 'majority', 'opinion', 'of', 'the', 'social', 'group', 'he', 'identifies', 'with', ')', 'and', 'de', 'facto', 'censorship', '(', 'down', '##vot', '##es', 'literally', 'hide', 'comments', 'from', 'view', ')', '.', 'combined', ',', 'this', 'will', 'lead', 'to', 'a', 'cycle', 'where', 'only', 'the', 'opinions', 'held', 'by', 'the', 'majority', 'will', 'considered', 'without', 'bias', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['daughter', 'has', 'ad', '##hd', ',', 'odd', ',', 'and', 'an', 'anxiety', 'disorder', '.', '.', '.', 'looking', 'for', 'advice', '.', 'my', '5', 'year', 'old', 'daughter', 'was', 'diagnosed', 'with', 'ad', '##hd', ',', 'odd', ',', 'and', 'didn', \"'\", 't', 'quite', 'meet', 'all', 'the', 'criteria', 'for', 'separation', 'anxiety', 'so', 'they', 'just', 'called', 'it', 'anxiety', 'disorder', '.', 'since', 'her', 'birthday', 'is', '9', 'days', 'past', 'the', 'cut', 'off', 'date', 'for', 'kindergarten', ',', 'she', 'did', 'not', 'start', 'school', 'this', 'year', '.', 'this', 'ended', 'up', 'being', 'a', 'blessing', 'as', 'it', 'gives', 'me', 'another', 'year', 'to', 'work', 'with', 'her', '.', 'she', 'is', 'highly', 'intelligent', '(', 'she', 'is', 'teaching', 'herself', 'to', 'read', ')', 'and', 'quite', 'the', 'handful', '.', 'she', 'sees', 'a', 'therapist', 'once', 'a', 'week', 'and', 'right', 'now', 'we', 'are', 'working', 'on', 'getting', 'a', 'handle', 'of', 'her', 'odd', '.', 'the', 'place', 'she', 'goes', 'to', 'is', 'awesome', 'as', 'they', 'believe', 'med', '##ica', '##ting', 'kids', 'is', 'a', 'last', 'resort', '.', 'this', 'means', 'she', 'is', 'not', 'taking', 'any', 'med', '##s', 'at', 'this', 'point', 'in', 'time', ',', 'but', 'it', 'might', 'change', 'if', 'she', 'is', 'still', 'having', 'major', 'issues', 'come', 'starting', 'school', 'next', 'year', '.', 'right', 'now', 'i', 'am', 'a', 'stay', 'at', 'home', 'mom', 'who', 'really', 'should', 'go', 'back', 'to', 'work', ',', 'but', 'with', 'everything', 'going', 'on', 'with', 'my', 'daughter', 'i', \"'\", 'm', 'really', 'he', '##sis', '##tan', '##t', '.', 'the', 'rest', 'of', 'our', 'family', 'includes', 'her', 'dad', ',', 'a', '17', 'year', 'old', 'brother', ',', 'a', '15', 'year', 'old', 'sister', '(', 'both', 'whom', 'have', 'a', 'different', 'dad', ')', ',', 'and', 'my', 'mom', '.', 'the', 'main', 'objective', 'of', 'my', 'post', 'is', 'to', 'get', 'advice', 'from', 'those', 'of', 'you', 'who', 'have', 'any', 'or', 'all', 'of', 'the', 'above', 'and', 'those', 'of', 'you', 'who', 'are', 'parents', '.', 'i', \"'\", 've', 'read', 'that', 'keeping', 'a', 'strict', 'schedule', 'for', 'them', 'really', 'helps', 'out', '.', '.', '.', 'which', 'is', 'something', 'really', 'hard', 'for', 'me', 'because', 'i', 'am', 'not', 'an', 'organized', 'person', 'by', 'any', 'means', '.', 'things', 'that', 'have', 'worked', '(', 'or', 'even', 'not', 'worked', ')', 'for', 'you', 'will', 'be', 'much', 'appreciated', '.']\n",
      "INFO:__main__:Number of tokens: 328\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['daughter', 'has', 'ad', '##hd', ',', 'odd', ',', 'and', 'an', 'anxiety', 'disorder', '.', '.', '.', 'looking', 'for', 'advice', '.', 'my', '5', 'year', 'old', 'daughter', 'was', 'diagnosed', 'with', 'ad', '##hd', ',', 'odd', ',', 'and', 'didn', \"'\", 't', 'quite', 'meet', 'all', 'the', 'criteria', 'for', 'separation', 'anxiety', 'so', 'they', 'just', 'called', 'it', 'anxiety', 'disorder', '.', 'since', 'her', 'birthday', 'is', '9', 'days', 'past', 'the', 'cut', 'off', 'date', 'for', 'kindergarten', ',', 'she', 'did', 'not', 'start', 'school', 'this', 'year', '.', 'this', 'ended', 'up', 'being', 'a', 'blessing', 'as', 'it', 'gives', 'me', 'another', 'year', 'to', 'work', 'with', 'her', '.', 'she', 'is', 'highly', 'intelligent', '(', 'she', 'is', 'teaching', 'herself', 'to', 'read', ')', 'and', 'quite', 'the', 'handful', '.', 'she', 'sees', 'a', 'therapist', 'once', 'a', 'week', 'and', 'right', 'now', 'we', 'are', 'working', 'on', 'getting', 'a', 'handle', 'of', 'her', 'odd', '.', 'the', 'place', 'she', 'goes', 'to', 'is', 'awesome', 'as', 'they', 'believe', 'med', '##ica', '##ting', 'kids', 'is', 'a', 'last', 'resort', '.', 'this', 'means', 'she', 'is', 'not', 'taking', 'any', 'med', '##s', 'at', 'this', 'point', 'in', 'time', ',', 'but', 'it', 'might', 'change', 'if', 'she', 'is', 'still', 'having', 'major', 'issues', 'come', 'starting', 'school', 'next', 'year', '.', 'right', 'now', 'i', 'am', 'a', 'stay', 'at', 'home', 'mom', 'who', 'really', 'should', 'go', 'back', 'to', 'work', ',', 'but', 'with', 'everything', 'going', 'on', 'with', 'my', 'daughter', 'i', \"'\", 'm', 'really', 'he', '##sis', '##tan', '##t', '.', 'the', 'rest', 'of', 'our', 'family', 'includes', 'her', 'dad', ',', 'a', '17', 'year', 'old', 'brother', ',', 'a', '15', 'year', 'old', 'sister', '(', 'both', 'whom', 'have', 'a', 'different', 'dad', ')', ',', 'and', 'my', 'mom', '.', 'the', 'main', 'objective', 'of', 'my', 'post', 'is', 'to', 'get', 'advice', 'from', 'those', 'of', 'you', 'who', 'have', 'any', 'or', 'all', 'of', 'the', 'above', 'and', 'those', 'of', 'you', 'who', 'are', 'parents', '.', 'i', \"'\", 've', 'read', 'that', 'keeping', 'a', 'strict', 'schedule', 'for', 'them', 'really', 'helps', 'out', '.', '.', '.', 'which', 'is', 'something', 'really', 'hard', 'for', 'me', 'because', 'i', 'am', 'not', 'an', 'organized', 'person', 'by', 'any', 'means', '.', 'things', 'that', 'have', 'worked', '(', 'or', 'even', 'not', 'worked', ')', 'for', 'you', 'will', 'be', 'much', 'appreciated', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'tips', 'on', 'surviving', 'meditation', '?', 'awhile', 'back', ',', 'after', 'doing', 'a', 'lot', 'of', 'reading', 'about', 'ad', '##hd', ',', 'one', 'of', 'the', 'things', 'that', 'really', 'impressed', 'me', 'was', 'the', 'amount', 'of', 'evidence', 'indicating', 'that', 'regular', 'meditation', 'could', 'go', 'a', 'long', 'way', 'towards', 'helping', 'improve', 'focus', ',', 'improve', 'the', 'ability', 'to', 'make', 'the', 'right', 'decision', 'even', 'when', 'it', \"'\", 's', 'difficult', ',', 'reduce', 'stress', '/', 'anxiety', ',', 'and', 'so', 'on', '.', 'i', 'resolved', 'to', 'sit', 'down', 'for', 'just', '5', 'minutes', 'a', 'day', ',', 'calmly', 'trying', 'to', 'just', 'experience', 'what', 'it', \"'\", 's', 'like', 'to', 'sit', 'calmly', '.', 'when', 'my', 'thoughts', 'started', 'to', 'wonder', ',', 'i', 'did', 'as', 'i', 'was', 'advised', '-', '-', 'i', 'noted', 'what', 'it', 'felt', 'like', 'when', 'my', 'thoughts', 'wandered', ',', 'and', 'went', 'back', 'to', 'trying', 'to', 'concentrate', 'on', 'my', 'breathing', ',', 'the', 'position', 'of', 'my', 'body', ',', 'and', 'so', 'on', '.', 'oh', 'my', 'god', 'what', 'horrible', 'torture', '!', 'i', 'thought', 'i', 'might', 'like', 'it', ',', 'but', 'i', 'seem', 'to', 'have', 'rebelled', 'against', 'it', '.', 'i', 'don', \"'\", 't', 'know', 'why', ',', 'but', 'by', 'the', 'end', 'of', 'that', '5', 'minutes', 'i', 'start', 'to', 'have', 'a', 'faster', 'heart', 'rate', ',', 'i', 'get', 'jump', '##y', ',', 'and', 'despite', 'my', 'best', 'efforts', ',', 'i', 'can', \"'\", 't', 'help', 'feeling', 'like', 'i', 'wasted', 'the', 'time', 'since', 'my', 'mind', 'wandered', 'far', 'too', 'much', '.', 'i', 'tried', 'cutting', 'it', 'down', 'to', 'shorter', 'times', ',', 'but', 'i', 'guess', 'my', 'initial', 'experience', 'has', 'created', 'an', '\"', 'u', '##gh', 'field', '\"', 'or', 'a', 'resistance', 'to', 'think', 'about', 'or', 'engage', 'in', 'the', 'behavior', ',', 'and', 'i', \"'\", 've', 'stopped', 'doing', 'it', '.', 'my', 'only', 'thought', 'it', 'so', 'make', 'it', 'somehow', 'reward', '##ing', '-', '-', 'e', '##g', ',', 'med', '##itate', 'for', '5', 'minutes', 'and', 'then', 'eat', 'an', 'ice', 'cream', 'sandwich', '.', 'alternately', ',', 'i', 'considered', 'med', '##itating', 'with', 'jelly', '##be', '##ans', 'or', 'something', '-', '-', 'i', 'know', 'some', 'sites', 'recommended', 'eating', 'your', 'meals', 'slowly', 'and', 'just', 'concentrating', 'on', 'that', ',', 'so', 'i', 'suppose', 'if', 'i', 'sit', 'down', 'with', 'some', 'hard', 'candy', '(', 'maybe', 'a', 'pepper', '##min', '##t', '?', ')', 'and', 'sucking', 'on', 'it', 'until', 'it', 'dissolve', '##s', 'while', 'trying', 'to', 'think', 'about', 'the', 'taste', 'might', 'help', '.', 'hmm', '##m', '.', '.', '.', 'okay', 'so', 'in', 'writing', 'my', 'question', ',', 'it', 'seems', 'i', \"'\", 've', 'discovered', 'that', 'i', 'may', 'already', 'have', 'some', 'answers', '.', 'anyway', ',', 'question', 'remains', ':', 'do', 'you', 'have', 'any', 'other', 'ideas', '?', 'do', 'you', 'think', 'the', 'ideas', 'i', \"'\", 've', 'offered', 'will', 'provide', 'some', 'benefits', 'of', 'meditation', 'without', 'making', 'the', 'experience', 'unpleasant', '?', 'do', 'you', 'think', 'the', 'entire', 'idea', 'of', 'meditation', 'is', 'bunk', '?', 'i', \"'\", 'd', 'love', 'to', 'hear', 'your', 'thoughts', '!']\n",
      "INFO:__main__:Number of tokens: 436\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'tips', 'on', 'surviving', 'meditation', '?', 'awhile', 'back', ',', 'after', 'doing', 'a', 'lot', 'of', 'reading', 'about', 'ad', '##hd', ',', 'one', 'of', 'the', 'things', 'that', 'really', 'impressed', 'me', 'was', 'the', 'amount', 'of', 'evidence', 'indicating', 'that', 'regular', 'meditation', 'could', 'go', 'a', 'long', 'way', 'towards', 'helping', 'improve', 'focus', ',', 'improve', 'the', 'ability', 'to', 'make', 'the', 'right', 'decision', 'even', 'when', 'it', \"'\", 's', 'difficult', ',', 'reduce', 'stress', '/', 'anxiety', ',', 'and', 'so', 'on', '.', 'i', 'resolved', 'to', 'sit', 'down', 'for', 'just', '5', 'minutes', 'a', 'day', ',', 'calmly', 'trying', 'to', 'just', 'experience', 'what', 'it', \"'\", 's', 'like', 'to', 'sit', 'calmly', '.', 'when', 'my', 'thoughts', 'started', 'to', 'wonder', ',', 'i', 'did', 'as', 'i', 'was', 'advised', '-', '-', 'i', 'noted', 'what', 'it', 'felt', 'like', 'when', 'my', 'thoughts', 'wandered', ',', 'and', 'went', 'back', 'to', 'trying', 'to', 'concentrate', 'on', 'my', 'breathing', ',', 'the', 'position', 'of', 'my', 'body', ',', 'and', 'so', 'on', '.', 'oh', 'my', 'god', 'what', 'horrible', 'torture', '!', 'i', 'thought', 'i', 'might', 'like', 'it', ',', 'but', 'i', 'seem', 'to', 'have', 'rebelled', 'against', 'it', '.', 'i', 'don', \"'\", 't', 'know', 'why', ',', 'but', 'by', 'the', 'end', 'of', 'that', '5', 'minutes', 'i', 'start', 'to', 'have', 'a', 'faster', 'heart', 'rate', ',', 'i', 'get', 'jump', '##y', ',', 'and', 'despite', 'my', 'best', 'efforts', ',', 'i', 'can', \"'\", 't', 'help', 'feeling', 'like', 'i', 'wasted', 'the', 'time', 'since', 'my', 'mind', 'wandered', 'far', 'too', 'much', '.', 'i', 'tried', 'cutting', 'it', 'down', 'to', 'shorter', 'times', ',', 'but', 'i', 'guess', 'my', 'initial', 'experience', 'has', 'created', 'an', '\"', 'u', '##gh', 'field', '\"', 'or', 'a', 'resistance', 'to', 'think', 'about', 'or', 'engage', 'in', 'the', 'behavior', ',', 'and', 'i', \"'\", 've', 'stopped', 'doing', 'it', '.', 'my', 'only', 'thought', 'it', 'so', 'make', 'it', 'somehow', 'reward', '##ing', '-', '-', 'e', '##g', ',', 'med', '##itate', 'for', '5', 'minutes', 'and', 'then', 'eat', 'an', 'ice', 'cream', 'sandwich', '.', 'alternately', ',', 'i', 'considered', 'med', '##itating', 'with', 'jelly', '##be', '##ans', 'or', 'something', '-', '-', 'i', 'know', 'some', 'sites', 'recommended', 'eating', 'your', 'meals', 'slowly', 'and', 'just', 'concentrating', 'on', 'that', ',', 'so', 'i', 'suppose', 'if', 'i', 'sit', 'down', 'with', 'some', 'hard', 'candy', '(', 'maybe', 'a', 'pepper', '##min', '##t', '?', ')', 'and', 'sucking', 'on', 'it', 'until', 'it', 'dissolve', '##s', 'while', 'trying', 'to', 'think', 'about', 'the', 'taste', 'might', 'help', '.', 'hmm', '##m', '.', '.', '.', 'okay', 'so', 'in', 'writing', 'my', 'question', ',', 'it', 'seems', 'i', \"'\", 've', 'discovered', 'that', 'i', 'may', 'already', 'have', 'some', 'answers', '.', 'anyway', ',', 'question', 'remains', ':', 'do', 'you', 'have', 'any', 'other', 'ideas', '?', 'do', 'you', 'think', 'the', 'ideas', 'i', \"'\", 've', 'offered', 'will', 'provide', 'some', 'benefits', 'of', 'meditation', 'without', 'making', 'the', 'experience', 'unpleasant', '?', 'do', 'you', 'think', 'the', 'entire', 'idea', 'of', 'meditation', 'is', 'bunk', '?', 'i', \"'\", 'd', 'love', 'to', 'hear', 'your', 'thoughts', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['and', 'it', 'works', 'every', 'time']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['and', 'it', 'works', 'every', 'time']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'prevent', 'your', 'anxiety', 'from', 'wreck', '##ing', 'havoc', 'on', 'motivation', '?', 'i', 'can', \"'\", 't', 'seem', 'to', 'do', 'anything', 'lately', '.', 'it', \"'\", 's', 'like', 'everything', 'is', 'a', 'huge', 'deal', 'and', 'i', \"'\", 'm', 'so', 'afraid', 'of', 'running', 'out', 'of', 'time', 'that', 'i', 'just', 'don', \"'\", 't', 'do', 'anything', '.', 'it', \"'\", 's', 'even', 'worse', 'because', 'i', 'don', \"'\", 't', 'feel', 'like', 'my', 'i', 'actually', 'know', 'how', 'long', 'any', 'particular', 'task', 'might', 'take', '.', 'and', 'since', 'i', \"'\", 've', 'already', 'pro', '##cr', '##ast', '##inated', 'i', \"'\", 'm', 'more', 'unlikely', 'to', 'have', 'enough', 'time', 'to', 'get', 'anything', 'done', '.', 'it', \"'\", 's', 'like', 'batting', 'cages', 'with', 'the', 'balls', 'como', '##ng', 'too', 'fast', '.', 'but', 'it', \"'\", 's', 'just', 'laundry', '!', 'but', 'since', 'i', 'put', 'it', 'off', 'it', \"'\", 's', 'either', 'no', 'clothes', 'but', 'get', 'homework', 'done', ',', 'or', 'don', \"'\", 't', 'do', 'homework', '&', 'have', 'nothing', 'to', 'wear', 'to', 'class', '.', 'so', 'instead', 'of', 'choosing', 'i', 'came', 'here', 'because', 'its', 'just', 'overwhelming', '.', 'jesus', '.', 'i', \"'\", 'm', 'a', 'graduate', 'student', ',', 'this', 'should', 'be', 'cake', '!', 'sorry', 'for', 'the', 'ran', '##t', ',', 'but', 'wanted', 'to', 'know', 'what', 'y', \"'\", 'all', 'do', '.']\n",
      "INFO:__main__:Number of tokens: 194\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'prevent', 'your', 'anxiety', 'from', 'wreck', '##ing', 'havoc', 'on', 'motivation', '?', 'i', 'can', \"'\", 't', 'seem', 'to', 'do', 'anything', 'lately', '.', 'it', \"'\", 's', 'like', 'everything', 'is', 'a', 'huge', 'deal', 'and', 'i', \"'\", 'm', 'so', 'afraid', 'of', 'running', 'out', 'of', 'time', 'that', 'i', 'just', 'don', \"'\", 't', 'do', 'anything', '.', 'it', \"'\", 's', 'even', 'worse', 'because', 'i', 'don', \"'\", 't', 'feel', 'like', 'my', 'i', 'actually', 'know', 'how', 'long', 'any', 'particular', 'task', 'might', 'take', '.', 'and', 'since', 'i', \"'\", 've', 'already', 'pro', '##cr', '##ast', '##inated', 'i', \"'\", 'm', 'more', 'unlikely', 'to', 'have', 'enough', 'time', 'to', 'get', 'anything', 'done', '.', 'it', \"'\", 's', 'like', 'batting', 'cages', 'with', 'the', 'balls', 'como', '##ng', 'too', 'fast', '.', 'but', 'it', \"'\", 's', 'just', 'laundry', '!', 'but', 'since', 'i', 'put', 'it', 'off', 'it', \"'\", 's', 'either', 'no', 'clothes', 'but', 'get', 'homework', 'done', ',', 'or', 'don', \"'\", 't', 'do', 'homework', '&', 'have', 'nothing', 'to', 'wear', 'to', 'class', '.', 'so', 'instead', 'of', 'choosing', 'i', 'came', 'here', 'because', 'its', 'just', 'overwhelming', '.', 'jesus', '.', 'i', \"'\", 'm', 'a', 'graduate', 'student', ',', 'this', 'should', 'be', 'cake', '!', 'sorry', 'for', 'the', 'ran', '##t', ',', 'but', 'wanted', 'to', 'know', 'what', 'y', \"'\", 'all', 'do', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dia', '##gno', '##sing', 'ad', '##hd', ':', 'what', 'every', 'parent', 'should', 'know']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dia', '##gno', '##sing', 'ad', '##hd', ':', 'what', 'every', 'parent', 'should', 'know']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['yesterday', ',', 'i', 'was', 'finally', 'diagnosed', 'with', 'ad', '##hd', '.', 'after', '19', 'years', 'of', 'struggling', 'on', 'my', 'own', ',', 'i', 'picked', 'up', 'some', 'strategies', 'to', 'cope', '.', 'come', 'share', 'yours', '!']\n",
      "INFO:__main__:Number of tokens: 31\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['yesterday', ',', 'i', 'was', 'finally', 'diagnosed', 'with', 'ad', '##hd', '.', 'after', '19', 'years', 'of', 'struggling', 'on', 'my', 'own', ',', 'i', 'picked', 'up', 'some', 'strategies', 'to', 'cope', '.', 'come', 'share', 'yours', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['if', 'add', '##eral', '##l', 'effects', 'me', 'more', ',', 'what', 'does', 'that', 'mean', '?', 'so', 'i', 'was', 'just', 'diagnosed', 'with', 'ad', '##hd', 'and', 'i', 'just', 'started', 'medication', '.', 'i', \"'\", 'm', 'in', 'college', ',', 'and', 'while', 'it', 'may', 'not', 'be', 'legal', ',', 'to', 'help', 'my', 'room', 'mate', 'out', 'i', 'let', 'him', 'use', 'a', 'pill', '.', 'he', \"'\", 's', 'mentioned', 'to', 'me', 'a', 'couple', 'of', 'times', 'how', 'surprised', 'he', 'is', 'that', 'i', \"'\", 'm', 'so', 'effect', '##ed', 'by', 'it', '.', 'i', \"'\", 'm', 'just', 'up', 'a', 'lot', 'more', '.', 'i', 'got', 'up', 'at', '5', 'this', 'morning', 'i', \"'\", 'm', 'going', 'to', 'bed', 'at', '3', '.', 'i', \"'\", 've', 'had', 'reduced', 'appetite', '.', 'honestly', 'none', 'of', 'these', 'things', 'have', 'been', 'bad', 'in', 'my', 'opinion', '.', 'i', 'don', \"'\", 't', 'feel', 'shaky', 'or', 'weird', ',', 'just', 'alert', 'and', 'focused', ',', 'and', 'i', 'can', 'get', 'a', 'lot', 'of', 'stuff', 'done', '.', 'but', 'he', 'doesn', \"'\", 't', 'seem', 'to', 'feel', 'much', 'at', 'all', ',', 'and', 'neither', 'does', 'really', 'anyone', 'else', 'i', 'know', '(', 'except', 'for', 'a', 'friend', 'who', 'was', 'also', 'recently', 'prescribed', 'the', 'medicine', '.', 'but', 'he', 'might', 'be', 'a', 'special', 'case', '.', ')', 'what', '##s', 'ups', 'with', 'this', '?', 'does', 'this', 'indicate', 'that', 'i', '*', 'don', \"'\", 't', '*', 'have', 'a', 'problem', 'actually', ',', 'or', 'that', 'i', '*', 'do', '*', '?', 'is', 'this', 'indicative', 'of', 'anything', '?']\n",
      "INFO:__main__:Number of tokens: 222\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['if', 'add', '##eral', '##l', 'effects', 'me', 'more', ',', 'what', 'does', 'that', 'mean', '?', 'so', 'i', 'was', 'just', 'diagnosed', 'with', 'ad', '##hd', 'and', 'i', 'just', 'started', 'medication', '.', 'i', \"'\", 'm', 'in', 'college', ',', 'and', 'while', 'it', 'may', 'not', 'be', 'legal', ',', 'to', 'help', 'my', 'room', 'mate', 'out', 'i', 'let', 'him', 'use', 'a', 'pill', '.', 'he', \"'\", 's', 'mentioned', 'to', 'me', 'a', 'couple', 'of', 'times', 'how', 'surprised', 'he', 'is', 'that', 'i', \"'\", 'm', 'so', 'effect', '##ed', 'by', 'it', '.', 'i', \"'\", 'm', 'just', 'up', 'a', 'lot', 'more', '.', 'i', 'got', 'up', 'at', '5', 'this', 'morning', 'i', \"'\", 'm', 'going', 'to', 'bed', 'at', '3', '.', 'i', \"'\", 've', 'had', 'reduced', 'appetite', '.', 'honestly', 'none', 'of', 'these', 'things', 'have', 'been', 'bad', 'in', 'my', 'opinion', '.', 'i', 'don', \"'\", 't', 'feel', 'shaky', 'or', 'weird', ',', 'just', 'alert', 'and', 'focused', ',', 'and', 'i', 'can', 'get', 'a', 'lot', 'of', 'stuff', 'done', '.', 'but', 'he', 'doesn', \"'\", 't', 'seem', 'to', 'feel', 'much', 'at', 'all', ',', 'and', 'neither', 'does', 'really', 'anyone', 'else', 'i', 'know', '(', 'except', 'for', 'a', 'friend', 'who', 'was', 'also', 'recently', 'prescribed', 'the', 'medicine', '.', 'but', 'he', 'might', 'be', 'a', 'special', 'case', '.', ')', 'what', '##s', 'ups', 'with', 'this', '?', 'does', 'this', 'indicate', 'that', 'i', '*', 'don', \"'\", 't', '*', 'have', 'a', 'problem', 'actually', ',', 'or', 'that', 'i', '*', 'do', '*', '?', 'is', 'this', 'indicative', 'of', 'anything', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'fl', 'parents', 'with', 'experience', 'dealing', 'with', 'public', 'school', 'systems', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'fl', 'parents', 'with', 'experience', 'dealing', 'with', 'public', 'school', 'systems', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', '##s', 'your', 'favorite', 'scheduling', '/', 'time', 'management', 'software', '?', 'i', 'have', 'been', 'reading', 'that', 'the', 'intentional', 'planning', 'of', 'one', \"'\", 's', 'day', 'is', 'a', 'good', 'thing', 'for', 'those', 'with', 'ad', '##hd', '.', 'just', 'wondering', 'if', 'you', 'do', 'this', 'electronically', ',', 'what', 'is', 'your', 'favorite', 'software', 'tool', '?']\n",
      "INFO:__main__:Number of tokens: 48\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', '##s', 'your', 'favorite', 'scheduling', '/', 'time', 'management', 'software', '?', 'i', 'have', 'been', 'reading', 'that', 'the', 'intentional', 'planning', 'of', 'one', \"'\", 's', 'day', 'is', 'a', 'good', 'thing', 'for', 'those', 'with', 'ad', '##hd', '.', 'just', 'wondering', 'if', 'you', 'do', 'this', 'electronically', ',', 'what', 'is', 'your', 'favorite', 'software', 'tool', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'some', 'techniques', 'i', 'can', 'use', 'to', 'observe', 'the', 'effects', 'of', 'my', 'med', '##s', '?']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'some', 'techniques', 'i', 'can', 'use', 'to', 'observe', 'the', 'effects', 'of', 'my', 'med', '##s', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'experience', 'using', 'pot', 'to', 'self', 'med', '##icate', '(', 'from', 'a', 'virtually', 'non', '-', 'smoke', '##r', ')', 'i', \"'\", 'm', '28', 'years', 'old', 'and', 'i', 'can', 'honestly', 'i', \"'\", 've', 'never', 'tried', 'smoking', 'pot', '-', 'just', 'not', 'my', 'thing', 'and', 'have', 'plenty', 'of', 'friends', 'who', 'do', '.', 'anyway', ',', 'last', 'night', 'i', 'tried', 'smoking', 'with', 'a', 'buddy', 'of', 'mine', '.', 'i', \"'\", 'm', 'a', 'college', 'gr', '##ad', ',', 'with', 'a', 'great', 'career', 'but', 'have', 'always', 'felt', 'held', 'back', 'from', 'my', 'true', 'potential', 'because', 'my', 'mind', 'is', 'always', 'racing', '.', 'i', 'figured', 'i', \"'\", 'd', 'give', 'pot', 'a', 'try', 'to', 'see', 'if', 'it', 'helped', 'any', '.', 'well', ',', 'it', 'did', '.', 'sort', '##a', '.', 'smoking', 'pot', 'really', 'slowed', 'me', 'down', '.', 'in', 'a', 'good', 'way', '.', 'all', 'my', 'anxiety', 'and', 'tension', 'went', 'away', '.', 'i', 'usually', 'clench', 'my', 'jaw', 'for', 'no', 'reason', 'and', 'have', 'managed', 'to', 'damage', 'my', 'teeth', 'because', 'of', 'that', '.', 'yesterday', 'was', 'seriously', 'one', 'of', 'the', 'only', 'times', 'i', 'remember', 'truly', 'relaxing', '.', 'the', 'other', 'side', 'of', 'the', 'coin', 'is', 'that', 'i', 'couldn', \"'\", 't', 'do', 'anything', '.', 'i', 'was', 'too', 'stone', '##d', 'and', 'just', 'sat', 'on', 'my', 'couch', '.', 'walking', 'to', 'get', 'a', 'glass', 'of', 'water', 'turned', 'into', 'a', 'journey', '.', 'halfway', 'to', 'get', 'water', 'i', 'forgot', 'what', 'i', 'had', 'gone', 'for', '.', 'while', 'i', 'appreciate', 'the', 'release', 'of', 'tension', ',', 'i', 'wouldn', \"'\", 't', 'give', 'it', 'up', 'for', 'the', 'ability', 'to', 'be', 'aware', 'of', 'my', 'so', '##rro', '##und', '##ings', '.', 'that', \"'\", 's', 'how', 'it', 'affected', 'me', '.', 'anyone', 'have', 'any', 'similar', 'stories', '?', 't', '##l', ';', 'dr', 'i', 'smoked', 'pot', ',', 'it', 'helped', 'me', 'relax', ',', 'but', 'i', 'wouldn', \"'\", 't', 'keep', 'doing', 'it', '.', 'ninja', 'edit', ':', 'pot', 'is', 'legal', 'in', 'california', 'for', 'the', 'record', 'so', 'this', 'doesn', \"'\", 't', 'violate', 'side', '##bar', 'rules']\n",
      "INFO:__main__:Number of tokens: 300\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'experience', 'using', 'pot', 'to', 'self', 'med', '##icate', '(', 'from', 'a', 'virtually', 'non', '-', 'smoke', '##r', ')', 'i', \"'\", 'm', '28', 'years', 'old', 'and', 'i', 'can', 'honestly', 'i', \"'\", 've', 'never', 'tried', 'smoking', 'pot', '-', 'just', 'not', 'my', 'thing', 'and', 'have', 'plenty', 'of', 'friends', 'who', 'do', '.', 'anyway', ',', 'last', 'night', 'i', 'tried', 'smoking', 'with', 'a', 'buddy', 'of', 'mine', '.', 'i', \"'\", 'm', 'a', 'college', 'gr', '##ad', ',', 'with', 'a', 'great', 'career', 'but', 'have', 'always', 'felt', 'held', 'back', 'from', 'my', 'true', 'potential', 'because', 'my', 'mind', 'is', 'always', 'racing', '.', 'i', 'figured', 'i', \"'\", 'd', 'give', 'pot', 'a', 'try', 'to', 'see', 'if', 'it', 'helped', 'any', '.', 'well', ',', 'it', 'did', '.', 'sort', '##a', '.', 'smoking', 'pot', 'really', 'slowed', 'me', 'down', '.', 'in', 'a', 'good', 'way', '.', 'all', 'my', 'anxiety', 'and', 'tension', 'went', 'away', '.', 'i', 'usually', 'clench', 'my', 'jaw', 'for', 'no', 'reason', 'and', 'have', 'managed', 'to', 'damage', 'my', 'teeth', 'because', 'of', 'that', '.', 'yesterday', 'was', 'seriously', 'one', 'of', 'the', 'only', 'times', 'i', 'remember', 'truly', 'relaxing', '.', 'the', 'other', 'side', 'of', 'the', 'coin', 'is', 'that', 'i', 'couldn', \"'\", 't', 'do', 'anything', '.', 'i', 'was', 'too', 'stone', '##d', 'and', 'just', 'sat', 'on', 'my', 'couch', '.', 'walking', 'to', 'get', 'a', 'glass', 'of', 'water', 'turned', 'into', 'a', 'journey', '.', 'halfway', 'to', 'get', 'water', 'i', 'forgot', 'what', 'i', 'had', 'gone', 'for', '.', 'while', 'i', 'appreciate', 'the', 'release', 'of', 'tension', ',', 'i', 'wouldn', \"'\", 't', 'give', 'it', 'up', 'for', 'the', 'ability', 'to', 'be', 'aware', 'of', 'my', 'so', '##rro', '##und', '##ings', '.', 'that', \"'\", 's', 'how', 'it', 'affected', 'me', '.', 'anyone', 'have', 'any', 'similar', 'stories', '?', 't', '##l', ';', 'dr', 'i', 'smoked', 'pot', ',', 'it', 'helped', 'me', 'relax', ',', 'but', 'i', 'wouldn', \"'\", 't', 'keep', 'doing', 'it', '.', 'ninja', 'edit', ':', 'pot', 'is', 'legal', 'in', 'california', 'for', 'the', 'record', 'so', 'this', 'doesn', \"'\", 't', 'violate', 'side', '##bar', 'rules']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', 'trade', 'off', '.', 'a', 'never', '-', 'ending', 'stream', '.', '.', '.', 'of', 'saber', '-', 'tooth', '##ed', 'tiger', '.', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', 'trade', 'off', '.', 'a', 'never', '-', 'ending', 'stream', '.', '.', '.', 'of', 'saber', '-', 'tooth', '##ed', 'tiger', '.', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['drawing', 'board', '-', 'trail', 'off', '(', 'what', 'ad', '##hd', 'is', 'like', ')']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['drawing', 'board', '-', 'trail', 'off', '(', 'what', 'ad', '##hd', 'is', 'like', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['st', '##rate', '##rra', 'dos', '##age', 'question', 't', '##l', ';', 'dr', ':', 'doc', 'wanted', 'to', 'put', 'me', 'on', 'full', 'adult', 'dose', ',', 'i', \"'\", 'm', 'built', 'like', 'a', 'kid', 'so', 'i', 'want', 'to', 'ti', '##tra', '##te', 'up', 'and', 'only', 'use', 'the', 'full', 'dose', 'if', 'i', 'need', '/', 'can', 'tolerate', 'it', '.', 'i', 'finally', 'got', 'in', 'to', 'see', 'a', 'psychiatrist', 'today', ',', 'and', 'he', 'confirmed', 'both', 'the', 'diagnosis', 'and', 'also', 'that', 'i', \"'\", 'm', 'at', 'the', 'point', 'that', 'i', 'do', 'need', 'med', '##s', '.', 'we', 'agreed', 'on', 'st', '##rate', '##rra', ',', 'but', 'we', 'had', 'a', 'pretty', 'long', 'discussion', 'about', 'the', 'dos', '##ing', '.', 'i', \"'\", 'm', 'about', '100', '##lb', '##s', 'on', 'a', 'good', 'day', ',', 'so', 'i', \"'\", 'm', 'concerned', 'about', 'taking', 'large', 'doses', 'of', 'any', 'medication', '.', 'the', 'full', 'adult', 'dose', 'is', '80', '##mg', '/', 'day', ',', 'and', 'he', 'wanted', 'to', 'start', 'me', 'on', 'that', 'right', 'away', '.', 'my', 'brother', 'also', 'takes', 'st', '##rate', '##rra', 'and', 'he', 'needs', 'to', 'divide', 'his', 'doses', 'because', 'of', 'the', 'side', 'effects', '-', 'and', 'he', 'out', '##weig', '##hs', 'me', 'by', '50', '-', '60', 'lbs', '.', 'i', 'flat', 'out', 'refused', 'to', 'start', 'at', '80', '##mg', 'and', 'after', 'listening', 'to', 'my', 'concerns', ',', 'he', 'agreed', 'to', 'start', 'at', '40', '##mg', '/', 'day', 'and', 'work', 'up', 'to', '60', '##mg', ',', 'then', 'possibly', '80', '##mg', 'if', 'i', \"'\", 'm', 'doing', 'ok', '.', 'this', 'is', 'with', 'my', 'full', 'understanding', 'that', 'it', 'may', 'take', 'longer', 'to', 'get', 'the', 'maximal', 'benefit', '.', 'so', 'i', 'have', 'two', 'questions', ':', '-', 'any', 'other', 'adults', 'doing', 'fine', 'with', 'less', 'than', '80', '##mg', '/', 'day', '?', '-', 'if', 'so', ',', 'how', 'did', 'you', 'arrive', 'at', 'that', 'dose', '?', 'was', 'it', 'because', 'of', 'side', 'effects', 'or', 'because', 'you', 'found', 'you', 'were', 'able', 'to', 'get', 'the', 'full', 'benefit', 'with', 'the', 'smaller', 'dose', '?']\n",
      "INFO:__main__:Number of tokens: 294\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['st', '##rate', '##rra', 'dos', '##age', 'question', 't', '##l', ';', 'dr', ':', 'doc', 'wanted', 'to', 'put', 'me', 'on', 'full', 'adult', 'dose', ',', 'i', \"'\", 'm', 'built', 'like', 'a', 'kid', 'so', 'i', 'want', 'to', 'ti', '##tra', '##te', 'up', 'and', 'only', 'use', 'the', 'full', 'dose', 'if', 'i', 'need', '/', 'can', 'tolerate', 'it', '.', 'i', 'finally', 'got', 'in', 'to', 'see', 'a', 'psychiatrist', 'today', ',', 'and', 'he', 'confirmed', 'both', 'the', 'diagnosis', 'and', 'also', 'that', 'i', \"'\", 'm', 'at', 'the', 'point', 'that', 'i', 'do', 'need', 'med', '##s', '.', 'we', 'agreed', 'on', 'st', '##rate', '##rra', ',', 'but', 'we', 'had', 'a', 'pretty', 'long', 'discussion', 'about', 'the', 'dos', '##ing', '.', 'i', \"'\", 'm', 'about', '100', '##lb', '##s', 'on', 'a', 'good', 'day', ',', 'so', 'i', \"'\", 'm', 'concerned', 'about', 'taking', 'large', 'doses', 'of', 'any', 'medication', '.', 'the', 'full', 'adult', 'dose', 'is', '80', '##mg', '/', 'day', ',', 'and', 'he', 'wanted', 'to', 'start', 'me', 'on', 'that', 'right', 'away', '.', 'my', 'brother', 'also', 'takes', 'st', '##rate', '##rra', 'and', 'he', 'needs', 'to', 'divide', 'his', 'doses', 'because', 'of', 'the', 'side', 'effects', '-', 'and', 'he', 'out', '##weig', '##hs', 'me', 'by', '50', '-', '60', 'lbs', '.', 'i', 'flat', 'out', 'refused', 'to', 'start', 'at', '80', '##mg', 'and', 'after', 'listening', 'to', 'my', 'concerns', ',', 'he', 'agreed', 'to', 'start', 'at', '40', '##mg', '/', 'day', 'and', 'work', 'up', 'to', '60', '##mg', ',', 'then', 'possibly', '80', '##mg', 'if', 'i', \"'\", 'm', 'doing', 'ok', '.', 'this', 'is', 'with', 'my', 'full', 'understanding', 'that', 'it', 'may', 'take', 'longer', 'to', 'get', 'the', 'maximal', 'benefit', '.', 'so', 'i', 'have', 'two', 'questions', ':', '-', 'any', 'other', 'adults', 'doing', 'fine', 'with', 'less', 'than', '80', '##mg', '/', 'day', '?', '-', 'if', 'so', ',', 'how', 'did', 'you', 'arrive', 'at', 'that', 'dose', '?', 'was', 'it', 'because', 'of', 'side', 'effects', 'or', 'because', 'you', 'found', 'you', 'were', 'able', 'to', 'get', 'the', 'full', 'benefit', 'with', 'the', 'smaller', 'dose', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['und', '##ia', '##gno', '##sed', 'thus', 'far', ',', 'looking', 'for', 'some', 'suggestions', '.', 'i', \"'\", 've', 'known', 'for', 'a', 'few', 'years', ',', 'just', 'friends', 'in', 'the', 'medical', 'and', 'psychological', 'profession', 'giving', 'an', 'evaluation', 'but', 'nothing', 'on', 'record', '.', 'it', \"'\", 's', 'finally', 'come', 'to', 'the', 'point', 'that', 'it', \"'\", 's', 'interfering', 'with', 'my', 'work', 'immensely', 'and', 'i', 'can', \"'\", 't', 'sat', '##ura', '##te', 'myself', 'in', 'something', 'work', '-', 'focused', 'or', 'complete', 'anything', 'i', 'need', 'to', '.', 'in', 'my', 'field', ',', 'neurological', 'disorders', 'and', 'prescription', '##s', 'are', 'frowned', 'upon', 'heavily', '.', 'what', 'i', \"'\", 'm', 'asking', 'is', 'what', 'is', 'the', 'most', 'effective', 'treatment', 'barr', '##ing', 'amp', '##het', '##amine', '##s', '?', 'edit', ':', 'also', ',', 'do', 'i', 'go', 'to', 'a', 'doctor', ',', 'psychologist', ',', 'or', 'psychiatrist', '?']\n",
      "INFO:__main__:Number of tokens: 124\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['und', '##ia', '##gno', '##sed', 'thus', 'far', ',', 'looking', 'for', 'some', 'suggestions', '.', 'i', \"'\", 've', 'known', 'for', 'a', 'few', 'years', ',', 'just', 'friends', 'in', 'the', 'medical', 'and', 'psychological', 'profession', 'giving', 'an', 'evaluation', 'but', 'nothing', 'on', 'record', '.', 'it', \"'\", 's', 'finally', 'come', 'to', 'the', 'point', 'that', 'it', \"'\", 's', 'interfering', 'with', 'my', 'work', 'immensely', 'and', 'i', 'can', \"'\", 't', 'sat', '##ura', '##te', 'myself', 'in', 'something', 'work', '-', 'focused', 'or', 'complete', 'anything', 'i', 'need', 'to', '.', 'in', 'my', 'field', ',', 'neurological', 'disorders', 'and', 'prescription', '##s', 'are', 'frowned', 'upon', 'heavily', '.', 'what', 'i', \"'\", 'm', 'asking', 'is', 'what', 'is', 'the', 'most', 'effective', 'treatment', 'barr', '##ing', 'amp', '##het', '##amine', '##s', '?', 'edit', ':', 'also', ',', 'do', 'i', 'go', 'to', 'a', 'doctor', ',', 'psychologist', ',', 'or', 'psychiatrist', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['people', 'who', 'wish', 'they', 'had', 'ad', '##hd', 'i', 'want', 'to', 'kill', 'them', '.', 'not', 'really', ',', 'but', 'oh', 'god', '.', 'they', 'make', 'me', 'even', 'more', 'mad', 'than', 'people', 'who', 'say', 'add', '/', 'ad', '##hd', 'is', 'a', 'myth', '.', 'i', 'just', 'hate', 'it', 'when', 'i', 'tell', 'someone', 'i', 'have', 'it', '(', 'because', 'sometimes', 'the', 'situation', 'comes', 'up', ',', 'i', 'don', '##t', 'run', 'around', 'saying', 'i', 'have', 'a', 'disorder', 'for', 'fun', ',', 'as', 'some', 'people', 'seem', 'to', 'think', '.', '.', '.', ')', 'and', 'they', 'go', '\"', 'oh', '!', 'man', 'i', 'wish', 'i', 'had', 'that', '.', '.', '.', 'you', 'guys', 'are', 'so', 'hyper', 'and', 'fun', 'and', 'creative', '!', '\"', '.', 'okay', ',', 'sure', 'that', 'is', 'sort', '##a', 'cool', '.', 'except', 'that', 'the', 'hyper', '##act', '##ivity', 'can', \"'\", 't', 'really', 'be', 'turned', 'off', ',', 'it', 'makes', 'me', 'seem', 'annoying', 'to', 'people', '.', 'i', \"'\", 'm', 'a', 'huge', 'attention', 'seeker', ',', 'which', 'isn', \"'\", 't', 'nec', '##ces', '##ari', '##ly', 'my', 'fault', ',', 'but', 'i', 'have', 'to', 'control', 'it', '(', 'which', 'sucks', ')', '.', 'i', \"'\", 'm', 'creative', 'because', 'i', 'can', 'tell', 'long', 'elaborate', 'stories', 'if', 'you', 'just', 'give', 'me', 'a', 'word', 'to', 'base', 'it', 'around', ',', 'i', 'love', 'writing', '.', 'stories', ',', 'notes', ',', 'descriptions', ',', 'etc', '.', 'i', \"'\", 'm', 'also', 'a', 'great', 'reader', ',', 'i', 'think', 'words', 'are', 'something', 'that', 'i', 'hyper', '##fo', '##cus', 'on', '(', 'never', 'used', 'that', 'word', ',', 'if', '##fy', 'on', 'what', 'it', 'means', ',', 'sorry', 'if', 'i', 'used', 'it', 'wrong', ')', '.', 'but', 'still', ',', 'you', 'can', 'do', 'these', 'things', 'and', 'not', 'be', 'ad', '##hd', '/', 'add', '.', 'it', 'makes', 'me', 'do', 'terrible', 'in', 'school', '(', 'i', 'pray', 'for', 'c', \"'\", 's', '.', ')', 'and', 'i', 'just', 'des', '##pis', '##e', 'chores', '.', 'i', 'would', 'definitely', 'rather', 'sit', 'in', 'a', 'room', 'and', 'stare', 'at', 'a', 'wall', 'for', 'hours', 'than', 'do', 'my', 'chores', ',', 'which', 'my', 'dad', 'just', 'can', \"'\", 't', 'understand', '.', 'my', 'friends', 'get', 'pissed', 'at', 'me', 'because', 'i', 'forget', 'what', 'they', 'said', 'right', 'after', 'they', 'said', 'it', ',', 'and', 'i', 'interrupt', 'a', 'lot', '.', 'oh', ',', 'and', 'i', 'now', 'have', 'an', 'amp', '##het', '##amine', 'addiction', 'because', 'of', 'medication', '.', '*', '*', 't', '##l', ';', 'dr', 'fuck', 'you', 'wanna', '##bes', '*', '*']\n",
      "INFO:__main__:Number of tokens: 363\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['people', 'who', 'wish', 'they', 'had', 'ad', '##hd', 'i', 'want', 'to', 'kill', 'them', '.', 'not', 'really', ',', 'but', 'oh', 'god', '.', 'they', 'make', 'me', 'even', 'more', 'mad', 'than', 'people', 'who', 'say', 'add', '/', 'ad', '##hd', 'is', 'a', 'myth', '.', 'i', 'just', 'hate', 'it', 'when', 'i', 'tell', 'someone', 'i', 'have', 'it', '(', 'because', 'sometimes', 'the', 'situation', 'comes', 'up', ',', 'i', 'don', '##t', 'run', 'around', 'saying', 'i', 'have', 'a', 'disorder', 'for', 'fun', ',', 'as', 'some', 'people', 'seem', 'to', 'think', '.', '.', '.', ')', 'and', 'they', 'go', '\"', 'oh', '!', 'man', 'i', 'wish', 'i', 'had', 'that', '.', '.', '.', 'you', 'guys', 'are', 'so', 'hyper', 'and', 'fun', 'and', 'creative', '!', '\"', '.', 'okay', ',', 'sure', 'that', 'is', 'sort', '##a', 'cool', '.', 'except', 'that', 'the', 'hyper', '##act', '##ivity', 'can', \"'\", 't', 'really', 'be', 'turned', 'off', ',', 'it', 'makes', 'me', 'seem', 'annoying', 'to', 'people', '.', 'i', \"'\", 'm', 'a', 'huge', 'attention', 'seeker', ',', 'which', 'isn', \"'\", 't', 'nec', '##ces', '##ari', '##ly', 'my', 'fault', ',', 'but', 'i', 'have', 'to', 'control', 'it', '(', 'which', 'sucks', ')', '.', 'i', \"'\", 'm', 'creative', 'because', 'i', 'can', 'tell', 'long', 'elaborate', 'stories', 'if', 'you', 'just', 'give', 'me', 'a', 'word', 'to', 'base', 'it', 'around', ',', 'i', 'love', 'writing', '.', 'stories', ',', 'notes', ',', 'descriptions', ',', 'etc', '.', 'i', \"'\", 'm', 'also', 'a', 'great', 'reader', ',', 'i', 'think', 'words', 'are', 'something', 'that', 'i', 'hyper', '##fo', '##cus', 'on', '(', 'never', 'used', 'that', 'word', ',', 'if', '##fy', 'on', 'what', 'it', 'means', ',', 'sorry', 'if', 'i', 'used', 'it', 'wrong', ')', '.', 'but', 'still', ',', 'you', 'can', 'do', 'these', 'things', 'and', 'not', 'be', 'ad', '##hd', '/', 'add', '.', 'it', 'makes', 'me', 'do', 'terrible', 'in', 'school', '(', 'i', 'pray', 'for', 'c', \"'\", 's', '.', ')', 'and', 'i', 'just', 'des', '##pis', '##e', 'chores', '.', 'i', 'would', 'definitely', 'rather', 'sit', 'in', 'a', 'room', 'and', 'stare', 'at', 'a', 'wall', 'for', 'hours', 'than', 'do', 'my', 'chores', ',', 'which', 'my', 'dad', 'just', 'can', \"'\", 't', 'understand', '.', 'my', 'friends', 'get', 'pissed', 'at', 'me', 'because', 'i', 'forget', 'what', 'they', 'said', 'right', 'after', 'they', 'said', 'it', ',', 'and', 'i', 'interrupt', 'a', 'lot', '.', 'oh', ',', 'and', 'i', 'now', 'have', 'an', 'amp', '##het', '##amine', 'addiction', 'because', 'of', 'medication', '.', '*', '*', 't', '##l', ';', 'dr', 'fuck', 'you', 'wanna', '##bes', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['danger', 'of', 'diagnosis', '-', 'beliefs', 'can', 'influence', 'anything', ',', 'even', 'the', 'way', 'we', 'interpret', 'everyday', 'mis', '##ha', '##ps', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['danger', 'of', 'diagnosis', '-', 'beliefs', 'can', 'influence', 'anything', ',', 'even', 'the', 'way', 'we', 'interpret', 'everyday', 'mis', '##ha', '##ps', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'computer', 'ate', 'my', 'homework', '-', 'dr', '.', 'jain', 'discusses', 'internet', 'addiction', 'and', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'computer', 'ate', 'my', 'homework', '-', 'dr', '.', 'jain', 'discusses', 'internet', 'addiction', 'and', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'guidance', 'to', 'whether', 'or', 'not', 'i', 'have', 'add', 'i', 'need', 'help', '.', 'first', 'i', \"'\", 'll', 'list', 'my', 'problems', ':', 'i', 'make', 'a', 'ton', 'of', 'careless', 'mistakes', ',', 'and', 'have', 'to', 'check', 'work', 'multiple', 'times', '.', 'i', 'get', 'anxious', 'really', 'easily', 'and', 'generally', 'worry', 'about', 'irrational', 'things', 'sometimes', 'my', 'brain', 'feels', 'really', 'fuzzy', 'and', 'my', 'thoughts', 'aren', \"'\", 't', 'right', ',', 'as', 'if', 'i', 'can', \"'\", 't', 'think', 'correctly', 'or', 'i', 'am', 'missing', 'tons', 'of', 'stuff', 'right', 'in', 'front', 'of', 'me', '.', 'i', 'day', '##dre', '##am', ',', 'a', 'lot', '.', 'generally', 'i', 'am', 'really', 'absent', '-', 'minded', 'and', 'up', 'in', 'the', 'clouds', 'per', 'se', '.', 'i', 'generally', 'have', 'so', 'little', 'awareness', 'around', 'me', 'that', 'i', 'give', 'no', 'thought', 'to', 'others', ',', 'when', 'i', 'look', 'at', 'someone', 'my', 'mind', 'is', 'already', 'on', 'something', 'else', ',', 'meaning', 'i', 'rarely', 'even', 'make', 'judgement', 'on', 'people', '.', 'i', 'have', 'very', 'little', 'motivation', '.', 'i', 'always', 'start', 'projects', 'and', 'never', 'finish', '.', 'i', 'rarely', 'even', 'start', 'projects', 'i', 'want', 'to', 'do', '.', 'i', 'spend', 'hours', 'on', 'end', 'on', 'the', 'internet', ',', 'or', 'playing', 'video', 'games', ',', 'if', 'i', 'am', 'not', ',', 'i', 'am', 'ob', '##ses', '##sing', 'over', 'an', 'activity', 'just', 'to', 'keep', 'myself', 'from', 'being', 'bored', '.', 'i', 'forget', 'about', 'my', 'friends', ',', 'i', 'don', \"'\", 't', 'remember', 'to', 'call', 'people', ',', 'forget', 'i', 'am', 'friends', 'with', 'people', ',', 'and', 'often', 'don', \"'\", 't', 'have', 'the', 'will', 'to', 'hang', 'out', 'with', 'people', '.', 'i', 'haven', \"'\", 't', 'read', 'a', 'book', 'on', 'my', 'own', 'that', 'wasn', \"'\", 't', 'an', 'assignment', 'for', 'school', 'since', 'the', 'middle', 'of', '9th', 'grade', ',', 'i', 'am', 'in', 'the', 'middle', 'of', '11th', 'grade', '.', 'i', 'fi', '##dget', 'constantly', 'in', 'school', ',', 'i', 'am', 'loud', ',', 'and', 'get', 'yelled', 'at', 'by', 'teachers', 'for', 'disturbing', 'the', 'class', 'i', 'have', 'trouble', 'organizing', 'myself', 'and', 'stay', 'in', 'a', 'constant', 'mess', '.', 'i', 'forget', 'things', 'very', 'easily', ',', 'dates', ',', 'appointments', '.', 'not', 'even', 'easily', 'almost', ',', 'i', 'always', 'forget', 'things', 'and', 'its', 'almost', 'a', 'given', 'that', 'i', 'will', '.', 'people', 'think', 'i', 'don', \"'\", 't', 'care', ',', 'but', 'they', 'just', 'don', \"'\", 't', 'realize', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'keep', 'up', '.', '\\\\', 'edit', ':', 'i', 'would', 'answer', 'all', 'the', 'responses', ',', 'but', 'i', 'really', 'don', \"'\", 't', 'feel', 'like', 'it', '.', 'ha', '##ha', '.', 'i', 'am', 'going', 'to', 'see', 'a', 'psychiatrist', '(', 'my', 'mom', 'set', 'it', 'up', ',', 'ya', '##y', 'mom', '##s', '!', ')', 'and', 'i', 'will', 'be', 'sure', 'to', 'show', 'them', 'this', 'list', '.', 'that', 'seems', 'to', 'be', 'the', 'consensus', '.']\n",
      "INFO:__main__:Number of tokens: 421\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'guidance', 'to', 'whether', 'or', 'not', 'i', 'have', 'add', 'i', 'need', 'help', '.', 'first', 'i', \"'\", 'll', 'list', 'my', 'problems', ':', 'i', 'make', 'a', 'ton', 'of', 'careless', 'mistakes', ',', 'and', 'have', 'to', 'check', 'work', 'multiple', 'times', '.', 'i', 'get', 'anxious', 'really', 'easily', 'and', 'generally', 'worry', 'about', 'irrational', 'things', 'sometimes', 'my', 'brain', 'feels', 'really', 'fuzzy', 'and', 'my', 'thoughts', 'aren', \"'\", 't', 'right', ',', 'as', 'if', 'i', 'can', \"'\", 't', 'think', 'correctly', 'or', 'i', 'am', 'missing', 'tons', 'of', 'stuff', 'right', 'in', 'front', 'of', 'me', '.', 'i', 'day', '##dre', '##am', ',', 'a', 'lot', '.', 'generally', 'i', 'am', 'really', 'absent', '-', 'minded', 'and', 'up', 'in', 'the', 'clouds', 'per', 'se', '.', 'i', 'generally', 'have', 'so', 'little', 'awareness', 'around', 'me', 'that', 'i', 'give', 'no', 'thought', 'to', 'others', ',', 'when', 'i', 'look', 'at', 'someone', 'my', 'mind', 'is', 'already', 'on', 'something', 'else', ',', 'meaning', 'i', 'rarely', 'even', 'make', 'judgement', 'on', 'people', '.', 'i', 'have', 'very', 'little', 'motivation', '.', 'i', 'always', 'start', 'projects', 'and', 'never', 'finish', '.', 'i', 'rarely', 'even', 'start', 'projects', 'i', 'want', 'to', 'do', '.', 'i', 'spend', 'hours', 'on', 'end', 'on', 'the', 'internet', ',', 'or', 'playing', 'video', 'games', ',', 'if', 'i', 'am', 'not', ',', 'i', 'am', 'ob', '##ses', '##sing', 'over', 'an', 'activity', 'just', 'to', 'keep', 'myself', 'from', 'being', 'bored', '.', 'i', 'forget', 'about', 'my', 'friends', ',', 'i', 'don', \"'\", 't', 'remember', 'to', 'call', 'people', ',', 'forget', 'i', 'am', 'friends', 'with', 'people', ',', 'and', 'often', 'don', \"'\", 't', 'have', 'the', 'will', 'to', 'hang', 'out', 'with', 'people', '.', 'i', 'haven', \"'\", 't', 'read', 'a', 'book', 'on', 'my', 'own', 'that', 'wasn', \"'\", 't', 'an', 'assignment', 'for', 'school', 'since', 'the', 'middle', 'of', '9th', 'grade', ',', 'i', 'am', 'in', 'the', 'middle', 'of', '11th', 'grade', '.', 'i', 'fi', '##dget', 'constantly', 'in', 'school', ',', 'i', 'am', 'loud', ',', 'and', 'get', 'yelled', 'at', 'by', 'teachers', 'for', 'disturbing', 'the', 'class', 'i', 'have', 'trouble', 'organizing', 'myself', 'and', 'stay', 'in', 'a', 'constant', 'mess', '.', 'i', 'forget', 'things', 'very', 'easily', ',', 'dates', ',', 'appointments', '.', 'not', 'even', 'easily', 'almost', ',', 'i', 'always', 'forget', 'things', 'and', 'its', 'almost', 'a', 'given', 'that', 'i', 'will', '.', 'people', 'think', 'i', 'don', \"'\", 't', 'care', ',', 'but', 'they', 'just', 'don', \"'\", 't', 'realize', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'keep', 'up', '.', '\\\\', 'edit', ':', 'i', 'would', 'answer', 'all', 'the', 'responses', ',', 'but', 'i', 'really', 'don', \"'\", 't', 'feel', 'like', 'it', '.', 'ha', '##ha', '.', 'i', 'am', 'going', 'to', 'see', 'a', 'psychiatrist', '(', 'my', 'mom', 'set', 'it', 'up', ',', 'ya', '##y', 'mom', '##s', '!', ')', 'and', 'i', 'will', 'be', 'sure', 'to', 'show', 'them', 'this', 'list', '.', 'that', 'seems', 'to', 'be', 'the', 'consensus', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['visual', 'and', 'auditory', 'processing', 'disorders', '-', 'portrait', 'health', 'centers', 'ad', '##hd', 'blog']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['visual', 'and', 'auditory', 'processing', 'disorders', '-', 'portrait', 'health', 'centers', 'ad', '##hd', 'blog']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'first', 'day', 'on', 'add', '##eral', '##l', '(', 'legal', ')', 'it', 'reminded', 'me', 'of', 'a', 'story', 'my', 'dad', 'once', 'told', 'me', '.', '\"', 'when', 'i', 'was', 'young', 'i', 'always', 'crashed', 'into', 'things', 'and', 'got', 'hurt', '-', 'eventually', 'the', 'doctors', 'realized', 'very', 'soon', 'that', 'i', 'had', 'ho', '##rre', '##ndo', '##us', 'eyes', '##ight', '.', 'when', 'i', 'was', '6', '-', '8', 'years', 'old', 'i', 'got', 'my', 'very', 'first', 'pair', 'of', 'glasses', 'that', 'were', 'custom', 'for', 'me', '.', 'he', 'hadn', \"'\", 't', 'even', 'know', 'that', 'leaves', 'were', 'not', 'just', 'a', 'bush', '##ily', 'single', 'entity', ',', 'this', 'was', 'the', 'first', 'time', 'he', 'had', 'noticed', 'the', 'edges', 'and', 'distinctions', '.', 'today', 'was', 'one', 'of', 'the', 'best', 'days', 'of', 'my', 'life', '.', 'i', 'was', 'able', 'to', 'get', 'to', 'class', ',', 'actually', 'pay', 'attention', ',', 'social', '##ize', 'with', 'neighbors', 'much', 'more', ',', 'i', 'did', 'all', 'my', 'household', 'chores', ',', 'basically', 'everything', 'but', 'actual', 'school', '##work', '.', 'this', 'was', 'one', 'of', 'the', 'first', 'days', 'in', 'my', 'life', 'where', 'i', 'was', 'able', 'to', 'just', 'be', 'me', '.', 'words', 'do', 'not', 'do', 'justice', 'to', 'a', 'veil', 'that', 'has', 'been', 'over', 'your', 'eyes', 'your', 'entire', 'life', 'being', 'lifted', 'up', '.', 'kind', 'of', 'an', 'all', '##ego', '##ry', 'of', 'the', 'cave', 'metaphor', '.', 'i', 'just', 'got', 'prescribed', '10', '##mg', 'x', '##r', ',', 'it', 'didn', \"'\", 't', \"'\", 'do', 'much', 'and', 'swim', 'has', 'done', 'way', 'to', 'much', 'of', 'this', 'drug', 'before', 'but', 'for', 'recreational', 'purposes', '.', 'i', 'took', '2', '10', '##mg', 'x', '##r', 'and', 'voyage', '##d', 'out', 'into', 'the', 'world', '.', 'the', '10', '##s', 'do', 'nothing', 'for', 'me', 'from', 'experience', ',', 'so', 'we', 'will', 'see', 'what', 'happens', 'next', '.', 'it', 'helps', 'as', 'well', 'that', 'i', 'have', 'k', '##lon', '##op', '##in', 'to', 'keep', 'the', 'anxiety', 'down', 'and', 'ambient', 'to', 'help', 'me', 'sleep', 'at', 'night', '.', 'my', 'psychiatrist', 'is', 'an', 'amazing', 'woman', 'and', 'has', 'been', 'helping', 'me', 'for', '3', 'years', '.']\n",
      "INFO:__main__:Number of tokens: 304\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'first', 'day', 'on', 'add', '##eral', '##l', '(', 'legal', ')', 'it', 'reminded', 'me', 'of', 'a', 'story', 'my', 'dad', 'once', 'told', 'me', '.', '\"', 'when', 'i', 'was', 'young', 'i', 'always', 'crashed', 'into', 'things', 'and', 'got', 'hurt', '-', 'eventually', 'the', 'doctors', 'realized', 'very', 'soon', 'that', 'i', 'had', 'ho', '##rre', '##ndo', '##us', 'eyes', '##ight', '.', 'when', 'i', 'was', '6', '-', '8', 'years', 'old', 'i', 'got', 'my', 'very', 'first', 'pair', 'of', 'glasses', 'that', 'were', 'custom', 'for', 'me', '.', 'he', 'hadn', \"'\", 't', 'even', 'know', 'that', 'leaves', 'were', 'not', 'just', 'a', 'bush', '##ily', 'single', 'entity', ',', 'this', 'was', 'the', 'first', 'time', 'he', 'had', 'noticed', 'the', 'edges', 'and', 'distinctions', '.', 'today', 'was', 'one', 'of', 'the', 'best', 'days', 'of', 'my', 'life', '.', 'i', 'was', 'able', 'to', 'get', 'to', 'class', ',', 'actually', 'pay', 'attention', ',', 'social', '##ize', 'with', 'neighbors', 'much', 'more', ',', 'i', 'did', 'all', 'my', 'household', 'chores', ',', 'basically', 'everything', 'but', 'actual', 'school', '##work', '.', 'this', 'was', 'one', 'of', 'the', 'first', 'days', 'in', 'my', 'life', 'where', 'i', 'was', 'able', 'to', 'just', 'be', 'me', '.', 'words', 'do', 'not', 'do', 'justice', 'to', 'a', 'veil', 'that', 'has', 'been', 'over', 'your', 'eyes', 'your', 'entire', 'life', 'being', 'lifted', 'up', '.', 'kind', 'of', 'an', 'all', '##ego', '##ry', 'of', 'the', 'cave', 'metaphor', '.', 'i', 'just', 'got', 'prescribed', '10', '##mg', 'x', '##r', ',', 'it', 'didn', \"'\", 't', \"'\", 'do', 'much', 'and', 'swim', 'has', 'done', 'way', 'to', 'much', 'of', 'this', 'drug', 'before', 'but', 'for', 'recreational', 'purposes', '.', 'i', 'took', '2', '10', '##mg', 'x', '##r', 'and', 'voyage', '##d', 'out', 'into', 'the', 'world', '.', 'the', '10', '##s', 'do', 'nothing', 'for', 'me', 'from', 'experience', ',', 'so', 'we', 'will', 'see', 'what', 'happens', 'next', '.', 'it', 'helps', 'as', 'well', 'that', 'i', 'have', 'k', '##lon', '##op', '##in', 'to', 'keep', 'the', 'anxiety', 'down', 'and', 'ambient', 'to', 'help', 'me', 'sleep', 'at', 'night', '.', 'my', 'psychiatrist', 'is', 'an', 'amazing', 'woman', 'and', 'has', 'been', 'helping', 'me', 'for', '3', 'years', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'tips', 'for', 'writing', 'long', '(', 'is', '##h', '.', '.', '1500', 'words', '/', 'about', '7', 'pages', ')', 'essays', 'in', 'a', 'short', 'amount', 'of', 'time', '(', 'due', 'at', 'midnight', 'tomorrow', ',', 'so', '24', 'hours', ')', '?', 'i', 'was', 'diagnosed', 'earlier', 'this', 'spring', '.', 'ever', 'since', 'i', 'started', 'university', '(', 'and', 'i', \"'\", 'm', 'a', 'third', 'year', 'now', '.', '.', ')', ',', 'i', \"'\", 've', 'been', 'having', 'more', 'and', 'more', 'trouble', 'writing', 'essays', '.', 'even', 'when', 'i', 'try', 'to', 'not', 'pro', '##cr', '##ast', '##inate', 'and', 'i', 'really', 'attempt', 'to', 'start', 'earlier', ',', 'i', 'sit', 'down', ',', 'stare', 'at', 'the', 'empty', 'page', 'and', 'my', 'brain', 'just', '.', '.', '.', '.', 'i', 'don', \"'\", 't', 'even', 'know', '.', 'i', 'just', 'have', 'so', 'many', 'issues', 'with', 'writing', 'papers', ',', 'i', 'don', \"'\", 't', 'even', 'know', 'where', 'to', 'start', 'talking', 'about', 'them', '.', 'i', 'look', 'up', 'sources', 'and', 'try', 'and', 'read', 'some', 'things', '.', 'and', 'then', 'i', 'stare', 'at', 'my', 'empty', 'page', 'again', 'and', 'try', 'to', 'vaguely', 'write', 'some', 'intro', 'sentences', '.', 'and', 'this', 'will', 'just', 'continue', 'for', 'hours', ',', 'even', 'if', 'i', \"'\", 'm', 'not', 'distracted', 'by', 'the', 'internet', '.', 'or', 'my', 'brain', 'will', 'just', 'run', 'off', 'and', 'just', 'be', 'like', 'tr', '##olo', '##lo', '##lo', '##l', '.', 'i', 'think', 'my', 'main', 'issue', 'is', 'starting', 'the', 'essay', '.', '(', 'then', 'again', ',', 'continuing', 'it', 'is', 'pretty', 'difficult', 'as', 'well', ',', 'of', 'course', ')', 'my', 'bad', 'work', 'habits', 'in', 'this', 'case', 'are', 'mostly', 'pro', '##cr', '##ast', '##inating', 'till', 'the', 'night', 'before', 'and', 'trying', 'to', 'work', 'on', 'it', 'by', 'staying', 'up', 'all', 'night', '.', '.', 'but', 'i', 'end', 'up', 'do', '##zing', 'off', 'every', '5', 'minutes', 'and', 'not', 'getting', 'shit', 'done', '.', 'even', 'for', 'short', 'writing', 'assignments', ',', 'i', 'end', 'up', 'doing', 'a', 'really', 'shitty', 'job', 'literally', 'minutes', 'before', 'i', 'have', 'to', 'leave', 'for', 'class', '.', 'and', 'of', 'course', ',', 'i', 'end', 'up', 'feeling', 'like', 'shit', 'about', 'myself', 'for', 'pro', '##cr', '##ast', '##inating', 'and', 'doing', 'a', 'bad', 'job', '.', '/', 'vicious', 'cycle', 'most', 'of', 'us', 'are', 'probably', 'familiar', 'with', '.', '.', '.', '?', 'so', 'anyway', ',', 'i', 'guess', 'i', \"'\", 'm', 'just', 'asking', 'for', 'tips', 'on', 'how', 'you', 'guys', 'handle', 'these', 'sort', 'of', 'long', '-', 'is', '##h', 'paper', 'writing', 'situations', '.', 'in', 'a', 'short', '##ish', 'amount', 'of', 'time', ',', 'since', 'i', 'have', 'a', 'little', 'less', 'than', '24', 'hours', '.', 'and', 'yes', ',', 'that', 'is', 'a', 'short', 'amount', 'of', 'time', 'for', 'me', ';', '_', '_', '_', '_', ';', 'feel', 'free', 'to', 'share', 'your', 'stories', '/', 'similar', 'situations', '!', 'thanks', 'ahead', 'of', 'time', '.', 'and', 'sorry', 'if', 'this', 'post', 'is', 'all', 'over', 'the', 'place', '.', '[', 'p', '.', 's', '.', 'the', 'paper', 'is', 'for', 'an', 'asian', 'american', 'fiction', 'class', 'and', 'it', \"'\", 's', 'the', 'final', 'paper', 'for', 'a', 'book', 'we', '(', 'are', 'supposed', 'to', 'have', ')', 'read', ',', 'if', 'that', \"'\", 's', 'relevant', '.', 'i', 'have', 'the', 'ebook', '.', ']', '*', '*', 'edit', ':', '*', '*', 'taking', 'a', 'break', 'from', 'writing', '.', 'thanks', 'for', 'the', 'comments', ',', 'guys', '!', 'i', 'just', 'made', 'it', 'to', 'the', 'second', 'page', '(', 'single', 'spaced', '!', ')', '.', 'i', \"'\", 'm', 'going', 'to', 'wait', 'till', 'i', 'get', 'to', 'the', 'bottom', 'of', 'the', 'second', 'page', 'to', 'double', 'space', 'it', '.', '.', 'reward', '?', 'ha', '##ha', '.', 'i', 'made', 'a', 'vague', 'schedule', 'to', 'write', 'about', 'one', 'page', 'per', 'hour', ',', 'so', 'if', 'my', 'rate', 'stays', 'like', 'this', ',', 'i', 'should', 'finish', '!', 'we', \"'\", 'll', 'see', '.', '.', '.', 'i', 'shall', 'report', 'back', '.']\n",
      "INFO:__main__:Number of tokens: 566\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['any', 'tips', 'for', 'writing', 'long', '(', 'is', '##h', '.', '.', '1500', 'words', '/', 'about', '7', 'pages', ')', 'essays', 'in', 'a', 'short', 'amount', 'of', 'time', '(', 'due', 'at', 'midnight', 'tomorrow', ',', 'so', '24', 'hours', ')', '?', 'i', 'was', 'diagnosed', 'earlier', 'this', 'spring', '.', 'ever', 'since', 'i', 'started', 'university', '(', 'and', 'i', \"'\", 'm', 'a', 'third', 'year', 'now', '.', '.', ')', ',', 'i', \"'\", 've', 'been', 'having', 'more', 'and', 'more', 'trouble', 'writing', 'essays', '.', 'even', 'when', 'i', 'try', 'to', 'not', 'pro', '##cr', '##ast', '##inate', 'and', 'i', 'really', 'attempt', 'to', 'start', 'earlier', ',', 'i', 'sit', 'down', ',', 'stare', 'at', 'the', 'empty', 'page', 'and', 'my', 'brain', 'just', '.', '.', '.', '.', 'i', 'don', \"'\", 't', 'even', 'know', '.', 'i', 'just', 'have', 'so', 'many', 'issues', 'with', 'writing', 'papers', ',', 'i', 'don', \"'\", 't', 'even', 'know', 'where', 'to', 'start', 'talking', 'about', 'them', '.', 'i', 'look', 'up', 'sources', 'and', 'try', 'and', 'read', 'some', 'things', '.', 'and', 'then', 'i', 'stare', 'at', 'my', 'empty', 'page', 'again', 'and', 'try', 'to', 'vaguely', 'write', 'some', 'intro', 'sentences', '.', 'and', 'this', 'will', 'just', 'continue', 'for', 'hours', ',', 'even', 'if', 'i', \"'\", 'm', 'not', 'distracted', 'by', 'the', 'internet', '.', 'or', 'my', 'brain', 'will', 'just', 'run', 'off', 'and', 'just', 'be', 'like', 'tr', '##olo', '##lo', '##lo', '##l', '.', 'i', 'think', 'my', 'main', 'issue', 'is', 'starting', 'the', 'essay', '.', '(', 'then', 'again', ',', 'continuing', 'it', 'is', 'pretty', 'difficult', 'as', 'well', ',', 'of', 'course', ')', 'my', 'bad', 'work', 'habits', 'in', 'this', 'case', 'are', 'mostly', 'pro', '##cr', '##ast', '##inating', 'till', 'the', 'night', 'before', 'and', 'trying', 'to', 'work', 'on', 'it', 'by', 'staying', 'up', 'all', 'night', '.', '.', 'but', 'i', 'end', 'up', 'do', '##zing', 'off', 'every', '5', 'minutes', 'and', 'not', 'getting', 'shit', 'done', '.', 'even', 'for', 'short', 'writing', 'assignments', ',', 'i', 'end', 'up', 'doing', 'a', 'really', 'shitty', 'job', 'literally', 'minutes', 'before', 'i', 'have', 'to', 'leave', 'for', 'class', '.', 'and', 'of', 'course', ',', 'i', 'end', 'up', 'feeling', 'like', 'shit', 'about', 'myself', 'for', 'pro', '##cr', '##ast', '##inating', 'and', 'doing', 'a', 'bad', 'job', '.', '/', 'vicious', 'cycle', 'most', 'of', 'us', 'are', 'probably', 'familiar', 'with', '.', '.', '.', '?', 'so', 'anyway', ',', 'i', 'guess', 'i', \"'\", 'm', 'just', 'asking', 'for', 'tips', 'on', 'how', 'you', 'guys', 'handle', 'these', 'sort', 'of', 'long', '-', 'is', '##h', 'paper', 'writing', 'situations', '.', 'in', 'a', 'short', '##ish', 'amount', 'of', 'time', ',', 'since', 'i', 'have', 'a', 'little', 'less', 'than', '24', 'hours', '.', 'and', 'yes', ',', 'that', 'is', 'a', 'short', 'amount', 'of', 'time', 'for', 'me', ';', '_', '_', '_', '_', ';', 'feel', 'free', 'to', 'share', 'your', 'stories', '/', 'similar', 'situations', '!', 'thanks', 'ahead', 'of', 'time', '.', 'and', 'sorry', 'if', 'this', 'post', 'is', 'all', 'over', 'the', 'place', '.', '[', 'p', '.', 's', '.', 'the', 'paper', 'is', 'for', 'an', 'asian', 'american', 'fiction', 'class', 'and', 'it', \"'\", 's', 'the', 'final', 'paper', 'for', 'a', 'book', 'we', '(', 'are', 'supposed', 'to', 'have', ')', 'read', ',', 'if', 'that', \"'\", 's', 'relevant', '.', 'i', 'have', 'the', 'ebook', '.', ']', '*', '*', 'edit', ':', '*', '*', 'taking', 'a', 'break', 'from', 'writing', '.', 'thanks', 'for', 'the', 'comments', ',', 'guys', '!', 'i', 'just', 'made', 'it', 'to', 'the', 'second', 'page', '(', 'single', 'spaced', '!', ')', '.', 'i', \"'\", 'm', 'going', 'to', 'wait', 'till', 'i', 'get', 'to'], ['the', 'bottom', 'of', 'the', 'second', 'page', 'to', 'double', 'space', 'it', '.', '.', 'reward', '?', 'ha', '##ha', '.', 'i', 'made', 'a', 'vague', 'schedule', 'to', 'write', 'about', 'one', 'page', 'per', 'hour', ',', 'so', 'if', 'my', 'rate', 'stays', 'like', 'this', ',', 'i', 'should', 'finish', '!', 'we', \"'\", 'll', 'see', '.', '.', '.', 'i', 'shall', 'report', 'back', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['somebody', 'please', 'help', 'straighten', 'out', 'this', 'idiot']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['somebody', 'please', 'help', 'straighten', 'out', 'this', 'idiot']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'have', 'ad', '##hd', '+', 'autism', 'or', 'know', 'the', 'difference', 'between', 'the', 'two', '?', 'i', \"'\", 've', 'just', 'been', 'noticing', 'that', 'they', 'seem', 'a', 'bit', 'similar', '.', 'i', 'have', 'ad', '##hd', '(', 'the', 'ina', '##tten', '##tive', 'type', ')', ',', 'but', 'i', \"'\", 've', 'been', 'wondering', 'if', 'perhaps', 'i', \"'\", 'm', 'slightly', 'au', '##tist', '##ic', 'too', '.', 'for', 'example', ',', 'i', 'had', 'speech', 'pathology', 'all', 'throughout', 'elementary', 'school', ',', 'and', 'i', \"'\", 'm', 'wondering', 'whether', 'this', 'is', 'more', 'characteristic', 'with', 'autism', '/', 'as', '##per', '##ger', \"'\", 's', 'than', 'it', 'is', 'with', 'ad', '##hd', '.', 'also', ',', 'even', 'now', ',', 'sometimes', 'it', 'takes', 'me', 'a', 'long', 'time', 'to', 'learn', 'how', 'to', 'pro', '##no', '##unce', 'a', 'word', '(', 'one', 'reason', 'i', 'don', \"'\", 't', 'think', 'i', 'could', 'ever', 'pick', 'up', 'a', 'second', 'language', ')', '.', 'i', \"'\", 'm', 'also', 'bad', 'at', 'interrupting', 'and', 'i', \"'\", 'm', 'a', 'bit', 'socially', 'in', '##ept', '(', 'at', 'least', 'i', 'think', 'so', ',', 'my', 'friends', 'say', 'i', \"'\", 'm', 'fine', 'but', 'a', 'little', 'qui', '##rky', ')', '.', 'i', 'also', 'read', 'that', 'au', '##tist', '##ic', 'people', 'like', 'to', 'argue', '/', 'debate', 'and', 'tend', 'to', 'be', 'analytic', 'or', 'rigid', 'thinkers', '.', '.', '.', 'well', ',', 'that', \"'\", 's', 'me', 'too', ',', 'lo', '##l', '.', 'so', 'that', \"'\", 's', 'the', 'social', 'stuff', '.', '.', '.', 'i', 'don', \"'\", 't', 'know', 'which', 'is', 'more', 'characteristic', 'of', 'autism', 'or', 'ad', '##hd', ',', 'but', 'they', 'both', 'seem', 'to', 'fit', '.', 'i', 'was', 'also', 'a', 'gu', '##lli', '##ble', 'kid', ',', 'pretty', 'oblivious', 'to', 'my', 'surroundings', ',', 'and', 'had', 'difficulty', 'picking', 'up', 'jokes', '(', 'especially', 'sarcasm', ')', '.', 'i', \"'\", 'm', 'still', 'not', 'the', 'best', 'at', 'detecting', 'jokes', 'or', 'telling', 'jokes', 'even', 'as', 'an', 'adult', '.', 'then', 'i', \"'\", 'm', 'also', 'quick', 'to', 'anger', ',', 'forget', '##ful', ',', 'and', 'fairly', 'imp', '##ulsive', '.', 'i', 'know', 'these', 'are', 'ad', '##hd', 'traits', ',', 'but', ',', 'again', ',', 'it', 'seems', 'appropriate', 'for', 'autism', '/', 'as', '##per', '##ger', \"'\", 's', 'too', '(', 'at', 'least', 'the', 'anger', 'and', 'imp', '##ulsive', '##ness', ')', '.', 'then', 'i', 'also', 'have', 'pretty', 'bad', 'anxiety', '-', 'like', 'i', 'don', \"'\", 't', 'sleep', 'if', 'i', \"'\", 'm', 'worried', 'even', 'about', 'the', 'simplest', 'thing', '(', 'then', 'spend', 'hours', 'or', 'days', 'pro', '##cr', '##ast', '##inating', ')', 'or', 'i', 'have', 'heart', 'pal', '##pit', '##ations', 'if', 'i', 'try', 'to', 'work', 'faster', 'when', 'i', \"'\", 'm', 'worried', 'about', 'doing', 'well', ';', 'not', 'to', 'mention', 'i', \"'\", 'm', 'a', 'nervous', 'driver', '.', 'i', 'know', 'anxiety', 'is', 'a', 'sy', '##mpt', '##om', 'of', 'ad', '##hd', ',', 'but', 'is', 'it', 'also', 'a', 'sy', '##mpt', '##om', 'of', 'autism', '?', 'i', 'also', 'cannot', 'sleep', 'unless', 'there', 'is', 'no', 'noise', 'and', 'i', \"'\", 'm', 'at', 'the', 'perfect', 'temperature', '.', 'it', 'drives', 'my', 'significant', 'other', 'nuts', ',', 'lo', '##l', '.']\n",
      "INFO:__main__:Number of tokens: 450\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'have', 'ad', '##hd', '+', 'autism', 'or', 'know', 'the', 'difference', 'between', 'the', 'two', '?', 'i', \"'\", 've', 'just', 'been', 'noticing', 'that', 'they', 'seem', 'a', 'bit', 'similar', '.', 'i', 'have', 'ad', '##hd', '(', 'the', 'ina', '##tten', '##tive', 'type', ')', ',', 'but', 'i', \"'\", 've', 'been', 'wondering', 'if', 'perhaps', 'i', \"'\", 'm', 'slightly', 'au', '##tist', '##ic', 'too', '.', 'for', 'example', ',', 'i', 'had', 'speech', 'pathology', 'all', 'throughout', 'elementary', 'school', ',', 'and', 'i', \"'\", 'm', 'wondering', 'whether', 'this', 'is', 'more', 'characteristic', 'with', 'autism', '/', 'as', '##per', '##ger', \"'\", 's', 'than', 'it', 'is', 'with', 'ad', '##hd', '.', 'also', ',', 'even', 'now', ',', 'sometimes', 'it', 'takes', 'me', 'a', 'long', 'time', 'to', 'learn', 'how', 'to', 'pro', '##no', '##unce', 'a', 'word', '(', 'one', 'reason', 'i', 'don', \"'\", 't', 'think', 'i', 'could', 'ever', 'pick', 'up', 'a', 'second', 'language', ')', '.', 'i', \"'\", 'm', 'also', 'bad', 'at', 'interrupting', 'and', 'i', \"'\", 'm', 'a', 'bit', 'socially', 'in', '##ept', '(', 'at', 'least', 'i', 'think', 'so', ',', 'my', 'friends', 'say', 'i', \"'\", 'm', 'fine', 'but', 'a', 'little', 'qui', '##rky', ')', '.', 'i', 'also', 'read', 'that', 'au', '##tist', '##ic', 'people', 'like', 'to', 'argue', '/', 'debate', 'and', 'tend', 'to', 'be', 'analytic', 'or', 'rigid', 'thinkers', '.', '.', '.', 'well', ',', 'that', \"'\", 's', 'me', 'too', ',', 'lo', '##l', '.', 'so', 'that', \"'\", 's', 'the', 'social', 'stuff', '.', '.', '.', 'i', 'don', \"'\", 't', 'know', 'which', 'is', 'more', 'characteristic', 'of', 'autism', 'or', 'ad', '##hd', ',', 'but', 'they', 'both', 'seem', 'to', 'fit', '.', 'i', 'was', 'also', 'a', 'gu', '##lli', '##ble', 'kid', ',', 'pretty', 'oblivious', 'to', 'my', 'surroundings', ',', 'and', 'had', 'difficulty', 'picking', 'up', 'jokes', '(', 'especially', 'sarcasm', ')', '.', 'i', \"'\", 'm', 'still', 'not', 'the', 'best', 'at', 'detecting', 'jokes', 'or', 'telling', 'jokes', 'even', 'as', 'an', 'adult', '.', 'then', 'i', \"'\", 'm', 'also', 'quick', 'to', 'anger', ',', 'forget', '##ful', ',', 'and', 'fairly', 'imp', '##ulsive', '.', 'i', 'know', 'these', 'are', 'ad', '##hd', 'traits', ',', 'but', ',', 'again', ',', 'it', 'seems', 'appropriate', 'for', 'autism', '/', 'as', '##per', '##ger', \"'\", 's', 'too', '(', 'at', 'least', 'the', 'anger', 'and', 'imp', '##ulsive', '##ness', ')', '.', 'then', 'i', 'also', 'have', 'pretty', 'bad', 'anxiety', '-', 'like', 'i', 'don', \"'\", 't', 'sleep', 'if', 'i', \"'\", 'm', 'worried', 'even', 'about', 'the', 'simplest', 'thing', '(', 'then', 'spend', 'hours', 'or', 'days', 'pro', '##cr', '##ast', '##inating', ')', 'or', 'i', 'have', 'heart', 'pal', '##pit', '##ations', 'if', 'i', 'try', 'to', 'work', 'faster', 'when', 'i', \"'\", 'm', 'worried', 'about', 'doing', 'well', ';', 'not', 'to', 'mention', 'i', \"'\", 'm', 'a', 'nervous', 'driver', '.', 'i', 'know', 'anxiety', 'is', 'a', 'sy', '##mpt', '##om', 'of', 'ad', '##hd', ',', 'but', 'is', 'it', 'also', 'a', 'sy', '##mpt', '##om', 'of', 'autism', '?', 'i', 'also', 'cannot', 'sleep', 'unless', 'there', 'is', 'no', 'noise', 'and', 'i', \"'\", 'm', 'at', 'the', 'perfect', 'temperature', '.', 'it', 'drives', 'my', 'significant', 'other', 'nuts', ',', 'lo', '##l', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'evidence', 'addresses', 'ad', '##hd', 'med', '##s', 'and', 'adult', 'cv', '##d', 'risk']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'evidence', 'addresses', 'ad', '##hd', 'med', '##s', 'and', 'adult', 'cv', '##d', 'risk']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['more', 'depressed', '/', 'hopeless', 'than', 'before', 'med', '##s', 'when', 'i', 'miss', 'a', 'little', 'bit', 'of', 'sleep', 'a', 'few', 'nights', 'in', 'a', 'row', '.', 'i', \"'\", 'm', 'wondering', 'if', 'anyone', 'else', 'gets', 'the', 'same', 'way', 'or', 'if', 'it', 'is', 'the', 'v', '##y', '##van', '##se', 'having', 'some', 'strange', 'effect', '?', 'i', 'used', 'to', 'get', 'like', 'this', 'all', 'the', 'time', '.', 'it', \"'\", 's', 'a', 'hopeless', '/', 'impending', 'doom', 'feeling', 'that', 'i', 'can', \"'\", 't', 'shake', '.', 'i', \"'\", 've', 'always', 'had', 'sleep', 'issues', 'and', 'have', 'moved', 'the', 'v', '##y', '##van', '##se', 'to', 'much', 'earlier', 'in', 'the', 'day', '.']\n",
      "INFO:__main__:Number of tokens: 96\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['more', 'depressed', '/', 'hopeless', 'than', 'before', 'med', '##s', 'when', 'i', 'miss', 'a', 'little', 'bit', 'of', 'sleep', 'a', 'few', 'nights', 'in', 'a', 'row', '.', 'i', \"'\", 'm', 'wondering', 'if', 'anyone', 'else', 'gets', 'the', 'same', 'way', 'or', 'if', 'it', 'is', 'the', 'v', '##y', '##van', '##se', 'having', 'some', 'strange', 'effect', '?', 'i', 'used', 'to', 'get', 'like', 'this', 'all', 'the', 'time', '.', 'it', \"'\", 's', 'a', 'hopeless', '/', 'impending', 'doom', 'feeling', 'that', 'i', 'can', \"'\", 't', 'shake', '.', 'i', \"'\", 've', 'always', 'had', 'sleep', 'issues', 'and', 'have', 'moved', 'the', 'v', '##y', '##van', '##se', 'to', 'much', 'earlier', 'in', 'the', 'day', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['red', '##dit', '+', 'add', '##eral', '##l']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['red', '##dit', '+', 'add', '##eral', '##l']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['(', 'comic', ')', 'drawing', 'board', '-', 'trail', 'off']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['(', 'comic', ')', 'drawing', 'board', '-', 'trail', 'off']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['studying', 'for', 'my', 'math', 'exam', 'for', 'the', 'first', 'time', ',', 'brought', 'to', 'you', 'by', ':']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['studying', 'for', 'my', 'math', 'exam', 'for', 'the', 'first', 'time', ',', 'brought', 'to', 'you', 'by', ':']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'many', 'people', 'with', 'ad', '##hd', 'does', 'it', 'take', 'to', 'change', 'a', 'light', 'bulb', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'many', 'people', 'with', 'ad', '##hd', 'does', 'it', 'take', 'to', 'change', 'a', 'light', 'bulb', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '&', 'me', '.', '[', 'uk', ']', 'first', 'things', 'first', '.', 'i', 'am', 'making', 'an', 'appointment', 'tomorrow', 'with', 'my', 'gp', 'to', 'see', 'a', 'specialist', '.', 'i', 'have', 'done', 'a', 'lot', 'of', 'research', 'on', 'this', ',', 'but', 'nothing', 'actually', 'productive', '.', 'when', 'i', 'was', 'at', 'school', ',', 'from', 'primary', 'school', '(', '5', 'years', 'old', '-', '12', '/', '13', '?', ')', 'i', 'was', 'always', 'hyper', '##active', 'and', 'usually', 'distracted', 'everyone', 'in', 'my', 'class', '.', 'nearly', 'all', 'my', 'reports', 'were', 'consisted', 'of', '*', '\"', 'intelligent', ',', 'but', 'needs', 'to', 'apply', 'himself', 'or', 'focus', 'on', 'the', 'task', 'at', 'hand', '\"', '*', 'and', 'i', 'ended', 'up', 'being', 'moved', 'to', 'a', 'different', 'school', 'due', 'to', 'my', 'parents', 'disagree', '##ing', 'with', 'the', 'teachers', 'who', 'called', 'me', 'a', 'problem', 'child', '.', 'so', 'i', 'ended', 'up', 'in', 'a', 'new', 'school', 'and', 'it', 'was', 'the', 'exact', 'same', ',', 'i', 'ended', 'up', 'doing', 'a', 'lot', 'of', 'stupid', 'stuff', ',', 'so', 'the', 'school', 'got', 'a', 'behaviour', '##al', 'dude', 'who', 'said', 'i', 'may', 'have', 'ad', '##hd', ',', 'something', 'which', ',', 'again', ',', 'my', 'parents', 'disagreed', 'with', 'them', 'and', 'nothing', 'was', 'done', ',', 'and', 'it', 'was', 'eventually', 'forgotten', 'about', '.', 'life', 'went', 'on', ',', 'highs', '##cho', '##ol', 'was', 'the', 'same', ',', 'college', '*', '(', 'the', 'first', '2', 'times', 'on', 'the', 'same', 'course', ')', '*', 'again', ',', 'the', 'same', '.', 'and', 'i', 'ended', 'up', 'doing', 'a', 'multitude', 'of', 'jobs', ',', 'none', 'of', 'which', 'lasted', 'more', 'than', 'a', 'year', ',', 'i', 'got', 'my', 'own', 'house', 'and', 'since', 'then', 'it', \"'\", 's', 'always', 'been', 'the', 'same', '.', 'i', 'am', 'terrible', 'at', 'organizing', 'things', '/', 'paying', 'bills', '/', 'doing', '*', '*', 'anything', '*', '*', 'that', 'needs', 'to', 'be', 'done', '.', 'my', 'home', 'is', 'a', 'mess', ',', 'i', 'haven', \"'\", 't', 'got', 'around', 'to', 'getting', 'carpets', 'yet', ',', 'i', 'am', 'useless', 'at', 'paying', 'things', 'on', 'time', 'and', 'i', 'have', 'pretty', 'much', ',', 'as', 'far', 'as', 'i', 'feel', ',', 'destroyed', 'my', 'life', '.', 'i', 'have', 'lost', 'my', 'job', ',', 'my', 'girlfriend', ',', 'and', 'pretty', 'much', 'on', 'the', 'verge', 'of', 'losing', 'my', 'house', '.', 'i', '*', '*', 'know', '*', '*', 'i', 'need', 'to', 'do', 'things', ',', 'i', 'need', 'to', 'pull', 'my', 'finger', 'out', 'of', 'my', 'ass', 'and', 'do', 'things', ',', 'but', 'it', 'feels', 'impossible', '.', 'i', 'wouldn', \"'\", 't', 'sleep', 'for', 'days', ',', 'and', 'went', 'to', 'the', 'doctors', 'about', 'it', ',', 'which', 'ended', 'up', 'with', 'me', 'on', 'z', '##op', '##ic', '##lone', 'to', 'help', 'me', 'sleep', '.', 'anyway', ',', 'one', 'night', 'as', 'i', 'was', 'internet', '##ting', ',', 'i', 'came', 'across', '[', 'this', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##3d', '##1', '##sw', '##ux', '##mc', '##0', ')', 'and', 'a', 'lot', 'of', 'the', 'stuff', 'he', 'said', 'fit', 'into', 'place', ',', 'i', 'actually', 'cried', 'and', 'laughed', 'as', 'i', 'could', 'relate', 'so', 'much', 'to', 'the', 'content', 'and', 'i', 'remembered', 'the', 'old', 'behaviour', '##al', 'dude', 'my', 'parents', 'ignored', '.', 'ever', 'since', 'then', 'i', 'have', 'soaked', 'up', 'everything', 'i', 'can', 'about', 'ad', '##hd', 'and', 'can', 'relate', 'to', 'near', 'it', 'all', ',', 'i', 'don', \"'\", 't', 'think', 'i', 'have', 'as', 'much', 'hyper', '##act', '##ivity', 'as', 'i', 'used', 'to', ',', 'i', 'still', 'have', 'spells', 'of', 'it', ',', 'but', 'no', 'where', 'near', 'as', 'i', 'used', 'to', 'be', '.', 'i', 'am', 'hoping', 'things', 'go', 'well', 'for', 'me', 'tomorrow', ',', 'i', 'am', 'not', 'expecting', 'a', 'quick', '-', 'fix', 'cure', '-', 'all', 'for', 'anything', ',', 'but', 'just', 'knowing', 'that', 'i', '*', '*', 'do', '*', '*', 'have', 'a', 'problem', 'and', 'can', 'at', 'least', 'make', 'steps', '/', 'get', 'help', 'with', 'my', 'disaster', 'of', 'a', 'life', 'is', 'helping', 'my', 'mind', '.', 'i', 'hope', 'through', 'methods', '/', 'medication', 'i', 'can', 'get', 'rid', 'of', 'that', '4a', '##m', '*', 'life', 'crashing', 'down', 'around', 'me', '*', 'feeling', 'and', 'i', 'can', 'finally', 'be', '\"', 'normal', '\"', '.', 'edit', ':', '-', 'i', \"'\", 'll', 'update', 'thread', 'tomorrow', 'on', 'the', 'outcome', 'of', 'the', 'appointment', '.', '*', 'someone', 'wake', 'me', 'up', '*', '*', '*', 'next', 'day', 'edit', ':', '-', '*', '*', '*', 'spoke', 'to', 'the', 'doctor', ',', 'a', 'different', 'one', 'than', 'usual', '(', 'good', 'thing', '?', ')', 'and', 'made', 'an', 'appointment', 'to', 'discuss', 'it', 'further', 'and', 'in', '-', 'depth', 'on', 'friday', '.', '*', 'made', 'it', 'on', 'time', 'though', '\\\\', 'o', '/']\n",
      "INFO:__main__:Number of tokens: 687\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '&', 'me', '.', '[', 'uk', ']', 'first', 'things', 'first', '.', 'i', 'am', 'making', 'an', 'appointment', 'tomorrow', 'with', 'my', 'gp', 'to', 'see', 'a', 'specialist', '.', 'i', 'have', 'done', 'a', 'lot', 'of', 'research', 'on', 'this', ',', 'but', 'nothing', 'actually', 'productive', '.', 'when', 'i', 'was', 'at', 'school', ',', 'from', 'primary', 'school', '(', '5', 'years', 'old', '-', '12', '/', '13', '?', ')', 'i', 'was', 'always', 'hyper', '##active', 'and', 'usually', 'distracted', 'everyone', 'in', 'my', 'class', '.', 'nearly', 'all', 'my', 'reports', 'were', 'consisted', 'of', '*', '\"', 'intelligent', ',', 'but', 'needs', 'to', 'apply', 'himself', 'or', 'focus', 'on', 'the', 'task', 'at', 'hand', '\"', '*', 'and', 'i', 'ended', 'up', 'being', 'moved', 'to', 'a', 'different', 'school', 'due', 'to', 'my', 'parents', 'disagree', '##ing', 'with', 'the', 'teachers', 'who', 'called', 'me', 'a', 'problem', 'child', '.', 'so', 'i', 'ended', 'up', 'in', 'a', 'new', 'school', 'and', 'it', 'was', 'the', 'exact', 'same', ',', 'i', 'ended', 'up', 'doing', 'a', 'lot', 'of', 'stupid', 'stuff', ',', 'so', 'the', 'school', 'got', 'a', 'behaviour', '##al', 'dude', 'who', 'said', 'i', 'may', 'have', 'ad', '##hd', ',', 'something', 'which', ',', 'again', ',', 'my', 'parents', 'disagreed', 'with', 'them', 'and', 'nothing', 'was', 'done', ',', 'and', 'it', 'was', 'eventually', 'forgotten', 'about', '.', 'life', 'went', 'on', ',', 'highs', '##cho', '##ol', 'was', 'the', 'same', ',', 'college', '*', '(', 'the', 'first', '2', 'times', 'on', 'the', 'same', 'course', ')', '*', 'again', ',', 'the', 'same', '.', 'and', 'i', 'ended', 'up', 'doing', 'a', 'multitude', 'of', 'jobs', ',', 'none', 'of', 'which', 'lasted', 'more', 'than', 'a', 'year', ',', 'i', 'got', 'my', 'own', 'house', 'and', 'since', 'then', 'it', \"'\", 's', 'always', 'been', 'the', 'same', '.', 'i', 'am', 'terrible', 'at', 'organizing', 'things', '/', 'paying', 'bills', '/', 'doing', '*', '*', 'anything', '*', '*', 'that', 'needs', 'to', 'be', 'done', '.', 'my', 'home', 'is', 'a', 'mess', ',', 'i', 'haven', \"'\", 't', 'got', 'around', 'to', 'getting', 'carpets', 'yet', ',', 'i', 'am', 'useless', 'at', 'paying', 'things', 'on', 'time', 'and', 'i', 'have', 'pretty', 'much', ',', 'as', 'far', 'as', 'i', 'feel', ',', 'destroyed', 'my', 'life', '.', 'i', 'have', 'lost', 'my', 'job', ',', 'my', 'girlfriend', ',', 'and', 'pretty', 'much', 'on', 'the', 'verge', 'of', 'losing', 'my', 'house', '.', 'i', '*', '*', 'know', '*', '*', 'i', 'need', 'to', 'do', 'things', ',', 'i', 'need', 'to', 'pull', 'my', 'finger', 'out', 'of', 'my', 'ass', 'and', 'do', 'things', ',', 'but', 'it', 'feels', 'impossible', '.', 'i', 'wouldn', \"'\", 't', 'sleep', 'for', 'days', ',', 'and', 'went', 'to', 'the', 'doctors', 'about', 'it', ',', 'which', 'ended', 'up', 'with', 'me', 'on', 'z', '##op', '##ic', '##lone', 'to', 'help', 'me', 'sleep', '.', 'anyway', ',', 'one', 'night', 'as', 'i', 'was', 'internet', '##ting', ',', 'i', 'came', 'across', '[', 'this', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##3d', '##1', '##sw', '##ux', '##mc', '##0', ')', 'and', 'a', 'lot', 'of', 'the', 'stuff', 'he', 'said', 'fit', 'into', 'place', ',', 'i', 'actually', 'cried', 'and', 'laughed', 'as', 'i', 'could', 'relate', 'so', 'much', 'to', 'the', 'content', 'and', 'i', 'remembered', 'the', 'old', 'behaviour', '##al', 'dude', 'my', 'parents', 'ignored', '.', 'ever', 'since', 'then', 'i', 'have', 'soaked', 'up', 'everything', 'i', 'can', 'about', 'ad', '##hd', 'and', 'can', 'relate', 'to', 'near', 'it', 'all', ',', 'i', 'don', \"'\", 't', 'think', 'i', 'have', 'as', 'much', 'hyper', '##act', '##ivity', 'as', 'i', 'used', 'to', ',', 'i'], ['still', 'have', 'spells', 'of', 'it', ',', 'but', 'no', 'where', 'near', 'as', 'i', 'used', 'to', 'be', '.', 'i', 'am', 'hoping', 'things', 'go', 'well', 'for', 'me', 'tomorrow', ',', 'i', 'am', 'not', 'expecting', 'a', 'quick', '-', 'fix', 'cure', '-', 'all', 'for', 'anything', ',', 'but', 'just', 'knowing', 'that', 'i', '*', '*', 'do', '*', '*', 'have', 'a', 'problem', 'and', 'can', 'at', 'least', 'make', 'steps', '/', 'get', 'help', 'with', 'my', 'disaster', 'of', 'a', 'life', 'is', 'helping', 'my', 'mind', '.', 'i', 'hope', 'through', 'methods', '/', 'medication', 'i', 'can', 'get', 'rid', 'of', 'that', '4a', '##m', '*', 'life', 'crashing', 'down', 'around', 'me', '*', 'feeling', 'and', 'i', 'can', 'finally', 'be', '\"', 'normal', '\"', '.', 'edit', ':', '-', 'i', \"'\", 'll', 'update', 'thread', 'tomorrow', 'on', 'the', 'outcome', 'of', 'the', 'appointment', '.', '*', 'someone', 'wake', 'me', 'up', '*', '*', '*', 'next', 'day', 'edit', ':', '-', '*', '*', '*', 'spoke', 'to', 'the', 'doctor', ',', 'a', 'different', 'one', 'than', 'usual', '(', 'good', 'thing', '?', ')', 'and', 'made', 'an', 'appointment', 'to', 'discuss', 'it', 'further', 'and', 'in', '-', 'depth', 'on', 'friday', '.', '*', 'made', 'it', 'on', 'time', 'though', '\\\\', 'o', '/']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['different', 'experiences', 'with', 'ad', '##hd', 'medication', '?', 'currently', 'i', 'am', 'taking', 'long', 'acting', 'methyl', '##ph', '##eni', '##date', '(', 'concert', '##a', ')', '.', 'i', 'have', 'been', 'taking', 'it', 'for', 'about', '4', 'years', '.', 'it', 'was', 'suggested', 'to', 'me', 'by', 'my', 'psychologist', 'that', 'i', 'should', 'consider', 'changing', 'medications', 'because', 'my', 'brain', 'is', '\"', 'normal', '##izing', '\"', 'or', '\"', 'natural', '##izing', '\"', '(', 'i', 'can', \"'\", 't', 'remember', 'what', 'word', 'he', 'used', ')', 'ye', '##il', '##ding', 'the', 'medication', 'less', 'effective', '.', 'what', 'medications', 'have', 'you', 'been', 'on', '?', 'have', 'you', 'tried', 'out', 'other', 'medication', '?', 'what', 'works', 'best', 'for', 'you', 'and', 'why', '(', 'what', 'do', 'you', 'notice', ')', '?']\n",
      "INFO:__main__:Number of tokens: 106\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['different', 'experiences', 'with', 'ad', '##hd', 'medication', '?', 'currently', 'i', 'am', 'taking', 'long', 'acting', 'methyl', '##ph', '##eni', '##date', '(', 'concert', '##a', ')', '.', 'i', 'have', 'been', 'taking', 'it', 'for', 'about', '4', 'years', '.', 'it', 'was', 'suggested', 'to', 'me', 'by', 'my', 'psychologist', 'that', 'i', 'should', 'consider', 'changing', 'medications', 'because', 'my', 'brain', 'is', '\"', 'normal', '##izing', '\"', 'or', '\"', 'natural', '##izing', '\"', '(', 'i', 'can', \"'\", 't', 'remember', 'what', 'word', 'he', 'used', ')', 'ye', '##il', '##ding', 'the', 'medication', 'less', 'effective', '.', 'what', 'medications', 'have', 'you', 'been', 'on', '?', 'have', 'you', 'tried', 'out', 'other', 'medication', '?', 'what', 'works', 'best', 'for', 'you', 'and', 'why', '(', 'what', 'do', 'you', 'notice', ')', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'a', 'good', 'add', '##eral', '##l', '10', '##mg', 'substitute', '?', 'this', 'shortage', 'had', 'to', 'happen', 'just', 'as', 'i', 'ran', 'out', 'of', 'add', '##eral', '##l', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'a', 'teenager', 'again', ',', 'unable', 'to', 'complete', 'any', 'tasks', '.', 'i', 'keep', 'hearing', 'about', 'v', '##y', '##van', '##se', 'but', 'isn', \"'\", 't', 'that', 'an', 'add', '##eral', '##l', '*', '*', 'x', '##r', '*', '*', 'substitute', '?', 'any', 'help', 'would', 'be', 'appreciated', 'before', 'i', 'go', 'visit', 'my', 'psychiatrist', 'to', 'discuss', 'changing', 'my', 'prescription', '.']\n",
      "INFO:__main__:Number of tokens: 84\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'a', 'good', 'add', '##eral', '##l', '10', '##mg', 'substitute', '?', 'this', 'shortage', 'had', 'to', 'happen', 'just', 'as', 'i', 'ran', 'out', 'of', 'add', '##eral', '##l', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'a', 'teenager', 'again', ',', 'unable', 'to', 'complete', 'any', 'tasks', '.', 'i', 'keep', 'hearing', 'about', 'v', '##y', '##van', '##se', 'but', 'isn', \"'\", 't', 'that', 'an', 'add', '##eral', '##l', '*', '*', 'x', '##r', '*', '*', 'substitute', '?', 'any', 'help', 'would', 'be', 'appreciated', 'before', 'i', 'go', 'visit', 'my', 'psychiatrist', 'to', 'discuss', 'changing', 'my', 'prescription', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anybody', 'know', 'when', 'the', 'add', '##eral', '##l', 'shortage', 'is', 'supposed', 'to', 'end', '?', 'i', \"'\", 'm', 'prescribed', '10', 'mg', \"'\", 's', 'and', 'i', \"'\", 've', 'been', 'supplying', 'to', 'my', 'friends', 'who', 'can', \"'\", 't', 'get', 'any', 'lately', ',', 'but', 'i', \"'\", 'm', 'starting', 'to', 'run', 'low', '.', 'i', 'read', 'somewhere', 'it', \"'\", 's', 'fda', 'regulations', 'that', 'are', 'limiting', 'the', 'supply', ',', 'and', 'these', 'will', 'be', 'reset', 'at', 'the', 'beginning', 'of', '2012', ',', 'but', 'i', 'can', \"'\", 't', 'be', 'sure', '.', '.', 'any', 'leads', ',', 'r', '/', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 90\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anybody', 'know', 'when', 'the', 'add', '##eral', '##l', 'shortage', 'is', 'supposed', 'to', 'end', '?', 'i', \"'\", 'm', 'prescribed', '10', 'mg', \"'\", 's', 'and', 'i', \"'\", 've', 'been', 'supplying', 'to', 'my', 'friends', 'who', 'can', \"'\", 't', 'get', 'any', 'lately', ',', 'but', 'i', \"'\", 'm', 'starting', 'to', 'run', 'low', '.', 'i', 'read', 'somewhere', 'it', \"'\", 's', 'fda', 'regulations', 'that', 'are', 'limiting', 'the', 'supply', ',', 'and', 'these', 'will', 'be', 'reset', 'at', 'the', 'beginning', 'of', '2012', ',', 'but', 'i', 'can', \"'\", 't', 'be', 'sure', '.', '.', 'any', 'leads', ',', 'r', '/', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['starting', 'concert', '##a', 'xl', '18', '##mg', '.']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['starting', 'concert', '##a', 'xl', '18', '##mg', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'diet', 're', '##med', '##ies', 'greeting', '##s', '.', 'i', 'would', 'like', 'to', 'get', 'responses', 'from', 'people', 'who', 'have', 'tried', '\"', 'diet', '\"', 're', '##med', '##ies', 'either', 'themselves', 'or', 'on', 'their', 'kids', 'for', 'ad', '##hd', 'as', 'a', 'solution', 'to', 'ad', '##hd', '.', 'personally', 'i', 'don', \"'\", 't', 'have', 'much', 'believe', 'in', 'diet', 're', '##med', '##ies', 'as', 'a', 'solution', ',', 'i', 'can', \"'\", 't', 'personally', 'say', 'my', 'changes', 'in', 'diet', 'have', 'had', 'any', 'effect', 'on', 'my', 'ad', '##hd', 'but', 'then', 'again', 'i', 'haven', \"'\", 't', 'tried', 'it', 'explicitly', 'nor', 'probably', 'will', 'i', 'ever', '.', 'i', 'view', 'ad', '##hd', 'mainly', 'as', 'a', 'genetic', '/', 'neurological', 'disorder', ',', 'which', 'modern', 'research', 'at', 'least', 'the', 'last', '10', 'years', 'has', 'indicated', '.', 'i', 'believe', 'mal', '##nut', '##rit', '##ion', 'in', 'multiple', 'or', 'isolated', 'nutrients', 'can', 'cause', 'ad', '##hd', 'like', 'symptoms', 'and', 'even', 'ad', '##hd', 'mis', '##dia', '##gno', '##stic', '.', 'there', 'does', 'exist', 'research', 'that', 'hasn', \"'\", 't', 'been', 'identified', 'as', 'severely', 'flawed', 'to', 'my', 'knowledge', 'that', 'nutrients', 'such', 'as', 'omega', '-', '3', 'fatty', 'acids', 'can', 'help', 'children', 'with', 'ad', '##hd', 'and', 'specific', 'learning', 'disabilities', 'reduce', 'ad', '##hd', 'symptoms', 'with', 'a', \"'\", 'medium', \"'\", 'statistical', 'effect', '(', '.', '5', ')', 'on', 'some', 'factors', 'compared', 'to', 'a', 'place', '##bo', 'group', '.', 'lower', 'concentrations', 'of', 'omega', '-', '3', 'acids', 'have', 'been', 'found', 'in', 'ad', '##hd', 'people', 'but', 'scientists', 'do', 'not', 'know', 'why', 'which', 'could', 'perhaps', 'indicate', 'why', 'omega', '-', '3', 'nutrients', 'seem', 'to', 'help', 'ad', '##hd', 'children', 'over', 'the', 'place', '##bo', 'group', '.', 'if', 'you', 'can', 'de', '##bu', '##nk', 'this', 'research', \"'\", 's', 'methods', 'then', 'please', 'go', 'ahead', '.', 'research', 'is', 'located', 'here', '(', 'posted', 'on', 'a', 'omega', '-', '3', 'website', 'isn', \"'\", 't', 'ass', '##uring', 'but', 'google', 'search', 'suggests', 'its', 'leg', '##it', ')', 'http', ':', '/', '/', 'www', '.', 'omega', '-', '3', '##hea', '##lth', '.', 'com', '.', 'au', '/', 'up', '##load', '/', 'pdf', '/', 'effect', '%', '20', '##of', '%', '20s', '##up', '##ple', '##ment', '##ation', '%', '20', '##with', '%', '20', '##pol', '##yu', '##ns', '##at', '##ura', '##ted', '%', '20', '##fat', '##ty', '%', '20', '##ac', '##ids', '.', 'pdf', 'anyway', 'i', 'would', 'like', 'to', 'some', 'input', 'from', 'those', 'who', 'have', 'tried', 'any', 'kind', 'of', 'diet', 'as', 'a', 'remedy', 'to', 'ad', '##hd', '.', 'did', 'you', 'perceive', 'it', 'as', 'something', 'that', 'worked', 'or', 'not', '.', 'obviously', 'any', 'result', 'of', 'this', 'discussion', 'here', 'will', 'not', 'be', 'un', '##bia', '##sed', 'but', 'it', 'would', 'give', 'a', 'clue', 'of', 'how', 'people', 'regard', 'these', 're', '##med', '##ies', '.']\n",
      "INFO:__main__:Number of tokens: 400\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'diet', 're', '##med', '##ies', 'greeting', '##s', '.', 'i', 'would', 'like', 'to', 'get', 'responses', 'from', 'people', 'who', 'have', 'tried', '\"', 'diet', '\"', 're', '##med', '##ies', 'either', 'themselves', 'or', 'on', 'their', 'kids', 'for', 'ad', '##hd', 'as', 'a', 'solution', 'to', 'ad', '##hd', '.', 'personally', 'i', 'don', \"'\", 't', 'have', 'much', 'believe', 'in', 'diet', 're', '##med', '##ies', 'as', 'a', 'solution', ',', 'i', 'can', \"'\", 't', 'personally', 'say', 'my', 'changes', 'in', 'diet', 'have', 'had', 'any', 'effect', 'on', 'my', 'ad', '##hd', 'but', 'then', 'again', 'i', 'haven', \"'\", 't', 'tried', 'it', 'explicitly', 'nor', 'probably', 'will', 'i', 'ever', '.', 'i', 'view', 'ad', '##hd', 'mainly', 'as', 'a', 'genetic', '/', 'neurological', 'disorder', ',', 'which', 'modern', 'research', 'at', 'least', 'the', 'last', '10', 'years', 'has', 'indicated', '.', 'i', 'believe', 'mal', '##nut', '##rit', '##ion', 'in', 'multiple', 'or', 'isolated', 'nutrients', 'can', 'cause', 'ad', '##hd', 'like', 'symptoms', 'and', 'even', 'ad', '##hd', 'mis', '##dia', '##gno', '##stic', '.', 'there', 'does', 'exist', 'research', 'that', 'hasn', \"'\", 't', 'been', 'identified', 'as', 'severely', 'flawed', 'to', 'my', 'knowledge', 'that', 'nutrients', 'such', 'as', 'omega', '-', '3', 'fatty', 'acids', 'can', 'help', 'children', 'with', 'ad', '##hd', 'and', 'specific', 'learning', 'disabilities', 'reduce', 'ad', '##hd', 'symptoms', 'with', 'a', \"'\", 'medium', \"'\", 'statistical', 'effect', '(', '.', '5', ')', 'on', 'some', 'factors', 'compared', 'to', 'a', 'place', '##bo', 'group', '.', 'lower', 'concentrations', 'of', 'omega', '-', '3', 'acids', 'have', 'been', 'found', 'in', 'ad', '##hd', 'people', 'but', 'scientists', 'do', 'not', 'know', 'why', 'which', 'could', 'perhaps', 'indicate', 'why', 'omega', '-', '3', 'nutrients', 'seem', 'to', 'help', 'ad', '##hd', 'children', 'over', 'the', 'place', '##bo', 'group', '.', 'if', 'you', 'can', 'de', '##bu', '##nk', 'this', 'research', \"'\", 's', 'methods', 'then', 'please', 'go', 'ahead', '.', 'research', 'is', 'located', 'here', '(', 'posted', 'on', 'a', 'omega', '-', '3', 'website', 'isn', \"'\", 't', 'ass', '##uring', 'but', 'google', 'search', 'suggests', 'its', 'leg', '##it', ')', 'http', ':', '/', '/', 'www', '.', 'omega', '-', '3', '##hea', '##lth', '.', 'com', '.', 'au', '/', 'up', '##load', '/', 'pdf', '/', 'effect', '%', '20', '##of', '%', '20s', '##up', '##ple', '##ment', '##ation', '%', '20', '##with', '%', '20', '##pol', '##yu', '##ns', '##at', '##ura', '##ted', '%', '20', '##fat', '##ty', '%', '20', '##ac', '##ids', '.', 'pdf', 'anyway', 'i', 'would', 'like', 'to', 'some', 'input', 'from', 'those', 'who', 'have', 'tried', 'any', 'kind', 'of', 'diet', 'as', 'a', 'remedy', 'to', 'ad', '##hd', '.', 'did', 'you', 'perceive', 'it', 'as', 'something', 'that', 'worked', 'or', 'not', '.', 'obviously', 'any', 'result', 'of', 'this', 'discussion', 'here', 'will', 'not', 'be', 'un', '##bia', '##sed', 'but', 'it', 'would', 'give', 'a', 'clue', 'of', 'how', 'people', 'regard', 'these', 're', '##med', '##ies', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', \"'\", 's', 'the', 'difference', 'between', 'add', 'and', 'ad', '##hd', '?', 'i', \"'\", 'm', 'sure', 'that', 'this', 'question', 'gets', 'posted', 'all', 'the', 'time', 'but', 'i', 'thought', 'i', \"'\", 'd', 'just', 'ask', '.', 'i', \"'\", 've', 'recently', 'realized', 'that', 'i', 'need', 'to', 'get', 'a', 'diagnosis', 'and', 'am', 'waiting', 'until', 'after', 'exams', 'to', 'look', 'into', 'it', '.', 'when', 'i', 'bring', 'it', 'up', 'that', 'i', 'think', 'that', 'i', 'have', 'ad', '##hd', 'people', 'always', 'correct', 'me', 'and', 'say', '\"', 'add', '\"', 'because', 'i', \"'\", 'm', 'not', '\"', 'bouncing', 'off', 'the', 'walls', '.', '\"', 'my', 'understanding', 'is', 'that', 'add', 'and', 'ad', '##hd', 'are', 'the', 'same', ';', 'there', 'are', 'three', 'types', 'of', 'ad', '##hd', '-', 'imp', '##ulsive', ',', 'hyper', '##active', 'and', 'combined', '-', 'and', 'that', 'add', 'is', 'just', 'an', 'older', 'name', 'for', 'ad', '##hd', '.', 'please', 'correct', 'me', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'sound', 'like', 'an', 'idiot', 'when', 'i', 'talk', 'to', 'people', '.']\n",
      "INFO:__main__:Number of tokens: 149\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', \"'\", 's', 'the', 'difference', 'between', 'add', 'and', 'ad', '##hd', '?', 'i', \"'\", 'm', 'sure', 'that', 'this', 'question', 'gets', 'posted', 'all', 'the', 'time', 'but', 'i', 'thought', 'i', \"'\", 'd', 'just', 'ask', '.', 'i', \"'\", 've', 'recently', 'realized', 'that', 'i', 'need', 'to', 'get', 'a', 'diagnosis', 'and', 'am', 'waiting', 'until', 'after', 'exams', 'to', 'look', 'into', 'it', '.', 'when', 'i', 'bring', 'it', 'up', 'that', 'i', 'think', 'that', 'i', 'have', 'ad', '##hd', 'people', 'always', 'correct', 'me', 'and', 'say', '\"', 'add', '\"', 'because', 'i', \"'\", 'm', 'not', '\"', 'bouncing', 'off', 'the', 'walls', '.', '\"', 'my', 'understanding', 'is', 'that', 'add', 'and', 'ad', '##hd', 'are', 'the', 'same', ';', 'there', 'are', 'three', 'types', 'of', 'ad', '##hd', '-', 'imp', '##ulsive', ',', 'hyper', '##active', 'and', 'combined', '-', 'and', 'that', 'add', 'is', 'just', 'an', 'older', 'name', 'for', 'ad', '##hd', '.', 'please', 'correct', 'me', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'sound', 'like', 'an', 'idiot', 'when', 'i', 'talk', 'to', 'people', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'struggle', 'with', 'add', ':', 'using', 'cocaine', 'to', 'fight', 'the', 'symptoms', '-', '-', 'the', 'atlantic']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'struggle', 'with', 'add', ':', 'using', 'cocaine', 'to', 'fight', 'the', 'symptoms', '-', '-', 'the', 'atlantic']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'really', 'new', 'at', 'this', 'whole', 'thing', '.', 'i', 'was', 'recently', 'diagnosed', ',', 'and', 'not', 'sure', 'what', 'to', 'expect', '.', 'well', '.', 'this', 'is', 'all', 'a', 'huge', ',', 'new', 'thing', 'and', 'it', \"'\", 's', 'scary', 'and', 'exciting', 'all', 'at', 'once', '.', 'yesterday', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '.', 'i', 'didn', \"'\", 't', 'suspect', 'anything', 'like', 'this', ',', 'i', 'was', 'in', 'because', 'i', 'was', 'depressed', '.', 'but', 'while', 'i', 'was', 'there', ',', 'i', 'filled', 'out', 'a', 'sheet', 'where', 'i', 'said', '\"', 'yes', ',', 'i', 'do', 'this', 'often', ',', 'no', 'i', 'never', 'do', 'this', ',', 'sometimes', 'i', 'do', 'this', '\"', ',', 'etc', '.', 'well', ',', 'i', 'said', '\"', 'i', 'often', 'do', 'this', '\"', 'to', 'all', 'of', 'the', 'tell', '-', 'tale', 'signs', 'of', 'ad', '##hd', '.', 'the', 'doctor', 'explained', 'a', 'bit', 'what', 'was', 'going', 'on', ',', 'and', 'gave', 'me', 'a', 'prescription', 'for', 'v', '##yna', '##se', '.', 'i', 'haven', \"'\", 't', 'started', 'on', 'them', 'yet', '.', 'starting', 'saturday', ',', 'i', \"'\", 'm', 'taking', '20', 'mg', 'every', 'morning', 'for', '10', 'days', 'and', 'seeing', 'him', 'again', '.', 'i', 'told', 'some', 'of', 'my', 'friends', 'about', 'what', 'was', 'going', 'on', ',', 'and', 'some', 'of', 'them', 'are', '.', '.', '.', 'worried', '.', 'most', 'of', 'us', 'aren', \"'\", 't', '\"', 'normal', '\"', ',', 'really', ',', 'and', 'they', \"'\", 're', 'concerned', 'that', 'taking', 'this', 'kind', 'of', 'medication', 'will', 'turn', 'me', 'into', 'a', 'mind', '##less', 'zombie', '.', 'as', 'well', ',', 'a', 'significant', 'number', 'of', 'them', 'are', 'vocal', '##ly', 'against', 'ad', '##hd', 'medication', ',', 'claiming', 'that', 'there', 'are', 'better', 'ways', 'to', 'deal', 'with', 'it', '.', '(', 'i', 'wonder', 'if', 'they', 'realize', 'that', ',', 'though', 'they', \"'\", 're', 'allowed', 'to', 'believe', 'that', ',', 'bringing', 'it', 'up', 'after', 'i', 'tell', 'them', 'that', 'i', \"'\", 'm', 'going', 'to', 'start', 'taking', 'ad', '##hd', 'medication', 'isn', \"'\", 't', 'going', 'to', 'make', 'me', 'feel', 'any', 'better', '.', ')', 'anyway', ',', 'the', 'reason', 'i', \"'\", 'm', 'posting', 'is', 'that', 'things', 'people', 'have', 'said', 'are', 'awakening', 'fears', 'i', 'didn', \"'\", 't', 'know', 'i', 'had', '.', 'when', 'they', 'said', 'that', 'i', 'would', 'lose', 'my', 'personality', ',', 'even', 'though', 'i', 'know', 'that', \"'\", 's', 'silly', ',', 'it', 'got', 'me', 'thinking', '.', 'how', 'much', 'of', 'the', 'person', 'that', 'i', 'know', 'as', '\"', 'me', '\"', 'is', 'the', 'disorder', 'poking', 'through', '?', 'i', 'don', \"'\", 't', 'want', 'to', 'go', 'on', 'the', 'medicine', 'and', 'stop', 'liking', 'the', 'things', 'i', 'used', 'to', 'like', 'or', '(', 'even', 'worse', '!', ')', 'find', 'myself', 'unable', 'to', 'find', 'anything', 'in', 'common', 'with', 'my', 'friends', '.', 'we', \"'\", 're', 'all', 'kind', 'of', 'weird', ',', 'and', 'i', 'like', 'that', ',', 'but', 'i', \"'\", 'm', 'pretty', 'scared', 'of', 'losing', 'them', '.', 'if', 'i', 'remember', 'correctly', ',', 'it', 'said', 'on', 'the', 'page', 'i', 'filled', 'out', 'that', 'having', 'difficulty', 'writing', 'a', 'conclusion', 'after', 'all', 'the', 'hard', 'work', 'is', 'done', 'is', 'normal', 'for', 'people', 'with', 'ad', '##hd', ',', 'so', 'i', 'feel', 'no', 'shame', 'in', 'what', 'is', 'probably', 'going', 'to', 'be', 'a', 'shitty', 'final', 'paragraph', '.', 'i', \"'\", 'm', 'scared', ',', 'and', 'i', \"'\", 'm', 'excited', ',', 'and', 'i', 'just', 'need', 'to', 'talk', '.', 'for', '15', 'years', 'of', 'my', 'life', 'i', \"'\", 've', 'slipped', 'through', 'the', 'cracks', ',', 'and', 'now', 'i', \"'\", 'm', '(', 'kind', 'of', ')', 'getting', 'a', 'chance', 'to', 'be', 'normal', 'and', 'i', 'just', 'want', 'to', 'talk', 'to', 'someone', '.', 'you', 'guys', 'have', 'all', 'been', 'through', 'this', '(', 'or', 'something', 'similar', ')', ',', 'and', 'i', 'figured', 'this', 'is', 'where', 'i', 'could', 'turn', 'to', '.', 'so', '.', '.', '.', 'hi', '!', ':', ')']\n",
      "INFO:__main__:Number of tokens: 568\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'really', 'new', 'at', 'this', 'whole', 'thing', '.', 'i', 'was', 'recently', 'diagnosed', ',', 'and', 'not', 'sure', 'what', 'to', 'expect', '.', 'well', '.', 'this', 'is', 'all', 'a', 'huge', ',', 'new', 'thing', 'and', 'it', \"'\", 's', 'scary', 'and', 'exciting', 'all', 'at', 'once', '.', 'yesterday', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '.', 'i', 'didn', \"'\", 't', 'suspect', 'anything', 'like', 'this', ',', 'i', 'was', 'in', 'because', 'i', 'was', 'depressed', '.', 'but', 'while', 'i', 'was', 'there', ',', 'i', 'filled', 'out', 'a', 'sheet', 'where', 'i', 'said', '\"', 'yes', ',', 'i', 'do', 'this', 'often', ',', 'no', 'i', 'never', 'do', 'this', ',', 'sometimes', 'i', 'do', 'this', '\"', ',', 'etc', '.', 'well', ',', 'i', 'said', '\"', 'i', 'often', 'do', 'this', '\"', 'to', 'all', 'of', 'the', 'tell', '-', 'tale', 'signs', 'of', 'ad', '##hd', '.', 'the', 'doctor', 'explained', 'a', 'bit', 'what', 'was', 'going', 'on', ',', 'and', 'gave', 'me', 'a', 'prescription', 'for', 'v', '##yna', '##se', '.', 'i', 'haven', \"'\", 't', 'started', 'on', 'them', 'yet', '.', 'starting', 'saturday', ',', 'i', \"'\", 'm', 'taking', '20', 'mg', 'every', 'morning', 'for', '10', 'days', 'and', 'seeing', 'him', 'again', '.', 'i', 'told', 'some', 'of', 'my', 'friends', 'about', 'what', 'was', 'going', 'on', ',', 'and', 'some', 'of', 'them', 'are', '.', '.', '.', 'worried', '.', 'most', 'of', 'us', 'aren', \"'\", 't', '\"', 'normal', '\"', ',', 'really', ',', 'and', 'they', \"'\", 're', 'concerned', 'that', 'taking', 'this', 'kind', 'of', 'medication', 'will', 'turn', 'me', 'into', 'a', 'mind', '##less', 'zombie', '.', 'as', 'well', ',', 'a', 'significant', 'number', 'of', 'them', 'are', 'vocal', '##ly', 'against', 'ad', '##hd', 'medication', ',', 'claiming', 'that', 'there', 'are', 'better', 'ways', 'to', 'deal', 'with', 'it', '.', '(', 'i', 'wonder', 'if', 'they', 'realize', 'that', ',', 'though', 'they', \"'\", 're', 'allowed', 'to', 'believe', 'that', ',', 'bringing', 'it', 'up', 'after', 'i', 'tell', 'them', 'that', 'i', \"'\", 'm', 'going', 'to', 'start', 'taking', 'ad', '##hd', 'medication', 'isn', \"'\", 't', 'going', 'to', 'make', 'me', 'feel', 'any', 'better', '.', ')', 'anyway', ',', 'the', 'reason', 'i', \"'\", 'm', 'posting', 'is', 'that', 'things', 'people', 'have', 'said', 'are', 'awakening', 'fears', 'i', 'didn', \"'\", 't', 'know', 'i', 'had', '.', 'when', 'they', 'said', 'that', 'i', 'would', 'lose', 'my', 'personality', ',', 'even', 'though', 'i', 'know', 'that', \"'\", 's', 'silly', ',', 'it', 'got', 'me', 'thinking', '.', 'how', 'much', 'of', 'the', 'person', 'that', 'i', 'know', 'as', '\"', 'me', '\"', 'is', 'the', 'disorder', 'poking', 'through', '?', 'i', 'don', \"'\", 't', 'want', 'to', 'go', 'on', 'the', 'medicine', 'and', 'stop', 'liking', 'the', 'things', 'i', 'used', 'to', 'like', 'or', '(', 'even', 'worse', '!', ')', 'find', 'myself', 'unable', 'to', 'find', 'anything', 'in', 'common', 'with', 'my', 'friends', '.', 'we', \"'\", 're', 'all', 'kind', 'of', 'weird', ',', 'and', 'i', 'like', 'that', ',', 'but', 'i', \"'\", 'm', 'pretty', 'scared', 'of', 'losing', 'them', '.', 'if', 'i', 'remember', 'correctly', ',', 'it', 'said', 'on', 'the', 'page', 'i', 'filled', 'out', 'that', 'having', 'difficulty', 'writing', 'a', 'conclusion', 'after', 'all', 'the', 'hard', 'work', 'is', 'done', 'is', 'normal', 'for', 'people', 'with', 'ad', '##hd', ',', 'so', 'i', 'feel', 'no', 'shame', 'in', 'what', 'is', 'probably', 'going', 'to', 'be', 'a', 'shitty', 'final', 'paragraph', '.', 'i', \"'\", 'm', 'scared', ',', 'and', 'i', \"'\", 'm', 'excited', ',', 'and', 'i', 'just', 'need', 'to', 'talk', '.', 'for', '15', 'years', 'of', 'my', 'life', 'i', \"'\", 've', 'slipped', 'through', 'the', 'cracks', ','], ['and', 'now', 'i', \"'\", 'm', '(', 'kind', 'of', ')', 'getting', 'a', 'chance', 'to', 'be', 'normal', 'and', 'i', 'just', 'want', 'to', 'talk', 'to', 'someone', '.', 'you', 'guys', 'have', 'all', 'been', 'through', 'this', '(', 'or', 'something', 'similar', ')', ',', 'and', 'i', 'figured', 'this', 'is', 'where', 'i', 'could', 'turn', 'to', '.', 'so', '.', '.', '.', 'hi', '!', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'add', '##eral', '##l', 'ruined', 'my', 'life']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'add', '##eral', '##l', 'ruined', 'my', 'life']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ap', '##a', 'ds', '##m', '-', 'v', 'proposed', 'revision', 'for', 'a', '10', 'attention', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ap', '##a', 'ds', '##m', '-', 'v', 'proposed', 'revision', 'for', 'a', '10', 'attention', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', 'do', 'i', 'have', 'to', 'physically', 'go', 'to', 'the', 'dr', \"'\", 's', 'office', 'once', 'per', 'month', 'to', 'pick', 'up', 'my', 'prescription', 'for', 'add', '##eral', '##l', 'x', '##r', '?', 'i', \"'\", 've', 'been', 'told', 'that', 'i', 'need', 'to', 'get', 'a', 'hand', '-', 'signed', 'prescription', 'from', 'my', 'doctor', ',', 'pick', 'it', 'up', 'in', 'person', 'each', 'month', ',', 'and', 'physically', 'take', 'it', 'to', 'the', 'pharmacy', 'each', 'month', '.', 'the', 'office', 'is', 'about', '45', 'minutes', 'away', 'for', 'me', 'and', 'its', 'getting', 'frustrating', ',', 'time', 'consuming', 'and', 'expensive', '(', 'gas', '-', 'wise', ')', 'to', 'have', 'to', 'get', 'my', 'prescription', 'each', 'month', '.', 'when', 'i', 'factor', 'in', 'having', 'to', 'wait', 'for', 'the', 'prescription', ',', 'the', 'entire', 'process', 'takes', 'about', '2', '.', '5', 'hours', 'out', 'of', 'my', 'day', '.', 'is', 'this', 'necessary', '?', 'and', 'if', 'it', 'is', ',', 'why', '?']\n",
      "INFO:__main__:Number of tokens: 133\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', 'do', 'i', 'have', 'to', 'physically', 'go', 'to', 'the', 'dr', \"'\", 's', 'office', 'once', 'per', 'month', 'to', 'pick', 'up', 'my', 'prescription', 'for', 'add', '##eral', '##l', 'x', '##r', '?', 'i', \"'\", 've', 'been', 'told', 'that', 'i', 'need', 'to', 'get', 'a', 'hand', '-', 'signed', 'prescription', 'from', 'my', 'doctor', ',', 'pick', 'it', 'up', 'in', 'person', 'each', 'month', ',', 'and', 'physically', 'take', 'it', 'to', 'the', 'pharmacy', 'each', 'month', '.', 'the', 'office', 'is', 'about', '45', 'minutes', 'away', 'for', 'me', 'and', 'its', 'getting', 'frustrating', ',', 'time', 'consuming', 'and', 'expensive', '(', 'gas', '-', 'wise', ')', 'to', 'have', 'to', 'get', 'my', 'prescription', 'each', 'month', '.', 'when', 'i', 'factor', 'in', 'having', 'to', 'wait', 'for', 'the', 'prescription', ',', 'the', 'entire', 'process', 'takes', 'about', '2', '.', '5', 'hours', 'out', 'of', 'my', 'day', '.', 'is', 'this', 'necessary', '?', 'and', 'if', 'it', 'is', ',', 'why', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'if', 'einstein', 'had', 'taken', 'rita', '##lin', '?', 'ad', '##hd', \"'\", 's', 'impact', 'on', 'creativity', '-', 'w', '##s', '##j', '.', 'com', '(', '2005', ')']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'if', 'einstein', 'had', 'taken', 'rita', '##lin', '?', 'ad', '##hd', \"'\", 's', 'impact', 'on', 'creativity', '-', 'w', '##s', '##j', '.', 'com', '(', '2005', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ne', '##uro', '##fe', '##ed', '##back', '/', 'quantitative', 'ee', '##g', 'for', 'ad', '##hd', 'diagnosis', '(', '2008', ')']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ne', '##uro', '##fe', '##ed', '##back', '/', 'quantitative', 'ee', '##g', 'for', 'ad', '##hd', 'diagnosis', '(', '2008', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'long', 'will', 'it', 'take', 'for', 'medication', 'to', 'work', '?', 'i', \"'\", 've', 'been', 'on', 'v', '##yna', '##se', 'since', 'thursday', ',', 'and', 'the', 'first', 'day', 'i', 'felt', 'a', 'very', 'slight', 'improvement', ',', 'but', 'yesterday', 'and', 'today', 'there', \"'\", 's', 'been', 'no', 'difference', 'from', 'before', 'i', 'started', 'taking', 'it', '.', '(', 'in', 'fact', ',', 'i', 'think', 'thursday', 'was', 'more', 'power', 'of', 'suggestion', 'than', 'anything', '.', ')', 'from', 'what', 'i', \"'\", 've', 'read', ',', 'it', 'sometimes', 'takes', 'a', 'few', 'days', 'to', 'start', 'working', ',', 'but', 'i', \"'\", 'm', 'thinking', 'that', 'i', 'might', 'need', 'a', 'larger', 'dose', '(', 'the', 'doctor', 'gave', 'me', 'a', 'small', 'dose', 'to', 'test', 'it', 'out', ')', '.', 'still', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'jump', 'to', 'conclusions', '.', 'how', 'long', 'should', 'i', 'wait', 'before', 'i', 'start', 'being', 'concerned', '?']\n",
      "INFO:__main__:Number of tokens: 131\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'long', 'will', 'it', 'take', 'for', 'medication', 'to', 'work', '?', 'i', \"'\", 've', 'been', 'on', 'v', '##yna', '##se', 'since', 'thursday', ',', 'and', 'the', 'first', 'day', 'i', 'felt', 'a', 'very', 'slight', 'improvement', ',', 'but', 'yesterday', 'and', 'today', 'there', \"'\", 's', 'been', 'no', 'difference', 'from', 'before', 'i', 'started', 'taking', 'it', '.', '(', 'in', 'fact', ',', 'i', 'think', 'thursday', 'was', 'more', 'power', 'of', 'suggestion', 'than', 'anything', '.', ')', 'from', 'what', 'i', \"'\", 've', 'read', ',', 'it', 'sometimes', 'takes', 'a', 'few', 'days', 'to', 'start', 'working', ',', 'but', 'i', \"'\", 'm', 'thinking', 'that', 'i', 'might', 'need', 'a', 'larger', 'dose', '(', 'the', 'doctor', 'gave', 'me', 'a', 'small', 'dose', 'to', 'test', 'it', 'out', ')', '.', 'still', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'jump', 'to', 'conclusions', '.', 'how', 'long', 'should', 'i', 'wait', 'before', 'i', 'start', 'being', 'concerned', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'been', 'prescribed', 'too', 'high', 'a', 'dose', 'of', 'v', '##y', '##van', '##se', '?', 'what', 'were', 'your', 'side', 'effects', '?', 'i', \"'\", 'm', 'trying', 'to', 'rule', 'out', 'too', 'high', 'of', 'a', 'dose', '.', 'my', 'doctor', 'is', 'away', 'and', 'the', 'side', 'effects', 'certainly', 'done', 'qualify', 'as', 'remotely', 'close', 'to', 'an', 'emergency', '.', 'edit', '-', 'it', \"'\", 's', 'a', 'tired', ',', 'de', '##pressive', 'and', '/', 'or', 'impending', 'doom', 'feeling', '.', 'lately', 'i', \"'\", 've', 'had', 'some', 'sleep', 'problems', 'and', 'have', 'been', 'eating', 'rather', 'poorly', 'so', 'i', \"'\", 'm', 'trying', 'to', 'correct', 'those', 'and', 'see', 'if', 'they', 'help', '.']\n",
      "INFO:__main__:Number of tokens: 96\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'been', 'prescribed', 'too', 'high', 'a', 'dose', 'of', 'v', '##y', '##van', '##se', '?', 'what', 'were', 'your', 'side', 'effects', '?', 'i', \"'\", 'm', 'trying', 'to', 'rule', 'out', 'too', 'high', 'of', 'a', 'dose', '.', 'my', 'doctor', 'is', 'away', 'and', 'the', 'side', 'effects', 'certainly', 'done', 'qualify', 'as', 'remotely', 'close', 'to', 'an', 'emergency', '.', 'edit', '-', 'it', \"'\", 's', 'a', 'tired', ',', 'de', '##pressive', 'and', '/', 'or', 'impending', 'doom', 'feeling', '.', 'lately', 'i', \"'\", 've', 'had', 'some', 'sleep', 'problems', 'and', 'have', 'been', 'eating', 'rather', 'poorly', 'so', 'i', \"'\", 'm', 'trying', 'to', 'correct', 'those', 'and', 'see', 'if', 'they', 'help', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['are', 'there', 'any', 'medications', 'that', 'don', \"'\", 't', 'make', 'you', 'sweat', '?']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['are', 'there', 'any', 'medications', 'that', 'don', \"'\", 't', 'make', 'you', 'sweat', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['research', ':', 'cognitive', 'behaviour', 'therapy', 'helps', 'adults', 'with', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['research', ':', 'cognitive', 'behaviour', 'therapy', 'helps', 'adults', 'with', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['rita', '##lin', ',', 'first', 'thoughts', '.', 'any', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['rita', '##lin', ',', 'first', 'thoughts', '.', 'any', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'here', 'play', 'the', 'same', 'song', 'over', '&', 'over', '&', 'over', '.', '.', '.', '&', 'over', ',', 'until', 'it', \"'\", 's', 'out', 'of', 'your', 'head', '?', 'this', 'is', 'the', 'song', 'i', \"'\", 've', 'got', 'on', 'repeat', 'at', 'the', 'moment', '.']\n",
      "INFO:__main__:Number of tokens: 41\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'here', 'play', 'the', 'same', 'song', 'over', '&', 'over', '&', 'over', '.', '.', '.', '&', 'over', ',', 'until', 'it', \"'\", 's', 'out', 'of', 'your', 'head', '?', 'this', 'is', 'the', 'song', 'i', \"'\", 've', 'got', 'on', 'repeat', 'at', 'the', 'moment', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'greatest', 'fear']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'greatest', 'fear']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['\"', 'i', 'am', 'so', 'clever', 'that', 'sometimes', 'i', 'don', \"'\", 't', 'understand', 'a', 'single', 'word', 'of', 'what', 'i', \"'\", 'm', 'saying', '.', '\"', '-', 'oscar', 'wilde']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['\"', 'i', 'am', 'so', 'clever', 'that', 'sometimes', 'i', 'don', \"'\", 't', 'understand', 'a', 'single', 'word', 'of', 'what', 'i', \"'\", 'm', 'saying', '.', '\"', '-', 'oscar', 'wilde']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '-', 'pi', ',', 'cycling', 'of', 'ap', '##athy', ',', 'anxiety', ',', 'and', 'slight', 'h', '##yp', '##oman', '##ia']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '-', 'pi', ',', 'cycling', 'of', 'ap', '##athy', ',', 'anxiety', ',', 'and', 'slight', 'h', '##yp', '##oman', '##ia']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['b', '-', 'calm', \"'\", 's', 'noise', 'intervention', 'audio', 'tracks', 'now', 'available', 'for', 'download']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['b', '-', 'calm', \"'\", 's', 'noise', 'intervention', 'audio', 'tracks', 'now', 'available', 'for', 'download']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'feel', 'like', 'i', 'exhibit', 'no', 'symptoms', 'of', 'ad', '##hd', '.', 'after', 'so', 'many', 'appointments', ',', 'my', 'psychiatrist', 'prescribed', 'ad', '##hd', 'medication', 'for', 'me', '.', 'where', 'i', 'dropped', 'out', 'of', 'college', 'before', 'due', 'to', 'problems', 'concentrating', ',', 'i', 'now', 'make', 'honors', 'grades', '.', 'my', 'doctor', 'is', 'convinced', 'i', 'show', 'signs', 'of', 'a', 'certain', 'type', 'of', 'ad', '##hd', ',', 'but', 'i', 'am', 'doubtful', '.', 'my', 'only', 'sy', '##mpt', '##om', 'is', 'problem', 'concentrating', ',', 'and', 'abnormal', 'fatigue', 'after', 'very', 'short', 'periods', 'of', 'concentration', '.', 'i', \"'\", 've', 'always', 'been', 'quiet', ',', 'calm', ',', 'and', 'extremely', 'organized', '.', 'i', \"'\", 've', 'been', 'on', 'many', 'types', 'of', 'medication', ',', 'and', 'this', 'is', 'the', 'only', 'thing', 'that', 'has', 'ever', 'helped', '.', 'would', 'this', 'still', 'be', 'ad', '##hd', ',', 'in', 'common', 'opinion', '?']\n",
      "INFO:__main__:Number of tokens: 127\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'feel', 'like', 'i', 'exhibit', 'no', 'symptoms', 'of', 'ad', '##hd', '.', 'after', 'so', 'many', 'appointments', ',', 'my', 'psychiatrist', 'prescribed', 'ad', '##hd', 'medication', 'for', 'me', '.', 'where', 'i', 'dropped', 'out', 'of', 'college', 'before', 'due', 'to', 'problems', 'concentrating', ',', 'i', 'now', 'make', 'honors', 'grades', '.', 'my', 'doctor', 'is', 'convinced', 'i', 'show', 'signs', 'of', 'a', 'certain', 'type', 'of', 'ad', '##hd', ',', 'but', 'i', 'am', 'doubtful', '.', 'my', 'only', 'sy', '##mpt', '##om', 'is', 'problem', 'concentrating', ',', 'and', 'abnormal', 'fatigue', 'after', 'very', 'short', 'periods', 'of', 'concentration', '.', 'i', \"'\", 've', 'always', 'been', 'quiet', ',', 'calm', ',', 'and', 'extremely', 'organized', '.', 'i', \"'\", 've', 'been', 'on', 'many', 'types', 'of', 'medication', ',', 'and', 'this', 'is', 'the', 'only', 'thing', 'that', 'has', 'ever', 'helped', '.', 'would', 'this', 'still', 'be', 'ad', '##hd', ',', 'in', 'common', 'opinion', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'my', 'fellow', 'ad', '##hd', \"'\", 'er', '##s', 'who', 'take', 'st', '##im', '##ula', '##nts', 'do', 'for', 'a', 'deco', '##nge', '##stan', '##t', '?', 'i', \"'\", 've', 'tried', 'the', 'pe', 'based', 'versions', 'of', 'pseudo', '##ph', '##ed', '##rine', 'and', 'the', 'ingredients', 'make', 'me', 'dr', '##ows', '##y', '.', 'i', \"'\", 've', 'google', '##d', 'quite', 'a', 'bit', '.', 'i', \"'\", 'm', 'hoping', 'someone', 'has', 'a', 'remedy', 'i', 'missed', '.']\n",
      "INFO:__main__:Number of tokens: 65\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'my', 'fellow', 'ad', '##hd', \"'\", 'er', '##s', 'who', 'take', 'st', '##im', '##ula', '##nts', 'do', 'for', 'a', 'deco', '##nge', '##stan', '##t', '?', 'i', \"'\", 've', 'tried', 'the', 'pe', 'based', 'versions', 'of', 'pseudo', '##ph', '##ed', '##rine', 'and', 'the', 'ingredients', 'make', 'me', 'dr', '##ows', '##y', '.', 'i', \"'\", 've', 'google', '##d', 'quite', 'a', 'bit', '.', 'i', \"'\", 'm', 'hoping', 'someone', 'has', 'a', 'remedy', 'i', 'missed', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'is', 'similar', 'to', 'entrepreneur', \"'\", 's', '\"', 'idea', 'disease', '\"']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'is', 'similar', 'to', 'entrepreneur', \"'\", 's', '\"', 'idea', 'disease', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'does', 'r', '/', 'ad', '##hd', 'think', 'about', 'this', 'article', '?', 'specifically', 'the', 'parts', 'about', 'developing', 'tolerance', '.']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'does', 'r', '/', 'ad', '##hd', 'think', 'about', 'this', 'article', '?', 'specifically', 'the', 'parts', 'about', 'developing', 'tolerance', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'home', '##opa', '##thy']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'home', '##opa', '##thy']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'coaching', '–', 'critical', 'guidelines', 'for', 'finding', 'a', 'good', 'quality', 'coach']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'coaching', '–', 'critical', 'guidelines', 'for', 'finding', 'a', 'good', 'quality', 'coach']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'started', 'well', '##bu', '##tri', '##n', 'xl', '150', '##mg']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'started', 'well', '##bu', '##tri', '##n', 'xl', '150', '##mg']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', 'dietary', 'changes', 'alleviate', 'ad', '##hd', 'symptoms', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', 'dietary', 'changes', 'alleviate', 'ad', '##hd', 'symptoms', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 'things', 'done', 'with', 'adult', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 'things', 'done', 'with', 'adult', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['red', '##dit', ',', 'the', 'perfect', 'place', 'to', 'run', 'your', 'mouth', 'off', '.', 'if', 'only', 'people', 'wouldn', \"'\", 't', 'valid', '##ate', 'my', 'ran', '##ts', 'by', 'giving', 'karma', 'for', 'them', '.', 'i', 'dread', 'the', 'moment', 'when', 'i', 'check', 'my', 'comments', 'and', 'notice', 'several', 'dozen', 'votes', 'coming', 'my', 'way', '.', 'what', 'did', 'i', 'say', ',', 'did', 'i', 'make', 'sense', ',', 'am', 'i', 'as', 'usual', 'ran', '##ting', 'about', 'a', 'subject', 'i', 'have', 'little', 'or', 'no', 'expertise', 'on', 'and', 'for', 'some', 'reason', 'getting', 'away', 'with', 'it', '?', 'and', 'worst', 'of', 'all', ',', 'people', 'commenting', 'and', 'forcing', 'you', 'to', 'come', 'up', 'with', 'more', 'ran', '##ting', '.', 'i', \"'\", 'm', 'pretty', 'sure', 'someone', 'here', 'will', 'recognize', 'the', 'feeling', ',', 'red', '##dit', 'feels', 'like', 'a', 'trap', '.', 'edit', ':', 'ran', '##t', ',', 'ran', '##t', '.', 'i', 'didn', \"'\", 't', 'even', 'mention', 'that', 'even', 'though', 'i', \"'\", 've', 'been', 'diagnosed', ',', 'i', 'haven', \"'\", 't', 'been', 'prescribed', 'medication', 'for', 'over', 'a', 'year', 'now', ',', 'just', 'refer', '##ral', '##s', '.', 'ran', '##t', ',', 'ran', '##t', '.', 'what', 'a', 'great', 'day', '.', 'start', 'with', '1', 'incomplete', 'ran', '##t', ',', 'realize', 'it', \"'\", 's', 'going', 'to', 'take', 'an', 'awful', 'lot', 'of', 'time', 'of', 'editing', 'and', 'you', 'haven', \"'\", 't', 'had', 'breakfast', 'or', 'coffee', 'yet', ',', 'post', 'ran', '##t', 'with', 'a', 'comment', 'explaining', 'it', 'will', 'be', 'completed', 'at', 'a', 'later', 'stage', '.', 'brows', '##e', 'red', '##dit', 'trying', 'to', 'relax', ',', 'find', 'something', 'else', 'to', 'ran', '##t', ',', 'get', 'into', 'a', 'ran', '##t', 'where', 'you', 'also', 'know', 'you', 'will', 'need', 'a', 'lot', 'of', 'time', 'to', 'edit', ',', 'post', 'it', 'anyway', 'as', 'the', 'small', 'letter', '##box', 'really', 'doesn', \"'\", 't', 'allow', 'for', 'comfortable', 'editing', '.', 'especially', 'because', 'i', 'tend', 'to', 'skip', 'and', 'connect', 'so', 'most', 'of', 'my', 'sentences', 'end', 'reading', 'like', 'a', 'collision', 'of', 'different', 'sentences', 'by', 'the', 'time', 'i', 'reach', 'the', 'end', 'of', 'my', 'sentence', '.', 'not', 'to', 'mention', 'all', 'those', 'other', 'things', 'that', 'are', 'on', 'my', 'mind', ',', 'which', 'continually', 'clash', 'with', 'my', 'current', 'activities', '.', 'i', \"'\", 'm', 'tired', '.']\n",
      "INFO:__main__:Number of tokens: 328\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['red', '##dit', ',', 'the', 'perfect', 'place', 'to', 'run', 'your', 'mouth', 'off', '.', 'if', 'only', 'people', 'wouldn', \"'\", 't', 'valid', '##ate', 'my', 'ran', '##ts', 'by', 'giving', 'karma', 'for', 'them', '.', 'i', 'dread', 'the', 'moment', 'when', 'i', 'check', 'my', 'comments', 'and', 'notice', 'several', 'dozen', 'votes', 'coming', 'my', 'way', '.', 'what', 'did', 'i', 'say', ',', 'did', 'i', 'make', 'sense', ',', 'am', 'i', 'as', 'usual', 'ran', '##ting', 'about', 'a', 'subject', 'i', 'have', 'little', 'or', 'no', 'expertise', 'on', 'and', 'for', 'some', 'reason', 'getting', 'away', 'with', 'it', '?', 'and', 'worst', 'of', 'all', ',', 'people', 'commenting', 'and', 'forcing', 'you', 'to', 'come', 'up', 'with', 'more', 'ran', '##ting', '.', 'i', \"'\", 'm', 'pretty', 'sure', 'someone', 'here', 'will', 'recognize', 'the', 'feeling', ',', 'red', '##dit', 'feels', 'like', 'a', 'trap', '.', 'edit', ':', 'ran', '##t', ',', 'ran', '##t', '.', 'i', 'didn', \"'\", 't', 'even', 'mention', 'that', 'even', 'though', 'i', \"'\", 've', 'been', 'diagnosed', ',', 'i', 'haven', \"'\", 't', 'been', 'prescribed', 'medication', 'for', 'over', 'a', 'year', 'now', ',', 'just', 'refer', '##ral', '##s', '.', 'ran', '##t', ',', 'ran', '##t', '.', 'what', 'a', 'great', 'day', '.', 'start', 'with', '1', 'incomplete', 'ran', '##t', ',', 'realize', 'it', \"'\", 's', 'going', 'to', 'take', 'an', 'awful', 'lot', 'of', 'time', 'of', 'editing', 'and', 'you', 'haven', \"'\", 't', 'had', 'breakfast', 'or', 'coffee', 'yet', ',', 'post', 'ran', '##t', 'with', 'a', 'comment', 'explaining', 'it', 'will', 'be', 'completed', 'at', 'a', 'later', 'stage', '.', 'brows', '##e', 'red', '##dit', 'trying', 'to', 'relax', ',', 'find', 'something', 'else', 'to', 'ran', '##t', ',', 'get', 'into', 'a', 'ran', '##t', 'where', 'you', 'also', 'know', 'you', 'will', 'need', 'a', 'lot', 'of', 'time', 'to', 'edit', ',', 'post', 'it', 'anyway', 'as', 'the', 'small', 'letter', '##box', 'really', 'doesn', \"'\", 't', 'allow', 'for', 'comfortable', 'editing', '.', 'especially', 'because', 'i', 'tend', 'to', 'skip', 'and', 'connect', 'so', 'most', 'of', 'my', 'sentences', 'end', 'reading', 'like', 'a', 'collision', 'of', 'different', 'sentences', 'by', 'the', 'time', 'i', 'reach', 'the', 'end', 'of', 'my', 'sentence', '.', 'not', 'to', 'mention', 'all', 'those', 'other', 'things', 'that', 'are', 'on', 'my', 'mind', ',', 'which', 'continually', 'clash', 'with', 'my', 'current', 'activities', '.', 'i', \"'\", 'm', 'tired', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'or', 'normal', '?', 'i', \"'\", 'm', 'sort', 'of', 'lost', '.', '.', 'after', 'a', 'psychologist', \"'\", 's', 'report', 'that', 'said', 'i', 'didn', \"'\", 't', 'have', 'ad', '##hd', ',', 'i', 'got', 'referred', 'to', 'a', 'psychiatrist', 'for', 'depression', '.', 'after', 'i', 'mentioned', 'that', 'i', 'didn', \"'\", 't', 'feel', 'depressed', 'all', 'the', 'time', ',', 'and', 'that', 'i', 'felt', 'like', 'ad', '##hd', 'makes', 'more', 'sense', '.', 'after', 'about', '15', '-', '20', 'minutes', ',', 'he', 'wanted', 'to', 'pre', '##scribe', 'medications', 'because', 'i', 'displayed', 'massive', 'pro', '##cr', '##ast', '##ination', ',', 'distracted', '##ness', ',', 'fi', '##dget', '##ing', '.', '.', '.', 'he', 'also', 'said', 'i', 'didn', \"'\", 't', 'have', '\"', 'ad', '##hd', '\"', ',', 'but', 'just', 'a', 'lot', 'of', 'the', 'symptoms', '.', 'it', 'was', 'rather', 'shocking', 'to', 'me', 'because', 'it', 'happened', 'so', 'quickly', '.', 'i', 'was', 'expecting', 'something', 'longer', 'that', 'practically', 'went', 'over', 'my', 'entire', 'life', 'to', 'try', 'to', 'figure', 'it', 'out', '.', '.', '.', 'he', 'gave', 'me', 'focal', '##in', 'x', '##r', 'for', '30', 'days', '.', 'first', 'day', 'was', 'really', 'weird', '.', 'i', 'felt', 'like', '.', '.', 'maybe', 'zombie', '-', 'is', '##h', 'for', '5', 'hours', ',', 'after', 'which', 'i', 'was', 'as', 'hyper', '##active', 'and', 'day', '##dre', '##amy', 'as', 'ever', '(', 'perhaps', 'more', '?', ')', '.', 'i', 'thought', 'it', 'wore', 'off', ',', 'but', 'then', 'at', 'the', '12', 'hour', 'mark', ',', 'i', 'had', 'pal', '##pit', '##ations', 'and', 'ins', '##om', '##nia', 'and', 'such', '.', 'this', 'scared', 'me', 'away', 'for', 'about', 'a', 'week', '.', 'after', 'i', 'started', 'taking', 'it', 'again', ',', 'i', 'didn', \"'\", 't', 'feel', 'any', 'of', 'these', 'things', ',', 'but', 'i', 'also', 'didn', \"'\", 't', 'feel', 'like', 'it', 'helped', 'any', 'of', 'the', 'ad', '##hd', '-', 'like', 'things', 'very', 'much', '.', 'the', 'one', 'exception', 'is', 'that', 'after', 'i', 'took', 'it', 'in', 'the', 'morning', 'as', 'my', 'alarm', 'went', 'off', ',', 'it', \"'\", 's', 'almost', 'trivial', 'to', 'get', 'up', '30', '-', '45', 'minutes', 'later', '(', 'whereas', 'before', ',', 'it', 'was', 'absolutely', 'impossible', ')', '.', 'i', 'also', 'wasn', \"'\", 't', 'able', 'to', 'fall', 'asleep', 'again', 'like', 'i', 'usually', 'do', ',', 'which', 'doesn', \"'\", 't', 'make', 'much', 'sense', 'because', 'the', 'pill', 'probably', 'hasn', \"'\", 't', 'had', 'time', 'to', 'dissolve', 'or', 'whatever', 'yet', '.', 'so', 'i', \"'\", 'm', 'thinking', 'of', 'finding', 'a', 'new', 'psychiatrist', 'because', 'this', '30', '-', 'minute', 'appointment', 'seemed', 'ir', '##res', '##pon', '##si', '##bly', 'fast', 'to', 'dia', '##gno', '##se', 'me', '(', 'or', 'rather', ',', 'not', 'dia', '##gno', '##se', 'me', '?', ')', '.', 'another', 'issue', 'is', 'that', 'i', 'don', \"'\", 't', 'really', 'care', 'about', 'med', '##s', '.', 'i', 'just', 'want', 'to', 'know', 'whether', 'it', 'was', 'ad', '##hd', 'holding', 'me', 'back', 'all', 'these', 'years', '.', 'i', \"'\", 'm', 'still', 'somewhat', 'at', 'an', 'identity', 'crisis', 'with', 'that', '.', 'the', 'psychological', 'report', 'said', 'i', 'don', \"'\", 't', 'have', 'it', ',', 'but', 'i', 'find', 'my', 'scores', 'rather', 'odd', '.', 'i', 'took', 'two', 'intelligence', 'tests', '.', 'one', 'scored', 'me', 'at', '127', '(', 'not', 'administered', 'face', '-', 'to', '-', 'face', ',', 'was', 'via', 'computer', ')', 'and', 'the', 'other', 'at', '109', '(', 'through', 'the', 'ps', '##ych', ')', '.', 'however', ',', 'from', 'the', 'wai', '##s', ',', 'my', 'working', 'memory', 'was', '102', ',', 'and', 'my', 'processing', 'speed', 'was', '89', '.', 'however', ',', 'my', 'other', 'tests', 'seemed', 'to', 'go', 'okay', '.', 'the', 'connor', '##s', 'continuous', 'performance', 'test', 'said', 'everything', 'was', 'average', 'except', 'for', 'one', 'score', 'of', 'imp', '##ulsive', 'on', 'one', 'part', '.', 'the', 'final', 'conclusion', 'was', 'that', 'attention', '-', 'demanding', 'tasks', 'were', 'my', 'worst', 'scores', 'but', 'not', 'def', '##icient', 'enough', 'to', 'be', 'disorder', '-', 'level', '.', 'but', 'i', 'still', 'have', 'all', 'these', 'issues', '.', 'looking', 'back', 'on', 'my', 'transcript', ',', 'so', 'many', 'of', 'just', 'my', 'good', 'grades', 'were', 'border', '##line', ',', 'only', 'low', 'as', 'because', 'i', 'either', 'did', 'near', '-', 'perfect', 'on', 'tests', 'or', 'made', 'up', 'a', 'bunch', 'of', 'work', 'at', 'the', 'last', 'second', '.', 'and', 'chronic', 'tar', '##dine', '##ss', 'to', 'school', 'because', 'of', 'issues', 'getting', 'up', 'have', 'plagued', 'me', 'since', '8th', 'grade', 'and', 'have', 'gotten', 'me', 'detention', '##s', ',', 'failing', 'grades', ',', 'and', 'in', '-', 'school', 'suspension', '##s', '.', 'and', '(', 'usually', 'in', 'private', ')', 'i', 'day', '##dre', '##am', 'constantly', ',', 'and', 'if', 'i', 'get', 'too', 'lost', 'in', 'that', ',', 'i', \"'\", 'm', 'running', 'around', 'and', 'jumping', 'on', 'things', 'and', 'not', 'even', 'aware', 'of', 'it', '.', 'and', 'i', 'fi', '##dget', 'a', 'lot', '(', 'tap', 'my', 'leg', ',', 'play', 'with', 'my', 'shirt', 'or', 'some', 'fabric', ',', 'etc', '.', ')', 'but', 'then', 'when', 'i', 'see', 'someone', 'else', 'mention', 'that', 'they', 'pro', '##cr', '##ast', '##inated', 'the', 'entire', 'night', 'or', 'keep', 'hitting', 's', '##no', '##oz', '##e', ',', 'or', 'see', 'someone', 'tapping', 'their', 'foot', ',', 'i', 'get', 'discouraged', 'and', 'think', 'that', 'maybe', 'i', \"'\", 'm', 'just', 'normal', 'and', 'failed', 'all', 'this', 'time', 'because', 'of', 'my', 'own', 'will', '##power', 'or', 'whatever', '.', 'it', \"'\", 's', 'getting', 'ti', '##ring', 'not', 'having', 'a', 'yes', 'or', 'no', 'diagnosis', '.', '.', '(', 'well', 'that', 'was', 'a', 'rather', 'long', 'and', 'scattered', 'post', '.', '.', '.', ')', 't', '##l', ';', 'dr', ':', 'i', 'don', \"'\", 't', 'have', 'ad', '##hd', '.', 'i', 'got', 'med', '##s', '.', 'i', 'still', 'think', 'i', 'have', 'it', '.', 'but', 'what', 'if', 'i', \"'\", 'm', 'just', 'regular', '?', 'and', 'how', 'do', 'i', 'figure', 'it', 'out', 'when', 'the', 'process', 'of', 'going', 'from', 'doctor', 'to', 'doctor', 'is', 'taking', 'so', 'long', '?', ':', '(']\n",
      "INFO:__main__:Number of tokens: 849\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'or', 'normal', '?', 'i', \"'\", 'm', 'sort', 'of', 'lost', '.', '.', 'after', 'a', 'psychologist', \"'\", 's', 'report', 'that', 'said', 'i', 'didn', \"'\", 't', 'have', 'ad', '##hd', ',', 'i', 'got', 'referred', 'to', 'a', 'psychiatrist', 'for', 'depression', '.', 'after', 'i', 'mentioned', 'that', 'i', 'didn', \"'\", 't', 'feel', 'depressed', 'all', 'the', 'time', ',', 'and', 'that', 'i', 'felt', 'like', 'ad', '##hd', 'makes', 'more', 'sense', '.', 'after', 'about', '15', '-', '20', 'minutes', ',', 'he', 'wanted', 'to', 'pre', '##scribe', 'medications', 'because', 'i', 'displayed', 'massive', 'pro', '##cr', '##ast', '##ination', ',', 'distracted', '##ness', ',', 'fi', '##dget', '##ing', '.', '.', '.', 'he', 'also', 'said', 'i', 'didn', \"'\", 't', 'have', '\"', 'ad', '##hd', '\"', ',', 'but', 'just', 'a', 'lot', 'of', 'the', 'symptoms', '.', 'it', 'was', 'rather', 'shocking', 'to', 'me', 'because', 'it', 'happened', 'so', 'quickly', '.', 'i', 'was', 'expecting', 'something', 'longer', 'that', 'practically', 'went', 'over', 'my', 'entire', 'life', 'to', 'try', 'to', 'figure', 'it', 'out', '.', '.', '.', 'he', 'gave', 'me', 'focal', '##in', 'x', '##r', 'for', '30', 'days', '.', 'first', 'day', 'was', 'really', 'weird', '.', 'i', 'felt', 'like', '.', '.', 'maybe', 'zombie', '-', 'is', '##h', 'for', '5', 'hours', ',', 'after', 'which', 'i', 'was', 'as', 'hyper', '##active', 'and', 'day', '##dre', '##amy', 'as', 'ever', '(', 'perhaps', 'more', '?', ')', '.', 'i', 'thought', 'it', 'wore', 'off', ',', 'but', 'then', 'at', 'the', '12', 'hour', 'mark', ',', 'i', 'had', 'pal', '##pit', '##ations', 'and', 'ins', '##om', '##nia', 'and', 'such', '.', 'this', 'scared', 'me', 'away', 'for', 'about', 'a', 'week', '.', 'after', 'i', 'started', 'taking', 'it', 'again', ',', 'i', 'didn', \"'\", 't', 'feel', 'any', 'of', 'these', 'things', ',', 'but', 'i', 'also', 'didn', \"'\", 't', 'feel', 'like', 'it', 'helped', 'any', 'of', 'the', 'ad', '##hd', '-', 'like', 'things', 'very', 'much', '.', 'the', 'one', 'exception', 'is', 'that', 'after', 'i', 'took', 'it', 'in', 'the', 'morning', 'as', 'my', 'alarm', 'went', 'off', ',', 'it', \"'\", 's', 'almost', 'trivial', 'to', 'get', 'up', '30', '-', '45', 'minutes', 'later', '(', 'whereas', 'before', ',', 'it', 'was', 'absolutely', 'impossible', ')', '.', 'i', 'also', 'wasn', \"'\", 't', 'able', 'to', 'fall', 'asleep', 'again', 'like', 'i', 'usually', 'do', ',', 'which', 'doesn', \"'\", 't', 'make', 'much', 'sense', 'because', 'the', 'pill', 'probably', 'hasn', \"'\", 't', 'had', 'time', 'to', 'dissolve', 'or', 'whatever', 'yet', '.', 'so', 'i', \"'\", 'm', 'thinking', 'of', 'finding', 'a', 'new', 'psychiatrist', 'because', 'this', '30', '-', 'minute', 'appointment', 'seemed', 'ir', '##res', '##pon', '##si', '##bly', 'fast', 'to', 'dia', '##gno', '##se', 'me', '(', 'or', 'rather', ',', 'not', 'dia', '##gno', '##se', 'me', '?', ')', '.', 'another', 'issue', 'is', 'that', 'i', 'don', \"'\", 't', 'really', 'care', 'about', 'med', '##s', '.', 'i', 'just', 'want', 'to', 'know', 'whether', 'it', 'was', 'ad', '##hd', 'holding', 'me', 'back', 'all', 'these', 'years', '.', 'i', \"'\", 'm', 'still', 'somewhat', 'at', 'an', 'identity', 'crisis', 'with', 'that', '.', 'the', 'psychological', 'report', 'said', 'i', 'don', \"'\", 't', 'have', 'it', ',', 'but', 'i', 'find', 'my', 'scores', 'rather', 'odd', '.', 'i', 'took', 'two', 'intelligence', 'tests', '.', 'one', 'scored', 'me', 'at', '127', '(', 'not', 'administered', 'face', '-', 'to', '-', 'face', ',', 'was', 'via', 'computer', ')', 'and', 'the', 'other', 'at', '109', '(', 'through', 'the', 'ps', '##ych', ')', '.', 'however', ',', 'from', 'the', 'wai', '##s', ',', 'my', 'working', 'memory', 'was', '102', ',', 'and', 'my', 'processing', 'speed', 'was', '89', '.', 'however', ',', 'my', 'other', 'tests'], ['seemed', 'to', 'go', 'okay', '.', 'the', 'connor', '##s', 'continuous', 'performance', 'test', 'said', 'everything', 'was', 'average', 'except', 'for', 'one', 'score', 'of', 'imp', '##ulsive', 'on', 'one', 'part', '.', 'the', 'final', 'conclusion', 'was', 'that', 'attention', '-', 'demanding', 'tasks', 'were', 'my', 'worst', 'scores', 'but', 'not', 'def', '##icient', 'enough', 'to', 'be', 'disorder', '-', 'level', '.', 'but', 'i', 'still', 'have', 'all', 'these', 'issues', '.', 'looking', 'back', 'on', 'my', 'transcript', ',', 'so', 'many', 'of', 'just', 'my', 'good', 'grades', 'were', 'border', '##line', ',', 'only', 'low', 'as', 'because', 'i', 'either', 'did', 'near', '-', 'perfect', 'on', 'tests', 'or', 'made', 'up', 'a', 'bunch', 'of', 'work', 'at', 'the', 'last', 'second', '.', 'and', 'chronic', 'tar', '##dine', '##ss', 'to', 'school', 'because', 'of', 'issues', 'getting', 'up', 'have', 'plagued', 'me', 'since', '8th', 'grade', 'and', 'have', 'gotten', 'me', 'detention', '##s', ',', 'failing', 'grades', ',', 'and', 'in', '-', 'school', 'suspension', '##s', '.', 'and', '(', 'usually', 'in', 'private', ')', 'i', 'day', '##dre', '##am', 'constantly', ',', 'and', 'if', 'i', 'get', 'too', 'lost', 'in', 'that', ',', 'i', \"'\", 'm', 'running', 'around', 'and', 'jumping', 'on', 'things', 'and', 'not', 'even', 'aware', 'of', 'it', '.', 'and', 'i', 'fi', '##dget', 'a', 'lot', '(', 'tap', 'my', 'leg', ',', 'play', 'with', 'my', 'shirt', 'or', 'some', 'fabric', ',', 'etc', '.', ')', 'but', 'then', 'when', 'i', 'see', 'someone', 'else', 'mention', 'that', 'they', 'pro', '##cr', '##ast', '##inated', 'the', 'entire', 'night', 'or', 'keep', 'hitting', 's', '##no', '##oz', '##e', ',', 'or', 'see', 'someone', 'tapping', 'their', 'foot', ',', 'i', 'get', 'discouraged', 'and', 'think', 'that', 'maybe', 'i', \"'\", 'm', 'just', 'normal', 'and', 'failed', 'all', 'this', 'time', 'because', 'of', 'my', 'own', 'will', '##power', 'or', 'whatever', '.', 'it', \"'\", 's', 'getting', 'ti', '##ring', 'not', 'having', 'a', 'yes', 'or', 'no', 'diagnosis', '.', '.', '(', 'well', 'that', 'was', 'a', 'rather', 'long', 'and', 'scattered', 'post', '.', '.', '.', ')', 't', '##l', ';', 'dr', ':', 'i', 'don', \"'\", 't', 'have', 'ad', '##hd', '.', 'i', 'got', 'med', '##s', '.', 'i', 'still', 'think', 'i', 'have', 'it', '.', 'but', 'what', 'if', 'i', \"'\", 'm', 'just', 'regular', '?', 'and', 'how', 'do', 'i', 'figure', 'it', 'out', 'when', 'the', 'process', 'of', 'going', 'from', 'doctor', 'to', 'doctor', 'is', 'taking', 'so', 'long', '?', ':', '(']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'a', 'weird', 'personality', 'and', 'attitude', 'or', 'mild', '(', 'if', 'possible', '?', ')', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'a', 'weird', 'personality', 'and', 'attitude', 'or', 'mild', '(', 'if', 'possible', '?', ')', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['this', 'is', 'a', 'x', '/', 'post', 'from', 'ask', '##red', '##dit', '.', 'i', \"'\", 'm', 'going', 'through', 'some', 'really', ',', 'really', 'weird', 'shit', 'and', 'i', 'was', 'wondering', 'if', 'you', 'guys', 'could', 'help', '?']\n",
      "INFO:__main__:Number of tokens: 32\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['this', 'is', 'a', 'x', '/', 'post', 'from', 'ask', '##red', '##dit', '.', 'i', \"'\", 'm', 'going', 'through', 'some', 'really', ',', 'really', 'weird', 'shit', 'and', 'i', 'was', 'wondering', 'if', 'you', 'guys', 'could', 'help', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['lack', 'of', 'ad', '##hd', 'drugs', 'prompt', 'hundreds', 'of', 'complaints', 'daily', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['lack', 'of', 'ad', '##hd', 'drugs', 'prompt', 'hundreds', 'of', 'complaints', 'daily', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['resources', '?', 'i', 'have', 'ad', '##hd', '.', 'i', 'however', ',', 'do', 'not', 'have', 'a', 'full', 'time', 'job', '.', 'nor', 'do', 'i', 'have', 'health', 'insurance', '.', 'i', \"'\", 'm', 'a', 'full', 'time', 'student', '.', 'i', 'am', 'a', 'single', 'father', '.', 'does', 'anyone', 'know', 'of', 'any', 'resources', 'available', 'to', 'help', 'get', 'medication', '?', 'i', 'used', 'to', 'be', 'on', 'add', '##eral', '##l', 'x', '##r', ',', 'and', 'that', 'worked', 'wonders', 'for', 'me', '.', 'but', 'if', 'i', 'remember', 'correct', ',', 'it', 'was', 'super', 'expensive', '.', 'i', 'appreciate', 'any', 'help', '!']\n",
      "INFO:__main__:Number of tokens: 85\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['resources', '?', 'i', 'have', 'ad', '##hd', '.', 'i', 'however', ',', 'do', 'not', 'have', 'a', 'full', 'time', 'job', '.', 'nor', 'do', 'i', 'have', 'health', 'insurance', '.', 'i', \"'\", 'm', 'a', 'full', 'time', 'student', '.', 'i', 'am', 'a', 'single', 'father', '.', 'does', 'anyone', 'know', 'of', 'any', 'resources', 'available', 'to', 'help', 'get', 'medication', '?', 'i', 'used', 'to', 'be', 'on', 'add', '##eral', '##l', 'x', '##r', ',', 'and', 'that', 'worked', 'wonders', 'for', 'me', '.', 'but', 'if', 'i', 'remember', 'correct', ',', 'it', 'was', 'super', 'expensive', '.', 'i', 'appreciate', 'any', 'help', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['notes', 'from', 'ad', '##hd', 'in', 'women', ':', 'a', 'hidden', 'disorder', ',', 'a', 'lecture', 'by', 'dr', '.', 'patricia', 'o', '.', 'quinn', 'dr', '.', 'quinn', ',', 'and', 'expert', 'and', 'speaker', 'on', 'ad', '##hd', ',', 'came', 'to', 'my', 'university', 'a', 'few', 'months', 'ago', 'and', 'i', 'thought', 'i', \"'\", 'd', 'pass', 'along', 'the', 'notes', 'i', 'took', '.', 'i', 'know', 'it', \"'\", 's', 'lengthy', 'but', 'i', 'learned', 'a', 'lot', 'from', 'this', 'lecture', '.', 'as', 'a', 'female', 'who', 'got', 'diagnosed', 'with', 'ad', '##hd', 'in', 'college', ',', 'i', 'can', 'at', '##test', 'that', 'so', 'much', 'makes', 'sense', 'and', 'that', 'this', 'disorder', 'extends', 'further', 'than', 'i', 'was', 'aware', 'of', '.', 'i', 'hope', 'you', 'guys', 'find', 'this', 'as', 'helpful', 'as', 'i', 'have', '!', '*', '*', 'edit', '!', '*', '*', 'her', 'lecture', 'is', 'available', 'on', 'itunes', 'for', 'free', '.', '[', 'this', 'link', ']', '(', 'http', ':', '/', '/', 'www', '.', 'un', '##c', '.', 'ed', '##u', '/', 'as', '##p', '/', 'ad', '##hdi', '##ng', '##ir', '##ls', '##and', '##wo', '##ment', '##he', '##hid', '##den', '##dis', '##ord', '##er', '.', 'h', '##tm', ')', 'has', 'her', 'power', '##point', 'slides', 'from', 'her', 'lecture', ',', 'the', 'link', 'to', 'itunes', ',', 'and', 'recommended', 'books', '.', '*', '*', 'historical', 'impact', 'on', 'gender', 'issues', '*', '*', '*', '*', 'what', 'we', 'looked', 'at', ',', 'what', 'we', 'looked', 'for', 'pre', '##de', '##ter', '##mined', 'what', 'we', 'found', '*', '*', '*', 'mostly', 'white', 'male', 'hyper', '##active', 'sub', '##type', '*', 'separating', 'fact', 'from', 'fiction', '*', '*', 'many', 'similarities', 'between', 'the', 'gender', '##s', '*', 'ph', '##eno', '##typic', 'core', 'of', 'ad', '##hd', 'symptoms', 'similar', 'to', 'what', 'has', 'been', 'observed', 'in', 'boys', '*', 'recent', 'findings', 'stress', 'the', 'severity', 'of', 'disorder', 'in', 'females', '*', 'more', 'mood', 'and', 'anxiety', 'disorders', ',', 'less', 'conduct', 'disorder', '*', 'prep', '##ond', '##eran', '##ce', 'of', 'ina', '##tten', '##tive', 'symptoms', '*', '18', 'symptoms', ':', '9', 'in', 'ina', '##tten', '##tive', ',', '9', 'in', 'hyper', '##active', '/', 'imp', '##ulsive', '*', '6', 'out', 'of', '9', 'symptoms', 'from', 'specific', 'category', 'puts', 'you', 'in', 'specified', 'sub', '##type', ',', '6', 'of', '9', 'out', 'of', 'any', '(', 'each', '?', 'sorry', 'don', \"'\", 't', 'remember', ')', 'puts', 'you', 'in', 'combined', '*', 'como', '##rb', '##idi', '##ty', ':', 'anxiety', 'result', 'from', 'ad', '##hd', '*', '*', 'how', 'are', 'differences', 'manifested', '?', '*', '*', '*', 'receive', 'f', 'in', 'test', ':', 'boy', 'says', 'test', 'is', 'stupid', ';', 'girl', 'says', '\"', 'i', 'am', 'stupid', '.', '\"', '*', 'boys', 'external', '##ize', 'while', 'girls', 'internal', '##ize', '*', 'self', '-', 'blame', ',', 'self', '-', 'at', '##tri', '##bution', '*', 'low', 'self', '-', 'esteem', '*', 'demo', '##ral', '##ization', '(', 'looks', 'like', 'low', 'level', 'depression', ')', '*', 'moves', 'into', 'depression', '*', '*', 'need', 'to', 'investigate', 'internal', '##izing', 'patterns', '*', '*', '*', 'internal', '##izing', 'patterns', 'require', 'much', 'closer', 'observation', '*', 'require', 'self', '-', 'report', 'evaluation', '##s', '*', '*', 'mit', '##iga', '##ting', 'circumstances', '(', 'especially', 'important', 'in', 'understanding', 'why', 'girls', 'go', 'und', '##ia', '##gno', '##sed', ')', '*', '*', '*', 'fewer', 'ad', '##hd', 'symptoms', '*', 'grew', 'up', 'in', 'a', 'structured', 'environment', '(', 'high', 'school', ';', 'parents', 'doing', 'the', 'organizing', ')', '*', 'high', 'social', 'economic', 'status', '(', 'can', 'afford', 'tutor', '##ing', ',', 'academic', 'coaches', ',', 'etc', '.', ')', '*', 'no', 'opposition', '##al', 'defiant', 'disorder', '(', 'odd', ')', 'or', 'conduct', 'disorder', '(', 'cd', ')', '*', 'high', 'iq', '*', 'social', 'pressure', '/', 'need', 'for', 'approval', '*', '*', 'why', 'learn', 'about', 'ad', '##hd', 'in', 'women', '?', '*', '*', '*', '*', 'und', '##ia', '##gno', '##sed', 'ad', '##hd', 'is', 'a', 'significant', 'health', 'concern', '*', '*', '*', '2nd', 'most', 'common', 'psychological', 'problem', 'in', 'adults', '*', 'affects', '4', 'million', 'women', '-', '-', 'most', 'und', '##ia', '##gno', '##sed', '*', 'accidents', '/', 'injuries', '*', 'abuse', '-', '-', 'psychological', '/', 'physical', '/', 'sexual', 'abuse', 'and', 'trauma', '*', 'marital', 'difficulties', '/', 'sexual', 'desires', '-', 'story', ':', 'couple', 'in', 'the', 'middle', 'of', 'getting', 'it', 'on', ',', 'woman', 'remembers', 'that', 'the', 'kids', 'need', 'milk', ',', 'leaves', 'to', 'go', 'back', '.', 'needles', '##s', 'to', 'say', 'the', 'husband', 'was', 'upset', '.', 'she', 'didn', \"'\", 't', 'understand', 'why', '.', '*', 'un', '##pl', '##anne', '##d', 'pre', '##gnan', '##cies', '-', 'distract', '##ability', ',', 'not', 'planning', 'ahead', ',', 'low', 'self', '-', 'esteem', ',', 'not', 'good', 'at', 'remembering', 'to', 'take', 'birth', 'control', '*', 'parenting', 'problems', '-', '-', 'child', 'with', 'ad', '##hd', '-', 'couldn', \"'\", 't', 'keep', 'track', 'of', 'kids', '-', 'bad', 'at', 'problem', 'solving', '-', '-', 'in', '##fle', '##xi', '##ble', 'thinking', ',', 'trouble', 'figuring', 'out', 'alternative', 'solutions', '*', 'poor', 'self', '-', 'esteem', '/', 'depression', '/', 'fi', '##bro', '##my', '##al', '##gia', '-', 'ad', '##hd', 'med', '##s', 'also', 'treat', 'fi', '##bro', '##my', '##al', '##gia', '*', 'sleep', 'disorders', '-', '-', 'no', 'off', 'switch', '*', 'eating', 'disorders', '-', 'bing', '##e', 'eating', ',', 'lack', 'of', 'awareness', 'of', 'full', '##ness', '-', 'obe', '##se', '-', 'little', 'control', 'to', 'be', 'an', '##ore', '##xi', '##c', '*', '*', 'presenting', 'complaints', 'for', 'women', '*', '*', '*', 'feeling', 'overwhelmed', '*', 'chronic', '##ally', 'di', '##sor', '##gan', '##ized', '*', 'chronic', '##ally', 'late', ',', 'poor', 'time', 'management', '-', 'live', 'in', 'the', 'now', '-', '-', 'not', 'getting', 'much', 'done', '-', 'don', \"'\", 't', 'have', 'to', 'do', 'it', 'now', ',', 'ends', 'up', 'not', 'doing', 'it', 'at', 'all', '-', 'not', 'learning', 'from', 'past', 'experiences', 'so', 'not', 'worried', '*', 'sense', 'of', 'shame', ',', 'ina', '##de', '##qua', '##cy', '-', '-', 'can', \"'\", 't', 'live', 'up', 'to', 'typical', 'societal', 'expectations', '*', 'meal', '-', 'planning', '/', 'preparation', 'and', 'other', 'daily', 'living', 'activities', 'is', 'a', 'challenge', '*', 'problems', 'with', 'friendships', '-', 'lack', 'of', 'attention', 'to', 'details', '(', 'won', \"'\", 't', 'notice', 'new', 'hair', '##cut', 'or', 'new', 'earrings', 'etc', ',', 'friend', 'may', 'be', 'offended', ')', '-', 'hard', 'time', 'connecting', 'with', 'females', '-', 'often', 'have', 'male', 'friends', '*', 'doesn', \"'\", 't', 'know', 'how', 'to', '\"', 'na', '##g', '\"', 'herself', '*', '*', 'over', '##t', 'chaos', 'vs', '.', 'silent', 'suffering', '*', '*', '*', 'some', 'women', 'seek', 'treatment', 'for', 'ad', '##hd', 'because', 'their', 'lives', 'are', 'out', 'of', 'control', '*', 'other', 'women', 'are', 'more', 'successful', 'at', 'hiding', 'their', 'ad', '##hd', '-', '-', 'struggle', 'valiant', '##ly', '*', 'develop', 'ob', '##ses', '##sive', 'com', '##pu', '##ls', '##ive', 'disorder', 'personality', 'disorder', '/', 'perfection', '##ism', 'to', 'cope', '*', 'anxiety', 'and', 'depression', 'en', '##su', '##e', '*', 'learned', 'helpless', '##ness', '*', '*', 'ina', '##tten', '##tive', 'type', '*', '*', '*', 'more', 'passive', 'academic', '##ally', '*', 'shy', ',', 'tim', '##id', ',', 'easily', 'overwhelmed', '*', 'tendency', 'to', 'withdraw', ',', 'not', 'act', 'out', '*', 'expressive', 'language', '*', 'word', 'retrieval', 'problems', '*', 'expressive', 'language', 'difficulties', '*', 'h', '##yp', '##oa', '##ctive', ',', 'let', '##har', '##gic', ',', 'easily', 'discouraged', '*', 'day', '##dre', '##ame', '##rs', '*', '*', 'combined', 'type', '*', '*', '*', 'hyper', '-', 'talk', '##ative', ',', 'hyper', '-', 'social', ',', 'hyper', '-', 'reactive', '*', 'restless', 'and', 'fi', '##dget', '##y', '*', 'charismatic', ',', 'boss', '##y', ',', 'stubborn', ',', '\"', 'spoiled', '\"', '*', 'can', \"'\", 't', 'manage', 'daily', 'demands', ',', 'blame', '##s', 'others', '*', '*', 'hyper', '##active', '/', 'imp', '##ulsive', 'sub', '##type', '*', '*', '*', 'rare', '*', 'seen', 'as', '\"', 'difficult', '\"', 'by', 'age', '3', 'or', '4', '*', 'tan', '##trum', '##s', 'etc', '.', '*', '*', 'diagnostic', 'issues', 'for', 'ad', '##hd', 'in', 'women', 'and', 'older', 'girls', '*', '*', '*', 'often', 'no', 'report', 'of', 'early', 'childhood', 'issues', '*', 'mother', '/', 'daughter', 'd', '##yad', '(', 'relationship', ')', ':', 'good', 'in', 'school', ',', 'hyper', '##cr', '##itic', '##ized', 'by', 'mother', ',', 'opposition', '##al', 'daughter', '*', 'ina', '##tten', '##tive', 'type', 'often', 'overlooked', 'by', 'professionals', ',', 'parents', ',', 'and', 'teachers', '*', 'often', 'more', 'teacher', 'compliant', '*', 'good', 'academic', 'record', 'in', 'early', 'years', '*', 'hyper', '##act', '##ivity', '/', 'imp', '##uls', '##ivity', 'less', 'common', '*', 'symptoms', 'present', 'at', 'pub', '##erty', '*', 'high', 'iq', 'in', 'girls', 'may', 'function', 'well', '*', 'anxiety', 'at', '8', ',', 'depression', 'at', '[', 'didn', \"'\", 't', 'hear', ']', '*', 'como', '##rb', '##idi', '##ty', 'with', 'bipolar', 'disorder', 'but', 'both', 'need', 'to', 'be', 'treated', '*', '*', 'pub', '##erty', 'in', 'girls', 'with', 'ad', '##hd', '*', '*', '*', 'for', 'boys', 'hyper', '##act', '##iv', '##iy', 'goes', 'down', 'while', 'for', 'girls', 'it', 'goes', 'up', '*', 'increase', 'in', 'depression', 'and', 'anxiety', '*', 'increased', 'likelihood', 'of', 'pm', '##s', 'and', 'pm', '##dd', '*', '*', 'the', 'lost', 'girls', '*', '*', '*', 'often', 'mis', '##dia', '##gno', '##sed', 'as', 'having', 'depression', 'or', 'bipolar', 'disorder', '*', 'may', 'be', 'at', 'greater', 'risk', 'for', ':', 'chronic', 'under', '##achi', '##eve', '##ment', ',', 'smoking', ',', 'substance', 'abuse', ',', 'pregnancy', '/', 'st', '##ds', ',', 'driving', 'problems', '*', 'girls', 'are', 'more', 'likely', 'to', 'have', 'taken', 'anti', '-', 'de', '##press', '##ants', 'when', 'diagnosed', 'with', 'ad', '##hd', '[', 'boys', '-', '5', '%', 'girls', '14', '%', ')', '*', '*', 'program', 'for', 'achieving', 'success', '*', '*', '*', 'education', '(', 'self', '-', 'knowledge', ')', '*', 'self', '-', 'advocacy', '*', 'support', 'groups', '*', 'counseling', '/', 'psycho', '##therapy', '/', 'family', 'therapy', '*', 'creating', 'an', 'ad', '##hd', '-', 'friendly', 'lifestyle', '*', 'medication', '*', 'improving', 'organization', 'skills', '/', 'professional', 'organizer', '*', 'coaching', '*', 'pursuing', 'a', 'healthy', 'lifestyle', '*', '*', 'husbands', '*', '*', '*', 'first', 'one', 'usually', 'has', 'o', '##cd', 'to', 'over', '##com', '##pen', '##sat', '##e', 'for', 'lack', 'of', 'organization', '*', 'second', 'one', 'usually', 'has', 'ad', '##hd', 'because', 'he', 'understands', 'from', 'personal', 'experience', '*', 'third', 'one', 'is', 'usually', 'just', 'right', 'for', 'her', '*', '*', 'besides', 'medication', '*', '*', '*', 'exercise', '(', 'works', 'for', 'about', '2', '-', '3', 'hr', '##s', ')', '*', 'omega', '3', '*', 'getting', 'enough', 'sleep', '*', 'academic', 'coach', '*', 'yoga', 'and', 'meditation', '*', '*', 'treatment', 'issues', 'for', 'girls', '*', '*', '*', 'don', \"'\", 't', 'need', 'as', 'much', '\"', 'behavior', 'management', '\"', 'issues', '*', 'peer', 'issues', 'more', 'intense', '*', '*', 'medication', 'issues', '*', '*', '*', 'anxiety', 'and', 'depression', 'may', 'need', 'to', 'be', 'treated', 'along', 'with', 'ad', '##hd', 'in', 'adolescence', '*', 'ad', '##hd', 'treated', 'first', ',', '60', '%', 'will', 'no', 'longer', 'have', 'anxiety', '/', 'depression', '*', 'pm', '##s', 'issues', '*', 'prem', '##ens', '##tr', '##ual', 'mag', '##ni', '##fication', 'of', 'ad', '##hd', 'symptoms', '*', 'st', '##im', '##ula', '##nts', 'are', 'more', 'effective', 'during', 'f', '##oll', '##icular', 'phase', 'of', 'cycle', '(', 'when', 'men', '##st', '##ru', '##ation', 'begins', 'until', 'o', '##vu', '##lation', ',', 'days', '1', '-', '13', ')', '*', '*', 'q', '&', 'a', '*', '*', '*', '*', 'risk', 'factors', '?', '*', '*', '*', 'problems', 'in', 'first', 'trim', '##ester', '*', 'genetic', 'factors', '*', 'pest', '##icide', '*', 'prem', '##at', '##urity', 'risk', 'factor', '*', 'men', '##ing', '##itis', '*', 'each', 'child', 'born', 'from', 'a', 'mother', 'with', 'ad', '##hd', 'have', 'a', '50', '-', '60', '%', 'chance', 'of', 'getting', 'it', '*', '*', 'at', 'what', 'point', 'is', 'it', 'a', 'disorder', 'and', 'not', 'blame', '?', '*', '*', '*', '9', '.', '6', '%', 'of', 'the', 'population', 'in', 'the', 'us', 'and', '9', '.', '3', '%', 'in', 'the', 'rest', 'of', 'the', 'world', 'have', 'it', '*', 'japan', 'has', 'a', 'lot', 'of', 'support', 'groups', '*', 'we', 'include', 'ina', '##tten', '##tive', '##ness', ',', 'age', ',', 'adolescents', ',', 'and', 'broad', '##ened', 'the', 'criteria', 'so', 'there', 'are', 'more', 'people', 'diagnosed', '*', 'two', 'people', 'with', 'same', 'problem', ':', 'messy', 'office', '-', '-', 'task', ':', 'clean', 'in', 'one', 'day', '-', 'person', 'with', 'no', 'ad', '##hd', ':', '~', 'clean', '##s', 'office', '~', 'has', 'time', 'to', 'do', 'other', 'things', '(', 'sign', 'holiday', 'cards', '/', 'alphabet', '##ize', 'things', ')', '-', 'person', 'with', 'ad', '##hd', '~', 'begins', 'cleaning', '~', 'sees', 'dying', 'plant', '~', 'buys', 'soil', 'for', 'it', '~', 'doesn', \"'\", 't', 'finish', 'cleaning', '~', 'ends', 'with', 'mess', '##ier', 'office', '*', 'area', 'for', 'brain', 'cannot', 'be', 'fully', 'stimulated', '*', 'lack', 'of', 'motivation', ',', 'not', 'will', '*', 'great', 'engine', 'but', 'no', 'brakes', '*', '*', 'is', 'medication', 'needed', '?', '*', '*', '*', 'it', 'does', 'play', 'a', 'major', 'role', '*', 'starting', 'early', 'may', 'prevent', 'low', 'self', '-', 'esteem', 'and', 'anxiety', '/', 'depression', '*', 'not', 'needed', 'if', 'you', \"'\", 're', 'living', 'an', 'ad', '##hd', '-', 'friendly', 'lifestyle', '*', '*', 'other', 'thoughts', '*', '*', '*', 'dec', '##af', '##fe', '##inate', 'yourself', 'once', 'you', 'become', 'med', '##icated', '*', 'need', '10', 'cups', 'of', 'coffee', 'at', 'once', 'to', 'be', 'equivalent', 'to', 'medication', '(', 'ji', '##tter', '##y', ',', 'ins', '##om', '##nia', ',', 'anxiety', 'side', 'effects', 'though', ')', '*', 'no', 'lab', 'test', 'yet', '*', 'ad', '##hd', 'is', 'not', 'cured', '*', 'you', 'can', 'live', 'well', 'despite', 'disorder', '*', 'ad', '##hd', 'friendly', 'lifestyle', 'may', 'cult', '##ivate', 'function', 'w', '/', 'medication']\n",
      "INFO:__main__:Number of tokens: 1916\n",
      "INFO:__main__:Number of chunks: 4\n",
      "INFO:__main__:Chunks: [['notes', 'from', 'ad', '##hd', 'in', 'women', ':', 'a', 'hidden', 'disorder', ',', 'a', 'lecture', 'by', 'dr', '.', 'patricia', 'o', '.', 'quinn', 'dr', '.', 'quinn', ',', 'and', 'expert', 'and', 'speaker', 'on', 'ad', '##hd', ',', 'came', 'to', 'my', 'university', 'a', 'few', 'months', 'ago', 'and', 'i', 'thought', 'i', \"'\", 'd', 'pass', 'along', 'the', 'notes', 'i', 'took', '.', 'i', 'know', 'it', \"'\", 's', 'lengthy', 'but', 'i', 'learned', 'a', 'lot', 'from', 'this', 'lecture', '.', 'as', 'a', 'female', 'who', 'got', 'diagnosed', 'with', 'ad', '##hd', 'in', 'college', ',', 'i', 'can', 'at', '##test', 'that', 'so', 'much', 'makes', 'sense', 'and', 'that', 'this', 'disorder', 'extends', 'further', 'than', 'i', 'was', 'aware', 'of', '.', 'i', 'hope', 'you', 'guys', 'find', 'this', 'as', 'helpful', 'as', 'i', 'have', '!', '*', '*', 'edit', '!', '*', '*', 'her', 'lecture', 'is', 'available', 'on', 'itunes', 'for', 'free', '.', '[', 'this', 'link', ']', '(', 'http', ':', '/', '/', 'www', '.', 'un', '##c', '.', 'ed', '##u', '/', 'as', '##p', '/', 'ad', '##hdi', '##ng', '##ir', '##ls', '##and', '##wo', '##ment', '##he', '##hid', '##den', '##dis', '##ord', '##er', '.', 'h', '##tm', ')', 'has', 'her', 'power', '##point', 'slides', 'from', 'her', 'lecture', ',', 'the', 'link', 'to', 'itunes', ',', 'and', 'recommended', 'books', '.', '*', '*', 'historical', 'impact', 'on', 'gender', 'issues', '*', '*', '*', '*', 'what', 'we', 'looked', 'at', ',', 'what', 'we', 'looked', 'for', 'pre', '##de', '##ter', '##mined', 'what', 'we', 'found', '*', '*', '*', 'mostly', 'white', 'male', 'hyper', '##active', 'sub', '##type', '*', 'separating', 'fact', 'from', 'fiction', '*', '*', 'many', 'similarities', 'between', 'the', 'gender', '##s', '*', 'ph', '##eno', '##typic', 'core', 'of', 'ad', '##hd', 'symptoms', 'similar', 'to', 'what', 'has', 'been', 'observed', 'in', 'boys', '*', 'recent', 'findings', 'stress', 'the', 'severity', 'of', 'disorder', 'in', 'females', '*', 'more', 'mood', 'and', 'anxiety', 'disorders', ',', 'less', 'conduct', 'disorder', '*', 'prep', '##ond', '##eran', '##ce', 'of', 'ina', '##tten', '##tive', 'symptoms', '*', '18', 'symptoms', ':', '9', 'in', 'ina', '##tten', '##tive', ',', '9', 'in', 'hyper', '##active', '/', 'imp', '##ulsive', '*', '6', 'out', 'of', '9', 'symptoms', 'from', 'specific', 'category', 'puts', 'you', 'in', 'specified', 'sub', '##type', ',', '6', 'of', '9', 'out', 'of', 'any', '(', 'each', '?', 'sorry', 'don', \"'\", 't', 'remember', ')', 'puts', 'you', 'in', 'combined', '*', 'como', '##rb', '##idi', '##ty', ':', 'anxiety', 'result', 'from', 'ad', '##hd', '*', '*', 'how', 'are', 'differences', 'manifested', '?', '*', '*', '*', 'receive', 'f', 'in', 'test', ':', 'boy', 'says', 'test', 'is', 'stupid', ';', 'girl', 'says', '\"', 'i', 'am', 'stupid', '.', '\"', '*', 'boys', 'external', '##ize', 'while', 'girls', 'internal', '##ize', '*', 'self', '-', 'blame', ',', 'self', '-', 'at', '##tri', '##bution', '*', 'low', 'self', '-', 'esteem', '*', 'demo', '##ral', '##ization', '(', 'looks', 'like', 'low', 'level', 'depression', ')', '*', 'moves', 'into', 'depression', '*', '*', 'need', 'to', 'investigate', 'internal', '##izing', 'patterns', '*', '*', '*', 'internal', '##izing', 'patterns', 'require', 'much', 'closer', 'observation', '*', 'require', 'self', '-', 'report', 'evaluation', '##s', '*', '*', 'mit', '##iga', '##ting', 'circumstances', '(', 'especially', 'important', 'in', 'understanding', 'why', 'girls', 'go', 'und', '##ia', '##gno', '##sed', ')', '*', '*', '*', 'fewer', 'ad', '##hd', 'symptoms', '*', 'grew', 'up', 'in', 'a', 'structured', 'environment', '(', 'high', 'school', ';', 'parents', 'doing', 'the', 'organizing', ')', '*', 'high', 'social', 'economic', 'status', '(', 'can', 'afford', 'tutor', '##ing', ',', 'academic', 'coaches', ',', 'etc', '.', ')', '*', 'no', 'opposition', '##al', 'defiant', 'disorder', '(', 'odd', ')', 'or', 'conduct', 'disorder', '(', 'cd', ')', '*', 'high'], ['iq', '*', 'social', 'pressure', '/', 'need', 'for', 'approval', '*', '*', 'why', 'learn', 'about', 'ad', '##hd', 'in', 'women', '?', '*', '*', '*', '*', 'und', '##ia', '##gno', '##sed', 'ad', '##hd', 'is', 'a', 'significant', 'health', 'concern', '*', '*', '*', '2nd', 'most', 'common', 'psychological', 'problem', 'in', 'adults', '*', 'affects', '4', 'million', 'women', '-', '-', 'most', 'und', '##ia', '##gno', '##sed', '*', 'accidents', '/', 'injuries', '*', 'abuse', '-', '-', 'psychological', '/', 'physical', '/', 'sexual', 'abuse', 'and', 'trauma', '*', 'marital', 'difficulties', '/', 'sexual', 'desires', '-', 'story', ':', 'couple', 'in', 'the', 'middle', 'of', 'getting', 'it', 'on', ',', 'woman', 'remembers', 'that', 'the', 'kids', 'need', 'milk', ',', 'leaves', 'to', 'go', 'back', '.', 'needles', '##s', 'to', 'say', 'the', 'husband', 'was', 'upset', '.', 'she', 'didn', \"'\", 't', 'understand', 'why', '.', '*', 'un', '##pl', '##anne', '##d', 'pre', '##gnan', '##cies', '-', 'distract', '##ability', ',', 'not', 'planning', 'ahead', ',', 'low', 'self', '-', 'esteem', ',', 'not', 'good', 'at', 'remembering', 'to', 'take', 'birth', 'control', '*', 'parenting', 'problems', '-', '-', 'child', 'with', 'ad', '##hd', '-', 'couldn', \"'\", 't', 'keep', 'track', 'of', 'kids', '-', 'bad', 'at', 'problem', 'solving', '-', '-', 'in', '##fle', '##xi', '##ble', 'thinking', ',', 'trouble', 'figuring', 'out', 'alternative', 'solutions', '*', 'poor', 'self', '-', 'esteem', '/', 'depression', '/', 'fi', '##bro', '##my', '##al', '##gia', '-', 'ad', '##hd', 'med', '##s', 'also', 'treat', 'fi', '##bro', '##my', '##al', '##gia', '*', 'sleep', 'disorders', '-', '-', 'no', 'off', 'switch', '*', 'eating', 'disorders', '-', 'bing', '##e', 'eating', ',', 'lack', 'of', 'awareness', 'of', 'full', '##ness', '-', 'obe', '##se', '-', 'little', 'control', 'to', 'be', 'an', '##ore', '##xi', '##c', '*', '*', 'presenting', 'complaints', 'for', 'women', '*', '*', '*', 'feeling', 'overwhelmed', '*', 'chronic', '##ally', 'di', '##sor', '##gan', '##ized', '*', 'chronic', '##ally', 'late', ',', 'poor', 'time', 'management', '-', 'live', 'in', 'the', 'now', '-', '-', 'not', 'getting', 'much', 'done', '-', 'don', \"'\", 't', 'have', 'to', 'do', 'it', 'now', ',', 'ends', 'up', 'not', 'doing', 'it', 'at', 'all', '-', 'not', 'learning', 'from', 'past', 'experiences', 'so', 'not', 'worried', '*', 'sense', 'of', 'shame', ',', 'ina', '##de', '##qua', '##cy', '-', '-', 'can', \"'\", 't', 'live', 'up', 'to', 'typical', 'societal', 'expectations', '*', 'meal', '-', 'planning', '/', 'preparation', 'and', 'other', 'daily', 'living', 'activities', 'is', 'a', 'challenge', '*', 'problems', 'with', 'friendships', '-', 'lack', 'of', 'attention', 'to', 'details', '(', 'won', \"'\", 't', 'notice', 'new', 'hair', '##cut', 'or', 'new', 'earrings', 'etc', ',', 'friend', 'may', 'be', 'offended', ')', '-', 'hard', 'time', 'connecting', 'with', 'females', '-', 'often', 'have', 'male', 'friends', '*', 'doesn', \"'\", 't', 'know', 'how', 'to', '\"', 'na', '##g', '\"', 'herself', '*', '*', 'over', '##t', 'chaos', 'vs', '.', 'silent', 'suffering', '*', '*', '*', 'some', 'women', 'seek', 'treatment', 'for', 'ad', '##hd', 'because', 'their', 'lives', 'are', 'out', 'of', 'control', '*', 'other', 'women', 'are', 'more', 'successful', 'at', 'hiding', 'their', 'ad', '##hd', '-', '-', 'struggle', 'valiant', '##ly', '*', 'develop', 'ob', '##ses', '##sive', 'com', '##pu', '##ls', '##ive', 'disorder', 'personality', 'disorder', '/', 'perfection', '##ism', 'to', 'cope', '*', 'anxiety', 'and', 'depression', 'en', '##su', '##e', '*', 'learned', 'helpless', '##ness', '*', '*', 'ina', '##tten', '##tive', 'type', '*', '*', '*', 'more', 'passive', 'academic', '##ally', '*', 'shy', ',', 'tim', '##id', ',', 'easily', 'overwhelmed', '*', 'tendency', 'to', 'withdraw', ',', 'not', 'act', 'out', '*', 'expressive', 'language', '*', 'word', 'retrieval', 'problems', '*', 'expressive', 'language', 'difficulties', '*', 'h', '##yp', '##oa', '##ctive', ',', 'let', '##har', '##gic', ',', 'easily', 'discouraged', '*'], ['day', '##dre', '##ame', '##rs', '*', '*', 'combined', 'type', '*', '*', '*', 'hyper', '-', 'talk', '##ative', ',', 'hyper', '-', 'social', ',', 'hyper', '-', 'reactive', '*', 'restless', 'and', 'fi', '##dget', '##y', '*', 'charismatic', ',', 'boss', '##y', ',', 'stubborn', ',', '\"', 'spoiled', '\"', '*', 'can', \"'\", 't', 'manage', 'daily', 'demands', ',', 'blame', '##s', 'others', '*', '*', 'hyper', '##active', '/', 'imp', '##ulsive', 'sub', '##type', '*', '*', '*', 'rare', '*', 'seen', 'as', '\"', 'difficult', '\"', 'by', 'age', '3', 'or', '4', '*', 'tan', '##trum', '##s', 'etc', '.', '*', '*', 'diagnostic', 'issues', 'for', 'ad', '##hd', 'in', 'women', 'and', 'older', 'girls', '*', '*', '*', 'often', 'no', 'report', 'of', 'early', 'childhood', 'issues', '*', 'mother', '/', 'daughter', 'd', '##yad', '(', 'relationship', ')', ':', 'good', 'in', 'school', ',', 'hyper', '##cr', '##itic', '##ized', 'by', 'mother', ',', 'opposition', '##al', 'daughter', '*', 'ina', '##tten', '##tive', 'type', 'often', 'overlooked', 'by', 'professionals', ',', 'parents', ',', 'and', 'teachers', '*', 'often', 'more', 'teacher', 'compliant', '*', 'good', 'academic', 'record', 'in', 'early', 'years', '*', 'hyper', '##act', '##ivity', '/', 'imp', '##uls', '##ivity', 'less', 'common', '*', 'symptoms', 'present', 'at', 'pub', '##erty', '*', 'high', 'iq', 'in', 'girls', 'may', 'function', 'well', '*', 'anxiety', 'at', '8', ',', 'depression', 'at', '[', 'didn', \"'\", 't', 'hear', ']', '*', 'como', '##rb', '##idi', '##ty', 'with', 'bipolar', 'disorder', 'but', 'both', 'need', 'to', 'be', 'treated', '*', '*', 'pub', '##erty', 'in', 'girls', 'with', 'ad', '##hd', '*', '*', '*', 'for', 'boys', 'hyper', '##act', '##iv', '##iy', 'goes', 'down', 'while', 'for', 'girls', 'it', 'goes', 'up', '*', 'increase', 'in', 'depression', 'and', 'anxiety', '*', 'increased', 'likelihood', 'of', 'pm', '##s', 'and', 'pm', '##dd', '*', '*', 'the', 'lost', 'girls', '*', '*', '*', 'often', 'mis', '##dia', '##gno', '##sed', 'as', 'having', 'depression', 'or', 'bipolar', 'disorder', '*', 'may', 'be', 'at', 'greater', 'risk', 'for', ':', 'chronic', 'under', '##achi', '##eve', '##ment', ',', 'smoking', ',', 'substance', 'abuse', ',', 'pregnancy', '/', 'st', '##ds', ',', 'driving', 'problems', '*', 'girls', 'are', 'more', 'likely', 'to', 'have', 'taken', 'anti', '-', 'de', '##press', '##ants', 'when', 'diagnosed', 'with', 'ad', '##hd', '[', 'boys', '-', '5', '%', 'girls', '14', '%', ')', '*', '*', 'program', 'for', 'achieving', 'success', '*', '*', '*', 'education', '(', 'self', '-', 'knowledge', ')', '*', 'self', '-', 'advocacy', '*', 'support', 'groups', '*', 'counseling', '/', 'psycho', '##therapy', '/', 'family', 'therapy', '*', 'creating', 'an', 'ad', '##hd', '-', 'friendly', 'lifestyle', '*', 'medication', '*', 'improving', 'organization', 'skills', '/', 'professional', 'organizer', '*', 'coaching', '*', 'pursuing', 'a', 'healthy', 'lifestyle', '*', '*', 'husbands', '*', '*', '*', 'first', 'one', 'usually', 'has', 'o', '##cd', 'to', 'over', '##com', '##pen', '##sat', '##e', 'for', 'lack', 'of', 'organization', '*', 'second', 'one', 'usually', 'has', 'ad', '##hd', 'because', 'he', 'understands', 'from', 'personal', 'experience', '*', 'third', 'one', 'is', 'usually', 'just', 'right', 'for', 'her', '*', '*', 'besides', 'medication', '*', '*', '*', 'exercise', '(', 'works', 'for', 'about', '2', '-', '3', 'hr', '##s', ')', '*', 'omega', '3', '*', 'getting', 'enough', 'sleep', '*', 'academic', 'coach', '*', 'yoga', 'and', 'meditation', '*', '*', 'treatment', 'issues', 'for', 'girls', '*', '*', '*', 'don', \"'\", 't', 'need', 'as', 'much', '\"', 'behavior', 'management', '\"', 'issues', '*', 'peer', 'issues', 'more', 'intense', '*', '*', 'medication', 'issues', '*', '*', '*', 'anxiety', 'and', 'depression', 'may', 'need', 'to', 'be', 'treated', 'along', 'with', 'ad', '##hd', 'in', 'adolescence', '*', 'ad', '##hd', 'treated', 'first', ',', '60', '%', 'will', 'no', 'longer', 'have', 'anxiety', '/', 'depression', '*', 'pm', '##s', 'issues'], ['*', 'prem', '##ens', '##tr', '##ual', 'mag', '##ni', '##fication', 'of', 'ad', '##hd', 'symptoms', '*', 'st', '##im', '##ula', '##nts', 'are', 'more', 'effective', 'during', 'f', '##oll', '##icular', 'phase', 'of', 'cycle', '(', 'when', 'men', '##st', '##ru', '##ation', 'begins', 'until', 'o', '##vu', '##lation', ',', 'days', '1', '-', '13', ')', '*', '*', 'q', '&', 'a', '*', '*', '*', '*', 'risk', 'factors', '?', '*', '*', '*', 'problems', 'in', 'first', 'trim', '##ester', '*', 'genetic', 'factors', '*', 'pest', '##icide', '*', 'prem', '##at', '##urity', 'risk', 'factor', '*', 'men', '##ing', '##itis', '*', 'each', 'child', 'born', 'from', 'a', 'mother', 'with', 'ad', '##hd', 'have', 'a', '50', '-', '60', '%', 'chance', 'of', 'getting', 'it', '*', '*', 'at', 'what', 'point', 'is', 'it', 'a', 'disorder', 'and', 'not', 'blame', '?', '*', '*', '*', '9', '.', '6', '%', 'of', 'the', 'population', 'in', 'the', 'us', 'and', '9', '.', '3', '%', 'in', 'the', 'rest', 'of', 'the', 'world', 'have', 'it', '*', 'japan', 'has', 'a', 'lot', 'of', 'support', 'groups', '*', 'we', 'include', 'ina', '##tten', '##tive', '##ness', ',', 'age', ',', 'adolescents', ',', 'and', 'broad', '##ened', 'the', 'criteria', 'so', 'there', 'are', 'more', 'people', 'diagnosed', '*', 'two', 'people', 'with', 'same', 'problem', ':', 'messy', 'office', '-', '-', 'task', ':', 'clean', 'in', 'one', 'day', '-', 'person', 'with', 'no', 'ad', '##hd', ':', '~', 'clean', '##s', 'office', '~', 'has', 'time', 'to', 'do', 'other', 'things', '(', 'sign', 'holiday', 'cards', '/', 'alphabet', '##ize', 'things', ')', '-', 'person', 'with', 'ad', '##hd', '~', 'begins', 'cleaning', '~', 'sees', 'dying', 'plant', '~', 'buys', 'soil', 'for', 'it', '~', 'doesn', \"'\", 't', 'finish', 'cleaning', '~', 'ends', 'with', 'mess', '##ier', 'office', '*', 'area', 'for', 'brain', 'cannot', 'be', 'fully', 'stimulated', '*', 'lack', 'of', 'motivation', ',', 'not', 'will', '*', 'great', 'engine', 'but', 'no', 'brakes', '*', '*', 'is', 'medication', 'needed', '?', '*', '*', '*', 'it', 'does', 'play', 'a', 'major', 'role', '*', 'starting', 'early', 'may', 'prevent', 'low', 'self', '-', 'esteem', 'and', 'anxiety', '/', 'depression', '*', 'not', 'needed', 'if', 'you', \"'\", 're', 'living', 'an', 'ad', '##hd', '-', 'friendly', 'lifestyle', '*', '*', 'other', 'thoughts', '*', '*', '*', 'dec', '##af', '##fe', '##inate', 'yourself', 'once', 'you', 'become', 'med', '##icated', '*', 'need', '10', 'cups', 'of', 'coffee', 'at', 'once', 'to', 'be', 'equivalent', 'to', 'medication', '(', 'ji', '##tter', '##y', ',', 'ins', '##om', '##nia', ',', 'anxiety', 'side', 'effects', 'though', ')', '*', 'no', 'lab', 'test', 'yet', '*', 'ad', '##hd', 'is', 'not', 'cured', '*', 'you', 'can', 'live', 'well', 'despite', 'disorder', '*', 'ad', '##hd', 'friendly', 'lifestyle', 'may', 'cult', '##ivate', 'function', 'w', '/', 'medication']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 've', 'been', 'taking', 'add', '##eral', '##l', 'for', 'nearly', '10', 'years', ',', 'what', 'do', 'i', 'do', 'next', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 've', 'been', 'taking', 'add', '##eral', '##l', 'for', 'nearly', '10', 'years', ',', 'what', 'do', 'i', 'do', 'next', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['side', 'effects', 'after', 'break', 'from', 'medication', '?']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['side', 'effects', 'after', 'break', 'from', 'medication', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'to', 'do', 'when', 'medication', 'seems', 'ineffective', '?', 'hi', ',', 'so', 'i', \"'\", 'm', 'currently', 'taking', 'add', '##eral', '##l', 'x', '##r', '30', '##mg', 'with', 'a', 'supplementary', '10', 'mg', '(', 'regular', ',', 'not', 'x', '##r', ')', 'when', 'i', 'need', 'it', '.', 'prior', 'to', 'that', 'i', \"'\", 've', 'tried', '(', 'in', 'reverse', 'chronological', 'order', ')', ':', '30', 'mg', 'add', '##eral', '##l', 'x', '##r', ',', '20', 'mg', 'add', '##eral', '##l', 'x', '##r', ',', '40', 'mg', 'rita', '##lin', 'la', ',', '20', 'mg', 'rita', '##lin', 'la', ',', '20', 'mg', 'rita', '##lin', 'regular', 'and', 'a', 'few', 'doses', 'of', 'st', '##rate', '##ra', 'i', 'don', \"'\", 't', 'remember', '.', 'so', 'basically', '-', 'all', 'of', 'them', ',', 'in', 'every', 'dos', '##age', '.', ':', '/', 'each', 'was', 'for', 'at', 'least', '1', 'month', ',', 'so', 'i', 'don', \"'\", 't', 'know', 'what', 'the', 'problem', 'is', '.', 'i', \"'\", 'm', 'wary', 'of', 'bump', '##ing', 'it', 'up', 'again', ',', 'but', 'i', \"'\", 'm', 'so', 'desperate', 'to', 'find', 'a', 'dos', '##age', 'that', 'helps', '.', 'after', 'the', 'first', '5', 'days', ',', 'nothing', 'works', ',', 'even', 'if', 'i', 'double', 'my', 'pills', '(', 'even', 'with', 'the', '30', 'x', '##rs', '!', ')', ',', 'and', 'i', 'have', 'thus', 'made', 'little', 'progress', 'breaking', 'free', 'of', 'my', 'poor', 'behavior', 'or', 'improving', 'my', 'life', '.', 'has', 'anyone', 'else', 'had', 'this', 'problem', '?', 'what', 'do', 'you', 'suggest', '?', 'i', 'have', 'pretty', 'severe', 'ad', '##hd', '(', 'and', 'it', 'is', 'definitely', 'ad', '##hd', ')', 'and', 'it', 'negatively', 'affects', 'every', 'aspect', 'of', 'my', 'life', 'whether', 'or', 'not', 'i', \"'\", 'm', 'on', 'medication', '.', 'i', 'hate', 'it', 'so', 'much', ',', 'i', 'feel', 'hopeless', ',', 'and', 'i', 'feel', 'like', 'a', 'fuck', '-', 'up', '.', 'i', 'suppose', 'i', 'could', 'try', 'additional', 'behavioral', 'therapy', ',', 'but', 'it', 'just', 'seems', 'so', 'obvious', ',', 'like', 'they', \"'\", 're', 'telling', 'me', 'things', 'i', 'already', 'know', 'but', 'just', 'fail', 'to', 'do', '(', 'make', 'lists', ',', 'etc', ')', '.', 'is', 'this', 'just', 'my', 'mis', '##con', '##ception', 'or', 'has', 'anyone', 'made', 'major', 'progress', 'with', 'therapy', 'and', 'medication', '?', 'thanks', 'in', 'advance', 'for', 'your', 'support', '.']\n",
      "INFO:__main__:Number of tokens: 328\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'to', 'do', 'when', 'medication', 'seems', 'ineffective', '?', 'hi', ',', 'so', 'i', \"'\", 'm', 'currently', 'taking', 'add', '##eral', '##l', 'x', '##r', '30', '##mg', 'with', 'a', 'supplementary', '10', 'mg', '(', 'regular', ',', 'not', 'x', '##r', ')', 'when', 'i', 'need', 'it', '.', 'prior', 'to', 'that', 'i', \"'\", 've', 'tried', '(', 'in', 'reverse', 'chronological', 'order', ')', ':', '30', 'mg', 'add', '##eral', '##l', 'x', '##r', ',', '20', 'mg', 'add', '##eral', '##l', 'x', '##r', ',', '40', 'mg', 'rita', '##lin', 'la', ',', '20', 'mg', 'rita', '##lin', 'la', ',', '20', 'mg', 'rita', '##lin', 'regular', 'and', 'a', 'few', 'doses', 'of', 'st', '##rate', '##ra', 'i', 'don', \"'\", 't', 'remember', '.', 'so', 'basically', '-', 'all', 'of', 'them', ',', 'in', 'every', 'dos', '##age', '.', ':', '/', 'each', 'was', 'for', 'at', 'least', '1', 'month', ',', 'so', 'i', 'don', \"'\", 't', 'know', 'what', 'the', 'problem', 'is', '.', 'i', \"'\", 'm', 'wary', 'of', 'bump', '##ing', 'it', 'up', 'again', ',', 'but', 'i', \"'\", 'm', 'so', 'desperate', 'to', 'find', 'a', 'dos', '##age', 'that', 'helps', '.', 'after', 'the', 'first', '5', 'days', ',', 'nothing', 'works', ',', 'even', 'if', 'i', 'double', 'my', 'pills', '(', 'even', 'with', 'the', '30', 'x', '##rs', '!', ')', ',', 'and', 'i', 'have', 'thus', 'made', 'little', 'progress', 'breaking', 'free', 'of', 'my', 'poor', 'behavior', 'or', 'improving', 'my', 'life', '.', 'has', 'anyone', 'else', 'had', 'this', 'problem', '?', 'what', 'do', 'you', 'suggest', '?', 'i', 'have', 'pretty', 'severe', 'ad', '##hd', '(', 'and', 'it', 'is', 'definitely', 'ad', '##hd', ')', 'and', 'it', 'negatively', 'affects', 'every', 'aspect', 'of', 'my', 'life', 'whether', 'or', 'not', 'i', \"'\", 'm', 'on', 'medication', '.', 'i', 'hate', 'it', 'so', 'much', ',', 'i', 'feel', 'hopeless', ',', 'and', 'i', 'feel', 'like', 'a', 'fuck', '-', 'up', '.', 'i', 'suppose', 'i', 'could', 'try', 'additional', 'behavioral', 'therapy', ',', 'but', 'it', 'just', 'seems', 'so', 'obvious', ',', 'like', 'they', \"'\", 're', 'telling', 'me', 'things', 'i', 'already', 'know', 'but', 'just', 'fail', 'to', 'do', '(', 'make', 'lists', ',', 'etc', ')', '.', 'is', 'this', 'just', 'my', 'mis', '##con', '##ception', 'or', 'has', 'anyone', 'made', 'major', 'progress', 'with', 'therapy', 'and', 'medication', '?', 'thanks', 'in', 'advance', 'for', 'your', 'support', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'drugs', 'do', 'not', 'increase', 'risk', 'of', 'heart', 'disease', 'in', 'adults', ',', 'study', 'finds']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'drugs', 'do', 'not', 'increase', 'risk', 'of', 'heart', 'disease', 'in', 'adults', ',', 'study', 'finds']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['fda', 'finds', 'that', 'the', 'dea', 'has', 'created', 'an', 'artificial', 'shortage', 'of', 'ad', '##hd', 'medication', '[', 'x', '-', 'post', 'from', 'science', ']']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['fda', 'finds', 'that', 'the', 'dea', 'has', 'created', 'an', 'artificial', 'shortage', 'of', 'ad', '##hd', 'medication', '[', 'x', '-', 'post', 'from', 'science', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['r', '/', 'ad', '##hd', ',', 'do', 'you', 'have', 'any', 'tricks', 'that', 'help', 'you', 'focus', '?', 'hi', 'r', '/', 'ad', '##hd', '!', 'i', \"'\", 'm', 'just', 'curious', 'as', 'to', 'the', 'little', 'things', 'you', 'guys', 'do', 'to', 'keep', 'focused', ',', 'especially', 'if', 'you', 'aren', '’', 't', 'on', 'medication', '.', 'for', 'example', ',', 'i', 'was', 'off', 'med', '##s', 'when', 'i', 'found', 'a', 'smooth', 'rock', 'i', 'picked', 'up', 'on', 'a', 'school', 'trip', 'to', 'the', 'american', 'river', 'and', 'started', 'playing', 'with', 'it', 'like', 'some', 'kind', 'of', 'worry', 'stone', '.', 'i', 'found', 'that', 'it', 'helped', 'me', 'concentrate', 'and', 'also', 'stopped', 'me', 'from', 'picking', 'at', 'my', 'hair', ',', 'which', 'is', 'a', 'bad', 'habit', 'of', 'mine', 'that', 'really', 'comes', 'out', 'when', 'i', '’', 'm', 'off', 'med', '##s', '.', 'anyone', 'else', '?']\n",
      "INFO:__main__:Number of tokens: 123\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['r', '/', 'ad', '##hd', ',', 'do', 'you', 'have', 'any', 'tricks', 'that', 'help', 'you', 'focus', '?', 'hi', 'r', '/', 'ad', '##hd', '!', 'i', \"'\", 'm', 'just', 'curious', 'as', 'to', 'the', 'little', 'things', 'you', 'guys', 'do', 'to', 'keep', 'focused', ',', 'especially', 'if', 'you', 'aren', '’', 't', 'on', 'medication', '.', 'for', 'example', ',', 'i', 'was', 'off', 'med', '##s', 'when', 'i', 'found', 'a', 'smooth', 'rock', 'i', 'picked', 'up', 'on', 'a', 'school', 'trip', 'to', 'the', 'american', 'river', 'and', 'started', 'playing', 'with', 'it', 'like', 'some', 'kind', 'of', 'worry', 'stone', '.', 'i', 'found', 'that', 'it', 'helped', 'me', 'concentrate', 'and', 'also', 'stopped', 'me', 'from', 'picking', 'at', 'my', 'hair', ',', 'which', 'is', 'a', 'bad', 'habit', 'of', 'mine', 'that', 'really', 'comes', 'out', 'when', 'i', '’', 'm', 'off', 'med', '##s', '.', 'anyone', 'else', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'x', '##r', 'wearing', 'off']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'x', '##r', 'wearing', 'off']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'relationship', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'relationship', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['shortage', 'of', 'ad', '##hd', 'drug', 'add', '##eral', '##l', 'seen', 'persist', '##ing', 'in', '2012']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['shortage', 'of', 'ad', '##hd', 'drug', 'add', '##eral', '##l', 'seen', 'persist', '##ing', 'in', '2012']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['resource', ':', 'more', 'attention', 'less', 'deficit', 'podcast', 'really', 'good', 'resource', '.', 'this', 'free', 'podcast', 'begins', 'with', 'facts', 'and', 'examples', 'of', 'differences', 'between', 'a', 'normal', 'mind', 'and', 'a', 'more', 're', '##lat', '##able', 'ad', '##hd', 'mind', '.', 'it', 'then', 'goes', 'on', 'to', 'talk', 'about', 'practical', 'advice', '.', 'podcast', 'episodes', 'are', 'individually', 'short', ',', 'so', 'it', \"'\", 's', 'easy', 'to', 'sit', 'and', 'listen', 'to', '.', 'found', 'this', 'very', 'helpful', 'so', 'i', \"'\", 'm', 'sharing', 'it', 'with', 'you', 'guys', '*', '*', 'website', ':', '*', '*', 'http', ':', '/', '/', 'adult', '##ad', '##hd', '##book', '.', 'com', '/', '(', 'note', ',', 'the', 'side', '##bar', 'has', 'the', 'rs', '##s', 'feed', 'icon', 'with', 'click', '-', 'subscription', 'if', 'you', \"'\", 're', 'using', 'google', 'reader', '.', 'etc', ')', '*', '*', 'rs', '##s', 'feed', ':', '*', '*', 'http', ':', '/', '/', 'adult', '##ad', '##hd', '##book', '.', 'com', '/', 'feed', '/', '*', '*', 'itunes', 'link', ':', '*', '*', 'http', ':', '/', '/', 'itunes', '.', 'apple', '.', 'com', '/', 'podcast', '/', 'more', '-', 'attention', '-', 'less', '-', 'deficit', '/', 'id', '##31', '##28', '##31', '##48', '##5', 'please', 'up', '##vot', '##e', 'so', 'more', 'people', 'can', 'see', 'this', '!', 'submit', '##ting', 'this', 'as', 'a', 'self', 'post', 'so', 'i', 'don', \"'\", 't', 'get', 'any', 'karma', 'for', 'it']\n",
      "INFO:__main__:Number of tokens: 200\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['resource', ':', 'more', 'attention', 'less', 'deficit', 'podcast', 'really', 'good', 'resource', '.', 'this', 'free', 'podcast', 'begins', 'with', 'facts', 'and', 'examples', 'of', 'differences', 'between', 'a', 'normal', 'mind', 'and', 'a', 'more', 're', '##lat', '##able', 'ad', '##hd', 'mind', '.', 'it', 'then', 'goes', 'on', 'to', 'talk', 'about', 'practical', 'advice', '.', 'podcast', 'episodes', 'are', 'individually', 'short', ',', 'so', 'it', \"'\", 's', 'easy', 'to', 'sit', 'and', 'listen', 'to', '.', 'found', 'this', 'very', 'helpful', 'so', 'i', \"'\", 'm', 'sharing', 'it', 'with', 'you', 'guys', '*', '*', 'website', ':', '*', '*', 'http', ':', '/', '/', 'adult', '##ad', '##hd', '##book', '.', 'com', '/', '(', 'note', ',', 'the', 'side', '##bar', 'has', 'the', 'rs', '##s', 'feed', 'icon', 'with', 'click', '-', 'subscription', 'if', 'you', \"'\", 're', 'using', 'google', 'reader', '.', 'etc', ')', '*', '*', 'rs', '##s', 'feed', ':', '*', '*', 'http', ':', '/', '/', 'adult', '##ad', '##hd', '##book', '.', 'com', '/', 'feed', '/', '*', '*', 'itunes', 'link', ':', '*', '*', 'http', ':', '/', '/', 'itunes', '.', 'apple', '.', 'com', '/', 'podcast', '/', 'more', '-', 'attention', '-', 'less', '-', 'deficit', '/', 'id', '##31', '##28', '##31', '##48', '##5', 'please', 'up', '##vot', '##e', 'so', 'more', 'people', 'can', 'see', 'this', '!', 'submit', '##ting', 'this', 'as', 'a', 'self', 'post', 'so', 'i', 'don', \"'\", 't', 'get', 'any', 'karma', 'for', 'it']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'uk', ']', 'what', 'plan', 'of', 'action', 'should', 'i', 'take', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'uk', ']', 'what', 'plan', 'of', 'action', 'should', 'i', 'take', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['npr', 'report', 're', '##eva', '##lu', '##ating', 'the', 'dietary', 'research', 'done', 'on', 'add', '&', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['npr', 'report', 're', '##eva', '##lu', '##ating', 'the', 'dietary', 'research', 'done', 'on', 'add', '&', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'ad', '##hd', 'medication', 'is', 'in', 'short', 'supply', '!', 'now', 'what', 'do', 'i', 'do', '?', '-', 'portrait', 'health', 'centers', 'ad', '##hd', 'blog']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'ad', '##hd', 'medication', 'is', 'in', 'short', 'supply', '!', 'now', 'what', 'do', 'i', 'do', '?', '-', 'portrait', 'health', 'centers', 'ad', '##hd', 'blog']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'ron', 'paul', 'have', 'add', '?', 'the', 'number', '2', 'guy', 'in', 'the', 'republican', 'primary', 'right', 'now', 'say', 'that', 'his', 'biggest', 'weakness', 'is', 'getting', 'out', 'the', 'message', 'of', 'liberty', 'smoothly', '.', 'the', 'man', 'is', 'clearly', 'brilliant', 'in', 'many', 'respects', ',', 'and', 'he', 'is', 'older', ',', 'but', 'he', 'does', 'a', 'lot', 'of', 'st', '##am', '##mering', 'and', 'talking', 'in', 'semi', '##ci', '##rcle', '##s', '.']\n",
      "INFO:__main__:Number of tokens: 61\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'ron', 'paul', 'have', 'add', '?', 'the', 'number', '2', 'guy', 'in', 'the', 'republican', 'primary', 'right', 'now', 'say', 'that', 'his', 'biggest', 'weakness', 'is', 'getting', 'out', 'the', 'message', 'of', 'liberty', 'smoothly', '.', 'the', 'man', 'is', 'clearly', 'brilliant', 'in', 'many', 'respects', ',', 'and', 'he', 'is', 'older', ',', 'but', 'he', 'does', 'a', 'lot', 'of', 'st', '##am', '##mering', 'and', 'talking', 'in', 'semi', '##ci', '##rcle', '##s', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['med', '##s', 'causing', 'me', 'to', 'become', 'a', 'bing', '##e', 'eater', '.', '.', '.', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['med', '##s', 'causing', 'me', 'to', 'become', 'a', 'bing', '##e', 'eater', '.', '.', '.', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['\"', 'go', '##sh', 'i', 'hate', 'studying', 'for', 'exams', '.', 'ha', '##ha', 'i', 'probably', 'have', 'ad', '##hd', 'too', '!', 'i', 'feel', 'your', 'pain', 'ha', '##ha', '##ha', '!', '\"']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['\"', 'go', '##sh', 'i', 'hate', 'studying', 'for', 'exams', '.', 'ha', '##ha', 'i', 'probably', 'have', 'ad', '##hd', 'too', '!', 'i', 'feel', 'your', 'pain', 'ha', '##ha', '##ha', '!', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['everything', 'the', 'establishment', 'has', 'told', 'you', 'is', 'wrong', 'with', 'you', '-', 'is', 'actually', 'what', \"'\", 's', 'right', 'with', 'you', '.']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['everything', 'the', 'establishment', 'has', 'told', 'you', 'is', 'wrong', 'with', 'you', '-', 'is', 'actually', 'what', \"'\", 's', 'right', 'with', 'you', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['5', 'ways', 'to', 'protect', 'yourself', 'from', 'the', 'ad', '##hd', 'medication', 'shortage', '|', 'edge', 'ad', '##hd', 'coaching']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['5', 'ways', 'to', 'protect', 'yourself', 'from', 'the', 'ad', '##hd', 'medication', 'shortage', '|', 'edge', 'ad', '##hd', 'coaching']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'a', 'little', 'support', '/', 'advice', 'r', '/', 'ad', '##hd', '!']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'a', 'little', 'support', '/', 'advice', 'r', '/', 'ad', '##hd', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['living', ',', 'learning', ',', 'and', 'writing', 'with', 'attention', 'deficit', 'hyper', '##act', '##ivity', 'disorder', '(', 'ad', '##hd', ')', '.']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['living', ',', 'learning', ',', 'and', 'writing', 'with', 'attention', 'deficit', 'hyper', '##act', '##ivity', 'disorder', '(', 'ad', '##hd', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['attitudes', 'on', 'add', '##eral', '##l']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['attitudes', 'on', 'add', '##eral', '##l']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['let', \"'\", 's', 'talk', 'mornings', '/', 'breakfast', 'i', 'personally', 'prefer', 'to', 'take', 'my', 'medication', 'on', 'an', 'empty', 'stomach', 'with', 'a', 'cup', 'of', 'coffee', 'first', 'thing', 'in', 'the', 'morning', '.', 'with', 'food', ',', 'i', 'find', 'the', 'medications', 'effects', 'diminished', 'and', 'that', 'it', \"'\", 'kicks', 'in', \"'\", 'slower', 'and', 'more', 'gradually', '.', 'it', \"'\", 's', 'also', 'reassuring', 'to', 'feel', 'that', 'sudden', 'switch', 'from', \"'\", 'un', '##mot', '##ivated', '/', 'un', '##fo', '##cus', '##ed', \"'\", 'to', 'clear', ',', 'straightforward', 'thoughts', '.', 'so', ',', 'how', 'does', 'everyone', 'take', 'their', 'medication', 'in', 'the', 'morning', '?', 'with', 'or', 'without', 'food', '?', 'light', 'or', 'heavy', 'breakfast', '?', 'with', 'coffee', 'or', 'other', 'sources', 'of', 'caf', '##fe', '##in', '?']\n",
      "INFO:__main__:Number of tokens: 109\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['let', \"'\", 's', 'talk', 'mornings', '/', 'breakfast', 'i', 'personally', 'prefer', 'to', 'take', 'my', 'medication', 'on', 'an', 'empty', 'stomach', 'with', 'a', 'cup', 'of', 'coffee', 'first', 'thing', 'in', 'the', 'morning', '.', 'with', 'food', ',', 'i', 'find', 'the', 'medications', 'effects', 'diminished', 'and', 'that', 'it', \"'\", 'kicks', 'in', \"'\", 'slower', 'and', 'more', 'gradually', '.', 'it', \"'\", 's', 'also', 'reassuring', 'to', 'feel', 'that', 'sudden', 'switch', 'from', \"'\", 'un', '##mot', '##ivated', '/', 'un', '##fo', '##cus', '##ed', \"'\", 'to', 'clear', ',', 'straightforward', 'thoughts', '.', 'so', ',', 'how', 'does', 'everyone', 'take', 'their', 'medication', 'in', 'the', 'morning', '?', 'with', 'or', 'without', 'food', '?', 'light', 'or', 'heavy', 'breakfast', '?', 'with', 'coffee', 'or', 'other', 'sources', 'of', 'caf', '##fe', '##in', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['cancer', 'survivors', '(', 'che', '##mo', 'brain', ')', 'and', 'ad', '##hd', '?', 'no', 'help', 'from', 'my', 'on', '##col', '##ogist', ',', 'and', 'he', 'is', 'poisoning', 'my', 'ability', 'to', 'get', 'help', '!', 'i', 'have', 'always', 'had', 'problems', 'with', 'ad', '##hd', ',', 'but', 'because', 'of', 'the', 'attitude', 'of', 'my', 'family', 'toward', 'the', 'idea', 'of', 'it', '(', 'they', 'just', 'thought', 'i', 'was', 'lazy', 'and', 'un', '##mot', '##ivated', ',', 'and', 'simply', 'punished', 'me', '-', 'it', 'was', 'the', '70', \"'\", 's', ',', 'and', 'that', 'was', 'the', 'common', 'view', 'then', ')', 'was', 'never', 'diagnosed', 'or', 'treated', '.', 'after', 'graduating', 'college', 'in', '91', '(', 'barely', ')', ',', 'i', 'was', 'able', 'to', 'mud', '##dle', 'through', 'and', 'fake', 'my', 'way', 'through', 'interviews', '(', 'i', 'interview', 'very', 'well', ',', 'and', 'landed', 'some', 'great', 'jobs', ')', 'but', 'invariably', 'lost', 'them', 'because', 'of', 'my', 'inability', 'to', 'multi', '##tas', '##k', ',', 'get', 'tasks', 'completed', 'in', 'a', 'timely', 'manner', ',', 'organize', ',', 'etc', '.', 'the', 'usual', 'suspects', '.', 'i', \"'\", 've', 'done', 'everything', 'from', 'teaching', 'high', 'school', 'to', 'running', 'a', 'coffee', '##house', ',', 'and', 'every', 'job', 'in', 'between', '.', 'even', 'went', 'back', 'to', 'school', 'for', 'a', 'degree', 'in', 'graphic', 'art', '(', 'didn', \"'\", 't', 'finish', ')', ',', 'and', 'lost', 'the', 'job', 'i', 'got', 'at', 'a', 'design', 'firm', 'in', 'town', '.', 'the', 'problem', 'wasn', \"'\", 't', 'getting', 'jobs', ':', 'it', 'was', '*', '*', 'keeping', '*', '*', 'them', '.', 'fast', 'forward', 'to', '2007', '.', 'i', 'was', 'diagnosed', 'with', 'breast', 'cancer', ',', 'and', 'for', 'a', 'year', 'underwent', 'some', 'dr', '##aco', '##nian', 'treatments', '-', 'the', 'che', '##mo', 'was', 'harsh', '.', 'it', 'also', 'destroyed', 'whatever', 'was', 'left', 'of', 'my', 'ability', 'to', \"'\", 'fake', 'it', \"'\", '.', 'now', '.', '.', '.', 'i', 'now', 'face', 'losing', 'the', 'only', 'job', 'i', \"'\", 've', 'managed', 'to', 'hold', 'on', 'to', 'recently', ':', 'a', 'fairly', 'low', '-', 'pay', ',', 'low', '-', 'demand', 'job', '.', 'i', 'went', 'from', \"'\", 'one', 'of', 'the', 'best', 'employees', \"'\", '(', 'in', 'management', ')', 'before', 'cancer', 'to', 'part', '-', 'time', \"'\", 'sorry', 'we', 'have', 'to', 'cut', 'your', 'hours', ',', 'you', 'are', 'simply', 'under', '##per', '##form', '##ing', \"'\", 'in', 'just', 'three', 'years', '.', 'lost', 'my', 'benefits', 'as', 'well', 'since', 'i', 'was', 'bumped', 'off', 'of', 'full', 'time', '.', 'can', \"'\", 't', 'argue', 'my', 'case', 'because', 'my', 'doctor', 'refuses', 'to', 'acknowledge', 'i', 'have', 'a', 'problem', ',', 'much', 'less', 'intervene', '.', 'i', 'can', \"'\", 't', 'fake', 'it', 'in', 'an', 'interview', 'anymore', 'so', 'i', 'can', \"'\", 't', 'go', 'about', 'looking', 'for', 'a', 'better', 'job', 'with', 'more', 'hours', '.', 'i', 'can', \"'\", 't', 'even', 'manage', 'to', 'match', 'words', 'with', 'the', 'images', 'in', 'my', 'brain', '.', 'i', 'have', 'trouble', 'completing', 'sentences', ',', 'and', 'even', 'typing', 'has', 'become', 'a', 'task', 'of', \"'\", 'backs', '##pace', ',', 'correct', ',', 'backs', '##pace', ',', 'correct', \"'\", 'which', 'takes', 'forever', '.', 'then', 'i', 'have', 'to', 're', '-', 'read', 'what', 'i', 'type', 'so', 'that', 'i', 'can', 'correct', 'any', 'gi', '##bber', '##ish', 'or', 'wrong', 'word', 'usage', 'that', 'shows', 'up', '.', 'every', 'sy', '##mpt', '##om', 'of', 'cognitive', 'problems', 'i', 'had', 'before', 'have', 'increased', 'ten', '##fold', '.', 'this', 'post', 'has', 'taken', 'more', 'than', 'twenty', 'minutes', 'to', 'type', ',', 'bt', '##w', '.', 'close', 'to', 'thirty', 'now', '.', 'the', 'problem', '?', 'my', 'on', '##col', '##ogist', 'doesn', \"'\", 't', 'believe', 'me', '.', 'says', 'he', \"'\", 's', 'never', 'heard', 'of', 'che', '##mo', 'brain', ',', 'and', 'refuses', 'to', 'help', 'me', '.', 'i', 'have', 'tried', 'other', 'places', 'but', 'was', 'turned', 'away', '(', 'i', 'suspect', 'when', 'they', 'call', 'him', 'for', 'records', 'or', 'talk', 'to', 'him', 'he', 'tells', 'them', 'that', 'he', 'doesn', \"'\", 't', 'believe', 'i', 'have', 'a', 'problem', ')', '.', 'finally', 'i', 'called', 'and', 'begged', 'for', 'a', 'refer', '##ral', 'and', 'got', 'one', 'to', 'a', 'ne', '##uro', '##logy', '/', 'psychology', 'department', '.', 'i', 'am', 'praying', 'they', 'will', 'help', 'me', 'and', 'not', 'just', 'assume', 'that', 'i', \"'\", 'm', \"'\", 'doctor', 'shopping', \"'\", 'for', 'drugs', 'or', 'that', 'i', 'am', 'his', '##tri', '##onic', '.', 'i', 'don', \"'\", 't', 'want', \"'\", 'just', \"'\", 'drugs', '.', 'i', 'just', 'want', 'my', 'damned', 'life', 'back', '.', '*', '*', 't', '##l', ':', 'dr', '*', '*', ',', 'i', 'guess', 'i', 'am', 'hoping', 'to', 'hear', 'from', 'someone', 'who', 'has', 'ad', '##hd', '/', 'add', 'and', 'who', 'has', 'been', 'through', 'che', '##mo', ',', 'and', 'if', 'there', 'were', 'similar', 'problems', '.', 'what', 'do', 'you', 'do', 'when', 'the', 'only', 'people', 'who', 'can', 'help', 'you', ',', 'won', \"'\", 't', '?', 'i', 'live', 'in', 'a', 'small', 'town', ',', 'there', 'are', 'not', 'other', 'options', 'within', 'reasonable', 'driving', 'distance', ',', 'unfortunately', '.']\n",
      "INFO:__main__:Number of tokens: 716\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['cancer', 'survivors', '(', 'che', '##mo', 'brain', ')', 'and', 'ad', '##hd', '?', 'no', 'help', 'from', 'my', 'on', '##col', '##ogist', ',', 'and', 'he', 'is', 'poisoning', 'my', 'ability', 'to', 'get', 'help', '!', 'i', 'have', 'always', 'had', 'problems', 'with', 'ad', '##hd', ',', 'but', 'because', 'of', 'the', 'attitude', 'of', 'my', 'family', 'toward', 'the', 'idea', 'of', 'it', '(', 'they', 'just', 'thought', 'i', 'was', 'lazy', 'and', 'un', '##mot', '##ivated', ',', 'and', 'simply', 'punished', 'me', '-', 'it', 'was', 'the', '70', \"'\", 's', ',', 'and', 'that', 'was', 'the', 'common', 'view', 'then', ')', 'was', 'never', 'diagnosed', 'or', 'treated', '.', 'after', 'graduating', 'college', 'in', '91', '(', 'barely', ')', ',', 'i', 'was', 'able', 'to', 'mud', '##dle', 'through', 'and', 'fake', 'my', 'way', 'through', 'interviews', '(', 'i', 'interview', 'very', 'well', ',', 'and', 'landed', 'some', 'great', 'jobs', ')', 'but', 'invariably', 'lost', 'them', 'because', 'of', 'my', 'inability', 'to', 'multi', '##tas', '##k', ',', 'get', 'tasks', 'completed', 'in', 'a', 'timely', 'manner', ',', 'organize', ',', 'etc', '.', 'the', 'usual', 'suspects', '.', 'i', \"'\", 've', 'done', 'everything', 'from', 'teaching', 'high', 'school', 'to', 'running', 'a', 'coffee', '##house', ',', 'and', 'every', 'job', 'in', 'between', '.', 'even', 'went', 'back', 'to', 'school', 'for', 'a', 'degree', 'in', 'graphic', 'art', '(', 'didn', \"'\", 't', 'finish', ')', ',', 'and', 'lost', 'the', 'job', 'i', 'got', 'at', 'a', 'design', 'firm', 'in', 'town', '.', 'the', 'problem', 'wasn', \"'\", 't', 'getting', 'jobs', ':', 'it', 'was', '*', '*', 'keeping', '*', '*', 'them', '.', 'fast', 'forward', 'to', '2007', '.', 'i', 'was', 'diagnosed', 'with', 'breast', 'cancer', ',', 'and', 'for', 'a', 'year', 'underwent', 'some', 'dr', '##aco', '##nian', 'treatments', '-', 'the', 'che', '##mo', 'was', 'harsh', '.', 'it', 'also', 'destroyed', 'whatever', 'was', 'left', 'of', 'my', 'ability', 'to', \"'\", 'fake', 'it', \"'\", '.', 'now', '.', '.', '.', 'i', 'now', 'face', 'losing', 'the', 'only', 'job', 'i', \"'\", 've', 'managed', 'to', 'hold', 'on', 'to', 'recently', ':', 'a', 'fairly', 'low', '-', 'pay', ',', 'low', '-', 'demand', 'job', '.', 'i', 'went', 'from', \"'\", 'one', 'of', 'the', 'best', 'employees', \"'\", '(', 'in', 'management', ')', 'before', 'cancer', 'to', 'part', '-', 'time', \"'\", 'sorry', 'we', 'have', 'to', 'cut', 'your', 'hours', ',', 'you', 'are', 'simply', 'under', '##per', '##form', '##ing', \"'\", 'in', 'just', 'three', 'years', '.', 'lost', 'my', 'benefits', 'as', 'well', 'since', 'i', 'was', 'bumped', 'off', 'of', 'full', 'time', '.', 'can', \"'\", 't', 'argue', 'my', 'case', 'because', 'my', 'doctor', 'refuses', 'to', 'acknowledge', 'i', 'have', 'a', 'problem', ',', 'much', 'less', 'intervene', '.', 'i', 'can', \"'\", 't', 'fake', 'it', 'in', 'an', 'interview', 'anymore', 'so', 'i', 'can', \"'\", 't', 'go', 'about', 'looking', 'for', 'a', 'better', 'job', 'with', 'more', 'hours', '.', 'i', 'can', \"'\", 't', 'even', 'manage', 'to', 'match', 'words', 'with', 'the', 'images', 'in', 'my', 'brain', '.', 'i', 'have', 'trouble', 'completing', 'sentences', ',', 'and', 'even', 'typing', 'has', 'become', 'a', 'task', 'of', \"'\", 'backs', '##pace', ',', 'correct', ',', 'backs', '##pace', ',', 'correct', \"'\", 'which', 'takes', 'forever', '.', 'then', 'i', 'have', 'to', 're', '-', 'read', 'what', 'i', 'type', 'so', 'that', 'i', 'can', 'correct', 'any', 'gi', '##bber', '##ish', 'or', 'wrong', 'word', 'usage', 'that', 'shows', 'up', '.', 'every', 'sy', '##mpt', '##om', 'of', 'cognitive', 'problems', 'i', 'had', 'before', 'have', 'increased', 'ten', '##fold', '.', 'this', 'post', 'has', 'taken', 'more', 'than', 'twenty', 'minutes', 'to', 'type', ',', 'bt', '##w', '.', 'close', 'to', 'thirty', 'now', '.', 'the', 'problem', '?', 'my', 'on', '##col'], ['##ogist', 'doesn', \"'\", 't', 'believe', 'me', '.', 'says', 'he', \"'\", 's', 'never', 'heard', 'of', 'che', '##mo', 'brain', ',', 'and', 'refuses', 'to', 'help', 'me', '.', 'i', 'have', 'tried', 'other', 'places', 'but', 'was', 'turned', 'away', '(', 'i', 'suspect', 'when', 'they', 'call', 'him', 'for', 'records', 'or', 'talk', 'to', 'him', 'he', 'tells', 'them', 'that', 'he', 'doesn', \"'\", 't', 'believe', 'i', 'have', 'a', 'problem', ')', '.', 'finally', 'i', 'called', 'and', 'begged', 'for', 'a', 'refer', '##ral', 'and', 'got', 'one', 'to', 'a', 'ne', '##uro', '##logy', '/', 'psychology', 'department', '.', 'i', 'am', 'praying', 'they', 'will', 'help', 'me', 'and', 'not', 'just', 'assume', 'that', 'i', \"'\", 'm', \"'\", 'doctor', 'shopping', \"'\", 'for', 'drugs', 'or', 'that', 'i', 'am', 'his', '##tri', '##onic', '.', 'i', 'don', \"'\", 't', 'want', \"'\", 'just', \"'\", 'drugs', '.', 'i', 'just', 'want', 'my', 'damned', 'life', 'back', '.', '*', '*', 't', '##l', ':', 'dr', '*', '*', ',', 'i', 'guess', 'i', 'am', 'hoping', 'to', 'hear', 'from', 'someone', 'who', 'has', 'ad', '##hd', '/', 'add', 'and', 'who', 'has', 'been', 'through', 'che', '##mo', ',', 'and', 'if', 'there', 'were', 'similar', 'problems', '.', 'what', 'do', 'you', 'do', 'when', 'the', 'only', 'people', 'who', 'can', 'help', 'you', ',', 'won', \"'\", 't', '?', 'i', 'live', 'in', 'a', 'small', 'town', ',', 'there', 'are', 'not', 'other', 'options', 'within', 'reasonable', 'driving', 'distance', ',', 'unfortunately', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', 'good', 'discussion', 'of', 'rita', '##lin', ',', 'creativity', 'and', 'writing', '.', '(', 'x', '-', 'post', 'from', '/', 'r', '/', 'writing', ')']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', 'good', 'discussion', 'of', 'rita', '##lin', ',', 'creativity', 'and', 'writing', '.', '(', 'x', '-', 'post', 'from', '/', 'r', '/', 'writing', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', '\"', 'feel', '\"', 'the', 'difference', 'on', 'med', '##s', '?', 'i', 'recently', 'finally', 'mustered', 'up', 'the', 'strength', 'to', 'go', 'get', 'tested', 'for', 'ad', '##hd', '.', 'i', 'found', 'out', 'that', 'i', 'have', 'ad', '##hd', 'ina', '##tten', '##tive', 'type', 'and', 'so', 'after', 'wondering', 'for', 'most', 'of', 'the', '19', 'years', 'of', 'my', 'life', ',', 'i', 'finally', 'know', '.', 'the', 'doc', 'has', 'me', 'on', 'a', 'month', 'trial', 'of', 'v', '##y', '##van', '##se', '30', '##mg', '.', 'sometime', 'i', 'notice', 'little', 'things', 'that', 'have', 'improved', ',', 'but', 'i', 'don', \"'\", 't', 'really', 'feel', 'different', '.', 'so', 'my', 'question', 'is', 'when', 'you', 'guys', 'take', 'your', 'med', '##s', 'and', 'they', 'are', 'affect', '##ive', ',', 'do', 'you', '\"', 'feel', '\"', 'it', '?', 'i', 'mean', ',', 'do', 'you', 'actively', 'feel', 'a', 'drastic', 'change', 'in', 'your', 'day', 'while', 'your', 'on', 'it', 'compared', 'to', 'when', 'your', 'are', 'not', '?', 'also', ',', 'could', 'this', 'be', 'a', 'sign', 'that', 'i', \"'\", 'm', 'just', 'on', 'to', 'low', 'of', 'a', 'dos', '##age', '?', 'thanks', '=', ')']\n",
      "INFO:__main__:Number of tokens: 161\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', '\"', 'feel', '\"', 'the', 'difference', 'on', 'med', '##s', '?', 'i', 'recently', 'finally', 'mustered', 'up', 'the', 'strength', 'to', 'go', 'get', 'tested', 'for', 'ad', '##hd', '.', 'i', 'found', 'out', 'that', 'i', 'have', 'ad', '##hd', 'ina', '##tten', '##tive', 'type', 'and', 'so', 'after', 'wondering', 'for', 'most', 'of', 'the', '19', 'years', 'of', 'my', 'life', ',', 'i', 'finally', 'know', '.', 'the', 'doc', 'has', 'me', 'on', 'a', 'month', 'trial', 'of', 'v', '##y', '##van', '##se', '30', '##mg', '.', 'sometime', 'i', 'notice', 'little', 'things', 'that', 'have', 'improved', ',', 'but', 'i', 'don', \"'\", 't', 'really', 'feel', 'different', '.', 'so', 'my', 'question', 'is', 'when', 'you', 'guys', 'take', 'your', 'med', '##s', 'and', 'they', 'are', 'affect', '##ive', ',', 'do', 'you', '\"', 'feel', '\"', 'it', '?', 'i', 'mean', ',', 'do', 'you', 'actively', 'feel', 'a', 'drastic', 'change', 'in', 'your', 'day', 'while', 'your', 'on', 'it', 'compared', 'to', 'when', 'your', 'are', 'not', '?', 'also', ',', 'could', 'this', 'be', 'a', 'sign', 'that', 'i', \"'\", 'm', 'just', 'on', 'to', 'low', 'of', 'a', 'dos', '##age', '?', 'thanks', '=', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'take', 'focal', '##in', '?', 'i', 'take', 'focal', '##in', 'in', 'the', 'morning', 'because', 'of', 'my', 'ad', '##hd', '.', 'i', 'do', 'get', 'some', 'side', 'affects', 'such', 'as', 'being', 'shake', '##y', ',', 'paranoid', ',', 'nervous', ',', 'depressed', ',', 'and', 'not', 'hungry', '.', 'is', 'this', 'just', 'me', '?', 'i', 'would', 'like', 'to', 'see', 'what', 'other', 'users', 'think', ':', 'p']\n",
      "INFO:__main__:Number of tokens: 57\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'take', 'focal', '##in', '?', 'i', 'take', 'focal', '##in', 'in', 'the', 'morning', 'because', 'of', 'my', 'ad', '##hd', '.', 'i', 'do', 'get', 'some', 'side', 'affects', 'such', 'as', 'being', 'shake', '##y', ',', 'paranoid', ',', 'nervous', ',', 'depressed', ',', 'and', 'not', 'hungry', '.', 'is', 'this', 'just', 'me', '?', 'i', 'would', 'like', 'to', 'see', 'what', 'other', 'users', 'think', ':', 'p']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'feel', 'music', 'flows', 'at', 'different', 'speeds', 'at', 'different', 'parts', 'of', 'the', 'day', '?', 'i', 'noticed', 'this', 'a', 'year', 'ago', 'at', 'college', '.', 'i', 'don', \"'\", 't', 'know', 'if', 'it', \"'\", 's', 'an', 'ad', '##hd', 'thing', 'but', 'i', 'had', 'been', 'listening', 'to', 'this', 'song', 'for', 'a', 'while', 'and', 'upon', 'listening', 'to', 'it', 'right', 'before', 'bed', 'one', 'time', 'it', 'sounded', 'a', 'lot', 'slower', '!', 'that', 'was', 'the', 'first', 'thing', 'i', 'noticed', ',', 'but', 'upon', 'listening', 'closer', 'i', 'realized', 'i', 'could', 'comprehend', 'all', 'of', 'the', 'instruments', 'and', 'how', 'they', 'interact', '##ed', 'much', 'better', '.', 'hell', ',', 'it', 'could', 'have', 'been', 'sped', 'up', ',', 'it', 'could', 'have', 'been', 'slower', '-', '-', 'regardless', 'things', 'were', 'noticeably', 'different', '.', 'i', \"'\", 'm', 'listening', 'to', 'some', 'of', 'the', 'music', 'i', \"'\", 've', 'been', 'recording', 'now', 'and', 'all', 'of', 'a', 'sudden', 'it', 'seems', 'a', 'lot', 'slower', ',', 'almost', 'to', 'an', 'ob', '##no', '##xious', 'degree', '.', 'any', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 153\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'feel', 'music', 'flows', 'at', 'different', 'speeds', 'at', 'different', 'parts', 'of', 'the', 'day', '?', 'i', 'noticed', 'this', 'a', 'year', 'ago', 'at', 'college', '.', 'i', 'don', \"'\", 't', 'know', 'if', 'it', \"'\", 's', 'an', 'ad', '##hd', 'thing', 'but', 'i', 'had', 'been', 'listening', 'to', 'this', 'song', 'for', 'a', 'while', 'and', 'upon', 'listening', 'to', 'it', 'right', 'before', 'bed', 'one', 'time', 'it', 'sounded', 'a', 'lot', 'slower', '!', 'that', 'was', 'the', 'first', 'thing', 'i', 'noticed', ',', 'but', 'upon', 'listening', 'closer', 'i', 'realized', 'i', 'could', 'comprehend', 'all', 'of', 'the', 'instruments', 'and', 'how', 'they', 'interact', '##ed', 'much', 'better', '.', 'hell', ',', 'it', 'could', 'have', 'been', 'sped', 'up', ',', 'it', 'could', 'have', 'been', 'slower', '-', '-', 'regardless', 'things', 'were', 'noticeably', 'different', '.', 'i', \"'\", 'm', 'listening', 'to', 'some', 'of', 'the', 'music', 'i', \"'\", 've', 'been', 'recording', 'now', 'and', 'all', 'of', 'a', 'sudden', 'it', 'seems', 'a', 'lot', 'slower', ',', 'almost', 'to', 'an', 'ob', '##no', '##xious', 'degree', '.', 'any', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['some', 'questions', 'about', 'v', '##y', '##van', '##se', '(', 'cost', ',', 'dependence', ',', 'etc', ')']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['some', 'questions', 'about', 'v', '##y', '##van', '##se', '(', 'cost', ',', 'dependence', ',', 'etc', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['til', 'latest', 'on', 'will', '##power', '-', '-', '-', 'add', 'tip', 'o', 'the', 'day', '205']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['til', 'latest', 'on', 'will', '##power', '-', '-', '-', 'add', 'tip', 'o', 'the', 'day', '205']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mother', \"'\", 's', 'new', 'little', 'help', '##er', '—', 'add', '##eral', '##l']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mother', \"'\", 's', 'new', 'little', 'help', '##er', '—', 'add', '##eral', '##l']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ne', '##uro', '##psy', '##cho', '##logical', 'intra', '-', 'individual', 'variability', 'explains', 'unique', 'genetic', 'variance', 'of', 'ad', '##hd', 'and', 'shows', 'suggest', '##ive', 'link', '##age', 'to', 'chromosomes', '12', ',', '13', ',', 'and', '17']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ne', '##uro', '##psy', '##cho', '##logical', 'intra', '-', 'individual', 'variability', 'explains', 'unique', 'genetic', 'variance', 'of', 'ad', '##hd', 'and', 'shows', 'suggest', '##ive', 'link', '##age', 'to', 'chromosomes', '12', ',', '13', ',', 'and', '17']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['til', 'cause', 'or', 'effect', '?', 'add', 'tip', 'o', 'the', 'day', '206']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['til', 'cause', 'or', 'effect', '?', 'add', 'tip', 'o', 'the', 'day', '206']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'worried', 'about', 'my', 'son']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'worried', 'about', 'my', 'son']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['rita', '##lin', 'boost', '##s', 'learning', 'by', 'increasing', 'brain', 'plastic', '##ity']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['rita', '##lin', 'boost', '##s', 'learning', 'by', 'increasing', 'brain', 'plastic', '##ity']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['difference', 'between', 'being', \"'\", 'gifted', \"'\", 'and', 'having', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['difference', 'between', 'being', \"'\", 'gifted', \"'\", 'and', 'having', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['trouble', 'waking', 'up', 'in', 'the', 'morning', 'i', 'just', 'moved', 'to', 'london', 'as', 'part', 'of', 'a', 'semester', 'studying', 'abroad', '.', 'i', \"'\", 'm', 'doing', 'an', 'independent', 'study', 'program', ',', 'and', 'i', \"'\", 'm', 'having', 'a', 'terrible', 'time', 'waking', 'up', 'in', 'the', 'morning', '.', 'the', 'first', 'couple', 'of', 'days', 'it', 'was', 'no', 'problem', '.', 'i', 'figured', 'it', 'was', 'just', 'jet', 'la', '##g', 'or', 'something', ',', 'and', 'i', 'had', 'just', 'run', 'out', 'of', 'add', '##eral', '##l', '.', 'i', 'recently', 'got', 'my', 'prescription', 'in', 'the', 'mail', 'so', 'i', \"'\", 've', 'been', 'taking', 'it', '.', 'even', 'with', 'it', 'though', 'i', \"'\", 've', 'been', 'having', 'trouble', 'getting', 'up', '.', 'i', 'know', 'i', \"'\", 'm', 'a', 'heavy', 'sleeper', ',', 'but', 'i', \"'\", 'm', 'shooting', 'for', '8', 'hours', 'a', 'night', 'at', 'most', '(', 'way', 'more', 'than', 'average', 'for', 'a', 'college', 'kid', ')', 'and', 'i', \"'\", 'm', 'getting', 'more', 'like', '9', '.', 'at', 'first', 'i', 'thought', 'this', 'was', 'a', 'self', 'discipline', 'issue', ',', 'and', 'it', 'might', 'still', 'be', 'but', 'i', \"'\", 'm', 'starting', 'to', 'get', 'frustrated', 'with', 'this', '.', 'i', 'make', 'sure', 'to', 'go', 'to', 'bed', 'early', '.', 'i', 'stopped', 'taking', 'my', 'sleep', 'aid', '(', 'since', 'i', 'used', 'to', 'need', 'it', 'when', 'i', 'first', 'started', 'my', 'med', ')', '.', 'i', \"'\", 'm', 'not', 'sure', 'what', \"'\", 's', 'up', 'though', '.', 'this', 'semester', 'i', \"'\", 'm', 'much', 'more', 'independent', 'than', 'usual', '.', 'maybe', 'it', \"'\", 's', 'that', 'i', 'don', \"'\", 't', 'have', 'as', 'strict', 'a', 'schedule', '(', 'i', 'go', 'into', 'studio', 'when', 'i', 'want', '.', 'i', 'only', 'see', 'my', 'teacher', 'once', 'a', 'week', '.', 'otherwise', 'it', \"'\", 's', 'up', 'to', 'me', 'to', 'get', 'work', 'done', ')', 'i', \"'\", 'm', 'also', 'new', 'to', 'this', 'city', 'and', 'i', 'don', \"'\", 't', 'know', 'anyone', 'yet', '.', 'though', 'i', 'enjoy', 'social', 'stuff', ',', 'i', \"'\", 'm', 'also', 'intro', '##verted', '(', 'in', 'a', 'weird', 'way', '.', 'i', \"'\", 'm', 'also', 'outgoing', 'and', 'loud', 'etc', ')', ',', 'so', 'i', \"'\", 've', 'been', 'spending', 'wa', '##aa', '##ay', 'more', 'time', 'reading', 'books', '(', 'used', 'to', 'be', 'i', 'would', 'read', 'a', 'book', 'maybe', 'once', 'every', 'couple', 'of', 'months', '.', 'in', 'the', 'last', 'week', 'i', \"'\", 've', 'finished', '2', '.', '5', 'books', ')', 'i', 'wake', 'up', 'in', 'the', 'morning', 'and', 'i', 'feel', 'practically', 'drunk', ',', 'se', '##date', '##d', 'almost', '.', 'maybe', 'it', \"'\", 's', 'food', 'related', '?', 'since', 'moving', 'here', ',', 'with', 'money', 'being', 'tight', ',', 'i', \"'\", 've', 'also', 'cut', 'back', 'on', 'how', 'much', 'i', 'eat', ',', 'which', 'gives', 'me', 'no', 'problem', 'during', 'the', 'day', 'when', 'i', \"'\", 'm', 'focused', 'on', 'working', 'in', 'the', 'studio', '.', 'i', \"'\", 'm', 'fairly', 'healthy', '.', '21', 'years', 'old', '.', '5', 'foot', '9', ',', '185', 'pounds', 'with', '13', '%', 'body', 'fat', ',', 'and', 'a', 'decent', 'amount', 'of', 'lean', 'muscle', '(', 'i', 'lift', 'weights', ',', 'though', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'find', 'a', 'gym', 'around', 'here', 'yet', ')', 'i', \"'\", 'm', 'sure', 'a', 'lot', 'of', 'this', 'is', 'a', 'self', 'discipline', 'issue', ',', 'but', 'i', \"'\", 'm', 'also', 'not', 'sure', 'what', 'do', 'to', 'make', 'this', 'easier', '.', 'like', 'i', \"'\", 've', 'said', ',', 'i', \"'\", 've', 'always', 'been', 'a', 'heavier', 'sleeper', ',', 'but', 'this', 'is', 'way', 'out', 'there', '.', 'back', 'at', 'my', 'home', 'school', 'i', 'could', 'function', 'no', 'problem', 'on', '5', '-', '6', 'hours', 'a', 'day', '.', 'am', 'i', 'just', 'getting', 'older', '?', 'is', 'there', 'something', 'i', 'can', 'do', '?', '*', 'one', 'last', 'thing', '*', 'i', 'also', 'tend', 'to', 'be', 'a', 'night', 'owl', ',', 'and', 'recently', 'have', 'been', 'trying', 'to', 'switch', 'to', 'going', 'to', 'bed', 'earlier', 'and', 'waking', 'up', 'earlier', ',', 'because', 'it', \"'\", 's', 'winter', 'and', 'with', 'the', 'shorter', 'days', 'i', 'only', 'get', 'something', 'like', '5', 'hours', 'of', 'sunlight', ',', 'which', 'is', 'making', 'me', 'a', 'little', 'depressed', '.', 'i', 'want', 'more', 'sunlight', ',', 'so', 'i', 'want', 'to', 'get', 'up', 'in', 'the', 'morning', 'earlier', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'there', 'are', 'a', 'lot', 'of', 'factors', 'in', 'play', ',', 'but', 'i', 'want', 'to', 'get', 'up', 'earlier', ',', 'and', 'i', 'was', 'looking', 'for', 'suggestions', 'on', 'how', 'to', 'do', 'so', '.', 'some', 'of', 'this', 'is', 'self', 'discipline', ',', 'but', 'any', 'tips', 'would', 'help', '.', '*', '*']\n",
      "INFO:__main__:Number of tokens: 673\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['trouble', 'waking', 'up', 'in', 'the', 'morning', 'i', 'just', 'moved', 'to', 'london', 'as', 'part', 'of', 'a', 'semester', 'studying', 'abroad', '.', 'i', \"'\", 'm', 'doing', 'an', 'independent', 'study', 'program', ',', 'and', 'i', \"'\", 'm', 'having', 'a', 'terrible', 'time', 'waking', 'up', 'in', 'the', 'morning', '.', 'the', 'first', 'couple', 'of', 'days', 'it', 'was', 'no', 'problem', '.', 'i', 'figured', 'it', 'was', 'just', 'jet', 'la', '##g', 'or', 'something', ',', 'and', 'i', 'had', 'just', 'run', 'out', 'of', 'add', '##eral', '##l', '.', 'i', 'recently', 'got', 'my', 'prescription', 'in', 'the', 'mail', 'so', 'i', \"'\", 've', 'been', 'taking', 'it', '.', 'even', 'with', 'it', 'though', 'i', \"'\", 've', 'been', 'having', 'trouble', 'getting', 'up', '.', 'i', 'know', 'i', \"'\", 'm', 'a', 'heavy', 'sleeper', ',', 'but', 'i', \"'\", 'm', 'shooting', 'for', '8', 'hours', 'a', 'night', 'at', 'most', '(', 'way', 'more', 'than', 'average', 'for', 'a', 'college', 'kid', ')', 'and', 'i', \"'\", 'm', 'getting', 'more', 'like', '9', '.', 'at', 'first', 'i', 'thought', 'this', 'was', 'a', 'self', 'discipline', 'issue', ',', 'and', 'it', 'might', 'still', 'be', 'but', 'i', \"'\", 'm', 'starting', 'to', 'get', 'frustrated', 'with', 'this', '.', 'i', 'make', 'sure', 'to', 'go', 'to', 'bed', 'early', '.', 'i', 'stopped', 'taking', 'my', 'sleep', 'aid', '(', 'since', 'i', 'used', 'to', 'need', 'it', 'when', 'i', 'first', 'started', 'my', 'med', ')', '.', 'i', \"'\", 'm', 'not', 'sure', 'what', \"'\", 's', 'up', 'though', '.', 'this', 'semester', 'i', \"'\", 'm', 'much', 'more', 'independent', 'than', 'usual', '.', 'maybe', 'it', \"'\", 's', 'that', 'i', 'don', \"'\", 't', 'have', 'as', 'strict', 'a', 'schedule', '(', 'i', 'go', 'into', 'studio', 'when', 'i', 'want', '.', 'i', 'only', 'see', 'my', 'teacher', 'once', 'a', 'week', '.', 'otherwise', 'it', \"'\", 's', 'up', 'to', 'me', 'to', 'get', 'work', 'done', ')', 'i', \"'\", 'm', 'also', 'new', 'to', 'this', 'city', 'and', 'i', 'don', \"'\", 't', 'know', 'anyone', 'yet', '.', 'though', 'i', 'enjoy', 'social', 'stuff', ',', 'i', \"'\", 'm', 'also', 'intro', '##verted', '(', 'in', 'a', 'weird', 'way', '.', 'i', \"'\", 'm', 'also', 'outgoing', 'and', 'loud', 'etc', ')', ',', 'so', 'i', \"'\", 've', 'been', 'spending', 'wa', '##aa', '##ay', 'more', 'time', 'reading', 'books', '(', 'used', 'to', 'be', 'i', 'would', 'read', 'a', 'book', 'maybe', 'once', 'every', 'couple', 'of', 'months', '.', 'in', 'the', 'last', 'week', 'i', \"'\", 've', 'finished', '2', '.', '5', 'books', ')', 'i', 'wake', 'up', 'in', 'the', 'morning', 'and', 'i', 'feel', 'practically', 'drunk', ',', 'se', '##date', '##d', 'almost', '.', 'maybe', 'it', \"'\", 's', 'food', 'related', '?', 'since', 'moving', 'here', ',', 'with', 'money', 'being', 'tight', ',', 'i', \"'\", 've', 'also', 'cut', 'back', 'on', 'how', 'much', 'i', 'eat', ',', 'which', 'gives', 'me', 'no', 'problem', 'during', 'the', 'day', 'when', 'i', \"'\", 'm', 'focused', 'on', 'working', 'in', 'the', 'studio', '.', 'i', \"'\", 'm', 'fairly', 'healthy', '.', '21', 'years', 'old', '.', '5', 'foot', '9', ',', '185', 'pounds', 'with', '13', '%', 'body', 'fat', ',', 'and', 'a', 'decent', 'amount', 'of', 'lean', 'muscle', '(', 'i', 'lift', 'weights', ',', 'though', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'find', 'a', 'gym', 'around', 'here', 'yet', ')', 'i', \"'\", 'm', 'sure', 'a', 'lot', 'of', 'this', 'is', 'a', 'self', 'discipline', 'issue', ',', 'but', 'i', \"'\", 'm', 'also', 'not', 'sure', 'what', 'do', 'to', 'make', 'this', 'easier', '.', 'like', 'i', \"'\", 've', 'said', ',', 'i', \"'\", 've', 'always', 'been', 'a', 'heavier', 'sleeper', ',', 'but', 'this', 'is'], ['way', 'out', 'there', '.', 'back', 'at', 'my', 'home', 'school', 'i', 'could', 'function', 'no', 'problem', 'on', '5', '-', '6', 'hours', 'a', 'day', '.', 'am', 'i', 'just', 'getting', 'older', '?', 'is', 'there', 'something', 'i', 'can', 'do', '?', '*', 'one', 'last', 'thing', '*', 'i', 'also', 'tend', 'to', 'be', 'a', 'night', 'owl', ',', 'and', 'recently', 'have', 'been', 'trying', 'to', 'switch', 'to', 'going', 'to', 'bed', 'earlier', 'and', 'waking', 'up', 'earlier', ',', 'because', 'it', \"'\", 's', 'winter', 'and', 'with', 'the', 'shorter', 'days', 'i', 'only', 'get', 'something', 'like', '5', 'hours', 'of', 'sunlight', ',', 'which', 'is', 'making', 'me', 'a', 'little', 'depressed', '.', 'i', 'want', 'more', 'sunlight', ',', 'so', 'i', 'want', 'to', 'get', 'up', 'in', 'the', 'morning', 'earlier', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'there', 'are', 'a', 'lot', 'of', 'factors', 'in', 'play', ',', 'but', 'i', 'want', 'to', 'get', 'up', 'earlier', ',', 'and', 'i', 'was', 'looking', 'for', 'suggestions', 'on', 'how', 'to', 'do', 'so', '.', 'some', 'of', 'this', 'is', 'self', 'discipline', ',', 'but', 'any', 'tips', 'would', 'help', '.', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', 'do', 'i', 'sometimes', 'feel', 'really', 'tired', 'after', 'taking', 'concert', '##a', '?', '[', 'via', '/', 'r', '/', 'psycho', '##pha', '##rma', '##cology', ']']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', 'do', 'i', 'sometimes', 'feel', 'really', 'tired', 'after', 'taking', 'concert', '##a', '?', '[', 'via', '/', 'r', '/', 'psycho', '##pha', '##rma', '##cology', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['switched', 'from', 'st', '##rat', '##tera', 'to', 'add', '##eral', '##l', 'ir', ',', 'day', '8', '.', 'missing', 'something', '?', '*', '*', 'background', ':', '*', '*', 'was', 'diagnosed', 'ad', '##hd', 'at', 'about', '14', ',', 'on', 'rita', '##lin', 'off', 'and', 'on', 'into', 'university', '.', 'it', 'certainly', 'helped', ',', 'but', 'i', 'wasn', \"'\", 't', 'sold', 'on', 'the', 'whole', 'ad', '##hd', 'thing', ',', 'was', 'often', 'fairly', 'ok', 'without', 'med', '##s', ',', 'and', 'hated', 'the', 'feeling', 'of', 'taking', 'it', 'every', 'day', '.', 'finally', 'went', 'back', 'to', 'a', 'doc', 'a', 'few', 'months', 'ago', 'after', 'a', 'year', 'of', 'misery', '(', 'change', 'in', 'life', 'situation', ',', 'focus', 'going', 'away', 'completely', ',', 'starting', 'to', 'get', 'pretty', 'damn', 'low', 'with', 'professional', 'life', 'going', 'down', 'the', 'toilet', ')', ',', 'got', 'a', 'diagnosis', 'as', 'an', 'adult', '(', '29', 'now', ')', 'and', ',', 'it', 'really', 'hit', 'me', 'when', 'my', 'doc', 'said', ',', '\"', 'ad', '##hd', 'doesn', \"'\", 't', 'come', 'and', 'go', ',', 'just', 'your', 'life', 'circumstances', 'and', 'ability', 'to', 'deal', 'with', 'it', 'gets', 'better', 'and', 'worse', '\"', '.', '*', '*', 'now', ':', '*', '*', 'so', ',', 'we', \"'\", 've', 'been', 'trying', 'to', 'figure', 'out', 'a', 'medication', 'strategy', 'that', \"'\", 's', 'more', 'sustainable', 'long', 'term', '.', 'started', 'on', 'st', '##rat', '##tera', 'and', 'it', 'was', 'like', 'a', 'miracle', 'on', 'day', '1', '.', 'motivation', 'was', 'suddenly', 'there', 'and', 'i', 'was', 'starting', 'to', 'catch', 'up', 'on', 'things', '.', 'some', 'side', 'effects', 'but', 'nothing', 'bad', '.', 'on', 'day', '5', 'i', 'realized', 'brand', 'names', 'weren', \"'\", 't', 'covered', 'by', 'my', 'insurance', ',', 'so', 'i', 'asked', 'for', 'a', 'switch', 'to', 'his', 'other', 'non', '-', 'st', '##im', '##ula', '##nt', 'suggestion', 'that', 'had', 'a', 'generic', ',', 'well', '##bu', '##tri', '##n', '.', 'a', 'week', 'of', 'that', 'and', 'i', 'gave', 'up', 'because', 'even', 'though', 'i', 'know', 'it', 'could', 'work', 'in', '2', '-', '4', 'weeks', ',', 'patience', 'is', 'not', 'my', 'strong', 'suit', '.', '$', '200', '/', 'mo', 'for', 'st', '##rat', '##tera', 'suddenly', 'seemed', 'trivial', 'for', 'my', 'sanity', ':', 'p', 'fast', 'forward', 'to', 'a', 'month', 'on', 'the', 'full', '80', '##mg', 'dose', ',', 'and', 'it', 'felt', 'like', 'it', 'had', 'gradually', 'completely', 'stopped', 'working', ',', 'and', 'the', 'daily', 'hour', 'or', 'two', 'of', 'nausea', 'and', 'intensified', 'men', '##st', '##ru', '##al', 'cr', '##amps', 'really', 'didn', \"'\", 't', 'seem', 'worth', 'it', '.', 'doc', 'said', 'that', 'wasn', \"'\", 't', 'terribly', 'uncommon', 'and', 'we', 'decided', 'it', \"'\", 's', 'st', '##im', '##ula', '##nt', 'time', '.', 'i', \"'\", 've', 'been', 'working', 'up', 'to', '2', 'x', '20', '##mg', 'generic', 'add', '##eral', '##l', 'ir', '.', 'day', '1', '&', '2', 'were', 'amazing', '.', 'then', 'gradually', 'less', 'so', '.', 'it', 'started', 'to', 'feel', 'like', 'something', 'big', 'was', 'missing', 'again', '.', '*', '*', 'st', '##rat', '##tera', '=', '=', 'driver', 'behind', 'the', 'wheel', ',', 'but', 'no', 'engine', '*', '*', '*', '*', 'add', '##eral', '##l', '=', '=', 'engine', 'in', 'the', 'car', ',', 'but', 'no', 'one', 'behind', 'the', 'wheel', '*', '*', 'i', 'know', 'if', 'i', 'ask', 'him', 'about', 'changing', 'anything', 'now', ',', 'he', \"'\", 'll', 'want', 'me', 'to', 'stick', 'out', 'the', 'month', 'to', 'give', 'it', 'a', 'fair', 'shot', ',', 'and', 'that', 'seems', 'entirely', 'reasonable', '.', 'i', 'have', 'plenty', 'extra', 'of', 'both', 'st', '##rat', '##tera', 'and', 'well', '##bu', '##tri', '##n', 'and', 'it', \"'\", 's', 'taking', 'all', 'my', 'will', '##power', 'to', 'not', 'just', 'start', 'taking', 'one', 'of', 'them', ',', 'because', 'i', 'want', 'to', 'feel', '.', 'better', '.', 'now', '.', 'dammit', '!', 'i', 'can', \"'\", 't', 'help', 'but', 'think', 'that', 'the', 'initial', 'days', 'on', 'the', 'add', '##eral', '##l', 'were', 'so', 'much', 'better', 'because', 'the', 'st', '##rat', '##tera', 'takes', 'a', 'while', 'to', 'get', 'out', 'of', 'your', 'system', ',', 'so', 'i', 'had', 'both', 'going', 'at', 'first', '.', 'my', 'doc', 'was', 'fine', 'with', 'trying', 'them', 'together', ',', 'but', 'i', 'had', 'felt', 'that', 'the', 'st', '##rat', '##tera', 'wasn', \"'\", 't', 'doing', 'anything', 'anymore', ',', 'so', 'he', 'said', 'to', 'just', 'scrap', 'it', '.', 'plus', 'i', \"'\", 'd', 'probably', 'try', 'the', 'well', '##bu', '##tri', '##n', 'again', 'instead', 'for', 'the', 'cost', 'and', 'hopefully', 'less', 'nausea', '.', 'with', 'the', 'add', '##eral', '##l', 'at', 'least', '*', 'helping', '*', 'in', 'the', 'meantime', ',', 'it', 'would', 'be', 'easier', 'to', 'wait', 'it', 'out', '.', 'with', 'just', 'the', 'add', '##eral', '##l', ',', 'focus', 'is', 'improved', 'when', 'i', 'can', 'get', 'going', 'on', 'something', ',', 'but', 'i', \"'\", 'm', 'missing', 'the', 'lack', 'of', 'resistance', 'to', 'starting', 'things', 'that', 'i', 'had', 'previously', '.', 'i', \"'\", 'm', 'not', 'sure', 'exactly', 'why', 'i', \"'\", 'm', 'posting', 'here', 'other', 'than', 'for', 'support', ',', 'any', 'advice', 'or', 'common', 'experiences', ',', 'and', 'a', 'bit', 'of', 'vent', '##ing', ':', ')', 'as', 'someone', 'who', 'approaches', 'things', 'in', 'life', 'with', 'research', ',', 'logic', ',', 'and', 'confidence', ',', 'it', \"'\", 's', 'so', 'frustrating', 'to', 'play', 'the', 'experimentation', 'and', 'waiting', 'game', '!']\n",
      "INFO:__main__:Number of tokens: 748\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['switched', 'from', 'st', '##rat', '##tera', 'to', 'add', '##eral', '##l', 'ir', ',', 'day', '8', '.', 'missing', 'something', '?', '*', '*', 'background', ':', '*', '*', 'was', 'diagnosed', 'ad', '##hd', 'at', 'about', '14', ',', 'on', 'rita', '##lin', 'off', 'and', 'on', 'into', 'university', '.', 'it', 'certainly', 'helped', ',', 'but', 'i', 'wasn', \"'\", 't', 'sold', 'on', 'the', 'whole', 'ad', '##hd', 'thing', ',', 'was', 'often', 'fairly', 'ok', 'without', 'med', '##s', ',', 'and', 'hated', 'the', 'feeling', 'of', 'taking', 'it', 'every', 'day', '.', 'finally', 'went', 'back', 'to', 'a', 'doc', 'a', 'few', 'months', 'ago', 'after', 'a', 'year', 'of', 'misery', '(', 'change', 'in', 'life', 'situation', ',', 'focus', 'going', 'away', 'completely', ',', 'starting', 'to', 'get', 'pretty', 'damn', 'low', 'with', 'professional', 'life', 'going', 'down', 'the', 'toilet', ')', ',', 'got', 'a', 'diagnosis', 'as', 'an', 'adult', '(', '29', 'now', ')', 'and', ',', 'it', 'really', 'hit', 'me', 'when', 'my', 'doc', 'said', ',', '\"', 'ad', '##hd', 'doesn', \"'\", 't', 'come', 'and', 'go', ',', 'just', 'your', 'life', 'circumstances', 'and', 'ability', 'to', 'deal', 'with', 'it', 'gets', 'better', 'and', 'worse', '\"', '.', '*', '*', 'now', ':', '*', '*', 'so', ',', 'we', \"'\", 've', 'been', 'trying', 'to', 'figure', 'out', 'a', 'medication', 'strategy', 'that', \"'\", 's', 'more', 'sustainable', 'long', 'term', '.', 'started', 'on', 'st', '##rat', '##tera', 'and', 'it', 'was', 'like', 'a', 'miracle', 'on', 'day', '1', '.', 'motivation', 'was', 'suddenly', 'there', 'and', 'i', 'was', 'starting', 'to', 'catch', 'up', 'on', 'things', '.', 'some', 'side', 'effects', 'but', 'nothing', 'bad', '.', 'on', 'day', '5', 'i', 'realized', 'brand', 'names', 'weren', \"'\", 't', 'covered', 'by', 'my', 'insurance', ',', 'so', 'i', 'asked', 'for', 'a', 'switch', 'to', 'his', 'other', 'non', '-', 'st', '##im', '##ula', '##nt', 'suggestion', 'that', 'had', 'a', 'generic', ',', 'well', '##bu', '##tri', '##n', '.', 'a', 'week', 'of', 'that', 'and', 'i', 'gave', 'up', 'because', 'even', 'though', 'i', 'know', 'it', 'could', 'work', 'in', '2', '-', '4', 'weeks', ',', 'patience', 'is', 'not', 'my', 'strong', 'suit', '.', '$', '200', '/', 'mo', 'for', 'st', '##rat', '##tera', 'suddenly', 'seemed', 'trivial', 'for', 'my', 'sanity', ':', 'p', 'fast', 'forward', 'to', 'a', 'month', 'on', 'the', 'full', '80', '##mg', 'dose', ',', 'and', 'it', 'felt', 'like', 'it', 'had', 'gradually', 'completely', 'stopped', 'working', ',', 'and', 'the', 'daily', 'hour', 'or', 'two', 'of', 'nausea', 'and', 'intensified', 'men', '##st', '##ru', '##al', 'cr', '##amps', 'really', 'didn', \"'\", 't', 'seem', 'worth', 'it', '.', 'doc', 'said', 'that', 'wasn', \"'\", 't', 'terribly', 'uncommon', 'and', 'we', 'decided', 'it', \"'\", 's', 'st', '##im', '##ula', '##nt', 'time', '.', 'i', \"'\", 've', 'been', 'working', 'up', 'to', '2', 'x', '20', '##mg', 'generic', 'add', '##eral', '##l', 'ir', '.', 'day', '1', '&', '2', 'were', 'amazing', '.', 'then', 'gradually', 'less', 'so', '.', 'it', 'started', 'to', 'feel', 'like', 'something', 'big', 'was', 'missing', 'again', '.', '*', '*', 'st', '##rat', '##tera', '=', '=', 'driver', 'behind', 'the', 'wheel', ',', 'but', 'no', 'engine', '*', '*', '*', '*', 'add', '##eral', '##l', '=', '=', 'engine', 'in', 'the', 'car', ',', 'but', 'no', 'one', 'behind', 'the', 'wheel', '*', '*', 'i', 'know', 'if', 'i', 'ask', 'him', 'about', 'changing', 'anything', 'now', ',', 'he', \"'\", 'll', 'want', 'me', 'to', 'stick', 'out', 'the', 'month', 'to', 'give', 'it', 'a', 'fair', 'shot', ',', 'and', 'that', 'seems', 'entirely', 'reasonable', '.', 'i', 'have', 'plenty', 'extra', 'of', 'both', 'st', '##rat', '##tera', 'and', 'well', '##bu', '##tri', '##n', 'and', 'it', \"'\", 's', 'taking', 'all', 'my', 'will'], ['##power', 'to', 'not', 'just', 'start', 'taking', 'one', 'of', 'them', ',', 'because', 'i', 'want', 'to', 'feel', '.', 'better', '.', 'now', '.', 'dammit', '!', 'i', 'can', \"'\", 't', 'help', 'but', 'think', 'that', 'the', 'initial', 'days', 'on', 'the', 'add', '##eral', '##l', 'were', 'so', 'much', 'better', 'because', 'the', 'st', '##rat', '##tera', 'takes', 'a', 'while', 'to', 'get', 'out', 'of', 'your', 'system', ',', 'so', 'i', 'had', 'both', 'going', 'at', 'first', '.', 'my', 'doc', 'was', 'fine', 'with', 'trying', 'them', 'together', ',', 'but', 'i', 'had', 'felt', 'that', 'the', 'st', '##rat', '##tera', 'wasn', \"'\", 't', 'doing', 'anything', 'anymore', ',', 'so', 'he', 'said', 'to', 'just', 'scrap', 'it', '.', 'plus', 'i', \"'\", 'd', 'probably', 'try', 'the', 'well', '##bu', '##tri', '##n', 'again', 'instead', 'for', 'the', 'cost', 'and', 'hopefully', 'less', 'nausea', '.', 'with', 'the', 'add', '##eral', '##l', 'at', 'least', '*', 'helping', '*', 'in', 'the', 'meantime', ',', 'it', 'would', 'be', 'easier', 'to', 'wait', 'it', 'out', '.', 'with', 'just', 'the', 'add', '##eral', '##l', ',', 'focus', 'is', 'improved', 'when', 'i', 'can', 'get', 'going', 'on', 'something', ',', 'but', 'i', \"'\", 'm', 'missing', 'the', 'lack', 'of', 'resistance', 'to', 'starting', 'things', 'that', 'i', 'had', 'previously', '.', 'i', \"'\", 'm', 'not', 'sure', 'exactly', 'why', 'i', \"'\", 'm', 'posting', 'here', 'other', 'than', 'for', 'support', ',', 'any', 'advice', 'or', 'common', 'experiences', ',', 'and', 'a', 'bit', 'of', 'vent', '##ing', ':', ')', 'as', 'someone', 'who', 'approaches', 'things', 'in', 'life', 'with', 'research', ',', 'logic', ',', 'and', 'confidence', ',', 'it', \"'\", 's', 'so', 'frustrating', 'to', 'play', 'the', 'experimentation', 'and', 'waiting', 'game', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'anyone', 'using', 'med', '##ica', '##id', 'or', 'a', 'med', '##ica', '##id', 'managed', 'plan', 'having', 'trouble', 'getting', 'their', 'med', '##s', 'because', 'of', 'med', '##im', '##pac', '##t', \"'\", 's', 'formula', '##ry', '?', 'sorry', ',', 'i', 'combined', 'my', 'ad', '##hd', 'and', 'asthma', 'anger', '.', 'if', 'this', 'is', 'too', 'long', ',', 'the', 't', '##ld', '##r', 'version', 'is', ':', 'the', 'company', 'that', 'manages', 'my', 'formula', '##ry', 'has', 'been', 'playing', 'doctor', ',', 'trying', 'to', 'alter', 'my', 'prescription', '##s', ',', 'and', 'really', 'really', 'dick', '##ing', 'me', 'around', '.', 'someone', 'else', 'must', 'also', 'share', 'in', 'my', 'ran', '##t', '?', 'i', \"'\", 've', 'had', 'a', 'hell', '##ish', 'time', 'trying', 'to', 'get', 'my', 'insurance', 'to', 'cover', 'my', 'add', '##eral', '##l', 'x', '##r', 'and', 'my', 'in', '##hale', '##r', 'for', 'my', 'asthma', '.', 'med', '##im', '##pac', '##t', 'only', 'accepts', 'the', 'brand', 'name', 'add', '##eral', '##l', 'and', 'an', 'age', 'restriction', 'has', 'been', 'placed', 'on', 'the', 'medication', ';', 'if', 'you', \"'\", 're', 'over', '18', ',', 'you', 'now', 'need', 'prior', 'authorization', 'from', 'your', 'doctor', 'before', 'the', 'insurance', 'will', 'cover', 'the', 'cost', '.', 'it', \"'\", 's', 'supposed', 'to', 'take', 'about', '72', 'hours', 'for', 'med', '##im', '##pac', '##t', 'to', 'process', 'the', 'paperwork', 'fa', '##xed', 'over', 'by', 'the', 'doctor', 'that', 'gives', 'the', 'patient', 'permission', 'to', 'take', 'the', 'prescribed', 'medication', 'as', 'well', 'as', 'an', 'explanation', 'for', 'why', 'they', 'need', 'it', '.', 'i', \"'\", 've', 'been', 'waiting', 'nine', 'days', 'now', 'for', 'med', '##im', '##pac', '##t', 'to', 'okay', 'my', 'in', '##hale', '##r', 'and', 'this', 'month', \"'\", 's', 'and', 'last', 'month', \"'\", 's', 'add', '##eral', '##l', 'requests', 'were', 'just', 'shrugged', 'off', 'by', 'them', '.', 'i', '’', 'm', 'really', 'fed', 'up', ';', 'i', 'screamed', 'at', 'a', 'dozen', 'different', 'people', 'on', 'the', 'phone', 'today', '.', 'i', 'need', 'my', 'in', '##hale', '##r', ',', 'i', \"'\", 'm', 'sick', 'of', 'coughing', 'up', 'blood', 'and', 'scratching', 'together', 'money', ',', 'that', 'i', 'should', 'be', 'spending', 'on', 'food', ',', 'to', 'pay', 'for', 'my', 'add', '##eral', '##l', '.', 'i', 'was', 'told', 'on', 'the', 'phone', 'that', 'med', '##im', '##pac', '##t', 'would', 'like', 'me', 'to', 'try', 'other', 'medications', 'first', 'before', 'they', 'approve', 'the', 'medications', 'that', 'were', 'prescribed', 'to', 'me', 'by', 'my', 'doctors', '.', 'instead', 'of', 'ad', '##va', '##ir', ',', 'i', 'was', 'told', 'to', 'get', 'a', 'prescription', 'for', 'du', '##ler', '##a', ',', 'fill', 'it', ',', 'then', 'go', 'back', 'to', 'the', 'doctor', ',', 'get', 'another', 'ad', '##va', '##ir', 'prescription', ',', 'have', 'it', 'authorized', 'via', 'fa', '##x', 'once', 'again', ',', 'and', 'wait', 'for', 'it', 'to', 'be', 'approved', '.', 'instead', 'of', 'add', '##eral', '##l', 'i', 'was', 'told', 'to', 'get', 'a', 'prescription', 'for', '\"', 'one', 'of', 'those', 'other', 'ones', 'without', 'an', 'age', 'restriction', '.', '\"', 'i', 'tried', 'to', 'explain', 'that', 'my', 'doctor', 'pre', '##scribe', '##s', 'add', '##eral', '##l', 'because', 'it', \"'\", 's', 'the', 'drug', 'that', 'works', 'best', 'for', 'me', 'and', 'that', ',', 'even', 'though', ',', 'du', '##ler', '##a', 'and', 'ad', '##va', '##ir', 'both', 'treat', 'asthma', ',', 'they', 'are', 'different', 'medications', '.', 'i', 'also', 'pointed', 'out', 'that', 'the', 'two', 'medications', 'prescribed', 'to', 'me', 'are', 'listed', 'in', 'my', 'formula', '##ry', 'and', 'covered', 'by', 'my', 'insurance', 'when', 'authorized', 'by', 'a', 'doctor', 'and', 'it', 'doesn', '’', 't', 'make', 'sense', 'that', 'should', 'first', 'get', 'the', 'in', '##hale', '##r', 'that', 'i', 'don', '’', 't', 'need', 'and', 'then', 'go', 'get', 'the', 'one', 'that', 'i', 'do', '.', 'i', 'got', 'a', 'real', 'muffled', ',', 'reluctant', '“', 'yeah', '…', '”', 'from', 'the', 'other', 'side', 'of', 'the', 'phone', 'and', 'then', 'i', 'was', 'told', 'that', 'my', 'doctors', 'would', 'have', 'to', ',', 'again', ',', 'fa', '##x', 'over', 'paperwork', 'author', '##izing', 'and', 'explaining', 'my', 'need', 'for', 'the', 'medications', ',', 'but', 'this', 'time', 'they', 'needed', 'to', 'also', 'include', 'why', 'it', 'is', 'that', 'i', 'really', 'need', 'those', 'needed', 'medications', '.', 'right', 'now', ',', 'i', '’', 'm', 'making', 'due', 'with', 'my', 'emergency', 'in', '##hale', '##r', 'and', 'i', 'just', 'barely', 'paid', 'out', 'of', 'pocket', 'for', 'my', 'add', '##eral', '##l', '.', 'i', 'can', '’', 't', 'do', 'this', 'much', 'longer', '.', 'i', '’', 'm', 'going', 'broke', 'and', 'crazy', '.', 'i', '’', 'm', 'thinking', 'of', 'filing', 'a', 'complaint', 'with', 'health', 'and', 'human', 'services', '.', 'i', 'don', '’', 't', 'know', 'if', 'it', '’', 'll', 'do', 'much', '.']\n",
      "INFO:__main__:Number of tokens: 657\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['is', 'anyone', 'using', 'med', '##ica', '##id', 'or', 'a', 'med', '##ica', '##id', 'managed', 'plan', 'having', 'trouble', 'getting', 'their', 'med', '##s', 'because', 'of', 'med', '##im', '##pac', '##t', \"'\", 's', 'formula', '##ry', '?', 'sorry', ',', 'i', 'combined', 'my', 'ad', '##hd', 'and', 'asthma', 'anger', '.', 'if', 'this', 'is', 'too', 'long', ',', 'the', 't', '##ld', '##r', 'version', 'is', ':', 'the', 'company', 'that', 'manages', 'my', 'formula', '##ry', 'has', 'been', 'playing', 'doctor', ',', 'trying', 'to', 'alter', 'my', 'prescription', '##s', ',', 'and', 'really', 'really', 'dick', '##ing', 'me', 'around', '.', 'someone', 'else', 'must', 'also', 'share', 'in', 'my', 'ran', '##t', '?', 'i', \"'\", 've', 'had', 'a', 'hell', '##ish', 'time', 'trying', 'to', 'get', 'my', 'insurance', 'to', 'cover', 'my', 'add', '##eral', '##l', 'x', '##r', 'and', 'my', 'in', '##hale', '##r', 'for', 'my', 'asthma', '.', 'med', '##im', '##pac', '##t', 'only', 'accepts', 'the', 'brand', 'name', 'add', '##eral', '##l', 'and', 'an', 'age', 'restriction', 'has', 'been', 'placed', 'on', 'the', 'medication', ';', 'if', 'you', \"'\", 're', 'over', '18', ',', 'you', 'now', 'need', 'prior', 'authorization', 'from', 'your', 'doctor', 'before', 'the', 'insurance', 'will', 'cover', 'the', 'cost', '.', 'it', \"'\", 's', 'supposed', 'to', 'take', 'about', '72', 'hours', 'for', 'med', '##im', '##pac', '##t', 'to', 'process', 'the', 'paperwork', 'fa', '##xed', 'over', 'by', 'the', 'doctor', 'that', 'gives', 'the', 'patient', 'permission', 'to', 'take', 'the', 'prescribed', 'medication', 'as', 'well', 'as', 'an', 'explanation', 'for', 'why', 'they', 'need', 'it', '.', 'i', \"'\", 've', 'been', 'waiting', 'nine', 'days', 'now', 'for', 'med', '##im', '##pac', '##t', 'to', 'okay', 'my', 'in', '##hale', '##r', 'and', 'this', 'month', \"'\", 's', 'and', 'last', 'month', \"'\", 's', 'add', '##eral', '##l', 'requests', 'were', 'just', 'shrugged', 'off', 'by', 'them', '.', 'i', '’', 'm', 'really', 'fed', 'up', ';', 'i', 'screamed', 'at', 'a', 'dozen', 'different', 'people', 'on', 'the', 'phone', 'today', '.', 'i', 'need', 'my', 'in', '##hale', '##r', ',', 'i', \"'\", 'm', 'sick', 'of', 'coughing', 'up', 'blood', 'and', 'scratching', 'together', 'money', ',', 'that', 'i', 'should', 'be', 'spending', 'on', 'food', ',', 'to', 'pay', 'for', 'my', 'add', '##eral', '##l', '.', 'i', 'was', 'told', 'on', 'the', 'phone', 'that', 'med', '##im', '##pac', '##t', 'would', 'like', 'me', 'to', 'try', 'other', 'medications', 'first', 'before', 'they', 'approve', 'the', 'medications', 'that', 'were', 'prescribed', 'to', 'me', 'by', 'my', 'doctors', '.', 'instead', 'of', 'ad', '##va', '##ir', ',', 'i', 'was', 'told', 'to', 'get', 'a', 'prescription', 'for', 'du', '##ler', '##a', ',', 'fill', 'it', ',', 'then', 'go', 'back', 'to', 'the', 'doctor', ',', 'get', 'another', 'ad', '##va', '##ir', 'prescription', ',', 'have', 'it', 'authorized', 'via', 'fa', '##x', 'once', 'again', ',', 'and', 'wait', 'for', 'it', 'to', 'be', 'approved', '.', 'instead', 'of', 'add', '##eral', '##l', 'i', 'was', 'told', 'to', 'get', 'a', 'prescription', 'for', '\"', 'one', 'of', 'those', 'other', 'ones', 'without', 'an', 'age', 'restriction', '.', '\"', 'i', 'tried', 'to', 'explain', 'that', 'my', 'doctor', 'pre', '##scribe', '##s', 'add', '##eral', '##l', 'because', 'it', \"'\", 's', 'the', 'drug', 'that', 'works', 'best', 'for', 'me', 'and', 'that', ',', 'even', 'though', ',', 'du', '##ler', '##a', 'and', 'ad', '##va', '##ir', 'both', 'treat', 'asthma', ',', 'they', 'are', 'different', 'medications', '.', 'i', 'also', 'pointed', 'out', 'that', 'the', 'two', 'medications', 'prescribed', 'to', 'me', 'are', 'listed', 'in', 'my', 'formula', '##ry', 'and', 'covered', 'by', 'my', 'insurance', 'when', 'authorized', 'by', 'a', 'doctor', 'and', 'it', 'doesn', '’', 't', 'make', 'sense', 'that', 'should', 'first', 'get', 'the', 'in', '##hale', '##r', 'that', 'i', 'don', '’', 't', 'need', 'and'], ['then', 'go', 'get', 'the', 'one', 'that', 'i', 'do', '.', 'i', 'got', 'a', 'real', 'muffled', ',', 'reluctant', '“', 'yeah', '…', '”', 'from', 'the', 'other', 'side', 'of', 'the', 'phone', 'and', 'then', 'i', 'was', 'told', 'that', 'my', 'doctors', 'would', 'have', 'to', ',', 'again', ',', 'fa', '##x', 'over', 'paperwork', 'author', '##izing', 'and', 'explaining', 'my', 'need', 'for', 'the', 'medications', ',', 'but', 'this', 'time', 'they', 'needed', 'to', 'also', 'include', 'why', 'it', 'is', 'that', 'i', 'really', 'need', 'those', 'needed', 'medications', '.', 'right', 'now', ',', 'i', '’', 'm', 'making', 'due', 'with', 'my', 'emergency', 'in', '##hale', '##r', 'and', 'i', 'just', 'barely', 'paid', 'out', 'of', 'pocket', 'for', 'my', 'add', '##eral', '##l', '.', 'i', 'can', '’', 't', 'do', 'this', 'much', 'longer', '.', 'i', '’', 'm', 'going', 'broke', 'and', 'crazy', '.', 'i', '’', 'm', 'thinking', 'of', 'filing', 'a', 'complaint', 'with', 'health', 'and', 'human', 'services', '.', 'i', 'don', '’', 't', 'know', 'if', 'it', '’', 'll', 'do', 'much', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'can', 'i', 'tell', 'the', 'difference', 'between', '\"', 'can', \"'\", 't', '\"', 'and', '\"', 'don', \"'\", 't', 'want', 'to', '\"', '?', 'i', 'can', \"'\", 't', 'tell', 'the', 'difference', 'between', 'my', 'inability', 'and', 'my', 'la', '##zine', '##ss', '.', 'i', 'think', 'it', 'would', 'help', ',', 'if', 'i', 'could', '.', 'do', 'you', 'have', 'any', 'tips', ',', 'to', 'determine', 'whether', 'you', 'should', 'force', 'yourself', 'into', 'it', ',', 'or', 'to', 'take', 'a', 'break', 'and', 'work', 'on', 'your', 'mind', '?']\n",
      "INFO:__main__:Number of tokens: 74\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'can', 'i', 'tell', 'the', 'difference', 'between', '\"', 'can', \"'\", 't', '\"', 'and', '\"', 'don', \"'\", 't', 'want', 'to', '\"', '?', 'i', 'can', \"'\", 't', 'tell', 'the', 'difference', 'between', 'my', 'inability', 'and', 'my', 'la', '##zine', '##ss', '.', 'i', 'think', 'it', 'would', 'help', ',', 'if', 'i', 'could', '.', 'do', 'you', 'have', 'any', 'tips', ',', 'to', 'determine', 'whether', 'you', 'should', 'force', 'yourself', 'into', 'it', ',', 'or', 'to', 'take', 'a', 'break', 'and', 'work', 'on', 'your', 'mind', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'there', 'an', 'ad', '##hd', 'sub', '##red', '##dit', 'for', 'people', 'with', 'no', 'interest', 'in', 'medications', '?', 'i', 'am', 'wondering', 'if', 'there', \"'\", 's', 'a', 'way', 'i', 'don', \"'\", 't', 'have', 'to', 'scan', 'past', '2', '/', '3', 'of', 'the', 'posts', 'in', 'this', 'sub', '##red', '##dit', '.']\n",
      "INFO:__main__:Number of tokens: 45\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'there', 'an', 'ad', '##hd', 'sub', '##red', '##dit', 'for', 'people', 'with', 'no', 'interest', 'in', 'medications', '?', 'i', 'am', 'wondering', 'if', 'there', \"'\", 's', 'a', 'way', 'i', 'don', \"'\", 't', 'have', 'to', 'scan', 'past', '2', '/', '3', 'of', 'the', 'posts', 'in', 'this', 'sub', '##red', '##dit', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['totally', 'add', 'blog', ':', 'from', 'the', 'creators', 'of', \"'\", 'add', 'and', 'loving', 'it', \"'\"]\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['totally', 'add', 'blog', ':', 'from', 'the', 'creators', 'of', \"'\", 'add', 'and', 'loving', 'it', \"'\"]]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'notes', 'from', 'my', 'sociology', 'class', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'notes', 'from', 'my', 'sociology', 'class', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['very', 'little', '(', 'if', 'any', ')', 'talk', 'about', 'slug', '##gis', '##h', 'cognitive', 'tempo', ',', 'let', \"'\", 's', 'start', '.']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['very', 'little', '(', 'if', 'any', ')', 'talk', 'about', 'slug', '##gis', '##h', 'cognitive', 'tempo', ',', 'let', \"'\", 's', 'start', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'undergoing', 'ne', '##uro', '##co', '##gni', '##tive', 'testing', 'following', 'a', 'concussion', '.', 'previously', ',', 'my', 'therapist', '(', 'since', 'oct', ')', 'suggested', 'i', 'might', 'have', 'add', ',', 'is', 'it', 'worth', 'mentioning', 'her', 'observation', 'to', 'the', 'psychiatrist', '?', 'i', \"'\", 'm', 'wondering', 'if', 'that', 'might', 'change', 'the', 'perspective', 'from', 'which', 'the', 'testing', 'is', 'conducted', ',', 'if', 'that', 'makes', 'any', 'sense', '.', 'i', 'used', 'to', 'think', 'that', 'they', \"'\", 'd', 'automatically', 'ask', 'me', 'this', 'sort', 'of', 'thing', ',', 'but', 'i', \"'\", 've', 'noticed', 'that', 'i', 'have', 'to', 'advocate', 'for', 'myself', 'instead', '.']\n",
      "INFO:__main__:Number of tokens: 91\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'undergoing', 'ne', '##uro', '##co', '##gni', '##tive', 'testing', 'following', 'a', 'concussion', '.', 'previously', ',', 'my', 'therapist', '(', 'since', 'oct', ')', 'suggested', 'i', 'might', 'have', 'add', ',', 'is', 'it', 'worth', 'mentioning', 'her', 'observation', 'to', 'the', 'psychiatrist', '?', 'i', \"'\", 'm', 'wondering', 'if', 'that', 'might', 'change', 'the', 'perspective', 'from', 'which', 'the', 'testing', 'is', 'conducted', ',', 'if', 'that', 'makes', 'any', 'sense', '.', 'i', 'used', 'to', 'think', 'that', 'they', \"'\", 'd', 'automatically', 'ask', 'me', 'this', 'sort', 'of', 'thing', ',', 'but', 'i', \"'\", 've', 'noticed', 'that', 'i', 'have', 'to', 'advocate', 'for', 'myself', 'instead', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['medications', 'so', 'far', 'have', 'failed', ',', 'anyone', 'have', 'suggestions', 'add', '##eral', ';', 'well', 'it', 'makes', 'me', 'really', 'really', 'angry', ',', 'and', 'can', \"'\", 't', 'talk', 'at', 'all', ';', 'hell', 'lately', 'i', \"'\", 'm', 'not', 'anything', 'and', 'i', 'can', 'barely', 'talk', '.', 'i', 'have', 'a', 'decent', 'lexi', '##con', ',', 'but', 'i', 'stu', '##tter', 'a', 'lot', '.', '.', '.', 'i', 'didn', \"'\", 't', 'used', 'to', 'and', 'sometimes', 'i', \"'\", 'll', 'be', 'fine', 'but', 'other', 'times', 'id', '##k', '.', 'well', '##bu', '##tri', '##n', ';', 'made', 'me', 'feel', 'stupid', 'as', 'all', 'sin', 'could', 'barely', 'function', ',', 'i', 'could', 'focus', 'but', 'not', 'function', 'at', 'a', 'fast', 'speed', '.', 'anyone', 'have', 'any', 'other', 'good', 'suggestions', 'for', 'med', '##s', ',', 'i', 'also', 'have', 'hyper', '##thy', '##roid', '##ism', 'and', 'a', 'fast', 'metabolism', 'so', 'i', 'would', 'rather', 'not', 'take', 'st', '##im', '##ula', '##nts', 'because', 'i', 'want', 'to', 'gain', 'weight', 'and', 'put', 'on', 'mass', '.']\n",
      "INFO:__main__:Number of tokens: 145\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['medications', 'so', 'far', 'have', 'failed', ',', 'anyone', 'have', 'suggestions', 'add', '##eral', ';', 'well', 'it', 'makes', 'me', 'really', 'really', 'angry', ',', 'and', 'can', \"'\", 't', 'talk', 'at', 'all', ';', 'hell', 'lately', 'i', \"'\", 'm', 'not', 'anything', 'and', 'i', 'can', 'barely', 'talk', '.', 'i', 'have', 'a', 'decent', 'lexi', '##con', ',', 'but', 'i', 'stu', '##tter', 'a', 'lot', '.', '.', '.', 'i', 'didn', \"'\", 't', 'used', 'to', 'and', 'sometimes', 'i', \"'\", 'll', 'be', 'fine', 'but', 'other', 'times', 'id', '##k', '.', 'well', '##bu', '##tri', '##n', ';', 'made', 'me', 'feel', 'stupid', 'as', 'all', 'sin', 'could', 'barely', 'function', ',', 'i', 'could', 'focus', 'but', 'not', 'function', 'at', 'a', 'fast', 'speed', '.', 'anyone', 'have', 'any', 'other', 'good', 'suggestions', 'for', 'med', '##s', ',', 'i', 'also', 'have', 'hyper', '##thy', '##roid', '##ism', 'and', 'a', 'fast', 'metabolism', 'so', 'i', 'would', 'rather', 'not', 'take', 'st', '##im', '##ula', '##nts', 'because', 'i', 'want', 'to', 'gain', 'weight', 'and', 'put', 'on', 'mass', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['motivation', 'help', '?', 'i', \"'\", 'm', 'in', 'a', 'class', 'where', 'i', 'have', 'to', 'basically', 'read', 'a', 'chapter', 'of', 'the', 'textbook', 'a', 'day', ',', 'and', 'i', \"'\", 'm', 'having', 'trouble', 'finding', 'the', 'motivation', 'to', 'do', 'this', 'at', 'night', 'so', ',', 'the', 'next', 'day', ',', 'i', 'don', \"'\", 't', 'have', 'to', 'worry', 'about', 'overs', '##lee', '##ping', 'and', 'not', 'having', 'the', 'time', 'to', 'do', 'the', 'reading', '.', 'edit', ':', 'what', 'are', 'some', 'tips', 'to', 'help', 'motivation', '?']\n",
      "INFO:__main__:Number of tokens: 74\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['motivation', 'help', '?', 'i', \"'\", 'm', 'in', 'a', 'class', 'where', 'i', 'have', 'to', 'basically', 'read', 'a', 'chapter', 'of', 'the', 'textbook', 'a', 'day', ',', 'and', 'i', \"'\", 'm', 'having', 'trouble', 'finding', 'the', 'motivation', 'to', 'do', 'this', 'at', 'night', 'so', ',', 'the', 'next', 'day', ',', 'i', 'don', \"'\", 't', 'have', 'to', 'worry', 'about', 'overs', '##lee', '##ping', 'and', 'not', 'having', 'the', 'time', 'to', 'do', 'the', 'reading', '.', 'edit', ':', 'what', 'are', 'some', 'tips', 'to', 'help', 'motivation', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', 'parent', 'struggling', 'with', 'med', '##s', '/', 'energy', '/', 'general', 'life', 'hi', 'there', ',', 'i', \"'\", 'm', '25', 'with', 'an', '11', 'month', 'old', 'bu', '##b', '.', 'unless', 'i', \"'\", 'm', 'taking', 'dex', '##amp', '##het', '##amine', 'i', 'am', 'struggling', 'to', 'engage', 'in', 'normal', 'interaction', 'with', 'her', ',', 'due', 'in', 'part', 'to', 'a', 'consistently', 'interrupted', 'sleep', 'cycle', '.', 'my', 'temper', 'suffers', ',', 'my', 'ability', 'to', 'process', 'anything', 'is', 'greatly', 'reduced', 'and', 'on', 'the', 'whole', 'i', \"'\", 'm', 'feeling', 'like', 'i', 'am', 'failing', 'to', 'give', 'what', 'a', 'good', 'parent', 'does', '.', 'now', 'sure', 'the', 'seemingly', 'simple', 'answer', 'would', 'be', 'to', 'take', 'the', 'drugs', 'but', 'i', \"'\", 'm', 'finding', 'that', 'after', 'a', 'few', 'days', 'or', 'week', 'of', 'taking', 'the', 'dex', '##es', 'i', \"'\", 'm', 'feeling', 'almost', 'robotic', '.', 'how', 'else', 'can', 'i', 'have', 'the', 'energy', 'to', 'play', 'with', 'my', 'kid', 'and', 'make', 'the', 'rational', 'decisions', 'needed', 'to', 'be', 'a', 'decent', 'partner', 'and', 'father', '.', '.', 'its', 'starting', 'to', 'get', 'to', 'me', '.']\n",
      "INFO:__main__:Number of tokens: 158\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', 'parent', 'struggling', 'with', 'med', '##s', '/', 'energy', '/', 'general', 'life', 'hi', 'there', ',', 'i', \"'\", 'm', '25', 'with', 'an', '11', 'month', 'old', 'bu', '##b', '.', 'unless', 'i', \"'\", 'm', 'taking', 'dex', '##amp', '##het', '##amine', 'i', 'am', 'struggling', 'to', 'engage', 'in', 'normal', 'interaction', 'with', 'her', ',', 'due', 'in', 'part', 'to', 'a', 'consistently', 'interrupted', 'sleep', 'cycle', '.', 'my', 'temper', 'suffers', ',', 'my', 'ability', 'to', 'process', 'anything', 'is', 'greatly', 'reduced', 'and', 'on', 'the', 'whole', 'i', \"'\", 'm', 'feeling', 'like', 'i', 'am', 'failing', 'to', 'give', 'what', 'a', 'good', 'parent', 'does', '.', 'now', 'sure', 'the', 'seemingly', 'simple', 'answer', 'would', 'be', 'to', 'take', 'the', 'drugs', 'but', 'i', \"'\", 'm', 'finding', 'that', 'after', 'a', 'few', 'days', 'or', 'week', 'of', 'taking', 'the', 'dex', '##es', 'i', \"'\", 'm', 'feeling', 'almost', 'robotic', '.', 'how', 'else', 'can', 'i', 'have', 'the', 'energy', 'to', 'play', 'with', 'my', 'kid', 'and', 'make', 'the', 'rational', 'decisions', 'needed', 'to', 'be', 'a', 'decent', 'partner', 'and', 'father', '.', '.', 'its', 'starting', 'to', 'get', 'to', 'me', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'many', 'of', 'you', 'have', 'to', 'see', 'a', 'psychiatrist', 'for', 'med', '##s', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'many', 'of', 'you', 'have', 'to', 'see', 'a', 'psychiatrist', 'for', 'med', '##s', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['top', 'post', 'on', 'red', '##dit', 'hits', 'a', 'little', 'to', 'close', 'to', 'home', '(', 'x', '-', 'post', ')']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['top', 'post', 'on', 'red', '##dit', 'hits', 'a', 'little', 'to', 'close', 'to', 'home', '(', 'x', '-', 'post', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'very', 'few', 'friends', ',', 'and', 'only', 'hang', 'out', 'with', 'someone', 'once', 'to', 'twice', 'a', 'month', 'at', 'best', '.', 'all', 'because', 'of', 'my', 'hyper', '##act', '##ivity', '.', 'a', 'lot', 'of', 'people', 'understand', '##ably', 'are', 'annoyed', 'by', 'me', '.', 'some', 'hate', 'me', ',', 'some', 'realize', 'it', \"'\", 's', 'really', 'my', '\"', 'fault', '\"', 'but', 'that', 'doesn', \"'\", 't', 'help', 'the', 'fact', 'i', 'ann', '##oy', 'them', ',', 'so', 'they', 'don', \"'\", 't', 'want', 'to', 'be', 'around', 'me', '.', 'it', \"'\", 's', 'really', 'embarrassing', 'to', 'say', 'this', ',', 'because', 'i', 'know', 'why', 'i', 'ann', '##oy', 'people', ',', 'i', 'know', 'what', 'it', \"'\", 's', 'like', 'to', 'be', 'annoyed', ',', 'and', 'i', 'know', 'how', 'a', 'lot', 'of', 'people', 'feel', 'about', 'me', '.', 'i', 'have', 'few', 'friends', 'and', 'no', 'life', 'because', 'of', 'it', '.', 'the', 'last', 'party', 'i', 'was', 'invited', 'to', 'was', 'a', 'few', 'months', 'ago', ',', 'and', 'that', 'was', 'the', 'first', 'time', 'i', 'was', 'invited', 'to', 'a', 'party', 'since', '6th', 'grade', '.', 'it', \"'\", 's', 'not', 'that', 'i', 'am', 'anti', 'social', ',', 'it', \"'\", 's', 'just', 'that', 'i', 'am', 'a', 'pain', 'in', 'the', 'ass', '.', 'for', 'example', 'at', 'lunch', ',', 'i', \"'\", 'll', 'sit', 'there', 'with', 'my', 'friends', ',', 'and', 'i', \"'\", 'll', 'just', 'be', 'hyper', 'as', 'many', 'of', 'you', 'know', ',', 'and', 'i', \"'\", 'll', 'touch', 'their', 'stuff', ',', 'say', 'dumb', 'shit', ',', 'basically', 'everything', 'that', 'goes', 'along', 'with', 'being', 'hyper', '.', 'and', 'they', 'let', 'me', 'know', ',', 'and', 'i', 'can', \"'\", 't', 'blame', 'them', '.', 'even', 'when', 'i', \"'\", 'm', 'doing', 'it', 'i', 'want', 'to', 'stop', 'but', 'it', 'just', 'seems', 'like', 'i', 'can', \"'\", 't', '.', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'control', 'this', '.', 'i', 'am', 'currently', 'taking', 'herbal', 'med', '##s', ',', 'and', 'when', 'i', 'was', 'first', 'on', 'them', '4', '/', '5', 'years', 'ago', 'they', 'helped', 'me', 'out', 'a', 'lot', ',', 'but', 'they', 'worn', 'out', 'by', 'now', ',', 'even', 'after', 'increasing', 'dos', '##age', ',', 'and', 'help', 'very', 'little', '.', 'i', 'want', 'to', 'avoid', 'conventional', 'med', '##s', 'because', 'of', 'all', 'the', 'side', 'effects', 'i', 'hear', 'about', '.', 'just', 'some', 'background', 'info', ',', 'i', \"'\", 'm', '16', 'and', 'a', 'junior', '.', 'what', 'do', 'you', 'guys', 'think', '?', 'there', 'has', 'to', 'be', 'a', 'lot', 'of', 'people', 'here', 'who', \"'\", 's', 'been', 'in', 'my', 'situation', '.', 'what', 'should', 'i', 'do', '?', 'how', 'can', 'i', 'control', 'my', 'hyper', 'activity', '?', 'are', 'conventional', 'med', '##s', 'worth', 'it', '?', 'are', 'they', 'as', 'bad', 'as', 'what', \"'\", 's', 'said', 'about', 'them', ',', 'with', 'side', 'affects', 'like', 'no', 'appetite', ',', 'depression', 'etc', '?', 'i', 'try', 'to', 'control', 'it', 'and', 'some', 'days', 'are', 'better', 'than', 'others', ',', 'but', 'it', \"'\", 's', 'a', 'constant', 'battle', 'that', 'i', 'feel', 'like', 'i', \"'\", 'm', 'doomed', 'to', 'lose', ',', 'since', 'the', 'ad', '##hd', 'will', 'never', 'tire', 'out', ',', 'but', 'my', 'will', 'power', 'can', '.']\n",
      "INFO:__main__:Number of tokens: 460\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'very', 'few', 'friends', ',', 'and', 'only', 'hang', 'out', 'with', 'someone', 'once', 'to', 'twice', 'a', 'month', 'at', 'best', '.', 'all', 'because', 'of', 'my', 'hyper', '##act', '##ivity', '.', 'a', 'lot', 'of', 'people', 'understand', '##ably', 'are', 'annoyed', 'by', 'me', '.', 'some', 'hate', 'me', ',', 'some', 'realize', 'it', \"'\", 's', 'really', 'my', '\"', 'fault', '\"', 'but', 'that', 'doesn', \"'\", 't', 'help', 'the', 'fact', 'i', 'ann', '##oy', 'them', ',', 'so', 'they', 'don', \"'\", 't', 'want', 'to', 'be', 'around', 'me', '.', 'it', \"'\", 's', 'really', 'embarrassing', 'to', 'say', 'this', ',', 'because', 'i', 'know', 'why', 'i', 'ann', '##oy', 'people', ',', 'i', 'know', 'what', 'it', \"'\", 's', 'like', 'to', 'be', 'annoyed', ',', 'and', 'i', 'know', 'how', 'a', 'lot', 'of', 'people', 'feel', 'about', 'me', '.', 'i', 'have', 'few', 'friends', 'and', 'no', 'life', 'because', 'of', 'it', '.', 'the', 'last', 'party', 'i', 'was', 'invited', 'to', 'was', 'a', 'few', 'months', 'ago', ',', 'and', 'that', 'was', 'the', 'first', 'time', 'i', 'was', 'invited', 'to', 'a', 'party', 'since', '6th', 'grade', '.', 'it', \"'\", 's', 'not', 'that', 'i', 'am', 'anti', 'social', ',', 'it', \"'\", 's', 'just', 'that', 'i', 'am', 'a', 'pain', 'in', 'the', 'ass', '.', 'for', 'example', 'at', 'lunch', ',', 'i', \"'\", 'll', 'sit', 'there', 'with', 'my', 'friends', ',', 'and', 'i', \"'\", 'll', 'just', 'be', 'hyper', 'as', 'many', 'of', 'you', 'know', ',', 'and', 'i', \"'\", 'll', 'touch', 'their', 'stuff', ',', 'say', 'dumb', 'shit', ',', 'basically', 'everything', 'that', 'goes', 'along', 'with', 'being', 'hyper', '.', 'and', 'they', 'let', 'me', 'know', ',', 'and', 'i', 'can', \"'\", 't', 'blame', 'them', '.', 'even', 'when', 'i', \"'\", 'm', 'doing', 'it', 'i', 'want', 'to', 'stop', 'but', 'it', 'just', 'seems', 'like', 'i', 'can', \"'\", 't', '.', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'control', 'this', '.', 'i', 'am', 'currently', 'taking', 'herbal', 'med', '##s', ',', 'and', 'when', 'i', 'was', 'first', 'on', 'them', '4', '/', '5', 'years', 'ago', 'they', 'helped', 'me', 'out', 'a', 'lot', ',', 'but', 'they', 'worn', 'out', 'by', 'now', ',', 'even', 'after', 'increasing', 'dos', '##age', ',', 'and', 'help', 'very', 'little', '.', 'i', 'want', 'to', 'avoid', 'conventional', 'med', '##s', 'because', 'of', 'all', 'the', 'side', 'effects', 'i', 'hear', 'about', '.', 'just', 'some', 'background', 'info', ',', 'i', \"'\", 'm', '16', 'and', 'a', 'junior', '.', 'what', 'do', 'you', 'guys', 'think', '?', 'there', 'has', 'to', 'be', 'a', 'lot', 'of', 'people', 'here', 'who', \"'\", 's', 'been', 'in', 'my', 'situation', '.', 'what', 'should', 'i', 'do', '?', 'how', 'can', 'i', 'control', 'my', 'hyper', 'activity', '?', 'are', 'conventional', 'med', '##s', 'worth', 'it', '?', 'are', 'they', 'as', 'bad', 'as', 'what', \"'\", 's', 'said', 'about', 'them', ',', 'with', 'side', 'affects', 'like', 'no', 'appetite', ',', 'depression', 'etc', '?', 'i', 'try', 'to', 'control', 'it', 'and', 'some', 'days', 'are', 'better', 'than', 'others', ',', 'but', 'it', \"'\", 's', 'a', 'constant', 'battle', 'that', 'i', 'feel', 'like', 'i', \"'\", 'm', 'doomed', 'to', 'lose', ',', 'since', 'the', 'ad', '##hd', 'will', 'never', 'tire', 'out', ',', 'but', 'my', 'will', 'power', 'can', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', \"'\", 's', 'your', 'opinion', 'on', 'syn', '##ap', '##to', '##l', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', \"'\", 's', 'your', 'opinion', 'on', 'syn', '##ap', '##to', '##l', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'notice', 'a', 'difference', 'between', 'brand', 'and', 'generic', 'ad', '##hd', 'med', '##s', '?', 'i', \"'\", 'm', 'taking', '10', '##mg', 'focal', '##in', ',', 'and', 'i', 'notice', 'that', 'the', 'brand', 'name', 'stuff', 'seems', 'to', 'have', 'quite', 'a', 'bit', 'more', \"'\", 'kick', \"'\", 'to', 'it', 'than', 'the', 'generic', 'stuff', '.', 'it', \"'\", 's', 'like', 'the', 'difference', 'between', 'a', 'wall', 'outlet', 'and', 'a', '9', '##v', 'battery', 'to', 'me', '.', 'anyone', 'else', 'notice', 'a', 'big', 'difference', 'qu', '##ali', '##tative', '##ly', 'in', 'their', 'med', '##s', 'between', 'generic', 'and', 'brand', '?']\n",
      "INFO:__main__:Number of tokens: 85\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'notice', 'a', 'difference', 'between', 'brand', 'and', 'generic', 'ad', '##hd', 'med', '##s', '?', 'i', \"'\", 'm', 'taking', '10', '##mg', 'focal', '##in', ',', 'and', 'i', 'notice', 'that', 'the', 'brand', 'name', 'stuff', 'seems', 'to', 'have', 'quite', 'a', 'bit', 'more', \"'\", 'kick', \"'\", 'to', 'it', 'than', 'the', 'generic', 'stuff', '.', 'it', \"'\", 's', 'like', 'the', 'difference', 'between', 'a', 'wall', 'outlet', 'and', 'a', '9', '##v', 'battery', 'to', 'me', '.', 'anyone', 'else', 'notice', 'a', 'big', 'difference', 'qu', '##ali', '##tative', '##ly', 'in', 'their', 'med', '##s', 'between', 'generic', 'and', 'brand', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['confidence', '+', 'methyl', '##ph', '##eni', '##date', 'anyone', 'else', 'feel', 'their', 'confidence', 'flu', '##ct', '##uate', 'greatly', 'throughout', 'the', 'day', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['confidence', '+', 'methyl', '##ph', '##eni', '##date', 'anyone', 'else', 'feel', 'their', 'confidence', 'flu', '##ct', '##uate', 'greatly', 'throughout', 'the', 'day', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ina', '##tten', '##tive', ',', 'hyper', '##active', ',', 'and', 'mixed', ':', 'how', 'long', 'do', 'your', 'medications', 'need', 'to', 'take', 'effect', '?', 'the', 'more', 'responses', 'the', 'better', '.', 'i', \"'\", 'm', 'predominantly', 'ina', '##tten', '##tive', ',', 'and', 'my', 'v', '##y', '##van', '##se', 'starts', 'working', 'in', 'about', 'half', 'an', 'hour', ',', 'wears', 'off', 'in', 'six', '.', 'taking', 'it', 'for', 'several', 'days', 'in', 'a', 'row', 'has', 'no', 'additional', 'good', 'effects', '.', 'i', 'want', 'to', 'hear', 'from', 'those', 'diagnosed', 'with', 'hyper', '##active', '/', 'imp', '##ulsive', 'type', ',', 'as', 'well', 'as', 'other', 'ina', '##tten', '##tive', 'types', 'and', 'mixed', 'types', ':', '1', ')', 'medications', 'you', 'are', 'taking', ',', 'dos', '##age', ',', 'and', 'frequency', 'of', 'use', '?', '2', ')', 'how', 'long', 'do', 'your', 'medications', 'need', 'to', 'take', 'effect', '?', 'at', 'what', 'point', 'can', 'you', 'tell', 'they', 'are', 'working', '?', 'how', 'long', 'until', 'they', 'wear', 'off', '?', '3', ')', 'do', 'your', 'symptoms', 'reduce', 'more', 'if', 'you', 'take', 'them', 'consistently', 'over', 'several', 'days', '?', 'or', 'do', 'they', 'go', 'away', 'for', 'a', 'few', 'hours', 'and', 'then', 'return', 'immediately', '?', 'be', 'sure', 'to', 'specify', '*', 'which', '*', 'symptoms', '(', 'ina', '##tten', '##tive', 'or', 'imp', '##ulsive', '/', 'hyper', '##active', ')', 'reduce', 'when', '.', '4', ')', 'when', 'do', 'you', 'take', 'medications', ',', 'would', 'you', 'describe', 'the', 'experience', 'as', 'more', '~', '~', 'en', '##er', '##gi', '##zed', ',', 'or', 'calm', '?', 'more', 'stressed', ',', 'or', 'let', '##har', '##gic', '?', '~', '~', 'edit', ':', 'let', \"'\", 's', 'rep', '##hra', '##se', ',', 'that', 'was', 'too', 'vague', '.', '.', '.', 'does', 'it', 'make', 'you', 'feel', 'more', 'awake', '?', 'or', 'more', 'sleepy', '?', 'if', 'you', 'are', 'interested', ',', 'here', 'is', 'the', 'reason', 'i', 'am', 'asking', 'the', 'question', '.', 'no', 'need', 'to', 'worry', 'about', 'it', 'otherwise', ':', ')', 'please', 'answer', 'before', 'reading', 'so', 'as', 'not', 'to', 'bias', 'the', 'results', 'too', 'much', ',', 'you', 'can', 'edit', 'in', 'any', 'comments', 'about', 'the', 'text', 'below', 'after', '##words', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'there', 'is', 'some', 'controversy', 'surrounding', 'whether', 'people', 'with', 'ad', '##hd', 'are', 'hyper', '##do', '##pa', '##mine', '##rgen', '##ic', 'or', 'h', '##yp', '##od', '##opa', '##mine', '##rgen', '##ic', '.', 'hyper', '##do', '##pa', '##mine', '##rgen', '##ic', 'mice', 'seem', 'to', 'match', 'hyper', '##active', 'symptoms', 'and', 'h', '##yp', '##od', '##opa', '##mine', '##rgen', '##ic', 'mice', 'seem', 'to', 'be', 'ina', '##tten', '##tive', '.', 'paradox', '##ically', ',', 'both', 'types', 'of', 'mice', 'become', 'more', '\"', 'normal', '\"', 'under', 'st', '##im', '##ula', '##nt', 'medication', '.', '.', '.', 'i', 'think', 'the', 'effects', 'of', 'st', '##im', '##ula', '##nt', 'medication', 'work', 'by', 'different', 'mechanisms', 'for', 'various', 'symptoms', '.', 'i', 'suspect', 'st', '##im', '##ula', '##nts', 'control', 'hyper', '##act', '##ivity', 'by', 'affecting', 'receptor', 'up', '/', 'down', 'regulation', ',', 'while', 'controlling', 'ina', '##tten', '##tive', '##ness', 'by', 'effect', '##ing', 'do', '##pa', '##mine', 'concentration', 'at', 'the', 'syn', '##ap', '##se', '.', 'since', 'these', 'two', 'mechanisms', 'take', 'different', 'amounts', 'of', 'time', ',', 'i', 'want', 'to', 'know', 'how', 'long', 'it', 'takes', 'each', 'group', 'to', 'receive', 'benefits', 'from', 'medication', '.', 'i', 'am', 'asking', 'this', 'question', 'just', 'to', 'get', 'some', 'quick', 'and', 'dirty', 'an', '##ec', '##dot', '##al', 'data', 'to', 'see', 'if', 'people', \"'\", 's', 'experiences', 'line', 'up', 'with', 'my', 'predictions', ',', 'to', 'see', 'whether', 'this', 'line', 'of', 'questioning', 'is', 'worth', 'pursuing', '.', 'it', 'would', 'explain', 'some', 'of', 'the', 'paradox', '##ical', 'qualities', 'of', 'st', '##im', '##ula', '##nt', 'medication', 'on', 'hyper', '##act', '##ivity', '.', 'it', 'would', 'be', 'useful', 'too', ',', 'because', 'it', 'would', 'suggest', 'that', 'the', 'your', 'symptoms', 'determine', 'the', 'best', 'way', 'to', 'time', 'your', 'medications', '.', '\"', 'edit', ':', 'not', 'enough', 'responses', '.', 'i', 'has', 'a', 'sad', ':', \"'\", '(']\n",
      "INFO:__main__:Number of tokens: 577\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['ina', '##tten', '##tive', ',', 'hyper', '##active', ',', 'and', 'mixed', ':', 'how', 'long', 'do', 'your', 'medications', 'need', 'to', 'take', 'effect', '?', 'the', 'more', 'responses', 'the', 'better', '.', 'i', \"'\", 'm', 'predominantly', 'ina', '##tten', '##tive', ',', 'and', 'my', 'v', '##y', '##van', '##se', 'starts', 'working', 'in', 'about', 'half', 'an', 'hour', ',', 'wears', 'off', 'in', 'six', '.', 'taking', 'it', 'for', 'several', 'days', 'in', 'a', 'row', 'has', 'no', 'additional', 'good', 'effects', '.', 'i', 'want', 'to', 'hear', 'from', 'those', 'diagnosed', 'with', 'hyper', '##active', '/', 'imp', '##ulsive', 'type', ',', 'as', 'well', 'as', 'other', 'ina', '##tten', '##tive', 'types', 'and', 'mixed', 'types', ':', '1', ')', 'medications', 'you', 'are', 'taking', ',', 'dos', '##age', ',', 'and', 'frequency', 'of', 'use', '?', '2', ')', 'how', 'long', 'do', 'your', 'medications', 'need', 'to', 'take', 'effect', '?', 'at', 'what', 'point', 'can', 'you', 'tell', 'they', 'are', 'working', '?', 'how', 'long', 'until', 'they', 'wear', 'off', '?', '3', ')', 'do', 'your', 'symptoms', 'reduce', 'more', 'if', 'you', 'take', 'them', 'consistently', 'over', 'several', 'days', '?', 'or', 'do', 'they', 'go', 'away', 'for', 'a', 'few', 'hours', 'and', 'then', 'return', 'immediately', '?', 'be', 'sure', 'to', 'specify', '*', 'which', '*', 'symptoms', '(', 'ina', '##tten', '##tive', 'or', 'imp', '##ulsive', '/', 'hyper', '##active', ')', 'reduce', 'when', '.', '4', ')', 'when', 'do', 'you', 'take', 'medications', ',', 'would', 'you', 'describe', 'the', 'experience', 'as', 'more', '~', '~', 'en', '##er', '##gi', '##zed', ',', 'or', 'calm', '?', 'more', 'stressed', ',', 'or', 'let', '##har', '##gic', '?', '~', '~', 'edit', ':', 'let', \"'\", 's', 'rep', '##hra', '##se', ',', 'that', 'was', 'too', 'vague', '.', '.', '.', 'does', 'it', 'make', 'you', 'feel', 'more', 'awake', '?', 'or', 'more', 'sleepy', '?', 'if', 'you', 'are', 'interested', ',', 'here', 'is', 'the', 'reason', 'i', 'am', 'asking', 'the', 'question', '.', 'no', 'need', 'to', 'worry', 'about', 'it', 'otherwise', ':', ')', 'please', 'answer', 'before', 'reading', 'so', 'as', 'not', 'to', 'bias', 'the', 'results', 'too', 'much', ',', 'you', 'can', 'edit', 'in', 'any', 'comments', 'about', 'the', 'text', 'below', 'after', '##words', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'there', 'is', 'some', 'controversy', 'surrounding', 'whether', 'people', 'with', 'ad', '##hd', 'are', 'hyper', '##do', '##pa', '##mine', '##rgen', '##ic', 'or', 'h', '##yp', '##od', '##opa', '##mine', '##rgen', '##ic', '.', 'hyper', '##do', '##pa', '##mine', '##rgen', '##ic', 'mice', 'seem', 'to', 'match', 'hyper', '##active', 'symptoms', 'and', 'h', '##yp', '##od', '##opa', '##mine', '##rgen', '##ic', 'mice', 'seem', 'to', 'be', 'ina', '##tten', '##tive', '.', 'paradox', '##ically', ',', 'both', 'types', 'of', 'mice', 'become', 'more', '\"', 'normal', '\"', 'under', 'st', '##im', '##ula', '##nt', 'medication', '.', '.', '.', 'i', 'think', 'the', 'effects', 'of', 'st', '##im', '##ula', '##nt', 'medication', 'work', 'by', 'different', 'mechanisms', 'for', 'various', 'symptoms', '.', 'i', 'suspect', 'st', '##im', '##ula', '##nts', 'control', 'hyper', '##act', '##ivity', 'by', 'affecting', 'receptor', 'up', '/', 'down', 'regulation', ',', 'while', 'controlling', 'ina', '##tten', '##tive', '##ness', 'by', 'effect', '##ing', 'do', '##pa', '##mine', 'concentration', 'at', 'the', 'syn', '##ap', '##se', '.', 'since', 'these', 'two', 'mechanisms', 'take', 'different', 'amounts', 'of', 'time', ',', 'i', 'want', 'to', 'know', 'how', 'long', 'it', 'takes', 'each', 'group', 'to', 'receive', 'benefits', 'from', 'medication', '.', 'i', 'am', 'asking', 'this', 'question', 'just', 'to', 'get', 'some', 'quick', 'and', 'dirty', 'an', '##ec', '##dot', '##al', 'data', 'to', 'see', 'if', 'people', \"'\", 's', 'experiences', 'line', 'up', 'with', 'my', 'predictions', ',', 'to', 'see', 'whether'], ['this', 'line', 'of', 'questioning', 'is', 'worth', 'pursuing', '.', 'it', 'would', 'explain', 'some', 'of', 'the', 'paradox', '##ical', 'qualities', 'of', 'st', '##im', '##ula', '##nt', 'medication', 'on', 'hyper', '##act', '##ivity', '.', 'it', 'would', 'be', 'useful', 'too', ',', 'because', 'it', 'would', 'suggest', 'that', 'the', 'your', 'symptoms', 'determine', 'the', 'best', 'way', 'to', 'time', 'your', 'medications', '.', '\"', 'edit', ':', 'not', 'enough', 'responses', '.', 'i', 'has', 'a', 'sad', ':', \"'\", '(']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['group', 'assignments', 'in', 'college', 'do', 'people', 'here', 'hate', 'this', 'as', 'much', 'as', 'i', 'do', '?', 'i', \"'\", 'm', 'always', 'afraid', 'to', 'approach', 'people', 'for', 'the', 'same', 'reasons', 'i', \"'\", 'm', 'afraid', 'to', 'apply', 'for', 'jobs', ';', 'i', \"'\", 'm', 'scared', 'that', 'i', \"'\", 'll', 'mess', 'things', 'up', 'and', 'i', 'won', \"'\", 't', 'just', 'be', 'doing', 'damage', 'to', 'myself', 'but', 'to', 'other', 'people', '.', 'also', ',', 'i', 'don', \"'\", 't', 'seem', 'to', 'be', 'able', 'to', 'connect', 'in', 'a', 'way', 'that', 'takes', 'advantage', 'of', 'increased', 'manpower', '.', 'i', 'just', 'end', 'up', 'doing', 'everything', 'or', 'hardly', 'anything', 'at', 'all', '.']\n",
      "INFO:__main__:Number of tokens: 97\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['group', 'assignments', 'in', 'college', 'do', 'people', 'here', 'hate', 'this', 'as', 'much', 'as', 'i', 'do', '?', 'i', \"'\", 'm', 'always', 'afraid', 'to', 'approach', 'people', 'for', 'the', 'same', 'reasons', 'i', \"'\", 'm', 'afraid', 'to', 'apply', 'for', 'jobs', ';', 'i', \"'\", 'm', 'scared', 'that', 'i', \"'\", 'll', 'mess', 'things', 'up', 'and', 'i', 'won', \"'\", 't', 'just', 'be', 'doing', 'damage', 'to', 'myself', 'but', 'to', 'other', 'people', '.', 'also', ',', 'i', 'don', \"'\", 't', 'seem', 'to', 'be', 'able', 'to', 'connect', 'in', 'a', 'way', 'that', 'takes', 'advantage', 'of', 'increased', 'manpower', '.', 'i', 'just', 'end', 'up', 'doing', 'everything', 'or', 'hardly', 'anything', 'at', 'all', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'caf', '##fe', '##ine', 'a', 'good', 'alternative', 'st', '##im', '##ula', '##nt', 'when', 'taking', 'breaks', 'from', 'rita', '##lin', '/', 'add', '##eral', '##l', '?', 'and', 'how', 'long', 'should', 'medication', 'holidays', 'take', '?']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'caf', '##fe', '##ine', 'a', 'good', 'alternative', 'st', '##im', '##ula', '##nt', 'when', 'taking', 'breaks', 'from', 'rita', '##lin', '/', 'add', '##eral', '##l', '?', 'and', 'how', 'long', 'should', 'medication', 'holidays', 'take', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['if', 'd', '##1', 'do', '##pa', '##mine', 'receptors', 'stimulate', 'aden', '##yla', '##te', 'cy', '##cl', '##ase', '(', 'through', 'the', 'g', '-', 'protein', '-', 'coupled', 'receptor', 'process', ')', 'and', 'd', '##2', 'do', '##pa', '##mine', 'receptors', 'inhibit', 'it', ',', 'then', 'why', 'do', 'mutations', 'in', 'these', 'receptors', 'seem', 'to', 'produce', 'somewhat', 'similar', '\"', 'negative', '\"', 'effects', '?']\n",
      "INFO:__main__:Number of tokens: 52\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['if', 'd', '##1', 'do', '##pa', '##mine', 'receptors', 'stimulate', 'aden', '##yla', '##te', 'cy', '##cl', '##ase', '(', 'through', 'the', 'g', '-', 'protein', '-', 'coupled', 'receptor', 'process', ')', 'and', 'd', '##2', 'do', '##pa', '##mine', 'receptors', 'inhibit', 'it', ',', 'then', 'why', 'do', 'mutations', 'in', 'these', 'receptors', 'seem', 'to', 'produce', 'somewhat', 'similar', '\"', 'negative', '\"', 'effects', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['inc', '##omp', '##ete', '##nt', 'treat', '##ers', 'i', \"'\", 've', 'just', 'had', 'a', 'very', 'bad', 'experience', 'with', 'my', 'new', 'therapist', 'and', 'new', 'ps', '##ych', 'nurse', '.', 'i', 'have', 'a', 'prior', 'diagnosis', 'of', 'ad', '##hd', '-', 'pi', ',', 'but', 'have', 'let', 'it', 'sit', 'on', 'a', 'back', 'burn', '##er', 'after', 'drugs', 'weren', \"'\", 't', 'terrific', '##ally', 'effective', 'and', 'i', 'dealt', 'with', 'my', 'bipolar', '2', '.', 'now', 'my', 'mood', 'has', 'been', 'stable', 'for', 'several', 'months', 'and', 'my', 'brain', 'still', 'isn', \"'\", 't', 'cooper', '##ating', 'with', 'reality', ',', 'so', 'i', \"'\", 'd', 'like', 'to', 'rev', '##isi', '##t', 'ad', '##hd', 'and', 'try', 'to', 'get', 'it', 'treated', '.', 'though', 'my', 'therapist', 'was', 'supportive', 'of', 'getting', 'me', 'treatment', ',', 'she', 'gave', 'me', 'flat', 'out', 'incorrect', 'information', ':', '*', '*', '\"', 'the', 'good', 'news', 'is', 'that', 'ad', '##hd', 'gets', 'better', 'as', 'you', 'get', 'older', '.', '\"', '*', '*', 'nothing', 'i', \"'\", 've', 'read', 'indicates', 'that', 'it', 'gets', 'better', '.', 'the', 'physical', 'hyper', '##act', '##ivity', 'might', 'decline', ',', 'but', 'the', 'ina', '##tten', '##tive', '##ness', 'and', 'imp', '##uls', '##ivity', 'remain', '.', 'my', 'ps', '##ych', 'nurse', 'was', 'a', 'harder', 'sell', '.', 'he', 'attributes', 'any', 'ina', '##tten', '##tive', '##ness', '/', 'fl', '##aki', '##ness', 'i', 'have', 'to', 'depression', ',', 'even', 'though', 'i', 'tell', 'him', 'flat', 'out', 'tell', 'him', 'i', \"'\", 've', 'lost', 'interest', 'in', 'about', '50', 'different', 'activities', 'throughout', 'my', 'life', 'regardless', 'of', 'mood', '.', 'i', 'mean', ',', 'if', 'it', 'was', 'just', 'depression', ',', 'i', \"'\", 'd', 'think', 'i', \"'\", 'd', 'go', 'back', 'to', 'the', 'same', 'things', 'when', 'i', 'get', 'better', '.', 'i', 'don', \"'\", 't', '.', 'i', 'move', 'onto', 'something', 'else', '.', 'then', 'i', 'quit', ',', 'sometimes', 'co', '-', 'occurring', 'with', 'a', 'de', '##pressive', 'episode', ',', 'sometimes', 'not', ',', 'never', 'to', 'return', 'to', 'again', '.', 'so', 'while', 'i', \"'\", 'm', 'trying', 'to', 'convince', 'him', 'that', 'the', 'ad', '##hd', 'is', 'responsible', 'for', 'my', 'lack', 'of', 'motivation', 'to', 'pursue', 'consistent', 'activities', ',', 'he', 'responds', 'with', 'the', 'gem', '.', '*', '*', '\"', 'ad', '##hd', 'does', 'not', 'affect', 'motivation', '.', '\"', '*', '*', 'he', 'has', 'reluctantly', 'put', 'me', 'on', 'st', '##rat', '##tera', ',', 'but', 'i', 'feel', 'like', 'i', \"'\", 'm', 'going', 'to', 'be', 'fighting', 'with', 'him', 'about', 'whether', 'i', 'have', 'ad', '##hd', ';', 'and', 'if', 'he', 'doesn', \"'\", 't', 'think', 'ad', '##hd', 'effects', 'motivation', ',', 'i', 'don', \"'\", 't', 'really', 'see', 'how', 'he', 'is', 'competent', 'to', 'judge', 'whether', 'the', 'drug', 'is', 'working', '.', 'so', 'question', 'for', 'ra', '##dd', '##it', ':', 'have', 'you', 'dealt', 'with', 'inc', '##omp', '##ete', '##nt', 'providers', 'before', '?', 'how', 'have', 'you', 'dealt', 'with', 'it', '?', 'are', 'the', 'open', 'to', 'educating', 'themselves', '?', 'if', 'you', 'can', 'brow', '##beat', 'them', 'into', 'medical', 'treatment', ',', 'do', 'they', 'do', 'an', 'ok', 'job', 'of', 'pre', '##sc', '##ri', '##bing', ',', 'ti', '##tra', '##ting', 'and', 'monitoring', '?', '*', '*', 't', '##ld', '##r', ':', '*', '*', 'my', 'providers', 'have', 'plain', 'wrong', 'information', 'about', 'ad', '##hd', '.', 'what', 'to', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 468\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['inc', '##omp', '##ete', '##nt', 'treat', '##ers', 'i', \"'\", 've', 'just', 'had', 'a', 'very', 'bad', 'experience', 'with', 'my', 'new', 'therapist', 'and', 'new', 'ps', '##ych', 'nurse', '.', 'i', 'have', 'a', 'prior', 'diagnosis', 'of', 'ad', '##hd', '-', 'pi', ',', 'but', 'have', 'let', 'it', 'sit', 'on', 'a', 'back', 'burn', '##er', 'after', 'drugs', 'weren', \"'\", 't', 'terrific', '##ally', 'effective', 'and', 'i', 'dealt', 'with', 'my', 'bipolar', '2', '.', 'now', 'my', 'mood', 'has', 'been', 'stable', 'for', 'several', 'months', 'and', 'my', 'brain', 'still', 'isn', \"'\", 't', 'cooper', '##ating', 'with', 'reality', ',', 'so', 'i', \"'\", 'd', 'like', 'to', 'rev', '##isi', '##t', 'ad', '##hd', 'and', 'try', 'to', 'get', 'it', 'treated', '.', 'though', 'my', 'therapist', 'was', 'supportive', 'of', 'getting', 'me', 'treatment', ',', 'she', 'gave', 'me', 'flat', 'out', 'incorrect', 'information', ':', '*', '*', '\"', 'the', 'good', 'news', 'is', 'that', 'ad', '##hd', 'gets', 'better', 'as', 'you', 'get', 'older', '.', '\"', '*', '*', 'nothing', 'i', \"'\", 've', 'read', 'indicates', 'that', 'it', 'gets', 'better', '.', 'the', 'physical', 'hyper', '##act', '##ivity', 'might', 'decline', ',', 'but', 'the', 'ina', '##tten', '##tive', '##ness', 'and', 'imp', '##uls', '##ivity', 'remain', '.', 'my', 'ps', '##ych', 'nurse', 'was', 'a', 'harder', 'sell', '.', 'he', 'attributes', 'any', 'ina', '##tten', '##tive', '##ness', '/', 'fl', '##aki', '##ness', 'i', 'have', 'to', 'depression', ',', 'even', 'though', 'i', 'tell', 'him', 'flat', 'out', 'tell', 'him', 'i', \"'\", 've', 'lost', 'interest', 'in', 'about', '50', 'different', 'activities', 'throughout', 'my', 'life', 'regardless', 'of', 'mood', '.', 'i', 'mean', ',', 'if', 'it', 'was', 'just', 'depression', ',', 'i', \"'\", 'd', 'think', 'i', \"'\", 'd', 'go', 'back', 'to', 'the', 'same', 'things', 'when', 'i', 'get', 'better', '.', 'i', 'don', \"'\", 't', '.', 'i', 'move', 'onto', 'something', 'else', '.', 'then', 'i', 'quit', ',', 'sometimes', 'co', '-', 'occurring', 'with', 'a', 'de', '##pressive', 'episode', ',', 'sometimes', 'not', ',', 'never', 'to', 'return', 'to', 'again', '.', 'so', 'while', 'i', \"'\", 'm', 'trying', 'to', 'convince', 'him', 'that', 'the', 'ad', '##hd', 'is', 'responsible', 'for', 'my', 'lack', 'of', 'motivation', 'to', 'pursue', 'consistent', 'activities', ',', 'he', 'responds', 'with', 'the', 'gem', '.', '*', '*', '\"', 'ad', '##hd', 'does', 'not', 'affect', 'motivation', '.', '\"', '*', '*', 'he', 'has', 'reluctantly', 'put', 'me', 'on', 'st', '##rat', '##tera', ',', 'but', 'i', 'feel', 'like', 'i', \"'\", 'm', 'going', 'to', 'be', 'fighting', 'with', 'him', 'about', 'whether', 'i', 'have', 'ad', '##hd', ';', 'and', 'if', 'he', 'doesn', \"'\", 't', 'think', 'ad', '##hd', 'effects', 'motivation', ',', 'i', 'don', \"'\", 't', 'really', 'see', 'how', 'he', 'is', 'competent', 'to', 'judge', 'whether', 'the', 'drug', 'is', 'working', '.', 'so', 'question', 'for', 'ra', '##dd', '##it', ':', 'have', 'you', 'dealt', 'with', 'inc', '##omp', '##ete', '##nt', 'providers', 'before', '?', 'how', 'have', 'you', 'dealt', 'with', 'it', '?', 'are', 'the', 'open', 'to', 'educating', 'themselves', '?', 'if', 'you', 'can', 'brow', '##beat', 'them', 'into', 'medical', 'treatment', ',', 'do', 'they', 'do', 'an', 'ok', 'job', 'of', 'pre', '##sc', '##ri', '##bing', ',', 'ti', '##tra', '##ting', 'and', 'monitoring', '?', '*', '*', 't', '##ld', '##r', ':', '*', '*', 'my', 'providers', 'have', 'plain', 'wrong', 'information', 'about', 'ad', '##hd', '.', 'what', 'to', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', '’', 's', 'the', 'current', 'science', 'of', 'using', 'nico', '##tine', 'in', 'treatment', 'of', 'ad', '##hd', '?', '[', 'via', '/', 'r', '/', 'asks', '##cie', '##nce', ']']\n",
      "INFO:__main__:Number of tokens: 25\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', '’', 's', 'the', 'current', 'science', 'of', 'using', 'nico', '##tine', 'in', 'treatment', 'of', 'ad', '##hd', '?', '[', 'via', '/', 'r', '/', 'asks', '##cie', '##nce', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'sex', 'how', 'do', 'you', 'guys', 'feel', 'ad', '##hd', 'affects', 'your', 'sex', 'life', 'when', 'either', 'med', '##icated', 'or', 'not', '(', 'please', 'specify', 'and', 'if', 'med', '##icated', 'how', 'it', 'may', 'have', 'changed', 'from', 'before', 'med', '##s', ')', '.']\n",
      "INFO:__main__:Number of tokens: 39\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'sex', 'how', 'do', 'you', 'guys', 'feel', 'ad', '##hd', 'affects', 'your', 'sex', 'life', 'when', 'either', 'med', '##icated', 'or', 'not', '(', 'please', 'specify', 'and', 'if', 'med', '##icated', 'how', 'it', 'may', 'have', 'changed', 'from', 'before', 'med', '##s', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', '’', 's', 'the', 'current', 'science', 'of', 'using', 'nico', '##tine', 'in', 'treatment', 'of', 'ad', '##hd', '?', '[', 'via', '/', 'r', '/', 'asks', '##cie', '##nce', ']']\n",
      "INFO:__main__:Number of tokens: 25\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', '’', 's', 'the', 'current', 'science', 'of', 'using', 'nico', '##tine', 'in', 'treatment', 'of', 'ad', '##hd', '?', '[', 'via', '/', 'r', '/', 'asks', '##cie', '##nce', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['nico', '##tine', ':', 'it', 'may', 'have', 'a', 'good', 'side', '(', '2005', ',', 'harvard', 'health', 'publications', ')']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['nico', '##tine', ':', 'it', 'may', 'have', 'a', 'good', 'side', '(', '2005', ',', 'harvard', 'health', 'publications', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['nico', '##tine', ':', 'it', 'may', 'have', 'a', 'good', 'side', '(', '2006', ',', 'harvard', 'health', 'publications', ')']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['nico', '##tine', ':', 'it', 'may', 'have', 'a', 'good', 'side', '(', '2006', ',', 'harvard', 'health', 'publications', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'it', 'possible', 'that', 'add', 'med', '##s', '\"', 'stunt', '\"', 'growth', 'in', 'children', 'simply', 'because', 'they', 'induce', 'partial', 'appetite', 'loss', 'in', 'many', '?']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'it', 'possible', 'that', 'add', 'med', '##s', '\"', 'stunt', '\"', 'growth', 'in', 'children', 'simply', 'because', 'they', 'induce', 'partial', 'appetite', 'loss', 'in', 'many', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'people', 'with', 'ad', '##hd', 'have', 'a', 'tendency', 'to', 'over', '##cor', '##re', '##ct', 'for', 'everything', '?', 'like', ',', 'going', 'too', 'high', 'then', 'too', 'low', ',', 'then', 'too', 'high', 'again', ',', 'and', 'then', 'too', 'low', 'again', '?', '=', '=', 'and', 'damn', 'it', ',', 'i', 'think', 'i', 'fl', '##oss', '##ed', 'my', 'gum', '##s', 'so', 'hard', 'that', 'they', 'bleed', 'now', '.', '=', '/']\n",
      "INFO:__main__:Number of tokens: 60\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'people', 'with', 'ad', '##hd', 'have', 'a', 'tendency', 'to', 'over', '##cor', '##re', '##ct', 'for', 'everything', '?', 'like', ',', 'going', 'too', 'high', 'then', 'too', 'low', ',', 'then', 'too', 'high', 'again', ',', 'and', 'then', 'too', 'low', 'again', '?', '=', '=', 'and', 'damn', 'it', ',', 'i', 'think', 'i', 'fl', '##oss', '##ed', 'my', 'gum', '##s', 'so', 'hard', 'that', 'they', 'bleed', 'now', '.', '=', '/']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['til', 'see', 'the', 'beauty', '-', '-', 'add', 'tip', 'o', 'the', 'day', '208']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['til', 'see', 'the', 'beauty', '-', '-', 'add', 'tip', 'o', 'the', 'day', '208']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['weights', 'or', 'card', '##io', 'so', 'i', 'have', 'a', 'hard', 'time', 'bulk', '##ing', 'up', 'but', 'i', 'love', 'the', 'card', '##io', 'because', 'of', 'my', 'add', '.', '.', 'so', 'if', 'i', 'cut', 'the', 'card', '##io', 'out', 'and', 'just', 'focused', 'on', 'rapid', 'weight', 'training', 'will', 'that', 'do', 'the', 'same', 'thing', 'for', 'me', '(', 'mentally', ')', 'or', 'maybe', 'just', 'went', 'down', 'to', '10', 'minutes', 'of', 'sprint', '##ing', '.']\n",
      "INFO:__main__:Number of tokens: 63\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['weights', 'or', 'card', '##io', 'so', 'i', 'have', 'a', 'hard', 'time', 'bulk', '##ing', 'up', 'but', 'i', 'love', 'the', 'card', '##io', 'because', 'of', 'my', 'add', '.', '.', 'so', 'if', 'i', 'cut', 'the', 'card', '##io', 'out', 'and', 'just', 'focused', 'on', 'rapid', 'weight', 'training', 'will', 'that', 'do', 'the', 'same', 'thing', 'for', 'me', '(', 'mentally', ')', 'or', 'maybe', 'just', 'went', 'down', 'to', '10', 'minutes', 'of', 'sprint', '##ing', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['executive', 'function', 'disorder', '-', 'coping', 'methods', '?', 'this', 'submission', 'isn', \"'\", 't', 'strictly', 'about', 'ad', '##hd', 'but', 'please', 'bear', 'with', 'me', ',', 'this', 'is', 'the', 'closest', 'sub', '##red', '##dit', 'i', 'could', 'find', '.', 'more', 'than', 'a', 'decade', 'ago', 'when', 'i', 'was', 'a', 'junior', 'in', 'high', 'school', 'i', 'was', 'tested', 'for', 'learning', 'disabilities', 'at', 'my', 'own', 'request', '.', 'i', 'knew', 'something', 'wasn', \"'\", 't', 'quite', 'right', ',', 'thinking', 'it', 'might', 'be', 'ad', '##hd', '.', 'to', 'my', 'parents', 'surprise', '(', 'and', 'my', 'dismay', ')', 'the', 'eva', '##lu', '##ators', 'came', 'back', 'with', 'a', 'diagnosis', ':', '\"', 'some', 'kind', 'of', 'executive', 'function', 'disorder', '.', '\"', 'they', 'didn', \"'\", 't', 'have', 'too', 'much', 'information', 'for', 'me', 'on', 'what', 'it', 'was', 'and', 'what', 'causes', 'it', '.', 'just', 'that', 'i', 'would', 'qualify', 'for', 'special', 'considerations', 'during', 'exams', 'and', 'get', 'extra', 'study', 'periods', '.', 'fast', 'forward', 'to', 'present', 'day', ';', 'i', 'have', 'been', 'adequately', 'coping', 'thus', 'far', 'in', 'life', ',', 'i', 'have', 'a', 'college', 'degree', ',', 'a', 'good', 'job', 'and', 'a', 'family', 'but', 'every', 'day', 'i', 'find', 'myself', 'reminded', 'of', 'this', 'problem', '.', 'examples', 'include', ':', '*', 'the', 'inability', 'to', 'add', 'simple', 'numbers', 'confidently', 'without', 'counting', 'them', 'out', 'in', 'my', 'head', '(', 'e', '.', 'g', '.', 'adding', 'a', '$', '13', 'tip', 'to', 'a', '$', '53', 'meal', ')', '*', 'difficulty', 'recalling', 'adjacent', 'letters', 'in', 'the', 'alphabet', '(', 'e', '.', 'g', '.', 'i', 'couldn', \"'\", 't', 'tell', 'you', 'what', 'comes', 'before', 'or', 'after', 'h', 'without', 'singing', 'the', 'damn', 'alphabet', 'song', 'in', 'my', 'head', ')', '*', 'difficulty', 'remembering', 'and', 'keeping', 'schedules', '(', 'thank', 'goodness', 'for', 'smart', 'phones', ')', '*', 'the', 'near', 'complete', 'inability', 'to', 'read', 'music', 'and', 'play', 'the', 'correct', 'sequences', 'of', 'notes', '.', 'i', 'tried', 'learning', 'the', 'piano', 'for', 'several', 'years', 'when', 'i', 'was', 'younger', ',', 'it', 'led', 'to', 'ind', '##es', '##cr', '##iba', '##ble', 'frustration', 'as', 'i', 'watched', 'my', 'peers', 'leave', 'me', 'in', 'the', 'dust', '.', '*', 'i', 'often', 'leave', 'letters', 'off', 'the', 'end', 'of', 'words', 'and', 'sometimes', 'add', 'them', 'to', 'the', 'next', 'word', '(', 'e', '.', 'g', '.', 'the', 'boy', 'bo', '##ugh', 'the', 'basketball', ')', '*', 'i', 'cannot', 'remember', 'sequences', 'of', 'letters', 'and', 'numbers', 'more', 'than', 'a', 'few', 'characters', 'long', '.', 'recalling', 'even', 'short', 'sequences', 'usually', 'results', 'in', 'trans', '##posed', 'characters', '.', 'these', 'things', 'usually', 'range', 'from', 'mildly', 'annoying', 'to', 'embarrassing', '.', 'it', 'doesn', \"'\", 't', 'often', 'affect', 'my', 'work', 'or', 'personal', 'life', 'in', 'a', 'serious', 'way', 'but', 'there', 'are', 'times', 'when', 'i', 'make', 'a', 'fool', 'of', 'myself', 'with', 'simple', 'mistakes', '.', 'it', 'gets', 'much', 'worse', 'under', 'stress', '##ful', 'or', 'time', 'critical', 'situations', '.', 'i', 'read', 'that', 'many', 'with', 'ad', '##hd', 'have', 'symptoms', 'of', 'executive', 'function', 'disorder', 'which', 'is', 'why', 'i', 'came', 'here', '.', 'i', 'haven', \"'\", 't', 'knowing', '##ly', 'met', 'anyone', 'else', 'with', 'the', 'same', 'problem', '.', 'even', 'my', 'wife', 'doesn', \"'\", 't', 'really', 'understand', 'my', 'day', 'to', 'day', 'struggle', '(', 'however', 'minor', 'it', 'may', 'be', ')', ',', 'nor', 'do', 'i', 'really', 'want', 'to', 'draw', 'attention', 'to', 'it', '.', 'unfortunately', 'it', 'would', 'seem', 'there', 'isn', \"'\", 't', 'a', 'drug', 'i', 'can', 'take', 'that', 'will', 'help', '.', 'focusing', 'usually', 'isn', \"'\", 't', 'a', 'problem', ',', 'the', 'way', 'my', 'brain', 'processes', 'sequences', 'of', 'things', 'is', '.', 'i', 'know', 'i', \"'\", 'm', 'a', 'pretty', 'smart', 'person', 'but', 'i', 'constantly', 'feel', 'like', 'i', \"'\", 'm', 'being', 'held', 'back', '.', 'e', '##f', '##d', 'suffer', '##ers', ',', 'what', 'are', 'your', 'coping', 'methods', '?', 'is', 'there', 'anything', 'you', 'do', 'or', 'take', 'that', 'makes', 'it', 'better', '?']\n",
      "INFO:__main__:Number of tokens: 561\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['executive', 'function', 'disorder', '-', 'coping', 'methods', '?', 'this', 'submission', 'isn', \"'\", 't', 'strictly', 'about', 'ad', '##hd', 'but', 'please', 'bear', 'with', 'me', ',', 'this', 'is', 'the', 'closest', 'sub', '##red', '##dit', 'i', 'could', 'find', '.', 'more', 'than', 'a', 'decade', 'ago', 'when', 'i', 'was', 'a', 'junior', 'in', 'high', 'school', 'i', 'was', 'tested', 'for', 'learning', 'disabilities', 'at', 'my', 'own', 'request', '.', 'i', 'knew', 'something', 'wasn', \"'\", 't', 'quite', 'right', ',', 'thinking', 'it', 'might', 'be', 'ad', '##hd', '.', 'to', 'my', 'parents', 'surprise', '(', 'and', 'my', 'dismay', ')', 'the', 'eva', '##lu', '##ators', 'came', 'back', 'with', 'a', 'diagnosis', ':', '\"', 'some', 'kind', 'of', 'executive', 'function', 'disorder', '.', '\"', 'they', 'didn', \"'\", 't', 'have', 'too', 'much', 'information', 'for', 'me', 'on', 'what', 'it', 'was', 'and', 'what', 'causes', 'it', '.', 'just', 'that', 'i', 'would', 'qualify', 'for', 'special', 'considerations', 'during', 'exams', 'and', 'get', 'extra', 'study', 'periods', '.', 'fast', 'forward', 'to', 'present', 'day', ';', 'i', 'have', 'been', 'adequately', 'coping', 'thus', 'far', 'in', 'life', ',', 'i', 'have', 'a', 'college', 'degree', ',', 'a', 'good', 'job', 'and', 'a', 'family', 'but', 'every', 'day', 'i', 'find', 'myself', 'reminded', 'of', 'this', 'problem', '.', 'examples', 'include', ':', '*', 'the', 'inability', 'to', 'add', 'simple', 'numbers', 'confidently', 'without', 'counting', 'them', 'out', 'in', 'my', 'head', '(', 'e', '.', 'g', '.', 'adding', 'a', '$', '13', 'tip', 'to', 'a', '$', '53', 'meal', ')', '*', 'difficulty', 'recalling', 'adjacent', 'letters', 'in', 'the', 'alphabet', '(', 'e', '.', 'g', '.', 'i', 'couldn', \"'\", 't', 'tell', 'you', 'what', 'comes', 'before', 'or', 'after', 'h', 'without', 'singing', 'the', 'damn', 'alphabet', 'song', 'in', 'my', 'head', ')', '*', 'difficulty', 'remembering', 'and', 'keeping', 'schedules', '(', 'thank', 'goodness', 'for', 'smart', 'phones', ')', '*', 'the', 'near', 'complete', 'inability', 'to', 'read', 'music', 'and', 'play', 'the', 'correct', 'sequences', 'of', 'notes', '.', 'i', 'tried', 'learning', 'the', 'piano', 'for', 'several', 'years', 'when', 'i', 'was', 'younger', ',', 'it', 'led', 'to', 'ind', '##es', '##cr', '##iba', '##ble', 'frustration', 'as', 'i', 'watched', 'my', 'peers', 'leave', 'me', 'in', 'the', 'dust', '.', '*', 'i', 'often', 'leave', 'letters', 'off', 'the', 'end', 'of', 'words', 'and', 'sometimes', 'add', 'them', 'to', 'the', 'next', 'word', '(', 'e', '.', 'g', '.', 'the', 'boy', 'bo', '##ugh', 'the', 'basketball', ')', '*', 'i', 'cannot', 'remember', 'sequences', 'of', 'letters', 'and', 'numbers', 'more', 'than', 'a', 'few', 'characters', 'long', '.', 'recalling', 'even', 'short', 'sequences', 'usually', 'results', 'in', 'trans', '##posed', 'characters', '.', 'these', 'things', 'usually', 'range', 'from', 'mildly', 'annoying', 'to', 'embarrassing', '.', 'it', 'doesn', \"'\", 't', 'often', 'affect', 'my', 'work', 'or', 'personal', 'life', 'in', 'a', 'serious', 'way', 'but', 'there', 'are', 'times', 'when', 'i', 'make', 'a', 'fool', 'of', 'myself', 'with', 'simple', 'mistakes', '.', 'it', 'gets', 'much', 'worse', 'under', 'stress', '##ful', 'or', 'time', 'critical', 'situations', '.', 'i', 'read', 'that', 'many', 'with', 'ad', '##hd', 'have', 'symptoms', 'of', 'executive', 'function', 'disorder', 'which', 'is', 'why', 'i', 'came', 'here', '.', 'i', 'haven', \"'\", 't', 'knowing', '##ly', 'met', 'anyone', 'else', 'with', 'the', 'same', 'problem', '.', 'even', 'my', 'wife', 'doesn', \"'\", 't', 'really', 'understand', 'my', 'day', 'to', 'day', 'struggle', '(', 'however', 'minor', 'it', 'may', 'be', ')', ',', 'nor', 'do', 'i', 'really', 'want', 'to', 'draw', 'attention', 'to', 'it', '.', 'unfortunately', 'it', 'would', 'seem', 'there', 'isn', \"'\", 't', 'a', 'drug', 'i', 'can', 'take', 'that', 'will', 'help', '.', 'focusing', 'usually', 'isn', \"'\", 't', 'a', 'problem', ',', 'the', 'way', 'my', 'brain', 'processes', 'sequences'], ['of', 'things', 'is', '.', 'i', 'know', 'i', \"'\", 'm', 'a', 'pretty', 'smart', 'person', 'but', 'i', 'constantly', 'feel', 'like', 'i', \"'\", 'm', 'being', 'held', 'back', '.', 'e', '##f', '##d', 'suffer', '##ers', ',', 'what', 'are', 'your', 'coping', 'methods', '?', 'is', 'there', 'anything', 'you', 'do', 'or', 'take', 'that', 'makes', 'it', 'better', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['meta', '-', 'post', ':', 'losing', 'interest', 'in', 'ad', '##hd', 'i', 'visit', 'ad', '##hd', 'as', 'a', 'diagnosis', 'and', 'source', 'of', 'my', 'difficulties', 'every', 'few', 'years', '.', 'the', 'first', 'time', 'i', 'got', 'a', 'coach', ';', 'my', 'psychiatrist', 'wasn', \"'\", 't', 'on', 'board', 'with', 'a', 'diagnosis', ',', 'so', 'i', 'received', 'no', 'medication', '.', 'it', 'was', 'a', 'wasted', 'effort', '.', 'the', 'second', 'time', ',', 'i', 'received', 'treatment', 'briefly', ',', 'but', 'two', 'medication', 'trials', 'didn', \"'\", 't', 'make', 'a', 'significant', 'impact', 'in', 'my', 'life', ',', 'so', 'i', 'stopped', 'taking', 'them', 'and', 'didn', \"'\", 't', 'visit', 'the', 'issue', 'for', 'about', 'a', 'year', '.', 'until', 'about', 'two', 'weeks', 'ago', 'when', 'dia', '##gno', '##sing', 'and', 'treating', 'ad', '##hd', 'became', 'the', 'single', 'most', 'important', 'thing', 'in', 'my', 'life', '.', 'today', 'i', \"'\", 've', 'spent', 'hours', 'on', 'this', 'red', '##dit', 'commenting', 'on', 'ad', '##hd', 'and', 'reading', 'materials', 'on', 'it', '.', 'i', \"'\", 'm', 'per', '##ser', '##vera', '##ting', 'on', 'it', '-', '-', 'the', 'same', 'way', 'i', \"'\", 'd', 'per', '##ser', '##vera', '##te', 'on', 'playing', 'guitar', ',', 'clarinet', ',', 'ji', '##uj', '##its', '##u', ',', 'video', '##games', ',', 'carp', '##ent', '##ry', ',', 'clarinet', ',', 'etc', '.', 'i', \"'\", 'm', 'seeing', 'a', 'ps', '##ych', 'nurse', 'next', 'week', '.', 'but', 'i', \"'\", 'm', 'concerned', ',', 'in', 'this', 'moment', 'of', 'luc', '##idi', '##ty', ',', 'that', 'the', 'next', 'treatment', 'will', 'be', 'ineffective', 'in', 'the', 'short', 'term', 'and', 'that', 'i', 'will', 'lose', 'interest', 'in', 'treating', 'my', 'ad', '##hd', 'as', 'i', \"'\", 've', 'lost', 'interest', 'in', 'every', 'project', 'i', \"'\", 've', 'ever', 'undertaken', '.', 'my', 'career', 'is', 'already', 'over', '-', '-', 'i', \"'\", 'm', 'on', 'disability', '-', '-', 'and', 'that', 'was', 'my', 'biggest', 'motivation', 'throughout', 'my', 'life', 'for', 'treatment', '(', 'which', 'was', 'mostly', 'spent', 'missing', 'the', 'diagnosis', 'of', 'bipolar', 'ii', ')', '.', 't', '##ld', '##r', ';', 'does', 'ad', '##hd', 'make', 'itself', 'more', 'difficult', 'to', 'treat', 'because', 'you', 'lose', 'interest', 'in', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 303\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['meta', '-', 'post', ':', 'losing', 'interest', 'in', 'ad', '##hd', 'i', 'visit', 'ad', '##hd', 'as', 'a', 'diagnosis', 'and', 'source', 'of', 'my', 'difficulties', 'every', 'few', 'years', '.', 'the', 'first', 'time', 'i', 'got', 'a', 'coach', ';', 'my', 'psychiatrist', 'wasn', \"'\", 't', 'on', 'board', 'with', 'a', 'diagnosis', ',', 'so', 'i', 'received', 'no', 'medication', '.', 'it', 'was', 'a', 'wasted', 'effort', '.', 'the', 'second', 'time', ',', 'i', 'received', 'treatment', 'briefly', ',', 'but', 'two', 'medication', 'trials', 'didn', \"'\", 't', 'make', 'a', 'significant', 'impact', 'in', 'my', 'life', ',', 'so', 'i', 'stopped', 'taking', 'them', 'and', 'didn', \"'\", 't', 'visit', 'the', 'issue', 'for', 'about', 'a', 'year', '.', 'until', 'about', 'two', 'weeks', 'ago', 'when', 'dia', '##gno', '##sing', 'and', 'treating', 'ad', '##hd', 'became', 'the', 'single', 'most', 'important', 'thing', 'in', 'my', 'life', '.', 'today', 'i', \"'\", 've', 'spent', 'hours', 'on', 'this', 'red', '##dit', 'commenting', 'on', 'ad', '##hd', 'and', 'reading', 'materials', 'on', 'it', '.', 'i', \"'\", 'm', 'per', '##ser', '##vera', '##ting', 'on', 'it', '-', '-', 'the', 'same', 'way', 'i', \"'\", 'd', 'per', '##ser', '##vera', '##te', 'on', 'playing', 'guitar', ',', 'clarinet', ',', 'ji', '##uj', '##its', '##u', ',', 'video', '##games', ',', 'carp', '##ent', '##ry', ',', 'clarinet', ',', 'etc', '.', 'i', \"'\", 'm', 'seeing', 'a', 'ps', '##ych', 'nurse', 'next', 'week', '.', 'but', 'i', \"'\", 'm', 'concerned', ',', 'in', 'this', 'moment', 'of', 'luc', '##idi', '##ty', ',', 'that', 'the', 'next', 'treatment', 'will', 'be', 'ineffective', 'in', 'the', 'short', 'term', 'and', 'that', 'i', 'will', 'lose', 'interest', 'in', 'treating', 'my', 'ad', '##hd', 'as', 'i', \"'\", 've', 'lost', 'interest', 'in', 'every', 'project', 'i', \"'\", 've', 'ever', 'undertaken', '.', 'my', 'career', 'is', 'already', 'over', '-', '-', 'i', \"'\", 'm', 'on', 'disability', '-', '-', 'and', 'that', 'was', 'my', 'biggest', 'motivation', 'throughout', 'my', 'life', 'for', 'treatment', '(', 'which', 'was', 'mostly', 'spent', 'missing', 'the', 'diagnosis', 'of', 'bipolar', 'ii', ')', '.', 't', '##ld', '##r', ';', 'does', 'ad', '##hd', 'make', 'itself', 'more', 'difficult', 'to', 'treat', 'because', 'you', 'lose', 'interest', 'in', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'research', 'paper', 'that', 'first', 'introduced', 'ad', '##hd', '-', 'pi', 'as', 'a', 'different', 'disorder', 'from', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'research', 'paper', 'that', 'first', 'introduced', 'ad', '##hd', '-', 'pi', 'as', 'a', 'different', 'disorder', 'from', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['chad', '##d', 'annual', 'international', 'conference', 'on', 'ad', '##hd', '-', 'nov', ',', '2012', 'in', 'california']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['chad', '##d', 'annual', 'international', 'conference', 'on', 'ad', '##hd', '-', 'nov', ',', '2012', 'in', 'california']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ca', '##yen', '##ne', 'pepper', 'helps', 'me', 'a', 'lot', '!']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ca', '##yen', '##ne', 'pepper', 'helps', 'me', 'a', 'lot', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'tried', 'or', 'know', 'anything', 'about', 'prof', '##ider', '##all', '?', 'so', 'i', \"'\", 've', 'been', 'looking', 'for', 'a', 'long', 'time', 'for', 'an', 'ot', '##c', 'add', '##eral', '##l', 'replacement', 'and', 'the', 'other', 'day', 'i', 'found', 'out', 'about', 'something', 'called', 'prof', '##ider', '##all', '.', 'i', \"'\", 've', 'been', 'off', 'add', '##eral', '##l', 'for', 'a', 'couple', 'year', 'but', 'i', 'just', 'got', 'a', 'marketing', 'internship', 'a', 'couple', 'days', 'ago', 'and', 'i', \"'\", 'm', 'expected', 'to', 'be', 'crank', '##ing', 'out', 'articles', 'like', 'mad', ',', 'except', 'it', \"'\", 's', 'really', 'boring', 'subject', 'matter', 'and', 'i', 'can', \"'\", 't', 'concentrate', 'to', 'save', 'my', 'life', '.', 'out', 'of', 'desperation', 'i', 'went', 'ahead', 'and', 'ordered', 'a', 'bottle', 'of', 'prof', '##ider', '##all', 'off', 'their', 'site', 'but', 'i', 'thought', 'i', 'would', 'toss', 'this', 'out', 'there', 'and', 'see', 'if', 'any', 'of', 'you', 'red', '##dit', '##ors', 'new', 'anything', 'about', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 139\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'tried', 'or', 'know', 'anything', 'about', 'prof', '##ider', '##all', '?', 'so', 'i', \"'\", 've', 'been', 'looking', 'for', 'a', 'long', 'time', 'for', 'an', 'ot', '##c', 'add', '##eral', '##l', 'replacement', 'and', 'the', 'other', 'day', 'i', 'found', 'out', 'about', 'something', 'called', 'prof', '##ider', '##all', '.', 'i', \"'\", 've', 'been', 'off', 'add', '##eral', '##l', 'for', 'a', 'couple', 'year', 'but', 'i', 'just', 'got', 'a', 'marketing', 'internship', 'a', 'couple', 'days', 'ago', 'and', 'i', \"'\", 'm', 'expected', 'to', 'be', 'crank', '##ing', 'out', 'articles', 'like', 'mad', ',', 'except', 'it', \"'\", 's', 'really', 'boring', 'subject', 'matter', 'and', 'i', 'can', \"'\", 't', 'concentrate', 'to', 'save', 'my', 'life', '.', 'out', 'of', 'desperation', 'i', 'went', 'ahead', 'and', 'ordered', 'a', 'bottle', 'of', 'prof', '##ider', '##all', 'off', 'their', 'site', 'but', 'i', 'thought', 'i', 'would', 'toss', 'this', 'out', 'there', 'and', 'see', 'if', 'any', 'of', 'you', 'red', '##dit', '##ors', 'new', 'anything', 'about', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'tips', 'for', 'coping', 'with', 'erect', '##ile', 'dysfunction', 'on', 'add', '##eral', '##l', '?', 'feels', 'bad', 'man', ':', '(', '.', 'i', \"'\", 've', 'been', 'on', 'add', '##eral', '##l', 'for', 'about', 'two', 'years', 'now', '(', 'taking', 'as', 'long', 'breaks', 'as', 'possible', 'when', 'i', \"'\", 'm', 'able', 'to', ')', 'and', 'have', 'been', 'having', 'trouble', 'getting', 'it', 'up', 'and', 'keeping', 'it', 'up', '.', 'does', 'anyone', 'know', 'of', 'any', 'ways', 'to', 'deal', 'with', 'this', '?', 'any', 'alternate', 'med', '##s', 'that', 'don', \"'\", 't', 'have', 'this', 'side', 'effect', '?', 'i', 'can', \"'\", 't', 'avoid', 'medication', 'altogether', 'because', 'i', \"'\", 'm', 'at', 'a', 'particularly', 'ball', '-', 'bust', '##ing', 'university', '.', 'any', 'suggestions', 'are', 'much', 'appreciated', ':', ')']\n",
      "INFO:__main__:Number of tokens: 110\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'tips', 'for', 'coping', 'with', 'erect', '##ile', 'dysfunction', 'on', 'add', '##eral', '##l', '?', 'feels', 'bad', 'man', ':', '(', '.', 'i', \"'\", 've', 'been', 'on', 'add', '##eral', '##l', 'for', 'about', 'two', 'years', 'now', '(', 'taking', 'as', 'long', 'breaks', 'as', 'possible', 'when', 'i', \"'\", 'm', 'able', 'to', ')', 'and', 'have', 'been', 'having', 'trouble', 'getting', 'it', 'up', 'and', 'keeping', 'it', 'up', '.', 'does', 'anyone', 'know', 'of', 'any', 'ways', 'to', 'deal', 'with', 'this', '?', 'any', 'alternate', 'med', '##s', 'that', 'don', \"'\", 't', 'have', 'this', 'side', 'effect', '?', 'i', 'can', \"'\", 't', 'avoid', 'medication', 'altogether', 'because', 'i', \"'\", 'm', 'at', 'a', 'particularly', 'ball', '-', 'bust', '##ing', 'university', '.', 'any', 'suggestions', 'are', 'much', 'appreciated', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'know', 'of', 'any', 'computer', 'apps', 'that', 'can', 'assist', 'with', 'keeping', 'focused', '?', 'ah', '##oy', ',', 'i', \"'\", 'm', 'wondering', 'if', 'there', 'any', 'computer', 'apps', 'that', 'can', 'help', 'with', 'keeping', 'on', 'task', 'with', 'work', 'and', 'keeping', 'track', 'of', 'time', ',', 'etc', '.', 'i', 'only', 'own', 'a', 'net', '##book', ',', 'which', 'i', 'try', 'to', 'limit', 'the', 'amount', 'of', 'time', 'on', 'because', 'i', 'tend', 'to', 'loose', 'track', 'of', 'time', 'or', 'ignore', 'alarms', 'completely', '.', 'so', 'i', \"'\", 'm', 'wondering', 'if', 'there', \"'\", 's', 'an', 'application', 'that', 'can', 'help', 'make', 'the', 'whole', '\"', 'you', 'just', 'spent', '2', 'hours', 'lurking', 'on', 'red', '##dit', '\"', 'message', 'more', 'noticeable', 'and', 'less', 'like', '\"', 'bee', '##p', 'bee', '##p', 'bee', '##p', '\"', '.', '.', '.', 'what', 'was', 'that', 'for', 'again', '?', 'who', 'cares', '!', 'there', 'are', 'cat', 'pictures', 'to', 'be', 'capt', '##ioned', '.', 'help', '?', ';', '_', ';', '?']\n",
      "INFO:__main__:Number of tokens: 142\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'know', 'of', 'any', 'computer', 'apps', 'that', 'can', 'assist', 'with', 'keeping', 'focused', '?', 'ah', '##oy', ',', 'i', \"'\", 'm', 'wondering', 'if', 'there', 'any', 'computer', 'apps', 'that', 'can', 'help', 'with', 'keeping', 'on', 'task', 'with', 'work', 'and', 'keeping', 'track', 'of', 'time', ',', 'etc', '.', 'i', 'only', 'own', 'a', 'net', '##book', ',', 'which', 'i', 'try', 'to', 'limit', 'the', 'amount', 'of', 'time', 'on', 'because', 'i', 'tend', 'to', 'loose', 'track', 'of', 'time', 'or', 'ignore', 'alarms', 'completely', '.', 'so', 'i', \"'\", 'm', 'wondering', 'if', 'there', \"'\", 's', 'an', 'application', 'that', 'can', 'help', 'make', 'the', 'whole', '\"', 'you', 'just', 'spent', '2', 'hours', 'lurking', 'on', 'red', '##dit', '\"', 'message', 'more', 'noticeable', 'and', 'less', 'like', '\"', 'bee', '##p', 'bee', '##p', 'bee', '##p', '\"', '.', '.', '.', 'what', 'was', 'that', 'for', 'again', '?', 'who', 'cares', '!', 'there', 'are', 'cat', 'pictures', 'to', 'be', 'capt', '##ioned', '.', 'help', '?', ';', '_', ';', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'ever', 'mentioned', 'add', 'on', 'their', 'gr', '##ad', 'school', 'application', '?', 'just', 'wondering', '.', 'i', 'decided', 'not', 'to', 'pull', 'the', '\"', 'add', 'card', '\"', 'on', 'my', 'apps', ',', 'so', 'to', 'speak', ',', 'but', 'i', 'did', 'disclose', 'it', 'to', 'everyone', 'who', 'wrote', 'lo', '##rs', 'for', 'me', '.']\n",
      "INFO:__main__:Number of tokens: 47\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'ever', 'mentioned', 'add', 'on', 'their', 'gr', '##ad', 'school', 'application', '?', 'just', 'wondering', '.', 'i', 'decided', 'not', 'to', 'pull', 'the', '\"', 'add', 'card', '\"', 'on', 'my', 'apps', ',', 'so', 'to', 'speak', ',', 'but', 'i', 'did', 'disclose', 'it', 'to', 'everyone', 'who', 'wrote', 'lo', '##rs', 'for', 'me', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 'tested', 'for', 'ad', '##hd', 'wednesday', ';', 'should', 'i', 'tell', 'the', 'doc', 'about', 'my', 'independent', 'tests', 'with', 'ad', '##hd', 'med', '##s', '?']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 'tested', 'for', 'ad', '##hd', 'wednesday', ';', 'should', 'i', 'tell', 'the', 'doc', 'about', 'my', 'independent', 'tests', 'with', 'ad', '##hd', 'med', '##s', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['should', 'i', 'even', 'say', 'anything', 'to', 'my', 'doctor', 'if', 'i', 'disagree', 'with', 'his', 'diagnosis', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['should', 'i', 'even', 'say', 'anything', 'to', 'my', 'doctor', 'if', 'i', 'disagree', 'with', 'his', 'diagnosis', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['where', 'to', 'begin', '?', '(', 'getting', 'diagnosed', ')', 'ok', ',', 'so', 'i', \"'\", 've', 'decided', 'to', 'go', 'to', 'a', 'doctor', 'to', 'get', 'tested', 'for', 'ad', '##hd', '.', 'a', 'couple', 'of', 'months', 'ago', 'i', 'was', 'going', 'to', 'a', 'psychologist', 'for', 'anxiety', 'issues', 'and', 'was', 'told', 'that', 'their', 'office', 'couldn', \"'\", 't', 'test', 'for', 'ad', '##hd', 'because', 'the', 'doctor', 'that', 'did', 'the', 'testing', 'wasn', \"'\", 't', 'under', 'my', 'plan', '(', 'however', 'my', 'psychologist', 'was', ')', '.', 'i', 'stopped', 'going', 'because', 'i', 'didn', \"'\", 't', 'feels', 'as', 'if', 'we', 'were', 'getting', 'anywhere', '.', 'since', 'then', 'i', 'signed', 'up', 'with', 'a', 'new', 'insurance', 'plan', '(', 'pp', '##o', 'so', 'i', 'don', \"'\", 't', 'have', 'to', 'go', 'through', 'a', 'primary', 'doctor', ')', '.', 'i', \"'\", 'm', 'thinking', 'i', 'should', 'call', 'my', 'old', 'psychologist', \"'\", 's', 'office', 'to', 'see', 'if', 'i', 'can', 'get', 'tested', 'now', '.', 'or', 'should', 'i', 'just', 'go', 'ahead', 'and', 'find', 'a', 'psychiatrist', 'to', 'test', 'me', '?', 'this', 'probably', 'isn', \"'\", 't', 'the', 'right', 'sub', '##red', '##dit', 'for', 'this', ',', 'but', 'as', '##per', '##gers', '.', 'my', 'younger', 'cousins', 'are', 'possibly', 'on', 'the', 'au', '##tist', '##ic', 'spectrum', '.', 'i', 'began', 'talking', 'to', 'my', 'mom', 'about', 'the', 'way', 'i', 'acted', 'as', 'i', 'kid', 'and', 'made', 'a', 'list', 'and', 'took', 'that', 'to', 'my', 'psychologist', '.', 'he', 'said', 'that', 'it', 'is', 'possible', 'that', 'i', 'might', 'have', 'as', '##per', '##gers', ',', 'but', 'that', 'i', 'would', 'need', 'further', 'testing', '(', 'which', 'goes', 'back', 'to', 'my', 'insurance', 'didn', \"'\", 't', 'cover', 'it', 'at', 'the', 'time', ')', '.', 'when', 'i', 'look', 'for', 'au', '##tist', '##ic', 'doctors', 'they', 'are', 'all', 'listed', 'as', 'child', 'doctors', '.', 'so', 'who', 'tests', 'for', 'adult', 'autism', '?']\n",
      "INFO:__main__:Number of tokens: 269\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['where', 'to', 'begin', '?', '(', 'getting', 'diagnosed', ')', 'ok', ',', 'so', 'i', \"'\", 've', 'decided', 'to', 'go', 'to', 'a', 'doctor', 'to', 'get', 'tested', 'for', 'ad', '##hd', '.', 'a', 'couple', 'of', 'months', 'ago', 'i', 'was', 'going', 'to', 'a', 'psychologist', 'for', 'anxiety', 'issues', 'and', 'was', 'told', 'that', 'their', 'office', 'couldn', \"'\", 't', 'test', 'for', 'ad', '##hd', 'because', 'the', 'doctor', 'that', 'did', 'the', 'testing', 'wasn', \"'\", 't', 'under', 'my', 'plan', '(', 'however', 'my', 'psychologist', 'was', ')', '.', 'i', 'stopped', 'going', 'because', 'i', 'didn', \"'\", 't', 'feels', 'as', 'if', 'we', 'were', 'getting', 'anywhere', '.', 'since', 'then', 'i', 'signed', 'up', 'with', 'a', 'new', 'insurance', 'plan', '(', 'pp', '##o', 'so', 'i', 'don', \"'\", 't', 'have', 'to', 'go', 'through', 'a', 'primary', 'doctor', ')', '.', 'i', \"'\", 'm', 'thinking', 'i', 'should', 'call', 'my', 'old', 'psychologist', \"'\", 's', 'office', 'to', 'see', 'if', 'i', 'can', 'get', 'tested', 'now', '.', 'or', 'should', 'i', 'just', 'go', 'ahead', 'and', 'find', 'a', 'psychiatrist', 'to', 'test', 'me', '?', 'this', 'probably', 'isn', \"'\", 't', 'the', 'right', 'sub', '##red', '##dit', 'for', 'this', ',', 'but', 'as', '##per', '##gers', '.', 'my', 'younger', 'cousins', 'are', 'possibly', 'on', 'the', 'au', '##tist', '##ic', 'spectrum', '.', 'i', 'began', 'talking', 'to', 'my', 'mom', 'about', 'the', 'way', 'i', 'acted', 'as', 'i', 'kid', 'and', 'made', 'a', 'list', 'and', 'took', 'that', 'to', 'my', 'psychologist', '.', 'he', 'said', 'that', 'it', 'is', 'possible', 'that', 'i', 'might', 'have', 'as', '##per', '##gers', ',', 'but', 'that', 'i', 'would', 'need', 'further', 'testing', '(', 'which', 'goes', 'back', 'to', 'my', 'insurance', 'didn', \"'\", 't', 'cover', 'it', 'at', 'the', 'time', ')', '.', 'when', 'i', 'look', 'for', 'au', '##tist', '##ic', 'doctors', 'they', 'are', 'all', 'listed', 'as', 'child', 'doctors', '.', 'so', 'who', 'tests', 'for', 'adult', 'autism', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'need', 're', '##ass', '##urance', '.', 'doc', 'suggested', 'i', 'wasn', \"'\", 't', '\"', 'implementing', 'the', 'right', 'strategies', '\"', 'as', 'she', 'passed', 'me', 'the', \"'\", 'script', 'i', \"'\", 've', 'been', 'waiting', 'on', 'forever', '.', 'a', 'common', 'frustration', ',', 'right', '?', 'so', 'i', 'took', 'part', 'in', 'a', 'trial', 'which', 'is', 'looking', 'at', 'the', 'potential', 'for', 'cognitive', 'behaviour', '##al', 'therapy', ',', 'without', 'med', '##s', ',', 'to', 'treat', 'ad', '##hd', '.', 'i', 'was', 'diagnosed', 'shortly', 'before', 'the', '3', 'months', 'of', 'weekly', 'group', 'cb', '##t', 'began', '.', 'cb', '##t', 'was', 'very', 'helpful', '!', 'especially', 'because', 'i', 'tried', 'as', 'much', 'as', 'i', 'could', 'to', 'practice', 'and', 'implement', 'the', 'strategies', '.', 'and', 'i', 'didn', \"'\", 't', 'expect', 'it', 'to', 'work', 'miracles', '.', 'i', 'went', 'in', 'fully', 'expecting', 'only', '*', 'slow', '*', 'change', '(', 'if', 'any', ')', 'and', 'knowing', 'that', 'results', 'would', 'depend', 'on', 'the', 'effort', 'i', 'put', 'into', 'it', '.', 'it', 'ended', 'in', 'december', ',', 'and', 'now', 'there', 'are', 'occasional', 'follow', '-', 'up', 'sessions', '.', 'i', \"'\", 'm', 'still', 'trying', 'to', 'go', 'through', 'a', 'works', '##hee', '##t', 'now', 'and', 'then', ',', 'to', 'track', 'my', '\"', 'hot', 'thoughts', ',', '\"', 'and', 'to', 'take', 'good', 'care', 'of', 'myself', 'by', 'eating', 'well', ',', 'sleeping', ',', 'exercising', ',', 'etc', '.', 'i', 'would', 'love', 'it', 'if', 'i', 'honestly', 'felt', 'like', 'six', 'more', 'months', 'of', 'the', 'same', 'effort', 'could', 'make', 'as', 'much', 'of', 'a', 'difference', 'with', 'my', 'organization', 'and', 'focus', 'as', 'it', 'has', 'already', 'with', 'my', 'emotions', 'and', 'confidence', '(', 'ie', ',', 'not', 'a', 'big', 'difference', ',', 'but', 'subtle', 'and', 'enough', 'for', 'me', 'to', 'notice', 'a', 'bit', ')', '.', 'but', 'it', 'just', 'hasn', \"'\", 't', '.', 'so', ',', 'after', 'much', 'del', '##ibe', '##ration', ',', 'i', 'finally', 'decided', 'i', 'shouldn', \"'\", 't', 'wait', 'any', 'longer', 'to', 'try', 'med', '##s', ',', 'if', 'anything', 'to', 'help', 'me', 'implement', 'the', 'cb', '##t', 'strategies', 'more', 'effectively', 'and', 'more', 'often', '.', 'so', 'the', 'doctor', 'in', 'charge', 'of', 'the', 'study', 'wrote', 'me', 'a', 'prescription', 'in', 'an', 'appointment', 'today', ',', 'and', 'added', ',', 'just', 'as', 'i', 'was', 'leaving', ',', '\"', 'the', 'thing', 'about', 'the', 'cb', '##t', 'is', 'that', 'you', 'actually', 'have', 'to', '*', 'implement', '*', 'the', 'strategies', '.', '\"', 'au', '##ug', '##gh', '##hh', '!', 'no', 'kidding', ',', 'doc', '!', '!', 'they', \"'\", 've', 'only', 'told', 'us', 'that', 'about', 'a', 'hundred', 'times', '.', 'i', 'smiling', '##ly', 'replied', 'that', 'it', '*', 'had', '*', 'been', 'very', 'helpful', 'in', 'certain', 'areas', 'of', 'my', 'life', 'and', 'that', 'i', 'had', 'deliberate', '##d', 'quite', 'a', 'lot', 'before', 'finally', 'deciding', 'to', 'ask', 'for', 'med', '##s', '.', 'gr', '##rr', '.', 't', '##l', '/', 'dr', ':', 'doc', 'suggested', 'i', 'hadn', \"'\", 't', 'been', 'trying', 'hard', 'enough', 'at', 'cb', '##t', 'as', 'she', 'handed', 'me', 'the', \"'\", 'script', '.', 'i', \"'\", 'm', 'trying', 'to', 'let', 'this', 'go', '(', 'as', 'i', \"'\", 've', 'learned', 'to', 'do', '!', ')', 'but', 'it', 'really', 'bothered', 'me', '.', 'obviously', 'nothing', 'is', 'a', '\"', 'quick', 'fix', '.', '\"', 'i', \"'\", 'm', 'almost', '26', 'and', 'my', 'ad', '##hd', 'has', 'caused', 'me', 'a', 'lot', 'of', 'problems', '.', 'i', 'just', 'kinda', 'wanna', 'get', 'this', 'show', 'on', 'the', 'road', 'and', 'see', 'if', 'med', '##s', 'will', 'actually', 'do', 'anything', 'for', 'me', 'already', '.', 'and', 'i', \"'\", 'm', 'looking', 'for', 'your', 're', '##ass', '##urance', 'that', 'i', 'am', ',', 'in', 'fact', ',', 'justified', 'in', 'finally', 'trying', 'med', '##s', 'after', 'cb', '##t', '(', 'which', 'i', 'plan', 'to', 'continue', 'on', 'my', 'own', ')', '.']\n",
      "INFO:__main__:Number of tokens: 546\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['just', 'need', 're', '##ass', '##urance', '.', 'doc', 'suggested', 'i', 'wasn', \"'\", 't', '\"', 'implementing', 'the', 'right', 'strategies', '\"', 'as', 'she', 'passed', 'me', 'the', \"'\", 'script', 'i', \"'\", 've', 'been', 'waiting', 'on', 'forever', '.', 'a', 'common', 'frustration', ',', 'right', '?', 'so', 'i', 'took', 'part', 'in', 'a', 'trial', 'which', 'is', 'looking', 'at', 'the', 'potential', 'for', 'cognitive', 'behaviour', '##al', 'therapy', ',', 'without', 'med', '##s', ',', 'to', 'treat', 'ad', '##hd', '.', 'i', 'was', 'diagnosed', 'shortly', 'before', 'the', '3', 'months', 'of', 'weekly', 'group', 'cb', '##t', 'began', '.', 'cb', '##t', 'was', 'very', 'helpful', '!', 'especially', 'because', 'i', 'tried', 'as', 'much', 'as', 'i', 'could', 'to', 'practice', 'and', 'implement', 'the', 'strategies', '.', 'and', 'i', 'didn', \"'\", 't', 'expect', 'it', 'to', 'work', 'miracles', '.', 'i', 'went', 'in', 'fully', 'expecting', 'only', '*', 'slow', '*', 'change', '(', 'if', 'any', ')', 'and', 'knowing', 'that', 'results', 'would', 'depend', 'on', 'the', 'effort', 'i', 'put', 'into', 'it', '.', 'it', 'ended', 'in', 'december', ',', 'and', 'now', 'there', 'are', 'occasional', 'follow', '-', 'up', 'sessions', '.', 'i', \"'\", 'm', 'still', 'trying', 'to', 'go', 'through', 'a', 'works', '##hee', '##t', 'now', 'and', 'then', ',', 'to', 'track', 'my', '\"', 'hot', 'thoughts', ',', '\"', 'and', 'to', 'take', 'good', 'care', 'of', 'myself', 'by', 'eating', 'well', ',', 'sleeping', ',', 'exercising', ',', 'etc', '.', 'i', 'would', 'love', 'it', 'if', 'i', 'honestly', 'felt', 'like', 'six', 'more', 'months', 'of', 'the', 'same', 'effort', 'could', 'make', 'as', 'much', 'of', 'a', 'difference', 'with', 'my', 'organization', 'and', 'focus', 'as', 'it', 'has', 'already', 'with', 'my', 'emotions', 'and', 'confidence', '(', 'ie', ',', 'not', 'a', 'big', 'difference', ',', 'but', 'subtle', 'and', 'enough', 'for', 'me', 'to', 'notice', 'a', 'bit', ')', '.', 'but', 'it', 'just', 'hasn', \"'\", 't', '.', 'so', ',', 'after', 'much', 'del', '##ibe', '##ration', ',', 'i', 'finally', 'decided', 'i', 'shouldn', \"'\", 't', 'wait', 'any', 'longer', 'to', 'try', 'med', '##s', ',', 'if', 'anything', 'to', 'help', 'me', 'implement', 'the', 'cb', '##t', 'strategies', 'more', 'effectively', 'and', 'more', 'often', '.', 'so', 'the', 'doctor', 'in', 'charge', 'of', 'the', 'study', 'wrote', 'me', 'a', 'prescription', 'in', 'an', 'appointment', 'today', ',', 'and', 'added', ',', 'just', 'as', 'i', 'was', 'leaving', ',', '\"', 'the', 'thing', 'about', 'the', 'cb', '##t', 'is', 'that', 'you', 'actually', 'have', 'to', '*', 'implement', '*', 'the', 'strategies', '.', '\"', 'au', '##ug', '##gh', '##hh', '!', 'no', 'kidding', ',', 'doc', '!', '!', 'they', \"'\", 've', 'only', 'told', 'us', 'that', 'about', 'a', 'hundred', 'times', '.', 'i', 'smiling', '##ly', 'replied', 'that', 'it', '*', 'had', '*', 'been', 'very', 'helpful', 'in', 'certain', 'areas', 'of', 'my', 'life', 'and', 'that', 'i', 'had', 'deliberate', '##d', 'quite', 'a', 'lot', 'before', 'finally', 'deciding', 'to', 'ask', 'for', 'med', '##s', '.', 'gr', '##rr', '.', 't', '##l', '/', 'dr', ':', 'doc', 'suggested', 'i', 'hadn', \"'\", 't', 'been', 'trying', 'hard', 'enough', 'at', 'cb', '##t', 'as', 'she', 'handed', 'me', 'the', \"'\", 'script', '.', 'i', \"'\", 'm', 'trying', 'to', 'let', 'this', 'go', '(', 'as', 'i', \"'\", 've', 'learned', 'to', 'do', '!', ')', 'but', 'it', 'really', 'bothered', 'me', '.', 'obviously', 'nothing', 'is', 'a', '\"', 'quick', 'fix', '.', '\"', 'i', \"'\", 'm', 'almost', '26', 'and', 'my', 'ad', '##hd', 'has', 'caused', 'me', 'a', 'lot', 'of', 'problems', '.', 'i', 'just', 'kinda', 'wanna', 'get', 'this', 'show', 'on', 'the', 'road', 'and', 'see', 'if', 'med', '##s', 'will', 'actually', 'do', 'anything', 'for', 'me', 'already', '.', 'and', 'i', \"'\"], ['m', 'looking', 'for', 'your', 're', '##ass', '##urance', 'that', 'i', 'am', ',', 'in', 'fact', ',', 'justified', 'in', 'finally', 'trying', 'med', '##s', 'after', 'cb', '##t', '(', 'which', 'i', 'plan', 'to', 'continue', 'on', 'my', 'own', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'does', 'methyl', '##in', 'er', 'compare', 'and', 'contrast', 'with', 'concert', '##a', '?', 'is', 'it', 'less', 'effective', 'in', 'some', 'individuals', '?']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'does', 'methyl', '##in', 'er', 'compare', 'and', 'contrast', 'with', 'concert', '##a', '?', 'is', 'it', 'less', 'effective', 'in', 'some', 'individuals', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'i', 'feel', 'like', 'when', 'finishing', 'a', 'big', 'project', 'and', 'coming', 'out', 'of', 'hyper', '##fo', '##cus']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'i', 'feel', 'like', 'when', 'finishing', 'a', 'big', 'project', 'and', 'coming', 'out', 'of', 'hyper', '##fo', '##cus']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['til', 'excerpt', '?', '-', '-', '-', 'add', 'tip', 'o', 'the', 'day', '209']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['til', 'excerpt', '?', '-', '-', '-', 'add', 'tip', 'o', 'the', 'day', '209']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', 'advice', 'about', 'med', '##s', '.', 'so', 'after', 'finding', 'out', 'i', 'had', 'ad', '##hd', 'last', 'year', 'i', 'started', 'taking', 'med', '##s', '.', 'i', 'was', 'in', '9th', 'grade', ',', 'my', 'parents', 'had', 'known', 'since', 'i', 'was', 'in', '2nd', 'grade', 'but', 'didn', \"'\", 't', 'think', 'it', 'per', '##tine', '##nt', 'until', 'i', 'started', 'highs', '##cho', '##ol', 'and', 'my', 'grades', 'actually', 'started', 'to', 'count', '.', 'as', 'soon', 'as', 'i', 'found', 'the', 'right', 'medicine', '(', 'concert', '##a', '-', '54', '##mg', 'i', 'think', ')', 'my', 'grades', 'increased', 'exponential', '##ly', '.', 'my', 'problem', 'is', 'i', 'hate', 'the', 'side', 'affects', 'with', 'a', 'burning', 'passion', '.', 'i', 'hate', 'no', 'being', 'hungry', ',', 'i', 'hate', 'being', 'moody', 'when', 'i', 'get', 'home', ',', 'and', 'i', 'hate', 'getting', 'headache', '##s', 'and', 'stomach', 'ache', '##s', 'from', 'the', 'lack', 'of', 'nutrition', 'i', 'absorb', 'throughout', 'the', 'day', '.', 'at', 'the', 'same', 'time', ',', 'i', 'really', 'like', 'doing', 'well', 'in', 'school', '.', 'i', 'was', 'just', 'wondering', 'if', 'anyone', 'had', ',', 'had', 'a', 'similar', 'experience', 'and', 'could', 'shed', 'some', 'light', 'on', 'the', 'subject', 'for', 'me', 'or', 'give', 'me', 'some', 'advice', 'etc', '.', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 177\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', 'advice', 'about', 'med', '##s', '.', 'so', 'after', 'finding', 'out', 'i', 'had', 'ad', '##hd', 'last', 'year', 'i', 'started', 'taking', 'med', '##s', '.', 'i', 'was', 'in', '9th', 'grade', ',', 'my', 'parents', 'had', 'known', 'since', 'i', 'was', 'in', '2nd', 'grade', 'but', 'didn', \"'\", 't', 'think', 'it', 'per', '##tine', '##nt', 'until', 'i', 'started', 'highs', '##cho', '##ol', 'and', 'my', 'grades', 'actually', 'started', 'to', 'count', '.', 'as', 'soon', 'as', 'i', 'found', 'the', 'right', 'medicine', '(', 'concert', '##a', '-', '54', '##mg', 'i', 'think', ')', 'my', 'grades', 'increased', 'exponential', '##ly', '.', 'my', 'problem', 'is', 'i', 'hate', 'the', 'side', 'affects', 'with', 'a', 'burning', 'passion', '.', 'i', 'hate', 'no', 'being', 'hungry', ',', 'i', 'hate', 'being', 'moody', 'when', 'i', 'get', 'home', ',', 'and', 'i', 'hate', 'getting', 'headache', '##s', 'and', 'stomach', 'ache', '##s', 'from', 'the', 'lack', 'of', 'nutrition', 'i', 'absorb', 'throughout', 'the', 'day', '.', 'at', 'the', 'same', 'time', ',', 'i', 'really', 'like', 'doing', 'well', 'in', 'school', '.', 'i', 'was', 'just', 'wondering', 'if', 'anyone', 'had', ',', 'had', 'a', 'similar', 'experience', 'and', 'could', 'shed', 'some', 'light', 'on', 'the', 'subject', 'for', 'me', 'or', 'give', 'me', 'some', 'advice', 'etc', '.', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['advice', 'needed', 'on', 'coping', 'mechanisms', 'hi', '/', 'r', '/', 'ad', '##hd', '.', 'to', 'get', 'this', 'out', 'of', 'the', 'way', ',', 'i', \"'\", 've', 'never', 'been', 'formally', 'diagnosed', 'with', 'ad', '##hd', '.', 'it', \"'\", 's', 'only', 'been', 'very', 'recently', 'that', 'i', \"'\", 've', 'really', 'noticed', 'that', 'i', 'don', \"'\", 't', 'seem', 'to', 'be', 'able', 'to', 'work', 'the', 'same', 'as', 'everyone', 'else', 'in', 'my', 'team', ',', 'and', 'that', \"'\", 's', 'when', 'i', 'started', 'to', 'do', 'some', 'reading', '.', 'background', ':', 'mid', '-', '20s', ',', 'work', 'in', 'it', '.', 'jack', '##ed', 'in', 'my', 'last', 'job', 'because', 'i', 'was', 'bored', '.', 'worked', 'a', 'year', 'in', 'my', 'current', 'job', ',', 'in', 'the', 'process', 'of', 'jack', '##ing', 'it', 'in', 'because', 'i', \"'\", 'm', 'bored', '.', 'start', 'a', 'new', 'job', 'in', 'a', 'few', 'weeks', '.', 'i', 'work', 'fine', 'when', 'it', \"'\", 's', 'something', 'interesting', ',', 'but', 'i', 'can', \"'\", 't', 'seem', 'to', 'do', 'anything', 'men', '##ial', 'or', 'anything', 'that', 'i', 'don', \"'\", 't', 'find', 'interesting', '.', 'this', 'was', 'less', 'apparent', 'at', 'my', 'last', 'job', ',', 'where', 'i', 'was', 'a', 'senior', 'guy', 'and', 'i', 'had', 'people', 'reporting', 'to', 'me', '.', 'i', 'found', 'that', 'i', 'delegate', '##d', 'a', 'lot', 'of', 'the', 'less', 'interesting', 'stuff', ',', 'and', 'fixed', 'all', 'the', 'complicated', 'stuff', '.', 'i', 'moved', 'to', 'my', 'current', 'job', ',', 'where', 'my', 'boss', 'immediately', 'noticed', 'that', 'i', 'did', 'all', 'of', 'the', 'complicated', 'jobs', ',', 'but', 'left', 'all', 'of', 'the', 'boring', 'jobs', ',', 'despite', 'some', 'of', 'them', 'being', 'a', 'higher', 'priority', '.', 'i', 'also', 'have', 'trouble', 'verbal', '##ising', 'what', 'i', \"'\", 'm', 'thinking', ',', 'which', 'leads', 'to', 'all', 'sorts', 'of', 'pictures', 'being', 'drawn', 'and', 'hand', 'movements', 'to', 'try', 'to', 'explain', 'what', 'i', 'mean', '.', 'i', 'often', \"'\", 'get', \"'\", 'an', 'underlying', 'workings', 'of', 'something', ',', 'but', 'that', 'don', \"'\", 't', 'know', 'how', 'to', 'explain', 'it', ',', 'which', 'leads', 'people', 'to', 'not', 'believe', 'that', 'i', 'know', 'how', 'to', 'fix', 'something', '.', 'quite', 'often', 'i', 'will', 'be', 'proved', 'to', 'have', 'been', 'correct', ',', 'but', 'it', \"'\", 's', 'very', 'frustrating', '.', 'so', 'anyway', ',', 'back', 'on', 'topic', '.', 'i', 'suspect', 'i', 'don', \"'\", 't', 'have', 'ad', '##hd', 'as', 'severely', 'as', 'a', 'lot', 'of', 'guys', 'in', 'this', 'sub', '##red', '##dit', ',', 'but', 'i', 'would', 'appreciate', 'it', 'if', 'you', 'guys', 'could', 'share', 'some', 'of', 'your', 'coping', 'mechanisms', 'with', 'me', '.', 'at', 'the', 'moment', 'i', \"'\", 'm', 'writing', 'a', 'lot', 'of', 'notes', 'on', 'what', 'i', \"'\", 'm', 'doing', '.', 'this', 'helps', ',', 'as', 'it', 'gives', 'me', 'time', 'to', 'process', 'what', 'i', \"'\", 'm', 'doing', '.', 'i', \"'\", 'm', 'also', 'drinking', 'a', 'fuck', '##load', 'of', 'caf', '##fe', '##ine', ',', 'and', 'going', 'for', 'frequent', 'walks', '.', 'so', 'every', 'half', 'an', 'hour', 'or', 'so', 'i', 'will', 'go', 'outside', 'and', 'just', 'do', 'a', 'lap', 'of', 'the', 'building', '.', 'then', 'when', 'i', 'come', 'back', 'i', 'can', 'work', 'more', '.', 'has', 'anyone', 'else', 'got', 'any', 'more', '?', 'i', 'know', 'working', 'in', 'it', 'is', 'no', 'excuse', ',', 'but', 'i', 'do', 'feel', 'the', 'lure', 'of', 'the', 'web', 'during', 'the', 'day', '.', 'i', 'even', 'tried', 'a', 'tool', '##bar', 'that', 'blocked', 'time', '-', 'wasting', 'sites', ',', 'but', 'then', 'i', 'just', 'saw', 'ci', '##rc', '##um', '##venting', 'it', 'as', 'a', 'challenge', '.', 'i', 'need', 'your', 'help', '.', 'i', 'start', 'a', 'new', 'job', 'soon', ',', 'and', 'i', 'don', \"'\", 't', 'want', 'my', 'new', 'boss', 'to', 'have', 'to', 'be', 'as', 'understanding', 'as', 'my', 'current', 'boss', 'is', '.', 'i', 'want', 'to', 'be', 'able', 'to', 'work', 'and', 'be', 'good', 'at', 'my', 'job', 'without', 're', '-', 'in', '##venting', 'un', '-', 'necessary', 'wheels', 'while', 'i', \"'\", 'm', 'in', 'the', 'middle', 'of', 'doing', 'something', 'else', '.', 'thank', 'you', 'in', 'advance', '.']\n",
      "INFO:__main__:Number of tokens: 585\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['advice', 'needed', 'on', 'coping', 'mechanisms', 'hi', '/', 'r', '/', 'ad', '##hd', '.', 'to', 'get', 'this', 'out', 'of', 'the', 'way', ',', 'i', \"'\", 've', 'never', 'been', 'formally', 'diagnosed', 'with', 'ad', '##hd', '.', 'it', \"'\", 's', 'only', 'been', 'very', 'recently', 'that', 'i', \"'\", 've', 'really', 'noticed', 'that', 'i', 'don', \"'\", 't', 'seem', 'to', 'be', 'able', 'to', 'work', 'the', 'same', 'as', 'everyone', 'else', 'in', 'my', 'team', ',', 'and', 'that', \"'\", 's', 'when', 'i', 'started', 'to', 'do', 'some', 'reading', '.', 'background', ':', 'mid', '-', '20s', ',', 'work', 'in', 'it', '.', 'jack', '##ed', 'in', 'my', 'last', 'job', 'because', 'i', 'was', 'bored', '.', 'worked', 'a', 'year', 'in', 'my', 'current', 'job', ',', 'in', 'the', 'process', 'of', 'jack', '##ing', 'it', 'in', 'because', 'i', \"'\", 'm', 'bored', '.', 'start', 'a', 'new', 'job', 'in', 'a', 'few', 'weeks', '.', 'i', 'work', 'fine', 'when', 'it', \"'\", 's', 'something', 'interesting', ',', 'but', 'i', 'can', \"'\", 't', 'seem', 'to', 'do', 'anything', 'men', '##ial', 'or', 'anything', 'that', 'i', 'don', \"'\", 't', 'find', 'interesting', '.', 'this', 'was', 'less', 'apparent', 'at', 'my', 'last', 'job', ',', 'where', 'i', 'was', 'a', 'senior', 'guy', 'and', 'i', 'had', 'people', 'reporting', 'to', 'me', '.', 'i', 'found', 'that', 'i', 'delegate', '##d', 'a', 'lot', 'of', 'the', 'less', 'interesting', 'stuff', ',', 'and', 'fixed', 'all', 'the', 'complicated', 'stuff', '.', 'i', 'moved', 'to', 'my', 'current', 'job', ',', 'where', 'my', 'boss', 'immediately', 'noticed', 'that', 'i', 'did', 'all', 'of', 'the', 'complicated', 'jobs', ',', 'but', 'left', 'all', 'of', 'the', 'boring', 'jobs', ',', 'despite', 'some', 'of', 'them', 'being', 'a', 'higher', 'priority', '.', 'i', 'also', 'have', 'trouble', 'verbal', '##ising', 'what', 'i', \"'\", 'm', 'thinking', ',', 'which', 'leads', 'to', 'all', 'sorts', 'of', 'pictures', 'being', 'drawn', 'and', 'hand', 'movements', 'to', 'try', 'to', 'explain', 'what', 'i', 'mean', '.', 'i', 'often', \"'\", 'get', \"'\", 'an', 'underlying', 'workings', 'of', 'something', ',', 'but', 'that', 'don', \"'\", 't', 'know', 'how', 'to', 'explain', 'it', ',', 'which', 'leads', 'people', 'to', 'not', 'believe', 'that', 'i', 'know', 'how', 'to', 'fix', 'something', '.', 'quite', 'often', 'i', 'will', 'be', 'proved', 'to', 'have', 'been', 'correct', ',', 'but', 'it', \"'\", 's', 'very', 'frustrating', '.', 'so', 'anyway', ',', 'back', 'on', 'topic', '.', 'i', 'suspect', 'i', 'don', \"'\", 't', 'have', 'ad', '##hd', 'as', 'severely', 'as', 'a', 'lot', 'of', 'guys', 'in', 'this', 'sub', '##red', '##dit', ',', 'but', 'i', 'would', 'appreciate', 'it', 'if', 'you', 'guys', 'could', 'share', 'some', 'of', 'your', 'coping', 'mechanisms', 'with', 'me', '.', 'at', 'the', 'moment', 'i', \"'\", 'm', 'writing', 'a', 'lot', 'of', 'notes', 'on', 'what', 'i', \"'\", 'm', 'doing', '.', 'this', 'helps', ',', 'as', 'it', 'gives', 'me', 'time', 'to', 'process', 'what', 'i', \"'\", 'm', 'doing', '.', 'i', \"'\", 'm', 'also', 'drinking', 'a', 'fuck', '##load', 'of', 'caf', '##fe', '##ine', ',', 'and', 'going', 'for', 'frequent', 'walks', '.', 'so', 'every', 'half', 'an', 'hour', 'or', 'so', 'i', 'will', 'go', 'outside', 'and', 'just', 'do', 'a', 'lap', 'of', 'the', 'building', '.', 'then', 'when', 'i', 'come', 'back', 'i', 'can', 'work', 'more', '.', 'has', 'anyone', 'else', 'got', 'any', 'more', '?', 'i', 'know', 'working', 'in', 'it', 'is', 'no', 'excuse', ',', 'but', 'i', 'do', 'feel', 'the', 'lure', 'of', 'the', 'web', 'during', 'the', 'day', '.', 'i', 'even', 'tried', 'a', 'tool', '##bar', 'that', 'blocked', 'time', '-', 'wasting', 'sites', ',', 'but', 'then', 'i', 'just', 'saw', 'ci', '##rc', '##um', '##venting', 'it', 'as', 'a', 'challenge'], ['.', 'i', 'need', 'your', 'help', '.', 'i', 'start', 'a', 'new', 'job', 'soon', ',', 'and', 'i', 'don', \"'\", 't', 'want', 'my', 'new', 'boss', 'to', 'have', 'to', 'be', 'as', 'understanding', 'as', 'my', 'current', 'boss', 'is', '.', 'i', 'want', 'to', 'be', 'able', 'to', 'work', 'and', 'be', 'good', 'at', 'my', 'job', 'without', 're', '-', 'in', '##venting', 'un', '-', 'necessary', 'wheels', 'while', 'i', \"'\", 'm', 'in', 'the', 'middle', 'of', 'doing', 'something', 'else', '.', 'thank', 'you', 'in', 'advance', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['finally', ',', 'a', 'tu', '##mb', '##lr', 'that', 'understands', ':', 'ad', '##hd', 'problems']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['finally', ',', 'a', 'tu', '##mb', '##lr', 'that', 'understands', ':', 'ad', '##hd', 'problems']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sh', '*', 't', 'no', 'add', '##er', 'says', '…', '(', 'sf', '##w', ')']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sh', '*', 't', 'no', 'add', '##er', 'says', '…', '(', 'sf', '##w', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['problems', 'with', 'concert', '##a', '36', '##mg', 'hi', 'everyone', '.', 'i', \"'\", 'm', '27', 'years', 'old', ',', 'and', 'i', 'just', 'started', 'taking', 'add', 'medication', ',', 'specifically', 'concert', '##a', '.', 'i', 'started', '-', 'off', 'on', 'a', 'low', 'dose', ',', '18', '##mg', 'per', 'day', ',', 'then', 'moved', 'up', 'to', '36', '##mg', 'per', 'day', '.', 'i', 'found', 'that', '18', '##mg', 'worked', 'for', 'maybe', 'one', 'day', ',', 'but', 'that', 'was', 'it', '.', 'with', 'the', '36', '##mg', 'dos', '##age', 'of', 'concert', '##a', 'i', 'started', 'noticing', 'a', 'difference', ',', 'especially', 'the', 'first', 'two', 'weeks', '.', 'now', 'it', \"'\", 's', 'not', 'doing', 'too', 'much', 'for', 'me', '.', 'i', 'want', 'to', 'keep', 'taking', 'concert', '##a', ',', 'because', 'i', 'have', 'almost', 'no', 'ill', 'side', '-', 'effects', 'on', 'it', ',', 'except', 'sometimes', 'i', 'will', 'be', 'moody', 'which', 'is', 'solved', 'by', 'simply', 'drinking', 'water', 'and', 'eating', 'a', 'snack', '.', 'typically', ',', 'a', 'guy', 'who', \"'\", 's', '27', 'years', 'old', ',', '170', 'lbs', ',', 'master', 'student', 'so', 'i', \"'\", 'm', 'constantly', 'working', '(', 'reading', 'a', 'lot', ')', ',', 'what', 'is', 'a', 'good', 'dose', '?', '?', 'because', 'my', 'doctor', 'told', 'me', ',', 'many', 'adults', 'that', 'only', 'take', '18', '-', '36', '##mg', 'of', 'concert', '##a', '.', 'also', ',', 'it', 'only', 'lasts', 'like', '6', '-', '8', 'hours', ',', 'the', '12', 'hour', 'claim', 'is', 'garbage', '.', 'thanks']\n",
      "INFO:__main__:Number of tokens: 210\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['problems', 'with', 'concert', '##a', '36', '##mg', 'hi', 'everyone', '.', 'i', \"'\", 'm', '27', 'years', 'old', ',', 'and', 'i', 'just', 'started', 'taking', 'add', 'medication', ',', 'specifically', 'concert', '##a', '.', 'i', 'started', '-', 'off', 'on', 'a', 'low', 'dose', ',', '18', '##mg', 'per', 'day', ',', 'then', 'moved', 'up', 'to', '36', '##mg', 'per', 'day', '.', 'i', 'found', 'that', '18', '##mg', 'worked', 'for', 'maybe', 'one', 'day', ',', 'but', 'that', 'was', 'it', '.', 'with', 'the', '36', '##mg', 'dos', '##age', 'of', 'concert', '##a', 'i', 'started', 'noticing', 'a', 'difference', ',', 'especially', 'the', 'first', 'two', 'weeks', '.', 'now', 'it', \"'\", 's', 'not', 'doing', 'too', 'much', 'for', 'me', '.', 'i', 'want', 'to', 'keep', 'taking', 'concert', '##a', ',', 'because', 'i', 'have', 'almost', 'no', 'ill', 'side', '-', 'effects', 'on', 'it', ',', 'except', 'sometimes', 'i', 'will', 'be', 'moody', 'which', 'is', 'solved', 'by', 'simply', 'drinking', 'water', 'and', 'eating', 'a', 'snack', '.', 'typically', ',', 'a', 'guy', 'who', \"'\", 's', '27', 'years', 'old', ',', '170', 'lbs', ',', 'master', 'student', 'so', 'i', \"'\", 'm', 'constantly', 'working', '(', 'reading', 'a', 'lot', ')', ',', 'what', 'is', 'a', 'good', 'dose', '?', '?', 'because', 'my', 'doctor', 'told', 'me', ',', 'many', 'adults', 'that', 'only', 'take', '18', '-', '36', '##mg', 'of', 'concert', '##a', '.', 'also', ',', 'it', 'only', 'lasts', 'like', '6', '-', '8', 'hours', ',', 'the', '12', 'hour', 'claim', 'is', 'garbage', '.', 'thanks']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['im', 'raging', '!']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['im', 'raging', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['overcome', 'shy', '##ness', ':', 'how', 'to', 'voice', 'your', 'opinions']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['overcome', 'shy', '##ness', ':', 'how', 'to', 'voice', 'your', 'opinions']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['have', 'you', 'been', \"'\", 'v', '##y', '##van', '##sed', \"'\", 'today', 'my', 'psychiatrist', 'said', 'stuff', 'about', 'abuse', 'potential', 'and', 'policy', 'and', 'before', 'i', 'knew', 'it', 'i', 'had', 'a', 'prescription', 'and', 'a', 'coup', '##on', 'for', 'v', '##y', '##van', '##se', 'to', 'replace', 'my', 'usual', 'dex', '##ed', '##rine', 'ir', '.', 'i', 'figure', ',', 'it', \"'\", 's', 'free', 'and', 'x', '##r', 'is', 'more', 'convenient', '(', 'less', 'pill', '-', 'taking', 'and', 'potential', 'ups', 'and', 'downs', ')', ',', 'so', 'i', \"'\", 'll', 'try', 'it', ',', 'but', 'i', \"'\", 'm', 'still', 'uneasy', 'about', 'the', 'motivation', 'for', 'getting', 'me', 'to', 'try', 'it', '.', 'has', 'your', 'medication', 'been', 'switched', 'with', 'something', 'the', 'doc', 'had', 'coup', '##ons', 'for', 'before', '?']\n",
      "INFO:__main__:Number of tokens: 109\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['have', 'you', 'been', \"'\", 'v', '##y', '##van', '##sed', \"'\", 'today', 'my', 'psychiatrist', 'said', 'stuff', 'about', 'abuse', 'potential', 'and', 'policy', 'and', 'before', 'i', 'knew', 'it', 'i', 'had', 'a', 'prescription', 'and', 'a', 'coup', '##on', 'for', 'v', '##y', '##van', '##se', 'to', 'replace', 'my', 'usual', 'dex', '##ed', '##rine', 'ir', '.', 'i', 'figure', ',', 'it', \"'\", 's', 'free', 'and', 'x', '##r', 'is', 'more', 'convenient', '(', 'less', 'pill', '-', 'taking', 'and', 'potential', 'ups', 'and', 'downs', ')', ',', 'so', 'i', \"'\", 'll', 'try', 'it', ',', 'but', 'i', \"'\", 'm', 'still', 'uneasy', 'about', 'the', 'motivation', 'for', 'getting', 'me', 'to', 'try', 'it', '.', 'has', 'your', 'medication', 'been', 'switched', 'with', 'something', 'the', 'doc', 'had', 'coup', '##ons', 'for', 'before', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['at', 'what', 'point', 'did', 'you', 'decide', 'you', 'were', 'ideally', 'med', '##icated', '?', 'note', '-', 'my', 'questions', 'are', 'aimed', 'more', 'towards', 'those', 'with', 'focus', '/', 'attention', 'issues', '(', 'ad', '##hd', '-', 'pi', ')', 'than', 'towards', 'hyper', '##act', '##ivity', 'and', 'impulse', '/', 'inhibition', 'control', 'issues', '.', 'however', ',', 'i', 'am', 'still', 'interested', 'in', 'what', 'everyone', 'has', 'to', 'say', '!', 'as', 'the', 'title', 'suggests', ',', 'at', 'what', 'point', 'did', 'you', 'and', 'your', 'doctor', 'decide', 'on', 'a', 'final', 'dos', '##age', 'of', '(', 'rita', '##lin', ',', 'add', '##eral', '##l', ',', 'etc', '.', ')', 'that', 'you', 'would', 'stick', 'to', 'for', 'an', 'extended', 'period', 'of', 'time', '?', 'do', 'you', 'find', 'this', 'dos', '##age', 'to', 'be', 'night', 'and', 'day', 'in', 'comparison', 'to', 'both', 'not', 'being', 'on', 'medication', 'and', 'to', 'being', 'on', 'the', 'same', 'drug', 'but', 'a', 'different', 'dos', '##age', '?', 'i', \"'\", 'm', 'aware', 'that', 'different', 'med', '##s', 'affect', 'people', 'differently', ',', 'same', 'with', 'sensitivity', 'to', 'dos', '##age', 'changes', 'and', 'such', '.', 'what', 'i', \"'\", 'm', 'interested', 'in', 'is', 'more', 'along', 'the', 'lines', 'of', 'what', 'to', 'look', 'for', 'when', 'i', \"'\", 've', 'hit', 'the', 'right', 'dos', '##age', '.', 'how', 'significant', 'is', 'taking', 'the', 'medication', 'consistently', '?', 'since', 'i', \"'\", 've', 'left', 'high', 'school', 'i', \"'\", 've', 'become', 'far', 'less', 'organized', '(', 'oh', '.', '.', '.', '*', 'shit', '*', ')', 'and', 'don', \"'\", 't', 'take', 'my', 'medication', 'as', 'consistently', 'as', 'i', 'should', '.', 'if', 'i', 'take', 'my', 'dose', ',', 'it', \"'\", 's', 'anywhere', 'from', '7', '##am', 'to', '12', '##pm', 'and', 'while', 'i', 'don', \"'\", 't', 'usually', 'miss', 'more', 'than', 'one', 'day', 'in', 'a', 'row', ',', 'i', 'probably', 'miss', 'at', 'least', '2', 'days', 'a', 'week', '.', 'is', 'that', 'significant', ',', 'and', 'would', 'i', 'see', 'better', '/', 'more', 'consistent', 'results', 'from', 'my', 'medication', 'if', 'i', 'were', 'to', 'make', 'taking', 'it', 'at', 'the', 'same', 'time', 'everyday', 'as', 'well', 'as', 'not', 'missing', 'so', 'many', 'days', 'each', 'week', 'a', 'priority', '?', 'in', 'your', 'opinion', ',', 'or', 'based', 'on', 'what', 'you', \"'\", 've', 'read', 'or', 'been', 'told', '(', 'cite', 'a', 'source', 'please', ')', 'how', 'long', 'should', 'someone', 'should', 'stay', 'on', 'a', 'certain', 'dos', '##age', 'of', 'a', 'drug', '(', 'taken', 'consistently', ',', 'not', 'in', 'a', 'sc', '##atter', '##bra', '##ined', 'manner', 'like', 'myself', ')', 'before', 'deciding', 'to', 'change', 'the', 'dose', '/', 'drug', '?', '(', 'as', 'long', 'as', 'it', 'isn', \"'\", 't', 'b', '##lat', '##antly', 'obvious', 'things', 'aren', \"'\", 't', 'working', 'out', 'because', 'of', 'side', 'effects', 'or', 'no', 'apparent', 'change', 'in', 'ability', 'to', 'focus', '.', ')', 'hopefully', 'this', \"'\", 'll', 'get', 'a', 'few', 'replies', 'so', 'i', 'can', 'sat', '##iate', 'my', 'curiosity', 'for', 'data', '(', 'that', 'also', 'happens', 'to', 'per', '##tain', 'to', 'myself', ')', '*', '*', 't', '##l', ';', 'dr', '-', 'n', '/', 'a', 'this', 'late', 'at', 'night', '.', '*', '*']\n",
      "INFO:__main__:Number of tokens: 442\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['at', 'what', 'point', 'did', 'you', 'decide', 'you', 'were', 'ideally', 'med', '##icated', '?', 'note', '-', 'my', 'questions', 'are', 'aimed', 'more', 'towards', 'those', 'with', 'focus', '/', 'attention', 'issues', '(', 'ad', '##hd', '-', 'pi', ')', 'than', 'towards', 'hyper', '##act', '##ivity', 'and', 'impulse', '/', 'inhibition', 'control', 'issues', '.', 'however', ',', 'i', 'am', 'still', 'interested', 'in', 'what', 'everyone', 'has', 'to', 'say', '!', 'as', 'the', 'title', 'suggests', ',', 'at', 'what', 'point', 'did', 'you', 'and', 'your', 'doctor', 'decide', 'on', 'a', 'final', 'dos', '##age', 'of', '(', 'rita', '##lin', ',', 'add', '##eral', '##l', ',', 'etc', '.', ')', 'that', 'you', 'would', 'stick', 'to', 'for', 'an', 'extended', 'period', 'of', 'time', '?', 'do', 'you', 'find', 'this', 'dos', '##age', 'to', 'be', 'night', 'and', 'day', 'in', 'comparison', 'to', 'both', 'not', 'being', 'on', 'medication', 'and', 'to', 'being', 'on', 'the', 'same', 'drug', 'but', 'a', 'different', 'dos', '##age', '?', 'i', \"'\", 'm', 'aware', 'that', 'different', 'med', '##s', 'affect', 'people', 'differently', ',', 'same', 'with', 'sensitivity', 'to', 'dos', '##age', 'changes', 'and', 'such', '.', 'what', 'i', \"'\", 'm', 'interested', 'in', 'is', 'more', 'along', 'the', 'lines', 'of', 'what', 'to', 'look', 'for', 'when', 'i', \"'\", 've', 'hit', 'the', 'right', 'dos', '##age', '.', 'how', 'significant', 'is', 'taking', 'the', 'medication', 'consistently', '?', 'since', 'i', \"'\", 've', 'left', 'high', 'school', 'i', \"'\", 've', 'become', 'far', 'less', 'organized', '(', 'oh', '.', '.', '.', '*', 'shit', '*', ')', 'and', 'don', \"'\", 't', 'take', 'my', 'medication', 'as', 'consistently', 'as', 'i', 'should', '.', 'if', 'i', 'take', 'my', 'dose', ',', 'it', \"'\", 's', 'anywhere', 'from', '7', '##am', 'to', '12', '##pm', 'and', 'while', 'i', 'don', \"'\", 't', 'usually', 'miss', 'more', 'than', 'one', 'day', 'in', 'a', 'row', ',', 'i', 'probably', 'miss', 'at', 'least', '2', 'days', 'a', 'week', '.', 'is', 'that', 'significant', ',', 'and', 'would', 'i', 'see', 'better', '/', 'more', 'consistent', 'results', 'from', 'my', 'medication', 'if', 'i', 'were', 'to', 'make', 'taking', 'it', 'at', 'the', 'same', 'time', 'everyday', 'as', 'well', 'as', 'not', 'missing', 'so', 'many', 'days', 'each', 'week', 'a', 'priority', '?', 'in', 'your', 'opinion', ',', 'or', 'based', 'on', 'what', 'you', \"'\", 've', 'read', 'or', 'been', 'told', '(', 'cite', 'a', 'source', 'please', ')', 'how', 'long', 'should', 'someone', 'should', 'stay', 'on', 'a', 'certain', 'dos', '##age', 'of', 'a', 'drug', '(', 'taken', 'consistently', ',', 'not', 'in', 'a', 'sc', '##atter', '##bra', '##ined', 'manner', 'like', 'myself', ')', 'before', 'deciding', 'to', 'change', 'the', 'dose', '/', 'drug', '?', '(', 'as', 'long', 'as', 'it', 'isn', \"'\", 't', 'b', '##lat', '##antly', 'obvious', 'things', 'aren', \"'\", 't', 'working', 'out', 'because', 'of', 'side', 'effects', 'or', 'no', 'apparent', 'change', 'in', 'ability', 'to', 'focus', '.', ')', 'hopefully', 'this', \"'\", 'll', 'get', 'a', 'few', 'replies', 'so', 'i', 'can', 'sat', '##iate', 'my', 'curiosity', 'for', 'data', '(', 'that', 'also', 'happens', 'to', 'per', '##tain', 'to', 'myself', ')', '*', '*', 't', '##l', ';', 'dr', '-', 'n', '/', 'a', 'this', 'late', 'at', 'night', '.', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sounds', '\"', 'slow', 'down', '\"', '?', 'maybe', 'this', 'is', 'just', 'me', 'being', 'a', 'little', 'broken', 'and', 'over', '-', 'med', '##icated', 'lately', ',', 'but', 'it', 'seems', 'like', 'sometimes', 'when', 'my', 'med', '##s', 'are', 'peaking', '(', 'i', 'take', 'add', '##eral', '##l', 'x', '##rs', ')', 'i', \"'\", 'll', 'listen', 'to', 'a', 'song', 'and', 'it', 'sounds', 'like', 'it', \"'\", 's', 'just', 'slow', '.', '.', '.', 'like', 'a', 'record', 'playing', 'at', 'half', 'or', 'partial', 'speed', 'or', 'something', '.', 'anyone', 'else', 'have', 'anything', 'similar', '?', 'or', 'am', 'i', 'just', 'a', 'crazy', 'person', '?']\n",
      "INFO:__main__:Number of tokens: 87\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sounds', '\"', 'slow', 'down', '\"', '?', 'maybe', 'this', 'is', 'just', 'me', 'being', 'a', 'little', 'broken', 'and', 'over', '-', 'med', '##icated', 'lately', ',', 'but', 'it', 'seems', 'like', 'sometimes', 'when', 'my', 'med', '##s', 'are', 'peaking', '(', 'i', 'take', 'add', '##eral', '##l', 'x', '##rs', ')', 'i', \"'\", 'll', 'listen', 'to', 'a', 'song', 'and', 'it', 'sounds', 'like', 'it', \"'\", 's', 'just', 'slow', '.', '.', '.', 'like', 'a', 'record', 'playing', 'at', 'half', 'or', 'partial', 'speed', 'or', 'something', '.', 'anyone', 'else', 'have', 'anything', 'similar', '?', 'or', 'am', 'i', 'just', 'a', 'crazy', 'person', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'dose', 'of', 'concert', '##a', '-', 'not', 'much', 'happening', 'about', 'three', 'and', 'a', 'half', 'hours', 'ago', 'i', 'took', 'one', '18', 'mg', 'capsule', 'of', 'concert', '##a', 'and', 'well', '.', '.', '.', 'i', 'don', \"'\", 't', 'feel', 'much', 'of', 'anything', '.', 'i', 'had', 'breakfast', 'around', 'half', 'an', 'hour', 'after', 'i', 'took', 'it', 'and', 'i', \"'\", 've', 'been', 'drinking', 'water', 'du', '##ti', '##fully', 'every', 'now', 'and', 'then', '.', 'i', \"'\", 've', 'also', 'had', 'a', 'cup', 'of', 'tea', 'and', 'a', 'few', 'cigarettes', '.', 'if', 'anything', 'i', 'feel', 'a', 'bit', 'let', '##har', '##gic', 'but', 'that', \"'\", 's', 'not', 'unusual', 'since', 'i', 'slept', 'a', 'little', 'less', 'than', 'is', 'usually', 'optimal', 'for', 'me', '.', 'is', 'this', 'normal', '?', 'i', 'was', 'supposed', 'to', 'boost', 'my', 'dose', 'by', 'another', 'capsule', 'a', 'week', 'in', 'but', 'i', 'have', 'half', 'a', 'mind', 'to', 'just', 'do', 'it', 'tomorrow', '.']\n",
      "INFO:__main__:Number of tokens: 136\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', 'dose', 'of', 'concert', '##a', '-', 'not', 'much', 'happening', 'about', 'three', 'and', 'a', 'half', 'hours', 'ago', 'i', 'took', 'one', '18', 'mg', 'capsule', 'of', 'concert', '##a', 'and', 'well', '.', '.', '.', 'i', 'don', \"'\", 't', 'feel', 'much', 'of', 'anything', '.', 'i', 'had', 'breakfast', 'around', 'half', 'an', 'hour', 'after', 'i', 'took', 'it', 'and', 'i', \"'\", 've', 'been', 'drinking', 'water', 'du', '##ti', '##fully', 'every', 'now', 'and', 'then', '.', 'i', \"'\", 've', 'also', 'had', 'a', 'cup', 'of', 'tea', 'and', 'a', 'few', 'cigarettes', '.', 'if', 'anything', 'i', 'feel', 'a', 'bit', 'let', '##har', '##gic', 'but', 'that', \"'\", 's', 'not', 'unusual', 'since', 'i', 'slept', 'a', 'little', 'less', 'than', 'is', 'usually', 'optimal', 'for', 'me', '.', 'is', 'this', 'normal', '?', 'i', 'was', 'supposed', 'to', 'boost', 'my', 'dose', 'by', 'another', 'capsule', 'a', 'week', 'in', 'but', 'i', 'have', 'half', 'a', 'mind', 'to', 'just', 'do', 'it', 'tomorrow', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'have', 'this', 'problem', '?', 'so', 'i', \"'\", 've', 'been', 'realizing', 'i', 'can', \"'\", 't', 'keep', 'focus', 'on', 'things', 'that', 'are', 'important', 'to', 'me', '.', 'it', \"'\", 's', 'almost', 'like', 'a', 'feedback', 'loop', ':', 'i', 'want', 'to', 'get', 'med', '##s', 'for', 'it', ',', 'but', 'i', 'hate', 'the', 'us', 'medical', 'bureau', '##cratic', 'process', 'so', 'much', 'that', 'i', 'just', 'end', 'up', 'putting', 'it', 'off', 'in', 'favor', 'of', 'doing', 'things', 'i', 'actually', 'enjoy', '.', 'i', 'figure', ',', 'if', 'i', 'had', 'the', 'med', '##s', ',', 'i', 'might', 'be', 'able', 'to', 'soldier', 'on', ',', 'but', 'i', 'don', \"'\", 't', '.', 'di', '##tto', 'with', 'banking', '/', 'school', '##work', '/', 'healthcare', '/', 'research', '.', 't', '##l', ';', 'dr', 'i', 'can', \"'\", 't', 'seem', 'to', 'find', 'the', 'motivation', 'to', 'get', 'pills', 'without', 'pills', '.']\n",
      "INFO:__main__:Number of tokens: 128\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'have', 'this', 'problem', '?', 'so', 'i', \"'\", 've', 'been', 'realizing', 'i', 'can', \"'\", 't', 'keep', 'focus', 'on', 'things', 'that', 'are', 'important', 'to', 'me', '.', 'it', \"'\", 's', 'almost', 'like', 'a', 'feedback', 'loop', ':', 'i', 'want', 'to', 'get', 'med', '##s', 'for', 'it', ',', 'but', 'i', 'hate', 'the', 'us', 'medical', 'bureau', '##cratic', 'process', 'so', 'much', 'that', 'i', 'just', 'end', 'up', 'putting', 'it', 'off', 'in', 'favor', 'of', 'doing', 'things', 'i', 'actually', 'enjoy', '.', 'i', 'figure', ',', 'if', 'i', 'had', 'the', 'med', '##s', ',', 'i', 'might', 'be', 'able', 'to', 'soldier', 'on', ',', 'but', 'i', 'don', \"'\", 't', '.', 'di', '##tto', 'with', 'banking', '/', 'school', '##work', '/', 'healthcare', '/', 'research', '.', 't', '##l', ';', 'dr', 'i', 'can', \"'\", 't', 'seem', 'to', 'find', 'the', 'motivation', 'to', 'get', 'pills', 'without', 'pills', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['book', 'review', ':', 'your', 'life', 'can', 'be', 'better', 'by', 'douglas', 'a', '.', 'pu', '##ry', '##ear', 'for', 'years', 'since', 'i', 'was', 'diagnosed', ',', 'i', 'felt', 'like', 'i', 'needed', 'some', 'sort', 'of', 'instruction', 'manual', 'for', 'dealing', 'with', 'my', 'mind', \"'\", 's', 'peculiar', '##ities', '.', 'where', 'other', 'books', ',', '*', 'driven', 'to', 'distraction', '*', 'included', ',', 'offer', 'an', '##ec', '##dote', '##s', 'and', 'general', 'strategies', ',', '*', 'your', 'life', 'can', 'be', 'better', '*', 'is', 'the', 'manual', 'i', \"'\", 've', 'been', 'looking', 'for', '.', 'the', 'author', ',', 'dr', '.', 'pu', '##ry', '##ear', ',', 'has', 'ad', '##hd', 'himself', ',', 'and', 'has', 'treated', 'the', 'subject', 'matter', 'in', 'a', 'personal', 'account', 'of', 'the', 'many', 'strategies', 'he', 'has', 'developed', 'in', 'the', 'course', 'of', 'his', 'life', 'for', 'coping', 'with', 'ad', '##hd', '.', 'the', 'book', 'is', 'laid', 'out', 'well', ',', 'and', 'each', 'chapter', 'is', 'short', 'and', 'con', '##cise', '.', 'i', 'was', 'ri', '##vet', '##ed', 'when', 'i', 'read', 'it', ',', 'and', 'immediately', 'started', 'using', 'a', 'number', 'of', 'the', 'tools', 'in', 'the', 'book', '.', 'this', 'is', 'definitely', 'my', 'missing', 'manual', ',', 'and', 'i', \"'\", 'd', 'recommend', 'it', 'to', 'anyone', 'with', 'ad', '##hd', ',', 'and', 'even', 'people', 'without', 'ad', '##hd', '.']\n",
      "INFO:__main__:Number of tokens: 187\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['book', 'review', ':', 'your', 'life', 'can', 'be', 'better', 'by', 'douglas', 'a', '.', 'pu', '##ry', '##ear', 'for', 'years', 'since', 'i', 'was', 'diagnosed', ',', 'i', 'felt', 'like', 'i', 'needed', 'some', 'sort', 'of', 'instruction', 'manual', 'for', 'dealing', 'with', 'my', 'mind', \"'\", 's', 'peculiar', '##ities', '.', 'where', 'other', 'books', ',', '*', 'driven', 'to', 'distraction', '*', 'included', ',', 'offer', 'an', '##ec', '##dote', '##s', 'and', 'general', 'strategies', ',', '*', 'your', 'life', 'can', 'be', 'better', '*', 'is', 'the', 'manual', 'i', \"'\", 've', 'been', 'looking', 'for', '.', 'the', 'author', ',', 'dr', '.', 'pu', '##ry', '##ear', ',', 'has', 'ad', '##hd', 'himself', ',', 'and', 'has', 'treated', 'the', 'subject', 'matter', 'in', 'a', 'personal', 'account', 'of', 'the', 'many', 'strategies', 'he', 'has', 'developed', 'in', 'the', 'course', 'of', 'his', 'life', 'for', 'coping', 'with', 'ad', '##hd', '.', 'the', 'book', 'is', 'laid', 'out', 'well', ',', 'and', 'each', 'chapter', 'is', 'short', 'and', 'con', '##cise', '.', 'i', 'was', 'ri', '##vet', '##ed', 'when', 'i', 'read', 'it', ',', 'and', 'immediately', 'started', 'using', 'a', 'number', 'of', 'the', 'tools', 'in', 'the', 'book', '.', 'this', 'is', 'definitely', 'my', 'missing', 'manual', ',', 'and', 'i', \"'\", 'd', 'recommend', 'it', 'to', 'anyone', 'with', 'ad', '##hd', ',', 'and', 'even', 'people', 'without', 'ad', '##hd', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['all', '-', 'in', '-', 'one', 'post', ':', 'fascinating', 'video', 'about', 'ad', '##hd', ',', 'plus', 'some', 'ran', '##ting', 'and', 'questions', 'of', 'my', 'own', '*', '*', 'video', '(', '29', 'minutes', ')', '*', '*', '*', '*', '[', 'russell', 'bark', '##ley', 'explains', 'ad', '##hd', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'l', '##yd', '##lit', '##0', '##g', '##z', '##pe', ')', '*', '*', 'i', \"'\", 'm', 'from', 'norway', '.', 'i', 'added', 'norwegian', 'sub', '##titles', 'so', 'my', 'mother', 'could', 'watch', 'it', '.', '*', '*', 'background', '*', '*', 'i', \"'\", 'm', 'a', '29', 'year', 'old', 'male', '.', 'i', 'suspected', 'ad', '##hd', 'for', 'many', 'years', '.', 'finally', 'made', 'my', 'way', 'to', 'a', 'private', '(', 'corporate', ')', 'clinic', 'that', 'specializes', 'in', 'ne', '##uro', '##psy', '##cho', '##logy', '.', 'the', 'clinic', 'found', '\"', 'prep', '##ond', '##eran', '##ce', 'for', 'ad', '##hd', 'problematic', ',', 'predominantly', 'ina', '##tten', '##tive', '.', '\"', 'not', 'very', 'definite', ',', 'and', 'i', 'needed', 'a', 'psychiatrist', 'to', 'verify', 'it', 'and', 'pre', '##scribe', 'med', '##s', ',', 'so', 'my', 'doctor', 'referred', 'me', 'to', 'a', 'public', '(', 'government', ')', 'poly', '##cl', '##ini', '##c', '.', '*', '*', 'in', '##patient', 'clinic', '*', '*', 'i', 'had', 'trouble', 'showing', 'up', 'for', 'appointments', 'and', 'spoke', 'of', 'being', 'depressed', ',', 'so', 'i', 'was', 'immediately', 'referred', 'to', 'an', 'in', '##patient', 'clinic', ',', 'where', 'i', 'stayed', 'for', '4', 'weeks', '.', 'i', 'was', 'not', 'taken', 'seriously', 'and', 'the', 'whole', 'place', 'felt', 'like', 'a', 'big', 'kindergarten', 'for', 'grown', '##ups', '.', 'the', 'discharge', 'report', 'from', 'the', 'public', 'in', '##patient', 'clinic', '*', 'dismissed', '*', 'the', 'discharge', 'report', 'from', 'the', 'private', 'clinic', 'as', 'incorrect', ',', 'and', 'i', 'was', 'told', 'that', '*', 'nothing', '*', 'was', 'wrong', 'with', 'me', ',', 'except', 'i', 'had', 'signs', 'of', 'personality', 'disorder', '(', 'not', 'otherwise', 'specified', ')', 'and', 'some', 'na', '##rc', '##iss', '##istic', 'traits', '.', 'it', 'was', 'a', 'ridiculous', 'read', '.', 'the', '*', 'first', 'sentence', '*', 'was', 'factual', '##ly', 'incorrect', ',', 'miss', '##tat', '##ing', 'my', 'town', 'of', 'birth', 'completely', '.', 'this', 'psychiatrist', 'hadn', \"'\", 't', 'even', '*', 'listened', '*', '.', 'the', 'report', 'was', 'stern', 'in', 'tone', ',', 'and', 'went', 'on', 'about', 'how', 'i', 'need', 'to', 'become', 'more', 'responsible', ',', 'which', 'would', 'be', 'hilarious', 'if', 'it', 'wasn', \"'\", 't', 'so', 'in', '##fur', '##iating', '.', 'i', 'should', 'have', 'known', 'it', 'was', 'a', 'mistake', '.', 'they', 'made', '*', 'no', 'attempt', '*', 'at', 'rep', '##rod', '##uc', '##ing', 'the', 'results', 'from', 'the', 'private', 'out', '##patient', 'clinic', ',', 'instead', 'opt', '##ing', 'to', 'merely', 'observe', 'my', 'day', '-', 'to', '-', 'day', 'behavior', '.', 'in', 'my', 'case', ',', 'this', 'consisted', 'mainly', 'of', 'surfing', 'red', '##dit', 'on', 'my', 'laptop', ',', 'and', 'eating', 'at', 'the', 'designated', 'meal', '##time', '##s', '.', 'the', 'report', 'ended', 'up', '*', 'criticizing', '*', 'me', 'for', 'failing', 'to', 'remember', 'appointments', ',', 'for', 'failing', 'to', 'remember', 'my', 'insulin', '(', 'i', 'have', 'diabetes', 'type', 'ii', ')', 'and', 'for', 'failing', 'to', 'get', 'up', 'in', 'the', 'morning', ',', 'saying', 'that', 'i', 'should', 'be', 'taking', 'this', 'responsibility', 'myself', '.', 'i', 'found', 'this', 'very', 'ridiculous', ',', 'considering', 'all', 'the', 'trouble', 'this', 'so', '-', 'called', 'lack', 'of', 'responsibility', 'has', 'caused', 'in', 'my', 'life', '.', '*', '*', 'helpless', '##ness', '*', '*', 'my', 'story', 'reflects', 'those', 'of', 'others', 'with', 'ad', '##hd', '.', 'it', 'almost', 'doesn', \"'\", 't', 'bear', 'repeating', ',', 'because', 'it', \"'\", 's', 'so', 'classic', '.', 'school', 'issues', ',', 'spa', '##cing', 'out', ',', 'shi', '##rkin', '##g', 'duties', ',', 'doesn', \"'\", 't', 'listen', 'when', 'spoken', 'to', ',', 'dropped', 'out', 'of', 'high', 'school', ',', 'fired', 'from', 'every', 'job', ',', 'tried', 'school', 'again', 'recently', 'and', 'failed', ',', 'forget', '##s', 'things', ',', 'forget', '##s', 'appointments', ',', 'forget', '##s', 'house', '##work', ',', 'constantly', 'pro', '##cr', '##ast', '##inates', ',', 'some', 'use', 'of', 'marijuana', ',', 'socially', 'una', '##sser', '##tive', 'and', 'anxious', ',', 'and', 'most', 'important', 'of', 'all', ':', 'i', 'feel', 'very', 'unhappy', 'about', 'all', 'of', 'these', 'things', ',', 'but', 'i', 'feel', 'helpless', 'about', 'doing', 'anything', 'about', 'them', '.', '*', '*', 'family', '*', '*', 'my', 'parents', 'are', 'gradually', 'beginning', 'to', 'understand', 'that', 'my', 'problem', 'is', 'a', '*', 'little', '*', 'worse', 'than', '\"', 'being', 'ir', '##res', '##pon', '##sible', '\"', 'and', 'i', 'have', 'basically', 'completed', 'the', 'puzzle', '.', 'all', 'my', 'unusual', 'issues', ',', 'the', 'ones', 'that', 'people', 'get', 'impatient', 'with', ',', 'can', 'be', 'attributed', 'to', 'ad', '##hd', '.', 'i', 'honestly', 'feel', 'that', 'i', \"'\", 've', 'done', 'my', 'best', 'to', 'be', 'a', 'productive', 'individual', ',', 'and', 'i', \"'\", 'm', 'tired', 'of', 'failing', '.', '*', '*', 'sympathy', '*', '*', 'i', 'found', 'a', 'norwegian', 'ad', '##hd', 'forum', '.', 'it', \"'\", 's', 'small', ',', 'like', 'most', 'ad', '##hd', 'communities', 'i', \"'\", 've', 'found', '.', 'apparently', ',', 'society', 'doesn', \"'\", 't', 'have', 'much', 'sympathy', 'for', 'the', 'issue', '.', 'i', \"'\", 've', 'encountered', '*', 'so', '*', 'many', 'people', 'who', 'are', 'ignorant', 'about', 'the', 'disease', '.', 'they', 'all', 'insist', 'on', 'at', '##tri', '##bu', '##ting', 'my', 'issues', 'to', '*', 'bad', 'morals', '*', '.', 'even', 'when', 'i', 'show', 'them', 'scientific', 'research', 'that', 'proves', 'beyond', 'doubt', 'that', 'will', '##power', 'is', 'a', 'result', 'of', 'ne', '##uro', '##biology', ',', 'and', 'not', 'how', 'your', 'parents', 'raised', 'you', ',', 'they', '*', 'refuse', '*', 'to', 'believe', 'it', '.', 'it', 'does', 'not', 'penetrate', 'their', 'dense', 'skulls', '.', 'even', 'my', 'mother', 'has', 'trouble', 'truly', '*', 'getting', '*', 'it', '.', 'at', 'least', 'she', \"'\", 's', 'supportive', 'now', '.', 'she', \"'\", 's', 'seen', 'all', 'the', 'trouble', 'i', \"'\", 've', 'been', 'through', '.', 'she', \"'\", 's', 'seen', 'what', 'ad', '##hd', 'has', 'done', 'to', 'my', 'sister', \"'\", 's', 'life', '.', 'i', 'think', 'it', \"'\", 's', 'beginning', 'to', 'penetrate', ',', 'but', 'god', 'damn', ',', 'people', 'are', 'slow', '!', 'ah', '##hh', '##hh', '!', 'god', ',', 'that', 'ran', '##t', 'felt', 'good', '.', '.', '.', 'the', 'people', 'in', 'the', 'norwegian', 'ad', '##hd', 'forums', 'seem', 'to', 'prefer', 'private', 'clinics', ',', 'so', 'i', \"'\", 'm', 'probably', 'going', 'to', 'try', 'that', 'next', ',', 'even', 'though', 'it', \"'\", 's', 'very', 'expensive', '.', '*', '*', 'questions', '*', '*', 'do', 'any', 'of', 'you', 'have', 'similar', 'experiences', 'to', 'this', '?', 'did', 'you', 'have', 'to', 'fight', 'to', 'get', 'recognition', ',', 'even', 'from', 'professionals', '?', 'did', 'you', 'have', 'to', 'switch', 'doctors', 'until', 'you', 'found', 'one', 'that', 'believed', 'in', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 977\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['all', '-', 'in', '-', 'one', 'post', ':', 'fascinating', 'video', 'about', 'ad', '##hd', ',', 'plus', 'some', 'ran', '##ting', 'and', 'questions', 'of', 'my', 'own', '*', '*', 'video', '(', '29', 'minutes', ')', '*', '*', '*', '*', '[', 'russell', 'bark', '##ley', 'explains', 'ad', '##hd', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'l', '##yd', '##lit', '##0', '##g', '##z', '##pe', ')', '*', '*', 'i', \"'\", 'm', 'from', 'norway', '.', 'i', 'added', 'norwegian', 'sub', '##titles', 'so', 'my', 'mother', 'could', 'watch', 'it', '.', '*', '*', 'background', '*', '*', 'i', \"'\", 'm', 'a', '29', 'year', 'old', 'male', '.', 'i', 'suspected', 'ad', '##hd', 'for', 'many', 'years', '.', 'finally', 'made', 'my', 'way', 'to', 'a', 'private', '(', 'corporate', ')', 'clinic', 'that', 'specializes', 'in', 'ne', '##uro', '##psy', '##cho', '##logy', '.', 'the', 'clinic', 'found', '\"', 'prep', '##ond', '##eran', '##ce', 'for', 'ad', '##hd', 'problematic', ',', 'predominantly', 'ina', '##tten', '##tive', '.', '\"', 'not', 'very', 'definite', ',', 'and', 'i', 'needed', 'a', 'psychiatrist', 'to', 'verify', 'it', 'and', 'pre', '##scribe', 'med', '##s', ',', 'so', 'my', 'doctor', 'referred', 'me', 'to', 'a', 'public', '(', 'government', ')', 'poly', '##cl', '##ini', '##c', '.', '*', '*', 'in', '##patient', 'clinic', '*', '*', 'i', 'had', 'trouble', 'showing', 'up', 'for', 'appointments', 'and', 'spoke', 'of', 'being', 'depressed', ',', 'so', 'i', 'was', 'immediately', 'referred', 'to', 'an', 'in', '##patient', 'clinic', ',', 'where', 'i', 'stayed', 'for', '4', 'weeks', '.', 'i', 'was', 'not', 'taken', 'seriously', 'and', 'the', 'whole', 'place', 'felt', 'like', 'a', 'big', 'kindergarten', 'for', 'grown', '##ups', '.', 'the', 'discharge', 'report', 'from', 'the', 'public', 'in', '##patient', 'clinic', '*', 'dismissed', '*', 'the', 'discharge', 'report', 'from', 'the', 'private', 'clinic', 'as', 'incorrect', ',', 'and', 'i', 'was', 'told', 'that', '*', 'nothing', '*', 'was', 'wrong', 'with', 'me', ',', 'except', 'i', 'had', 'signs', 'of', 'personality', 'disorder', '(', 'not', 'otherwise', 'specified', ')', 'and', 'some', 'na', '##rc', '##iss', '##istic', 'traits', '.', 'it', 'was', 'a', 'ridiculous', 'read', '.', 'the', '*', 'first', 'sentence', '*', 'was', 'factual', '##ly', 'incorrect', ',', 'miss', '##tat', '##ing', 'my', 'town', 'of', 'birth', 'completely', '.', 'this', 'psychiatrist', 'hadn', \"'\", 't', 'even', '*', 'listened', '*', '.', 'the', 'report', 'was', 'stern', 'in', 'tone', ',', 'and', 'went', 'on', 'about', 'how', 'i', 'need', 'to', 'become', 'more', 'responsible', ',', 'which', 'would', 'be', 'hilarious', 'if', 'it', 'wasn', \"'\", 't', 'so', 'in', '##fur', '##iating', '.', 'i', 'should', 'have', 'known', 'it', 'was', 'a', 'mistake', '.', 'they', 'made', '*', 'no', 'attempt', '*', 'at', 'rep', '##rod', '##uc', '##ing', 'the', 'results', 'from', 'the', 'private', 'out', '##patient', 'clinic', ',', 'instead', 'opt', '##ing', 'to', 'merely', 'observe', 'my', 'day', '-', 'to', '-', 'day', 'behavior', '.', 'in', 'my', 'case', ',', 'this', 'consisted', 'mainly', 'of', 'surfing', 'red', '##dit', 'on', 'my', 'laptop', ',', 'and', 'eating', 'at', 'the', 'designated', 'meal', '##time', '##s', '.', 'the', 'report', 'ended', 'up', '*', 'criticizing', '*', 'me', 'for', 'failing', 'to', 'remember', 'appointments', ',', 'for', 'failing', 'to', 'remember', 'my', 'insulin', '(', 'i', 'have', 'diabetes', 'type', 'ii', ')', 'and', 'for', 'failing', 'to', 'get', 'up', 'in', 'the', 'morning', ',', 'saying', 'that', 'i', 'should', 'be', 'taking', 'this', 'responsibility', 'myself', '.', 'i', 'found', 'this', 'very', 'ridiculous', ',', 'considering', 'all', 'the', 'trouble', 'this', 'so', '-', 'called', 'lack', 'of', 'responsibility', 'has', 'caused', 'in', 'my', 'life', '.', '*', '*', 'helpless', '##ness', '*', '*', 'my', 'story', 'reflects', 'those', 'of', 'others', 'with', 'ad', '##hd', '.', 'it'], ['almost', 'doesn', \"'\", 't', 'bear', 'repeating', ',', 'because', 'it', \"'\", 's', 'so', 'classic', '.', 'school', 'issues', ',', 'spa', '##cing', 'out', ',', 'shi', '##rkin', '##g', 'duties', ',', 'doesn', \"'\", 't', 'listen', 'when', 'spoken', 'to', ',', 'dropped', 'out', 'of', 'high', 'school', ',', 'fired', 'from', 'every', 'job', ',', 'tried', 'school', 'again', 'recently', 'and', 'failed', ',', 'forget', '##s', 'things', ',', 'forget', '##s', 'appointments', ',', 'forget', '##s', 'house', '##work', ',', 'constantly', 'pro', '##cr', '##ast', '##inates', ',', 'some', 'use', 'of', 'marijuana', ',', 'socially', 'una', '##sser', '##tive', 'and', 'anxious', ',', 'and', 'most', 'important', 'of', 'all', ':', 'i', 'feel', 'very', 'unhappy', 'about', 'all', 'of', 'these', 'things', ',', 'but', 'i', 'feel', 'helpless', 'about', 'doing', 'anything', 'about', 'them', '.', '*', '*', 'family', '*', '*', 'my', 'parents', 'are', 'gradually', 'beginning', 'to', 'understand', 'that', 'my', 'problem', 'is', 'a', '*', 'little', '*', 'worse', 'than', '\"', 'being', 'ir', '##res', '##pon', '##sible', '\"', 'and', 'i', 'have', 'basically', 'completed', 'the', 'puzzle', '.', 'all', 'my', 'unusual', 'issues', ',', 'the', 'ones', 'that', 'people', 'get', 'impatient', 'with', ',', 'can', 'be', 'attributed', 'to', 'ad', '##hd', '.', 'i', 'honestly', 'feel', 'that', 'i', \"'\", 've', 'done', 'my', 'best', 'to', 'be', 'a', 'productive', 'individual', ',', 'and', 'i', \"'\", 'm', 'tired', 'of', 'failing', '.', '*', '*', 'sympathy', '*', '*', 'i', 'found', 'a', 'norwegian', 'ad', '##hd', 'forum', '.', 'it', \"'\", 's', 'small', ',', 'like', 'most', 'ad', '##hd', 'communities', 'i', \"'\", 've', 'found', '.', 'apparently', ',', 'society', 'doesn', \"'\", 't', 'have', 'much', 'sympathy', 'for', 'the', 'issue', '.', 'i', \"'\", 've', 'encountered', '*', 'so', '*', 'many', 'people', 'who', 'are', 'ignorant', 'about', 'the', 'disease', '.', 'they', 'all', 'insist', 'on', 'at', '##tri', '##bu', '##ting', 'my', 'issues', 'to', '*', 'bad', 'morals', '*', '.', 'even', 'when', 'i', 'show', 'them', 'scientific', 'research', 'that', 'proves', 'beyond', 'doubt', 'that', 'will', '##power', 'is', 'a', 'result', 'of', 'ne', '##uro', '##biology', ',', 'and', 'not', 'how', 'your', 'parents', 'raised', 'you', ',', 'they', '*', 'refuse', '*', 'to', 'believe', 'it', '.', 'it', 'does', 'not', 'penetrate', 'their', 'dense', 'skulls', '.', 'even', 'my', 'mother', 'has', 'trouble', 'truly', '*', 'getting', '*', 'it', '.', 'at', 'least', 'she', \"'\", 's', 'supportive', 'now', '.', 'she', \"'\", 's', 'seen', 'all', 'the', 'trouble', 'i', \"'\", 've', 'been', 'through', '.', 'she', \"'\", 's', 'seen', 'what', 'ad', '##hd', 'has', 'done', 'to', 'my', 'sister', \"'\", 's', 'life', '.', 'i', 'think', 'it', \"'\", 's', 'beginning', 'to', 'penetrate', ',', 'but', 'god', 'damn', ',', 'people', 'are', 'slow', '!', 'ah', '##hh', '##hh', '!', 'god', ',', 'that', 'ran', '##t', 'felt', 'good', '.', '.', '.', 'the', 'people', 'in', 'the', 'norwegian', 'ad', '##hd', 'forums', 'seem', 'to', 'prefer', 'private', 'clinics', ',', 'so', 'i', \"'\", 'm', 'probably', 'going', 'to', 'try', 'that', 'next', ',', 'even', 'though', 'it', \"'\", 's', 'very', 'expensive', '.', '*', '*', 'questions', '*', '*', 'do', 'any', 'of', 'you', 'have', 'similar', 'experiences', 'to', 'this', '?', 'did', 'you', 'have', 'to', 'fight', 'to', 'get', 'recognition', ',', 'even', 'from', 'professionals', '?', 'did', 'you', 'have', 'to', 'switch', 'doctors', 'until', 'you', 'found', 'one', 'that', 'believed', 'in', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['advice', 'on', 'disc', '##los', '##ing', 'add', 'during', 'interviews', 'a', 'little', 'background', '.', 'i', \"'\", 'm', '25', ',', 'graduated', 'with', 'a', 'finance', 'degree', 'and', 'looking', 'for', 'a', 'new', 'position', '.', 'i', 'was', 'diagnosed', 'with', 'add', 'last', 'january', 'after', 'my', 'life', 'almost', 'falling', 'apart', 'in', 'the', 'few', 'years', 'after', 'graduation', 'and', 'am', 'now', 'fairly', 'certain', 'what', 'i', 'need', 'in', 'a', 'position', 'to', 'feel', 'good', 'about', 'what', 'i', 'do', '.', 'with', 'the', 'stigma', 'add', 'holds', 'with', 'some', 'people', 'should', 'i', 'disclose', 'that', 'i', 'have', 'this', 'at', 'any', 'point', 'in', 'the', 'interview', 'process', '?']\n",
      "INFO:__main__:Number of tokens: 90\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['advice', 'on', 'disc', '##los', '##ing', 'add', 'during', 'interviews', 'a', 'little', 'background', '.', 'i', \"'\", 'm', '25', ',', 'graduated', 'with', 'a', 'finance', 'degree', 'and', 'looking', 'for', 'a', 'new', 'position', '.', 'i', 'was', 'diagnosed', 'with', 'add', 'last', 'january', 'after', 'my', 'life', 'almost', 'falling', 'apart', 'in', 'the', 'few', 'years', 'after', 'graduation', 'and', 'am', 'now', 'fairly', 'certain', 'what', 'i', 'need', 'in', 'a', 'position', 'to', 'feel', 'good', 'about', 'what', 'i', 'do', '.', 'with', 'the', 'stigma', 'add', 'holds', 'with', 'some', 'people', 'should', 'i', 'disclose', 'that', 'i', 'have', 'this', 'at', 'any', 'point', 'in', 'the', 'interview', 'process', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['louis', 'the', '##rou', '##x', 'americas', 'med', '##icated', 'kids']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['louis', 'the', '##rou', '##x', 'americas', 'med', '##icated', 'kids']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['growing', 'list', 'of', 'positive', 'effects', 'of', 'nico', '##tine', 'seen', 'in', 'ne', '##uro', '##de', '##gen', '##erative', 'disorders', '(', 'ne', '##uro', '##logy', 'today', ',', '2012', ')']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['growing', 'list', 'of', 'positive', 'effects', 'of', 'nico', '##tine', 'seen', 'in', 'ne', '##uro', '##de', '##gen', '##erative', 'disorders', '(', 'ne', '##uro', '##logy', 'today', ',', '2012', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['growing', 'list', 'of', 'positive', 'effects', 'of', 'nico', '##tine', 'seen', 'for', 'ne', '##uro', '##de', '##gen', '##erative', 'disorders', '(', 'ne', '##uro', '##logy', 'today', ',', '2012', ')']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['growing', 'list', 'of', 'positive', 'effects', 'of', 'nico', '##tine', 'seen', 'for', 'ne', '##uro', '##de', '##gen', '##erative', 'disorders', '(', 'ne', '##uro', '##logy', 'today', ',', '2012', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'tried', 'the', 'fein', '##gold', 'diet', '?', 'my', 'son', 'is', '5', 'and', ',', 'though', 'he', \"'\", 's', 'too', 'young', 'to', 'be', 'diagnosed', 'as', 'ad', '##hd', ',', 'that', 'is', 'the', 'treatment', 'plan', 'that', 'his', 'doctor', 'is', 'following', '.', 'we', \"'\", 've', 'told', 'her', 'that', 'we', 'don', \"'\", 't', 'want', 'to', 'put', 'him', 'on', 'medication', 'and', 'we', \"'\", 're', 'working', 'on', 'behavior', 'modification', '.', 'we', 'talked', 'some', 'about', 'his', 'diet', ',', 'though', 'we', 'already', 'don', \"'\", 't', 'do', 'artificial', 'sweet', '##ener', '##s', 'or', 'dye', '##s', ',', 'h', '##fc', '##s', ',', 'or', 'dairy', '.', 'she', 'did', 'mention', 'that', 'some', 'parents', 'think', 'going', 'g', '##lu', '##ten', 'free', 'helps', ',', 'but', 'the', 'studies', 'don', \"'\", 't', 'support', 'it', '.', 'from', 'the', 'research', 'i', \"'\", 've', 'been', 'doing', 'on', 'my', 'own', 'i', \"'\", 've', 'come', 'across', 'mention', 'of', 'the', 'fein', '##gold', 'diet', 'multiple', 'times', 'with', 'many', 'people', 'swearing', 'by', 'it', '-', 'the', 'diet', 'program', 'through', 'the', 'website', 'is', 'fairly', 'expensive', 'though', 'and', 'anything', 'like', 'that', ',', 'to', 'me', ',', 'automatically', 'smells', 'like', 'a', 'sc', '##am', '-', 'so', 'i', \"'\", 'm', 'wondering', 'if', 'anyone', 'has', 'tried', 'or', 'has', 'successfully', 'tried', 'this', 'diet', 'and', 'was', 'hoping', 'you', 'could', 'explain', 'it', 'a', 'little', 'more', '.', 'thanks', 'in', 'advance', '!']\n",
      "INFO:__main__:Number of tokens: 201\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'tried', 'the', 'fein', '##gold', 'diet', '?', 'my', 'son', 'is', '5', 'and', ',', 'though', 'he', \"'\", 's', 'too', 'young', 'to', 'be', 'diagnosed', 'as', 'ad', '##hd', ',', 'that', 'is', 'the', 'treatment', 'plan', 'that', 'his', 'doctor', 'is', 'following', '.', 'we', \"'\", 've', 'told', 'her', 'that', 'we', 'don', \"'\", 't', 'want', 'to', 'put', 'him', 'on', 'medication', 'and', 'we', \"'\", 're', 'working', 'on', 'behavior', 'modification', '.', 'we', 'talked', 'some', 'about', 'his', 'diet', ',', 'though', 'we', 'already', 'don', \"'\", 't', 'do', 'artificial', 'sweet', '##ener', '##s', 'or', 'dye', '##s', ',', 'h', '##fc', '##s', ',', 'or', 'dairy', '.', 'she', 'did', 'mention', 'that', 'some', 'parents', 'think', 'going', 'g', '##lu', '##ten', 'free', 'helps', ',', 'but', 'the', 'studies', 'don', \"'\", 't', 'support', 'it', '.', 'from', 'the', 'research', 'i', \"'\", 've', 'been', 'doing', 'on', 'my', 'own', 'i', \"'\", 've', 'come', 'across', 'mention', 'of', 'the', 'fein', '##gold', 'diet', 'multiple', 'times', 'with', 'many', 'people', 'swearing', 'by', 'it', '-', 'the', 'diet', 'program', 'through', 'the', 'website', 'is', 'fairly', 'expensive', 'though', 'and', 'anything', 'like', 'that', ',', 'to', 'me', ',', 'automatically', 'smells', 'like', 'a', 'sc', '##am', '-', 'so', 'i', \"'\", 'm', 'wondering', 'if', 'anyone', 'has', 'tried', 'or', 'has', 'successfully', 'tried', 'this', 'diet', 'and', 'was', 'hoping', 'you', 'could', 'explain', 'it', 'a', 'little', 'more', '.', 'thanks', 'in', 'advance', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'it', 'dangerous', 'to', 'drink', 'while', 'on', 'dex', '##ed', '##rine', '?', 'title', ':', ')']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'it', 'dangerous', 'to', 'drink', 'while', 'on', 'dex', '##ed', '##rine', '?', 'title', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'relate', 'to', 'this', ',', 'and', 'it', 'makes', 'me', 'sad', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'relate', 'to', 'this', ',', 'and', 'it', 'makes', 'me', 'sad', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'kind', 'of', 'jobs', 'can', 'a', 'non', '-', 'med', '##icated', 'add', '/', 'ad', '##hd', 'person', 'do', 'and', 'still', 'be', 'successful', 'red', '##dit', 'd', ':', '?', 'jobs', 'where', 'i', 'don', \"'\", 't', 'have', 'to', 'look', 'at', 'too', 'many', 'details', 'or', 'coordinate', 'things', '.', 'also', 'not', 'things', 'where', 'it', \"'\", 's', 'very', 'easy', 'to', 'get', 'other', 'people', 'and', 'myself', 'hurt', 'like', 'working', 'with', 'hot', 'things', 'or', 'driving', '.', 'i', 'don', \"'\", 't', 'need', 'a', 'lot', 'of', 'money', '.', 'edit', ':', 'thank', 'you', 'so', 'much', 'for', 'your', 'suggestions', 'guys', '!', 'i', 'forgot', 'to', 'say', 'what', 'i', \"'\", 'm', 'studying', 'in', 'my', 'haste', ':', 'computer', 'engineering', ',', 'i', 'hate', 'coding', 'and', 'math', '-', 'which', 'i', 'keep', 'learning', 'the', 'hard', 'way', '.', 'somehow', 'managed', 'to', 'pass', 'most', 'of', 'the', 'subjects', 'and', 'am', 'in', 'my', 'last', 'semester', 'with', 'a', 'shit', 'ton', '##ne', 'of', 'subjects', 'and', 'a', 'final', 'year', 'thesis', '.', 'i', 'really', 'want', 'to', 'quit', 'but', 'so', 'much', 'money', 'and', 'time', 'has', 'been', 'sunk', 'and', 'it', \"'\", 's', 'the', 'last', 'freaking', 'se', '##m', ':', '(', '(', '.', 'i', \"'\", 'm', 'in', 'singapore', 'and', 'a', 'bit', 'too', 'poor', 'for', 'the', 'private', 'medical', 'ps', '##ych', 'clinics', 'that', 'dia', '##gno', '##se', 'and', 'treat', 'add', ':', '(', '.', 'and', 'even', 'if', 'i', 'get', 'diagnosed', ',', 'that', 'process', 'will', 'take', 'months', 'and', 'it', \"'\", 's', 'a', 'bit', 'too', 'late', 'at', 'that', 'point', 'for', 'saving', 'my', 'degree', 'ha', '##ha', '.', '.']\n",
      "INFO:__main__:Number of tokens: 229\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'kind', 'of', 'jobs', 'can', 'a', 'non', '-', 'med', '##icated', 'add', '/', 'ad', '##hd', 'person', 'do', 'and', 'still', 'be', 'successful', 'red', '##dit', 'd', ':', '?', 'jobs', 'where', 'i', 'don', \"'\", 't', 'have', 'to', 'look', 'at', 'too', 'many', 'details', 'or', 'coordinate', 'things', '.', 'also', 'not', 'things', 'where', 'it', \"'\", 's', 'very', 'easy', 'to', 'get', 'other', 'people', 'and', 'myself', 'hurt', 'like', 'working', 'with', 'hot', 'things', 'or', 'driving', '.', 'i', 'don', \"'\", 't', 'need', 'a', 'lot', 'of', 'money', '.', 'edit', ':', 'thank', 'you', 'so', 'much', 'for', 'your', 'suggestions', 'guys', '!', 'i', 'forgot', 'to', 'say', 'what', 'i', \"'\", 'm', 'studying', 'in', 'my', 'haste', ':', 'computer', 'engineering', ',', 'i', 'hate', 'coding', 'and', 'math', '-', 'which', 'i', 'keep', 'learning', 'the', 'hard', 'way', '.', 'somehow', 'managed', 'to', 'pass', 'most', 'of', 'the', 'subjects', 'and', 'am', 'in', 'my', 'last', 'semester', 'with', 'a', 'shit', 'ton', '##ne', 'of', 'subjects', 'and', 'a', 'final', 'year', 'thesis', '.', 'i', 'really', 'want', 'to', 'quit', 'but', 'so', 'much', 'money', 'and', 'time', 'has', 'been', 'sunk', 'and', 'it', \"'\", 's', 'the', 'last', 'freaking', 'se', '##m', ':', '(', '(', '.', 'i', \"'\", 'm', 'in', 'singapore', 'and', 'a', 'bit', 'too', 'poor', 'for', 'the', 'private', 'medical', 'ps', '##ych', 'clinics', 'that', 'dia', '##gno', '##se', 'and', 'treat', 'add', ':', '(', '.', 'and', 'even', 'if', 'i', 'get', 'diagnosed', ',', 'that', 'process', 'will', 'take', 'months', 'and', 'it', \"'\", 's', 'a', 'bit', 'too', 'late', 'at', 'that', 'point', 'for', 'saving', 'my', 'degree', 'ha', '##ha', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['should', 'i', 'talk', 'to', 'a', 'doctor', '.', '.', '.', 'or', 'get', 'tested', ',', 'i', 'don', \"'\", 't', 'know', 'the', 'term']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['should', 'i', 'talk', 'to', 'a', 'doctor', '.', '.', '.', 'or', 'get', 'tested', ',', 'i', 'don', \"'\", 't', 'know', 'the', 'term']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['bad', 'dex', '##ed', '##rine', 'reaction', '?', 'when', 'i', 'was', 'on', 'medication', '(', 'i', \"'\", 've', 'been', 'off', 'for', '4', 'years', 'i', 'think', ')', 'i', 'found', 'it', 'made', 'me', 'focused', 'in', 'the', 'daytime', ',', 'but', 'in', 'the', 'night', ',', 'when', 'it', 'wore', 'off', ',', 'i', 'was', 'a', 'mess', '.', 'i', \"'\", 'd', 'be', 'so', 'depressed', 'and', 'just', 'cry', 'and', 'feel', 'so', 'numb', 'and', 'alone', 'and', 'suicidal', '.', 'i', 'don', \"'\", 't', 'doubt', 'this', 'is', 'due', 'to', 'the', 'changing', 'of', 'brain', 'chemicals', 'but', 'has', 'anyone', 'else', 'experienced', 'this', '?', 'i', 'went', 'off', 'it', 'because', 'i', 'was', 'sick', 'of', 'feeling', 'like', 'this', '.', 'i', \"'\", 'd', 'hurt', 'myself', 'and', 'people', 'around', 'me', 'enough', '.', 'however', ',', 'lately', 'i', 'feel', 'super', '\"', 'scattered', '\"', '(', 'for', 'lack', 'of', 'a', 'better', 'description', ')', 'and', 'wonder', 'if', 'i', 'should', 'go', 'back', 'on', 'med', '##s', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 141\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['bad', 'dex', '##ed', '##rine', 'reaction', '?', 'when', 'i', 'was', 'on', 'medication', '(', 'i', \"'\", 've', 'been', 'off', 'for', '4', 'years', 'i', 'think', ')', 'i', 'found', 'it', 'made', 'me', 'focused', 'in', 'the', 'daytime', ',', 'but', 'in', 'the', 'night', ',', 'when', 'it', 'wore', 'off', ',', 'i', 'was', 'a', 'mess', '.', 'i', \"'\", 'd', 'be', 'so', 'depressed', 'and', 'just', 'cry', 'and', 'feel', 'so', 'numb', 'and', 'alone', 'and', 'suicidal', '.', 'i', 'don', \"'\", 't', 'doubt', 'this', 'is', 'due', 'to', 'the', 'changing', 'of', 'brain', 'chemicals', 'but', 'has', 'anyone', 'else', 'experienced', 'this', '?', 'i', 'went', 'off', 'it', 'because', 'i', 'was', 'sick', 'of', 'feeling', 'like', 'this', '.', 'i', \"'\", 'd', 'hurt', 'myself', 'and', 'people', 'around', 'me', 'enough', '.', 'however', ',', 'lately', 'i', 'feel', 'super', '\"', 'scattered', '\"', '(', 'for', 'lack', 'of', 'a', 'better', 'description', ')', 'and', 'wonder', 'if', 'i', 'should', 'go', 'back', 'on', 'med', '##s', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 've', 'just', 'be', 'prescribed', 'atom', '##ox', '##eti', '##ne', 'as', ',', 'at', 'the', 'age', 'of', '39', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'add', '.', 'what', 'are', 'your', 'experiences', 'with', 'this', 'drug', '?']\n",
      "INFO:__main__:Number of tokens: 33\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 've', 'just', 'be', 'prescribed', 'atom', '##ox', '##eti', '##ne', 'as', ',', 'at', 'the', 'age', 'of', '39', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'add', '.', 'what', 'are', 'your', 'experiences', 'with', 'this', 'drug', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'deal', 'with', 'people', 'that', 'don', \"'\", 't', 'think', 'ad', '##hd', 'is', 'real', '?']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'deal', 'with', 'people', 'that', 'don', \"'\", 't', 'think', 'ad', '##hd', 'is', 'real', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tal', '##aa', '##sh', 'teaser', '-', 'bollywood', 'movie', 'trailers', '&', 'promo', '##s']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tal', '##aa', '##sh', 'teaser', '-', 'bollywood', 'movie', 'trailers', '&', 'promo', '##s']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['med', '##s', 'have', 'lost', 'their', 'efficacy', ',', 'need', 'advice']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['med', '##s', 'have', 'lost', 'their', 'efficacy', ',', 'need', 'advice']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['was', 'diagnosed', ',', 'med', '##s', 'didn', \"'\", 't', 'help', '.', 'i', 'have', 'decided', 'to', 'deal', 'with', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['was', 'diagnosed', ',', 'med', '##s', 'didn', \"'\", 't', 'help', '.', 'i', 'have', 'decided', 'to', 'deal', 'with', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['til', 'some', 'things', 'i', 'am', 'not', 'good', 'at', '-', '-', '-', 'add', 'tip', 'o', 'the', 'day', '210']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['til', 'some', 'things', 'i', 'am', 'not', 'good', 'at', '-', '-', '-', 'add', 'tip', 'o', 'the', 'day', '210']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['teasing', 'out', 'add', ',', 'depression', ',', 'anxiety', 'from', 'therapy', '.', '.', '.', 'how', 'long', 'does', 'it', 'take', '?', 'should', 'i', 'consider', 'switching', 'providers', '?', 'i', \"'\", 've', 'been', 'seeing', 'a', 'social', 'worker', 'for', 'therapy', 'since', 'late', 'october', 'and', 'i', \"'\", 'm', 'wondering', 'how', 'long', 'it', 'typically', 'takes', 'to', 'come', 'across', 'some', 'understanding', 'of', 'myself', 'or', 'what', 'my', 'troubles', 'might', 'be', 'stemming', 'from', '.', 'we', 'did', 'get', 'rather', 'side', '##tra', '##cked', 'for', 'six', 'weeks', 'with', 'a', 'concussion', 'i', 'got', 'and', 'the', 'holidays', ',', 'though', '.']\n",
      "INFO:__main__:Number of tokens: 84\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['teasing', 'out', 'add', ',', 'depression', ',', 'anxiety', 'from', 'therapy', '.', '.', '.', 'how', 'long', 'does', 'it', 'take', '?', 'should', 'i', 'consider', 'switching', 'providers', '?', 'i', \"'\", 've', 'been', 'seeing', 'a', 'social', 'worker', 'for', 'therapy', 'since', 'late', 'october', 'and', 'i', \"'\", 'm', 'wondering', 'how', 'long', 'it', 'typically', 'takes', 'to', 'come', 'across', 'some', 'understanding', 'of', 'myself', 'or', 'what', 'my', 'troubles', 'might', 'be', 'stemming', 'from', '.', 'we', 'did', 'get', 'rather', 'side', '##tra', '##cked', 'for', 'six', 'weeks', 'with', 'a', 'concussion', 'i', 'got', 'and', 'the', 'holidays', ',', 'though', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['stress', '/', 'anxiety', 'as', 'a', 'coping', 'strategy', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['stress', '/', 'anxiety', 'as', 'a', 'coping', 'strategy', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['while', 'back', 'in', 'therapy', ',', 'it', 'seems', 'most', 'of', 'my', 'problems', 'are', 'connected', 'to', 'add', '.', 'next', 'week', 'i', \"'\", 'm', 'starting', 'with', 'med', '##s', '.', '*', '*', 't', '##l', ';', 'dr', '-', '-', '>', 'i', \"'\", 'm', 'sharing', 'my', 'story', '.', 'if', 'you', \"'\", 're', 'newly', 'diagnosed', ',', 'you', 'might', 'recognize', 'some', 'stuff', ',', 'as', 'i', 'keep', 'recognizing', 'all', 'kinds', 'of', 'things', '.', 'especially', 'if', 'you', \"'\", 're', 'dealing', 'with', 'mood', 'swings', ',', 'depression', 'or', 'anxiety', 'as', 'well', '.', '*', '*', '*', '*', 'enjoy', 'the', 'read', '.', 'reply', 'if', 'you', 'like', 'it', ',', 'ama', 'if', 'you', 'wish', ',', 'or', 'share', 'your', 'own', 'stories', 'if', 'you', 'want', '*', '*', 'i', \"'\", 'm', 'trying', 'to', 'wrap', 'my', 'head', 'around', 'all', 'that', \"'\", 's', 'happening', ',', 'and', 'i', 'found', 'this', 'community', 'and', 'thought', 'i', 'might', 'as', 'well', 'write', 'it', 'down', 'here', '.', 'about', '4', 'years', 'ago', 'my', 'brother', 'told', 'me', 'he', 'had', 'ad', '##hd', '.', 'it', 'didn', \"'\", 't', 'come', 'as', 'a', 'total', 'surprise', 'because', 'he', 'has', 'always', 'been', 'prone', 'to', 'hyper', '##act', '##ivity', '.', 'but', 'as', 'he', 'learned', 'about', 'his', 'disorder', ',', 'he', 'also', 'recognized', 'me', 'in', 'the', 'symptoms', '.', 'he', 'described', 'them', 'to', 'me', 'one', 'night', ',', 'and', 'i', 'just', 'broke', 'down', ':', 'this', 'was', 'exactly', 'me', 'he', 'was', 'describing', ',', 'and', 'i', 'had', 'never', 'realized', 'it', 'was', 'something', 'i', 'could', 'get', 'help', 'with', '.', 'i', 'was', '28', 'at', 'the', 'time', '.', 'just', 'the', 'realization', 'that', 'i', 'wasn', \"'\", 't', 'just', 'lazy', 'and', 'lacking', 'of', 'ambition', ',', 'could', 'have', 'made', 'me', 'feel', 'better', ',', 'but', 'it', 'didn', \"'\", 't', '.', 'i', 'would', 'describe', 'the', 'period', 'after', 'that', 'as', 'a', 'time', 'of', 'mourning', ',', 'because', 'i', 'felt', 'i', 'wouldn', \"'\", 't', 'have', 'wasted', 'years', 'of', 'my', 'life', 'if', 'i', 'had', 'known', 'about', 'this', 'earlier', '.', 'while', 'i', 'already', 'had', 'made', 'an', 'appointment', 'with', 'my', 'gp', ',', 'i', 'got', 'depressed', '.', 'my', 'psychiatrist', 'put', 'me', 'on', 'anti', '##de', '##press', '##ants', 'for', 'a', 'while', 'to', 'get', 'a', 'grip', 'on', 'my', 'mood', ',', 'and', 'i', 'had', 'cognitive', 'therapy', 'to', 'help', 'me', 'counter', 'my', 'automatic', 'negative', 'thoughts', 'and', 'give', 'me', 'some', 'structure', 'in', 'my', 'life', '.', 'everyone', 'suspected', 'add', ',', 'but', 'the', 'diagnosis', 'wasn', \"'\", 't', 'made', '.', 'for', 'my', 'diagnosis', 'to', 'be', 'completed', 'they', 'wanted', 'more', 'info', 'about', 'my', 'early', 'childhood', '.', 'they', 'interviewed', 'me', 'and', 'my', 'parents', ',', 'but', 'didn', \"'\", 't', 'find', 'enough', 'evidence', '.', 'but', ',', 'my', 'psychiatrist', 'explained', ',', 'as', 'i', 'appear', 'to', 'be', 'predominantly', 'ina', '##tten', '##tive', ',', 'it', 'was', 'quite', 'possible', 'that', 'i', 'still', 'had', 'add', '.', 'my', 'family', 'provided', 'enough', 'structure', ',', 'while', 'allowing', 'the', 'kids', 'to', 'have', 'their', 'qui', '##rks', ',', 'and', 'my', 'high', 'intelligence', 'allowed', 'me', 'to', 'cope', 'for', 'a', 'long', 'time', '.', 'so', 'long', 'story', 'short', ':', 'no', 'official', 'diagnosis', ',', 'but', 'we', 'all', 'agreed', 'i', 'had', 'it', 'anyway', '.', 'i', 'didn', \"'\", 't', 'get', 'any', 'med', '##s', 'for', 'it', ',', 'and', 'we', 'focused', 'on', 'the', 'therapy', '.', 'they', 'taught', 'me', 'a', 'bit', 'about', 'providing', 'yourself', 'with', 'structure', 'and', 'making', 'ample', 'use', 'of', 'lists', '.', 'i', 'had', 'already', 'tried', 'getting', 'things', 'done', 'myself', ',', 'but', 'somehow', 'i', 'never', 'managed', 'to', 'implement', 'the', 'entire', 'system', ',', 'always', 'got', 'to', '60', 'or', '70', '%', 'of', 'all', 'the', 'steps', '(', 'now', 'i', 'know', 'why', ',', 'ha', '!', ')', '.', 'the', 'lists', 'never', 'worked', 'perfect', ',', 'nor', 'do', 'they', 'now', ',', 'but', 'it', 'was', 'a', 'huge', 'improvement', '.', 'no', 'more', 'bail', '##iff', '##s', 'at', 'the', 'door', ',', 'because', 'i', 'managed', 'to', 'pay', 'my', 'bills', 'only', 'a', 'month', 'late', ',', 'tops', '.', 'i', 'already', 'had', 'some', 'lingering', 'an', '##xie', '##ties', 'and', 'weird', 'behaviour', '##s', 'that', 'puzzled', 'me', '.', 'i', 'was', 'prone', 'to', 'mood', 'swings', 'and', 'with', 'me', 'recent', 'depression', 'i', 'recognized', 'more', 'periods', 'of', 'depression', 'in', 'my', 'past', '.', 'i', 'noticed', 'i', 'had', 'a', 'fear', 'for', 'telephone', '##s', 'and', 'emails', 'in', 'times', 'of', 'stress', ',', 'which', 'was', 'often', '.', 'i', 'neglected', 'my', 'friends', 'and', 'family', ',', 'while', 'i', 'didn', \"'\", 't', 'want', 'to', '.', 'and', 'when', 'i', 'hadn', \"'\", 't', 'talked', 'to', 'them', 'in', 'a', 'while', ',', 'anxiety', 'took', 'over', 'and', 'i', 'couldn', \"'\", 't', 'call', 'them', 'at', 'all', ',', 'because', 'it', 'had', 'been', 'so', 'long', '.', 'i', 'know', 'begin', 'to', 'see', 'how', 'all', 'that', 'could', 'be', 'a', 'part', 'of', 'add', ',', 'but', 'i', 'did', 'not', 'know', 'that', 'at', 'the', 'time', '.', 'i', 'did', 'express', 'my', 'wish', 'to', 'understand', '*', 'why', '*', 'i', 'did', 'those', 'strange', 'things', ',', 'but', 'my', 'therapist', 'didn', \"'\", 't', 'go', 'along', 'with', 'that', '.', 'she', 'wanted', 'me', 'to', 'focus', 'on', 'cognitive', '##ly', 'counter', '##ing', 'my', \"'\", 'faulty', \"'\", 'thoughts', ',', 'and', 'that', \"'\", 's', 'what', 'i', 'did', '.', 'fast', 'forward', 'to', 'now', '.', 'i', 'felt', 'better', ',', 'had', 'a', 'slightly', 'better', 'focus', 'because', 'of', 'reduced', 'anxiety', ',', 'due', 'to', 'more', 'structure', 'and', 'lists', '.', 'i', 'also', 'became', 'a', 'bit', 'more', 'for', '##giving', 'of', 'myself', ',', 'so', 'that', 'was', 'helpful', 'as', 'well', '.', 'i', 'then', 'went', 'through', 'some', 'very', 'intense', 'and', 'emotional', 'years', '.', 'i', 'became', 'a', 'father', ',', 'i', 'changed', 'jobs', ',', 'my', 'g', '##f', 'got', 'sick', 'after', 'giving', 'birth', 'and', 'had', 'to', 'stay', 'at', 'home', 'for', 'a', 'total', 'of', 'two', 'years', '.', 'all', 'this', 'meant', 'that', 'i', 'had', 'to', 'carry', 'a', 'lot', 'more', 'weight', 'on', 'my', 'shoulders', '.', 'it', 'was', 'hard', ',', 'and', 'i', 'didn', \"'\", 't', 'always', 'feel', 'great', 'about', 'it', ',', 'but', 'i', 'managed', '.', 'and', 'that', 'sometimes', 'amazed', 'me', '.', 'i', 'am', 'now', 'beginning', 'to', 'think', 'that', 'the', 'increased', 'stress', 'in', 'everyday', 'life', 'actually', 'helped', 'me', 'to', 'focus', 'and', 'get', 'shit', 'done', '.', 'the', 'stress', 'just', 'kept', 'going', 'though', '.', 'when', 'my', 'g', '##f', 'finally', 'started', 'feeling', 'better', ',', 'it', 'should', 'have', 'been', 'my', 'time', 'to', 'relax', '.', 'but', 'on', 'the', 'contrary', ',', 'i', 'only', 'felt', 'worse', '.', 'my', 'mood', 'swings', 'returned', '.', 'i', 'couldn', \"'\", 't', 'stand', 'any', 'of', 'her', 'emotions', 'anymore', ',', 'in', 'fear', 'of', 'more', 'stress', 'and', 'what', 'i', 'called', \"'\", 'drama', \"'\", '.', 'we', 'fought', 'a', 'lot', 'and', 'my', 'creeping', 'social', 'anxiety', 'became', 'more', 'and', 'more', 'apparent', '.', 'i', 'returned', 'to', 'the', 'same', 'clinic', ',', 'to', 'get', 'help', 'with', 'what', 'appeared', 'to', 'be', 'a', 'burn', '##out', ',', 'social', 'anxiety', 'and', 'some', 'general', 'problems', 'to', 'feel', 'and', 'undergo', 'my', 'own', 'emotions', '.', 'this', 'time', ',', 'the', 'psychologist', 'who', 'did', 'my', 'intake', 'interview', 'kept', 'mentioning', 'the', 'add', 'again', '.', 'for', 'the', 'first', 'time', 'i', 'heard', 'that', 'my', 'anxiety', 'and', 'mood', 'swings', 'could', 'very', 'well', 'be', 'explained', 'by', 'my', 'add', '.', 'partly', 'because', 'of', 'this', ',', 'i', 'started', 'to', 're', '##rea', '##d', 'the', 'book', '*', 'driven', 'to', 'distraction', '*', '.', 'i', 'started', 'that', 'book', '4', 'years', 'ago', ',', 'but', 'never', 'got', 'past', 'the', 'first', 'chapter', '.', 'not', 'being', 'able', 'to', 'finish', 'books', '(', 'while', 'i', 'love', 'reading', ')', 'was', 'a', 'big', 'issue', 'at', 'that', 'time', '.', 'last', 'week', ',', 'i', 'just', 'skipped', 'to', 'the', 'chapter', 'about', 'adult', 'add', ',', 'and', 'started', 'reading', '.', 'mother', '##fu', '##cking', 'wow', '.', 'while', 'reading', ',', 'i', 'keep', 'making', 'lightning', 'fast', 'connections', 'to', 'my', 'own', 'situation', 'and', 'my', 'conversations', 'with', 'my', 'therapist', '##s', ',', 'then', 'and', 'now', '.', 'you', 'may', 'be', 'familiar', 'with', 'the', 'mind', 'set', 'i', \"'\", 'm', 'describing', ',', 'a', 'great', 'focus', 'that', 'allows', 'you', 'to', 'view', 'things', 'from', 'multiple', 'angles', ',', 'only', 'needing', 'a', 'few', 'dots', 'and', 'already', 'connecting', 'them', 'to', 'create', 'the', 'picture', '.', 'i', 'read', 'about', 'the', 'use', 'of', 'stress', 'and', 'stress', '##ful', 'situations', '.', 'i', \"'\", 'm', 'starting', 'to', 'realize', 'i', 'always', 'did', 'my', 'best', 'work', 'on', 'the', 'brink', 'of', 'anxiety', '.', 'sometimes', 'i', 'screwed', 'up', ',', 'when', 'stress', 'became', 'to', 'big', ',', 'but', 'often', 'i', 'did', 'great', 'amounts', 'of', 'work', ',', 'of', 'high', 'quality', 'and', 'great', 'creativity', ',', 'in', 'small', 'amounts', 'of', 'time', '.', 'under', 'pressure', '.', 'i', 'remember', 'often', 'getting', 'into', 'new', 'projects', 'with', 'an', 'eager', '\"', 'sure', ',', 'i', \"'\", 'll', 'do', 'that', '\"', 'and', 'only', 'then', 'starting', 'to', 'wonder', 'how', 'the', 'hell', 'i', 'was', 'supposed', 'to', 'pull', 'that', 'of', '.', 'and', 'i', 'usually', 'pulled', 'it', 'of', ',', 'with', 'a', 'great', 'sense', 'of', 'focus', 'and', 'a', 'feeling', 'of', 'flow', '.', 'except', 'when', 'i', 'didn', \"'\", 't', ',', 'and', 'i', 'would', 'just', 'panic', '.', 'dreadful', 'feeling', '.', 'i', 'used', 'to', 'work', 'freelance', ',', 'and', 'i', 'had', 'to', 'panic', 'more', 'and', 'more', '.', 'i', 'switched', 'that', 'for', 'a', 'steady', 'job', '.', 'though', 'i', 'had', 'to', 'learn', 'to', 'work', 'in', 'a', 'team', 'and', 'make', 'use', 'of', 'the', 'structure', 'of', 'a', 'real', 'workplace', ',', 'that', 'has', 'been', 'a', 'great', 'help', '.', 'but', 'that', \"'\", 's', 'some', '##ting', 'else', '.', 'forgive', 'me', 'my', 'wandering', 'thoughts', '.', 'i', \"'\", 'm', 'typing', 'this', 'in', 'one', 'go', '.', 'i', 'also', 'read', 'about', 'mood', 'swings', ',', 'about', 'an', 'interior', 'mono', '##logue', 'going', 'off', 'and', 'not', 'being', 'able', 'to', 'break', 'away', 'from', 'it', '.', 'i', 'immediately', 'remember', 'lying', 'in', 'bed', ',', 'almost', 'frozen', ',', 'not', 'being', 'able', 'to', 'get', 'up', ',', 'with', 'my', 'interior', 'mono', '##logue', 'giving', 'me', 'shit', 'about', 'staying', 'in', 'bed', 'and', 'almost', 'screaming', 'at', 'myself', 'that', 'should', 'get', 'up', 'and', 'make', 'something', 'of', 'the', 'day', '.', 'and', 'i', 'would', 'just', 'lay', 'there', '.', 'i', 'used', 'to', 'have', 'that', 'all', 'the', 'time', ',', 'but', 'then', 'i', 'just', 'thought', 'i', 'really', 'was', 'lazy', 'and', 'would', 'just', 'have', 'to', 'push', 'myself', 'harder', '.', 'i', 'read', 'about', 'focus', '##sing', 'around', 'your', 'bad', 'moods', ',', 'getting', 'into', 'a', 'circle', 'of', 'ne', '##gat', '##ivity', ',', 'and', 'i', 'see', 'it', 'happening', 'almost', 'everyday', '.', 'i', 'suddenly', 'realize', 'that', 'my', 'problem', 'with', 'really', 'feeling', 'my', 'feelings', ',', 'has', 'to', 'have', 'strong', 'ties', 'with', 'add', 'as', 'well', '.', 'when', 'i', 'want', 'to', 'really', 'feel', ',', 'and', 'especially', 'when', 'i', 'try', 'to', 'art', '##iculate', 'my', 'feelings', ',', 'when', 'i', 'have', 'to', 'stand', 'still', 'and', 'experience', 'them', ',', 'i', 'just', 'can', \"'\", 't', '.', 'the', 'feelings', 'fade', 'away', ',', 'and', 'my', 'head', 'become', 'the', 'famous', 'radio', 'with', 'dozens', 'of', 'channels', 'switched', 'on', 'at', 'the', 'same', 'time', '.', 'i', 'won', \"'\", 't', 'be', 'troubled', 'with', 'negative', 'thoughts', ',', 'i', 'just', 'think', 'of', 'anything', ',', 'everything', '.', 'i', \"'\", 'll', 'have', 'that', 'interior', 'mono', '##logue', 'running', ',', 'commenting', 'on', 'everything', ',', 'i', \"'\", 'll', 'have', 'silly', 'songs', 'in', 'my', 'head', ',', 'i', \"'\", 'll', 'be', 'thinking', 'of', 'dinner', 'of', 'grocery', 'shopping', '.', 'anything', ',', 'except', 'those', 'feelings', '.', 'so', ',', 'it', 'seems', 'i', 'get', 'some', 'focus', 'and', 'some', 'sense', 'of', 'quiet', 'in', 'my', 'head', 'by', 'pushing', 'away', 'those', 'emotions', ',', 'neglect', '##ing', 'them', '.', 'which', 'i', \"'\", 've', 'become', 'very', 'good', 'at', ',', 'until', 'they', 'totally', 'eng', '##ulf', 'and', 'over', '##w', '##helm', 'me', '.', 'and', 'when', 'i', 'break', 'down', ',', 'crying', 'hysterical', '##ly', ',', 'i', 'often', 'find', 'my', 'self', 'stopping', 'mid', '-', 'sob', ',', 'distracted', 'by', 'a', 'thought', ',', 'nice', 'and', 'calm', 'again', ',', 'only', 'to', 'start', 'sobbing', 'again', 'soon', 'after', '.', '*', 'continued', 'in', 'the', 'comments', '*', '*', '*', 'edit', ':', 'fixed', 'some', 'ty', '##po', '##graphy', '*', '*']\n",
      "INFO:__main__:Number of tokens: 1784\n",
      "INFO:__main__:Number of chunks: 4\n",
      "INFO:__main__:Chunks: [['while', 'back', 'in', 'therapy', ',', 'it', 'seems', 'most', 'of', 'my', 'problems', 'are', 'connected', 'to', 'add', '.', 'next', 'week', 'i', \"'\", 'm', 'starting', 'with', 'med', '##s', '.', '*', '*', 't', '##l', ';', 'dr', '-', '-', '>', 'i', \"'\", 'm', 'sharing', 'my', 'story', '.', 'if', 'you', \"'\", 're', 'newly', 'diagnosed', ',', 'you', 'might', 'recognize', 'some', 'stuff', ',', 'as', 'i', 'keep', 'recognizing', 'all', 'kinds', 'of', 'things', '.', 'especially', 'if', 'you', \"'\", 're', 'dealing', 'with', 'mood', 'swings', ',', 'depression', 'or', 'anxiety', 'as', 'well', '.', '*', '*', '*', '*', 'enjoy', 'the', 'read', '.', 'reply', 'if', 'you', 'like', 'it', ',', 'ama', 'if', 'you', 'wish', ',', 'or', 'share', 'your', 'own', 'stories', 'if', 'you', 'want', '*', '*', 'i', \"'\", 'm', 'trying', 'to', 'wrap', 'my', 'head', 'around', 'all', 'that', \"'\", 's', 'happening', ',', 'and', 'i', 'found', 'this', 'community', 'and', 'thought', 'i', 'might', 'as', 'well', 'write', 'it', 'down', 'here', '.', 'about', '4', 'years', 'ago', 'my', 'brother', 'told', 'me', 'he', 'had', 'ad', '##hd', '.', 'it', 'didn', \"'\", 't', 'come', 'as', 'a', 'total', 'surprise', 'because', 'he', 'has', 'always', 'been', 'prone', 'to', 'hyper', '##act', '##ivity', '.', 'but', 'as', 'he', 'learned', 'about', 'his', 'disorder', ',', 'he', 'also', 'recognized', 'me', 'in', 'the', 'symptoms', '.', 'he', 'described', 'them', 'to', 'me', 'one', 'night', ',', 'and', 'i', 'just', 'broke', 'down', ':', 'this', 'was', 'exactly', 'me', 'he', 'was', 'describing', ',', 'and', 'i', 'had', 'never', 'realized', 'it', 'was', 'something', 'i', 'could', 'get', 'help', 'with', '.', 'i', 'was', '28', 'at', 'the', 'time', '.', 'just', 'the', 'realization', 'that', 'i', 'wasn', \"'\", 't', 'just', 'lazy', 'and', 'lacking', 'of', 'ambition', ',', 'could', 'have', 'made', 'me', 'feel', 'better', ',', 'but', 'it', 'didn', \"'\", 't', '.', 'i', 'would', 'describe', 'the', 'period', 'after', 'that', 'as', 'a', 'time', 'of', 'mourning', ',', 'because', 'i', 'felt', 'i', 'wouldn', \"'\", 't', 'have', 'wasted', 'years', 'of', 'my', 'life', 'if', 'i', 'had', 'known', 'about', 'this', 'earlier', '.', 'while', 'i', 'already', 'had', 'made', 'an', 'appointment', 'with', 'my', 'gp', ',', 'i', 'got', 'depressed', '.', 'my', 'psychiatrist', 'put', 'me', 'on', 'anti', '##de', '##press', '##ants', 'for', 'a', 'while', 'to', 'get', 'a', 'grip', 'on', 'my', 'mood', ',', 'and', 'i', 'had', 'cognitive', 'therapy', 'to', 'help', 'me', 'counter', 'my', 'automatic', 'negative', 'thoughts', 'and', 'give', 'me', 'some', 'structure', 'in', 'my', 'life', '.', 'everyone', 'suspected', 'add', ',', 'but', 'the', 'diagnosis', 'wasn', \"'\", 't', 'made', '.', 'for', 'my', 'diagnosis', 'to', 'be', 'completed', 'they', 'wanted', 'more', 'info', 'about', 'my', 'early', 'childhood', '.', 'they', 'interviewed', 'me', 'and', 'my', 'parents', ',', 'but', 'didn', \"'\", 't', 'find', 'enough', 'evidence', '.', 'but', ',', 'my', 'psychiatrist', 'explained', ',', 'as', 'i', 'appear', 'to', 'be', 'predominantly', 'ina', '##tten', '##tive', ',', 'it', 'was', 'quite', 'possible', 'that', 'i', 'still', 'had', 'add', '.', 'my', 'family', 'provided', 'enough', 'structure', ',', 'while', 'allowing', 'the', 'kids', 'to', 'have', 'their', 'qui', '##rks', ',', 'and', 'my', 'high', 'intelligence', 'allowed', 'me', 'to', 'cope', 'for', 'a', 'long', 'time', '.', 'so', 'long', 'story', 'short', ':', 'no', 'official', 'diagnosis', ',', 'but', 'we', 'all', 'agreed', 'i', 'had', 'it', 'anyway', '.', 'i', 'didn', \"'\", 't', 'get', 'any', 'med', '##s', 'for', 'it', ',', 'and', 'we', 'focused', 'on', 'the', 'therapy', '.', 'they', 'taught', 'me', 'a', 'bit', 'about', 'providing', 'yourself', 'with', 'structure', 'and', 'making', 'ample', 'use', 'of', 'lists', '.', 'i', 'had', 'already', 'tried', 'getting', 'things', 'done', 'myself', ',', 'but', 'somehow'], ['i', 'never', 'managed', 'to', 'implement', 'the', 'entire', 'system', ',', 'always', 'got', 'to', '60', 'or', '70', '%', 'of', 'all', 'the', 'steps', '(', 'now', 'i', 'know', 'why', ',', 'ha', '!', ')', '.', 'the', 'lists', 'never', 'worked', 'perfect', ',', 'nor', 'do', 'they', 'now', ',', 'but', 'it', 'was', 'a', 'huge', 'improvement', '.', 'no', 'more', 'bail', '##iff', '##s', 'at', 'the', 'door', ',', 'because', 'i', 'managed', 'to', 'pay', 'my', 'bills', 'only', 'a', 'month', 'late', ',', 'tops', '.', 'i', 'already', 'had', 'some', 'lingering', 'an', '##xie', '##ties', 'and', 'weird', 'behaviour', '##s', 'that', 'puzzled', 'me', '.', 'i', 'was', 'prone', 'to', 'mood', 'swings', 'and', 'with', 'me', 'recent', 'depression', 'i', 'recognized', 'more', 'periods', 'of', 'depression', 'in', 'my', 'past', '.', 'i', 'noticed', 'i', 'had', 'a', 'fear', 'for', 'telephone', '##s', 'and', 'emails', 'in', 'times', 'of', 'stress', ',', 'which', 'was', 'often', '.', 'i', 'neglected', 'my', 'friends', 'and', 'family', ',', 'while', 'i', 'didn', \"'\", 't', 'want', 'to', '.', 'and', 'when', 'i', 'hadn', \"'\", 't', 'talked', 'to', 'them', 'in', 'a', 'while', ',', 'anxiety', 'took', 'over', 'and', 'i', 'couldn', \"'\", 't', 'call', 'them', 'at', 'all', ',', 'because', 'it', 'had', 'been', 'so', 'long', '.', 'i', 'know', 'begin', 'to', 'see', 'how', 'all', 'that', 'could', 'be', 'a', 'part', 'of', 'add', ',', 'but', 'i', 'did', 'not', 'know', 'that', 'at', 'the', 'time', '.', 'i', 'did', 'express', 'my', 'wish', 'to', 'understand', '*', 'why', '*', 'i', 'did', 'those', 'strange', 'things', ',', 'but', 'my', 'therapist', 'didn', \"'\", 't', 'go', 'along', 'with', 'that', '.', 'she', 'wanted', 'me', 'to', 'focus', 'on', 'cognitive', '##ly', 'counter', '##ing', 'my', \"'\", 'faulty', \"'\", 'thoughts', ',', 'and', 'that', \"'\", 's', 'what', 'i', 'did', '.', 'fast', 'forward', 'to', 'now', '.', 'i', 'felt', 'better', ',', 'had', 'a', 'slightly', 'better', 'focus', 'because', 'of', 'reduced', 'anxiety', ',', 'due', 'to', 'more', 'structure', 'and', 'lists', '.', 'i', 'also', 'became', 'a', 'bit', 'more', 'for', '##giving', 'of', 'myself', ',', 'so', 'that', 'was', 'helpful', 'as', 'well', '.', 'i', 'then', 'went', 'through', 'some', 'very', 'intense', 'and', 'emotional', 'years', '.', 'i', 'became', 'a', 'father', ',', 'i', 'changed', 'jobs', ',', 'my', 'g', '##f', 'got', 'sick', 'after', 'giving', 'birth', 'and', 'had', 'to', 'stay', 'at', 'home', 'for', 'a', 'total', 'of', 'two', 'years', '.', 'all', 'this', 'meant', 'that', 'i', 'had', 'to', 'carry', 'a', 'lot', 'more', 'weight', 'on', 'my', 'shoulders', '.', 'it', 'was', 'hard', ',', 'and', 'i', 'didn', \"'\", 't', 'always', 'feel', 'great', 'about', 'it', ',', 'but', 'i', 'managed', '.', 'and', 'that', 'sometimes', 'amazed', 'me', '.', 'i', 'am', 'now', 'beginning', 'to', 'think', 'that', 'the', 'increased', 'stress', 'in', 'everyday', 'life', 'actually', 'helped', 'me', 'to', 'focus', 'and', 'get', 'shit', 'done', '.', 'the', 'stress', 'just', 'kept', 'going', 'though', '.', 'when', 'my', 'g', '##f', 'finally', 'started', 'feeling', 'better', ',', 'it', 'should', 'have', 'been', 'my', 'time', 'to', 'relax', '.', 'but', 'on', 'the', 'contrary', ',', 'i', 'only', 'felt', 'worse', '.', 'my', 'mood', 'swings', 'returned', '.', 'i', 'couldn', \"'\", 't', 'stand', 'any', 'of', 'her', 'emotions', 'anymore', ',', 'in', 'fear', 'of', 'more', 'stress', 'and', 'what', 'i', 'called', \"'\", 'drama', \"'\", '.', 'we', 'fought', 'a', 'lot', 'and', 'my', 'creeping', 'social', 'anxiety', 'became', 'more', 'and', 'more', 'apparent', '.', 'i', 'returned', 'to', 'the', 'same', 'clinic', ',', 'to', 'get', 'help', 'with', 'what', 'appeared', 'to', 'be', 'a', 'burn', '##out', ',', 'social', 'anxiety', 'and', 'some', 'general', 'problems', 'to', 'feel', 'and', 'undergo', 'my', 'own', 'emotions'], ['.', 'this', 'time', ',', 'the', 'psychologist', 'who', 'did', 'my', 'intake', 'interview', 'kept', 'mentioning', 'the', 'add', 'again', '.', 'for', 'the', 'first', 'time', 'i', 'heard', 'that', 'my', 'anxiety', 'and', 'mood', 'swings', 'could', 'very', 'well', 'be', 'explained', 'by', 'my', 'add', '.', 'partly', 'because', 'of', 'this', ',', 'i', 'started', 'to', 're', '##rea', '##d', 'the', 'book', '*', 'driven', 'to', 'distraction', '*', '.', 'i', 'started', 'that', 'book', '4', 'years', 'ago', ',', 'but', 'never', 'got', 'past', 'the', 'first', 'chapter', '.', 'not', 'being', 'able', 'to', 'finish', 'books', '(', 'while', 'i', 'love', 'reading', ')', 'was', 'a', 'big', 'issue', 'at', 'that', 'time', '.', 'last', 'week', ',', 'i', 'just', 'skipped', 'to', 'the', 'chapter', 'about', 'adult', 'add', ',', 'and', 'started', 'reading', '.', 'mother', '##fu', '##cking', 'wow', '.', 'while', 'reading', ',', 'i', 'keep', 'making', 'lightning', 'fast', 'connections', 'to', 'my', 'own', 'situation', 'and', 'my', 'conversations', 'with', 'my', 'therapist', '##s', ',', 'then', 'and', 'now', '.', 'you', 'may', 'be', 'familiar', 'with', 'the', 'mind', 'set', 'i', \"'\", 'm', 'describing', ',', 'a', 'great', 'focus', 'that', 'allows', 'you', 'to', 'view', 'things', 'from', 'multiple', 'angles', ',', 'only', 'needing', 'a', 'few', 'dots', 'and', 'already', 'connecting', 'them', 'to', 'create', 'the', 'picture', '.', 'i', 'read', 'about', 'the', 'use', 'of', 'stress', 'and', 'stress', '##ful', 'situations', '.', 'i', \"'\", 'm', 'starting', 'to', 'realize', 'i', 'always', 'did', 'my', 'best', 'work', 'on', 'the', 'brink', 'of', 'anxiety', '.', 'sometimes', 'i', 'screwed', 'up', ',', 'when', 'stress', 'became', 'to', 'big', ',', 'but', 'often', 'i', 'did', 'great', 'amounts', 'of', 'work', ',', 'of', 'high', 'quality', 'and', 'great', 'creativity', ',', 'in', 'small', 'amounts', 'of', 'time', '.', 'under', 'pressure', '.', 'i', 'remember', 'often', 'getting', 'into', 'new', 'projects', 'with', 'an', 'eager', '\"', 'sure', ',', 'i', \"'\", 'll', 'do', 'that', '\"', 'and', 'only', 'then', 'starting', 'to', 'wonder', 'how', 'the', 'hell', 'i', 'was', 'supposed', 'to', 'pull', 'that', 'of', '.', 'and', 'i', 'usually', 'pulled', 'it', 'of', ',', 'with', 'a', 'great', 'sense', 'of', 'focus', 'and', 'a', 'feeling', 'of', 'flow', '.', 'except', 'when', 'i', 'didn', \"'\", 't', ',', 'and', 'i', 'would', 'just', 'panic', '.', 'dreadful', 'feeling', '.', 'i', 'used', 'to', 'work', 'freelance', ',', 'and', 'i', 'had', 'to', 'panic', 'more', 'and', 'more', '.', 'i', 'switched', 'that', 'for', 'a', 'steady', 'job', '.', 'though', 'i', 'had', 'to', 'learn', 'to', 'work', 'in', 'a', 'team', 'and', 'make', 'use', 'of', 'the', 'structure', 'of', 'a', 'real', 'workplace', ',', 'that', 'has', 'been', 'a', 'great', 'help', '.', 'but', 'that', \"'\", 's', 'some', '##ting', 'else', '.', 'forgive', 'me', 'my', 'wandering', 'thoughts', '.', 'i', \"'\", 'm', 'typing', 'this', 'in', 'one', 'go', '.', 'i', 'also', 'read', 'about', 'mood', 'swings', ',', 'about', 'an', 'interior', 'mono', '##logue', 'going', 'off', 'and', 'not', 'being', 'able', 'to', 'break', 'away', 'from', 'it', '.', 'i', 'immediately', 'remember', 'lying', 'in', 'bed', ',', 'almost', 'frozen', ',', 'not', 'being', 'able', 'to', 'get', 'up', ',', 'with', 'my', 'interior', 'mono', '##logue', 'giving', 'me', 'shit', 'about', 'staying', 'in', 'bed', 'and', 'almost', 'screaming', 'at', 'myself', 'that', 'should', 'get', 'up', 'and', 'make', 'something', 'of', 'the', 'day', '.', 'and', 'i', 'would', 'just', 'lay', 'there', '.', 'i', 'used', 'to', 'have', 'that', 'all', 'the', 'time', ',', 'but', 'then', 'i', 'just', 'thought', 'i', 'really', 'was', 'lazy', 'and', 'would', 'just', 'have', 'to', 'push', 'myself', 'harder', '.', 'i', 'read', 'about', 'focus', '##sing', 'around', 'your', 'bad', 'moods', ',', 'getting', 'into', 'a', 'circle', 'of', 'ne', '##gat', '##ivity'], [',', 'and', 'i', 'see', 'it', 'happening', 'almost', 'everyday', '.', 'i', 'suddenly', 'realize', 'that', 'my', 'problem', 'with', 'really', 'feeling', 'my', 'feelings', ',', 'has', 'to', 'have', 'strong', 'ties', 'with', 'add', 'as', 'well', '.', 'when', 'i', 'want', 'to', 'really', 'feel', ',', 'and', 'especially', 'when', 'i', 'try', 'to', 'art', '##iculate', 'my', 'feelings', ',', 'when', 'i', 'have', 'to', 'stand', 'still', 'and', 'experience', 'them', ',', 'i', 'just', 'can', \"'\", 't', '.', 'the', 'feelings', 'fade', 'away', ',', 'and', 'my', 'head', 'become', 'the', 'famous', 'radio', 'with', 'dozens', 'of', 'channels', 'switched', 'on', 'at', 'the', 'same', 'time', '.', 'i', 'won', \"'\", 't', 'be', 'troubled', 'with', 'negative', 'thoughts', ',', 'i', 'just', 'think', 'of', 'anything', ',', 'everything', '.', 'i', \"'\", 'll', 'have', 'that', 'interior', 'mono', '##logue', 'running', ',', 'commenting', 'on', 'everything', ',', 'i', \"'\", 'll', 'have', 'silly', 'songs', 'in', 'my', 'head', ',', 'i', \"'\", 'll', 'be', 'thinking', 'of', 'dinner', 'of', 'grocery', 'shopping', '.', 'anything', ',', 'except', 'those', 'feelings', '.', 'so', ',', 'it', 'seems', 'i', 'get', 'some', 'focus', 'and', 'some', 'sense', 'of', 'quiet', 'in', 'my', 'head', 'by', 'pushing', 'away', 'those', 'emotions', ',', 'neglect', '##ing', 'them', '.', 'which', 'i', \"'\", 've', 'become', 'very', 'good', 'at', ',', 'until', 'they', 'totally', 'eng', '##ulf', 'and', 'over', '##w', '##helm', 'me', '.', 'and', 'when', 'i', 'break', 'down', ',', 'crying', 'hysterical', '##ly', ',', 'i', 'often', 'find', 'my', 'self', 'stopping', 'mid', '-', 'sob', ',', 'distracted', 'by', 'a', 'thought', ',', 'nice', 'and', 'calm', 'again', ',', 'only', 'to', 'start', 'sobbing', 'again', 'soon', 'after', '.', '*', 'continued', 'in', 'the', 'comments', '*', '*', '*', 'edit', ':', 'fixed', 'some', 'ty', '##po', '##graphy', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'am', 'really', 'struggling', 'with', 'these', 'ad', '##hd', 'symptoms', '.', 'take', '15', '##mg', 'add', '##eral', '##l', 'twice', 'a', 'day', '.', 'i', 'never', 'thought', 'i', 'really', 'had', 'ad', '##hd', 'growing', 'up', 'and', 'now', 'as', 'an', 'adult', 'my', 'life', 'makes', 'a', 'ton', 'of', 'sense', '.', 'the', 'medication', 'is', 'helping', 'but', 'i', 'just', 'made', 'a', 'list', 'of', 'the', 'symptoms', 'i', \"'\", 'm', 'still', 'struggling', 'with', 'and', 'realized', 'i', 'still', 'have', 'a', 'long', 'way', 'to', 'go', '.', 'i', \"'\", 'm', 'relatively', 'successful', '/', 'ambitious', 'in', 'some', 'parts', 'of', 'my', 'life', 'and', 'a', 'near', 'failure', 'in', 'others', '.', 'it', \"'\", 's', 'a', 'struggle', '.', 'will', 'i', 'ever', 'get', 'these', 'symptoms', 'under', 'control', '?', '-', 'sensitive', 'start', '##le', 'response', '-', 'clumsy', '-', 'poor', 'handwriting', '-', 'always', 'spilling', 'things', '-', \"'\", 'zoning', 'out', \"'\", '-', 'easily', 'distracted', '-', 'poor', 'organization', 'skills', '-', 'pro', '##cr', '##ast', '##ination', '-', 'chronic', 'late', '##ness', '-', 'trouble', 'starting', 'and', 'finishing', 'projects', '-', 'forgetting', 'appointments', ',', 'commitments', ',', 'and', 'deadline', '##s', '-', 'losing', 'or', 'mis', '##pl', '##acing', 'things', '-', 'under', '##est', '##imating', 'the', 'time', 'it', 'takes', 'to', 'complete', 'a', 'task', '-', 'frequently', 'interrupt', 'others', 'or', 'talk', 'over', 'them', '-', 'have', 'poor', 'self', '-', 'control', '-', 'blur', '##t', 'out', 'thoughts', 'that', 'are', 'rude', 'or', 'inappropriate', 'without', 'thinking', '-', 'have', 'addict', '##ive', 'tendencies', '-', 'act', 'reckless', '##ly', 'or', 'spontaneously', 'without', 'regard', 'for', 'consequences', '-', 'have', 'trouble', 'be', '##ha', '##ving', 'in', 'socially', 'appropriate', 'ways', '(', 'such', 'as', 'sitting', 'still', 'during', 'a', 'long', 'meeting', ')', '-', 'sense', 'of', 'under', '##achi', '##eve', '##ment', '-', 'doesn', '’', 't', 'deal', 'well', 'with', 'frustration', '-', 'easily', 'flu', '##stered', 'and', 'stressed', 'out', '-', 'ir', '##rita', '##bility', 'or', 'mood', 'swings', '-', 'hyper', '##sen', '##sit', '##ivity', 'to', 'criticism', '-', 'short', ',', 'often', 'explosive', ',', 'temper', '-', 'low', 'self', '-', 'esteem', 'and', 'sense', 'of', 'ins', '##ec', '##urity', '-', 'feelings', 'of', 'inner', 'restless', '##ness', ',', 'agitation', '-', 'tendency', 'to', 'take', 'risks', '-', 'getting', 'bored', 'easily', '-', 'racing', 'thoughts', '-', 'trouble', 'sitting', 'still', ';', 'constant', 'fi', '##dget', '##ing', '-', 'doing', 'a', 'million', 'things', 'at', 'once']\n",
      "INFO:__main__:Number of tokens: 328\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'am', 'really', 'struggling', 'with', 'these', 'ad', '##hd', 'symptoms', '.', 'take', '15', '##mg', 'add', '##eral', '##l', 'twice', 'a', 'day', '.', 'i', 'never', 'thought', 'i', 'really', 'had', 'ad', '##hd', 'growing', 'up', 'and', 'now', 'as', 'an', 'adult', 'my', 'life', 'makes', 'a', 'ton', 'of', 'sense', '.', 'the', 'medication', 'is', 'helping', 'but', 'i', 'just', 'made', 'a', 'list', 'of', 'the', 'symptoms', 'i', \"'\", 'm', 'still', 'struggling', 'with', 'and', 'realized', 'i', 'still', 'have', 'a', 'long', 'way', 'to', 'go', '.', 'i', \"'\", 'm', 'relatively', 'successful', '/', 'ambitious', 'in', 'some', 'parts', 'of', 'my', 'life', 'and', 'a', 'near', 'failure', 'in', 'others', '.', 'it', \"'\", 's', 'a', 'struggle', '.', 'will', 'i', 'ever', 'get', 'these', 'symptoms', 'under', 'control', '?', '-', 'sensitive', 'start', '##le', 'response', '-', 'clumsy', '-', 'poor', 'handwriting', '-', 'always', 'spilling', 'things', '-', \"'\", 'zoning', 'out', \"'\", '-', 'easily', 'distracted', '-', 'poor', 'organization', 'skills', '-', 'pro', '##cr', '##ast', '##ination', '-', 'chronic', 'late', '##ness', '-', 'trouble', 'starting', 'and', 'finishing', 'projects', '-', 'forgetting', 'appointments', ',', 'commitments', ',', 'and', 'deadline', '##s', '-', 'losing', 'or', 'mis', '##pl', '##acing', 'things', '-', 'under', '##est', '##imating', 'the', 'time', 'it', 'takes', 'to', 'complete', 'a', 'task', '-', 'frequently', 'interrupt', 'others', 'or', 'talk', 'over', 'them', '-', 'have', 'poor', 'self', '-', 'control', '-', 'blur', '##t', 'out', 'thoughts', 'that', 'are', 'rude', 'or', 'inappropriate', 'without', 'thinking', '-', 'have', 'addict', '##ive', 'tendencies', '-', 'act', 'reckless', '##ly', 'or', 'spontaneously', 'without', 'regard', 'for', 'consequences', '-', 'have', 'trouble', 'be', '##ha', '##ving', 'in', 'socially', 'appropriate', 'ways', '(', 'such', 'as', 'sitting', 'still', 'during', 'a', 'long', 'meeting', ')', '-', 'sense', 'of', 'under', '##achi', '##eve', '##ment', '-', 'doesn', '’', 't', 'deal', 'well', 'with', 'frustration', '-', 'easily', 'flu', '##stered', 'and', 'stressed', 'out', '-', 'ir', '##rita', '##bility', 'or', 'mood', 'swings', '-', 'hyper', '##sen', '##sit', '##ivity', 'to', 'criticism', '-', 'short', ',', 'often', 'explosive', ',', 'temper', '-', 'low', 'self', '-', 'esteem', 'and', 'sense', 'of', 'ins', '##ec', '##urity', '-', 'feelings', 'of', 'inner', 'restless', '##ness', ',', 'agitation', '-', 'tendency', 'to', 'take', 'risks', '-', 'getting', 'bored', 'easily', '-', 'racing', 'thoughts', '-', 'trouble', 'sitting', 'still', ';', 'constant', 'fi', '##dget', '##ing', '-', 'doing', 'a', 'million', 'things', 'at', 'once']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'this', 'ad', '##hd', 'or', 'is', 'it', 'just', 'life', '?', 'hello', 'red', '##dit', ',', 'i', \"'\", 'm', 'wondering', 'whether', 'my', 'problems', 'might', 'be', 'due', 'to', 'some', 'kind', 'of', 'ad', '##hd', ',', 'or', 'whether', 'this', 'is', 'just', 'what', 'every', 'human', 'has', 'to', 'deal', 'with', '.', 'mainly', 'i', 'have', 'serious', 'pro', '##cr', '##ast', '##ination', ',', 'which', 'weren', \"'\", 't', 'much', 'of', 'a', 'problem', 'until', 'college', '.', 'i', 'get', 'by', ',', 'but', 'it', 'seems', 'like', 'it', \"'\", 'll', 'prevent', 'me', 'from', 'doing', 'anything', 'but', 'barely', 'getting', 'by', '.', 'i', 'put', 'off', 'just', 'about', 'everything', 'until', 'the', 'day', 'before', ',', 'so', 'i', 'never', 'do', 'a', 'very', 'good', 'job', 'on', 'anything', ',', 'and', 'don', \"'\", 't', 'retain', 'the', 'material', 'much', 'past', 'the', 'end', 'of', 'semester', '.', 'i', 'have', 'a', 'list', 'of', 'things', 'i', 'need', 'to', 'do', ',', 'but', 'especially', 'when', 'i', 'try', 'to', 'read', 'a', 'textbook', ',', 'i', 'find', 'it', 'really', 'hard', 'to', 'focus', 'on', 'it', 'for', 'more', 'than', 'a', 'few', 'minutes', '.', 'it', \"'\", 's', 'so', 'easy', 'to', 'hop', 'onto', 'red', '##dit', 'or', 'some', 'site', 'and', 'brows', '##e', 'links', 'for', 'an', 'hour', 'or', 'two', '(', 'even', 'if', 'it', \"'\", 's', 'only', 'been', '5', 'minutes', 'since', 'i', 'last', 'hit', 'red', '##dit', ')', '.', 'i', 'also', 'have', 'trouble', 'remembering', 'a', 'lot', 'of', 'what', 'i', 'read', '.', 'to', 'some', 'degree', 'this', 'applies', 'to', 'fiction', 'too', ',', 'but', 'sometimes', 'there', 'i', 'can', 'get', 'into', 'a', 'book', 'and', 'read', 'it', 'for', 'longer', '.', 'i', 'find', 'a', 'lot', 'of', 'things', 'with', 'instant', 'rewards', '(', 'video', 'games', ',', 'alcohol', ',', 'coffee', ',', 'red', '##dit', ',', 'youtube', ')', 'very', 'addict', '##ive', 'even', 'when', 'i', 'know', 'i', 'shouldn', \"'\", 't', 'be', 'doing', 'them', '.', 'i', \"'\", 'm', 'not', 'sure', 'whether', 'drinking', 'coffee', 'helps', 'me', 'or', 'not', ',', 'but', 'it', \"'\", 's', 'hard', 'to', 'avoid', 'drinking', 'it', 'long', 'enough', 'to', 'tell', 'whether', 'stopping', 'would', 'help', 'or', 'not', '.', 'on', 'the', 'totally', '##ad', '##d', 'virtual', 'doctor', 'quiz', ',', 'i', 'score', '9', '/', '9', 'on', 'the', 'ina', '##tten', '##tive', '##ness', 'section', '(', 'but', 'the', 'questions', 'are', 'vague', ',', 'and', 'doesn', \"'\", 't', 'everyone', 'have', 'times', 'when', 'they', 'can', \"'\", 't', 'focus', '?', ')', '.', 'i', 'don', \"'\", 't', 'think', 'i', 'have', 'as', 'much', 'of', 'the', 'hyper', '##active', 'side', 'of', 'things', ',', 'though', 'i', 'do', 'tend', 'to', 'get', 'up', 'and', 'pace', 'a', 'lot', '.', 'does', 'this', 'sound', 'like', 'real', 'ad', '##hd', ',', 'and', 'would', 'there', 'be', 'any', 'benefit', 'to', 'getting', 'a', 'real', 'diagnosis', '?', 'i', \"'\", 'm', 'not', 'sure', 'if', 'i', \"'\", 'd', 'want', 'to', 'go', 'down', 'the', 'route', 'of', 'daily', 'medication', '(', 'though', 'i', \"'\", 'd', 'probably', 'try', 'it', 'to', 'see', 'if', 'it', 'would', 'make', 'a', 'big', 'difference', ')', '.']\n",
      "INFO:__main__:Number of tokens: 434\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'this', 'ad', '##hd', 'or', 'is', 'it', 'just', 'life', '?', 'hello', 'red', '##dit', ',', 'i', \"'\", 'm', 'wondering', 'whether', 'my', 'problems', 'might', 'be', 'due', 'to', 'some', 'kind', 'of', 'ad', '##hd', ',', 'or', 'whether', 'this', 'is', 'just', 'what', 'every', 'human', 'has', 'to', 'deal', 'with', '.', 'mainly', 'i', 'have', 'serious', 'pro', '##cr', '##ast', '##ination', ',', 'which', 'weren', \"'\", 't', 'much', 'of', 'a', 'problem', 'until', 'college', '.', 'i', 'get', 'by', ',', 'but', 'it', 'seems', 'like', 'it', \"'\", 'll', 'prevent', 'me', 'from', 'doing', 'anything', 'but', 'barely', 'getting', 'by', '.', 'i', 'put', 'off', 'just', 'about', 'everything', 'until', 'the', 'day', 'before', ',', 'so', 'i', 'never', 'do', 'a', 'very', 'good', 'job', 'on', 'anything', ',', 'and', 'don', \"'\", 't', 'retain', 'the', 'material', 'much', 'past', 'the', 'end', 'of', 'semester', '.', 'i', 'have', 'a', 'list', 'of', 'things', 'i', 'need', 'to', 'do', ',', 'but', 'especially', 'when', 'i', 'try', 'to', 'read', 'a', 'textbook', ',', 'i', 'find', 'it', 'really', 'hard', 'to', 'focus', 'on', 'it', 'for', 'more', 'than', 'a', 'few', 'minutes', '.', 'it', \"'\", 's', 'so', 'easy', 'to', 'hop', 'onto', 'red', '##dit', 'or', 'some', 'site', 'and', 'brows', '##e', 'links', 'for', 'an', 'hour', 'or', 'two', '(', 'even', 'if', 'it', \"'\", 's', 'only', 'been', '5', 'minutes', 'since', 'i', 'last', 'hit', 'red', '##dit', ')', '.', 'i', 'also', 'have', 'trouble', 'remembering', 'a', 'lot', 'of', 'what', 'i', 'read', '.', 'to', 'some', 'degree', 'this', 'applies', 'to', 'fiction', 'too', ',', 'but', 'sometimes', 'there', 'i', 'can', 'get', 'into', 'a', 'book', 'and', 'read', 'it', 'for', 'longer', '.', 'i', 'find', 'a', 'lot', 'of', 'things', 'with', 'instant', 'rewards', '(', 'video', 'games', ',', 'alcohol', ',', 'coffee', ',', 'red', '##dit', ',', 'youtube', ')', 'very', 'addict', '##ive', 'even', 'when', 'i', 'know', 'i', 'shouldn', \"'\", 't', 'be', 'doing', 'them', '.', 'i', \"'\", 'm', 'not', 'sure', 'whether', 'drinking', 'coffee', 'helps', 'me', 'or', 'not', ',', 'but', 'it', \"'\", 's', 'hard', 'to', 'avoid', 'drinking', 'it', 'long', 'enough', 'to', 'tell', 'whether', 'stopping', 'would', 'help', 'or', 'not', '.', 'on', 'the', 'totally', '##ad', '##d', 'virtual', 'doctor', 'quiz', ',', 'i', 'score', '9', '/', '9', 'on', 'the', 'ina', '##tten', '##tive', '##ness', 'section', '(', 'but', 'the', 'questions', 'are', 'vague', ',', 'and', 'doesn', \"'\", 't', 'everyone', 'have', 'times', 'when', 'they', 'can', \"'\", 't', 'focus', '?', ')', '.', 'i', 'don', \"'\", 't', 'think', 'i', 'have', 'as', 'much', 'of', 'the', 'hyper', '##active', 'side', 'of', 'things', ',', 'though', 'i', 'do', 'tend', 'to', 'get', 'up', 'and', 'pace', 'a', 'lot', '.', 'does', 'this', 'sound', 'like', 'real', 'ad', '##hd', ',', 'and', 'would', 'there', 'be', 'any', 'benefit', 'to', 'getting', 'a', 'real', 'diagnosis', '?', 'i', \"'\", 'm', 'not', 'sure', 'if', 'i', \"'\", 'd', 'want', 'to', 'go', 'down', 'the', 'route', 'of', 'daily', 'medication', '(', 'though', 'i', \"'\", 'd', 'probably', 'try', 'it', 'to', 'see', 'if', 'it', 'would', 'make', 'a', 'big', 'difference', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'the', 'recovering', 'addict', 'hello', 'all', '.', '.', 'so', ',', 'basically', ',', 'i', 'am', 'a', 'recovering', 'addict', ',', 'mid', '20', \"'\", 's', '.', 'i', 'spent', 'nearly', '10', 'years', 'being', 'addicted', 'to', 'alcohol', 'and', 'heroin', '.', 'i', 'got', 'clean', '6', 'months', 'ago', 'with', 'the', 'help', 'of', 'some', 'programs', 'that', 'i', 'am', 'still', 'a', 'part', 'of', ',', 'but', 'started', 'having', 'difficulties', 'with', 'my', 'add', '/', 'hd', '.', 'i', 'was', 'diagnosed', 'as', 'a', 'kid', 'and', 'spent', 'years', 'on', 'add', '##eral', '##l', '.', 'around', 'the', 'time', 'i', 'stopped', 'taking', 'add', '##eral', '##l', 'is', 'when', 'i', 'moved', 'into', 'heavy', 'drugs', '.', 'i', \"'\", 'm', 'not', 'sure', 'the', 'exact', 'relationship', 'between', 'the', 'two', ',', 'and', 'i', 'really', 'can', \"'\", 't', 'make', 'a', 'link', 'there', '.', 'i', 'started', 'taking', '10', '##mg', 'of', 'add', '##eral', '##l', 'a', 'day', 'last', 'week', 'via', 'a', 'psychologist', \"'\", 's', 'recommendation', '.', 'i', 'was', 'very', 'skeptical', 'about', 'doing', 'this', 'because', 'of', 'my', 'past', ',', 'and', 'most', 'people', 'in', 'recovery', 'would', 'highly', 'di', '##s', '-', 'advise', 'taking', 'mind', '-', 'altering', 'drugs', '.', 'i', 'hear', 'a', 'lot', 'of', 'people', 'in', 'this', 'thread', 'talking', 'about', 'the', '\"', 'high', '\"', ',', 'etc', '.', 'and', 'this', 'really', 'worries', 'me', '.', 'i', 'feel', 'like', 'this', 'is', 'je', '##opa', '##rdi', '##zing', 'my', 'sob', '##riety', '.', 'i', 'cannot', 'afford', 'to', 'do', 'that', ',', 'because', 'if', 'i', 'start', 'drinking', 'or', 'doing', 'drugs', 'again', ',', 'i', 'will', 'die', '.', '.', 't', '##ld', '##r', ':', 'in', 'recovery', 'for', 'drugs', 'and', 'alcohol', ',', 'worried', 'about', 'taking', 'add', '##eral', '##l', '.', 'any', 'suggestions', '?', '?']\n",
      "INFO:__main__:Number of tokens: 251\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'the', 'recovering', 'addict', 'hello', 'all', '.', '.', 'so', ',', 'basically', ',', 'i', 'am', 'a', 'recovering', 'addict', ',', 'mid', '20', \"'\", 's', '.', 'i', 'spent', 'nearly', '10', 'years', 'being', 'addicted', 'to', 'alcohol', 'and', 'heroin', '.', 'i', 'got', 'clean', '6', 'months', 'ago', 'with', 'the', 'help', 'of', 'some', 'programs', 'that', 'i', 'am', 'still', 'a', 'part', 'of', ',', 'but', 'started', 'having', 'difficulties', 'with', 'my', 'add', '/', 'hd', '.', 'i', 'was', 'diagnosed', 'as', 'a', 'kid', 'and', 'spent', 'years', 'on', 'add', '##eral', '##l', '.', 'around', 'the', 'time', 'i', 'stopped', 'taking', 'add', '##eral', '##l', 'is', 'when', 'i', 'moved', 'into', 'heavy', 'drugs', '.', 'i', \"'\", 'm', 'not', 'sure', 'the', 'exact', 'relationship', 'between', 'the', 'two', ',', 'and', 'i', 'really', 'can', \"'\", 't', 'make', 'a', 'link', 'there', '.', 'i', 'started', 'taking', '10', '##mg', 'of', 'add', '##eral', '##l', 'a', 'day', 'last', 'week', 'via', 'a', 'psychologist', \"'\", 's', 'recommendation', '.', 'i', 'was', 'very', 'skeptical', 'about', 'doing', 'this', 'because', 'of', 'my', 'past', ',', 'and', 'most', 'people', 'in', 'recovery', 'would', 'highly', 'di', '##s', '-', 'advise', 'taking', 'mind', '-', 'altering', 'drugs', '.', 'i', 'hear', 'a', 'lot', 'of', 'people', 'in', 'this', 'thread', 'talking', 'about', 'the', '\"', 'high', '\"', ',', 'etc', '.', 'and', 'this', 'really', 'worries', 'me', '.', 'i', 'feel', 'like', 'this', 'is', 'je', '##opa', '##rdi', '##zing', 'my', 'sob', '##riety', '.', 'i', 'cannot', 'afford', 'to', 'do', 'that', ',', 'because', 'if', 'i', 'start', 'drinking', 'or', 'doing', 'drugs', 'again', ',', 'i', 'will', 'die', '.', '.', 't', '##ld', '##r', ':', 'in', 'recovery', 'for', 'drugs', 'and', 'alcohol', ',', 'worried', 'about', 'taking', 'add', '##eral', '##l', '.', 'any', 'suggestions', '?', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'addiction', 'problems', '.', 'i', \"'\", 've', 'been', 'working', 'on', 'getting', 'on', 'medication', 'for', 'a', 'couple', 'weeks', 'now', ',', 'but', 'i', \"'\", 'm', 'starting', 'to', 'have', 'second', 'thoughts', '.', 'everything', 'i', 'read', 'about', 'most', 'of', 'the', 'medications', 'throws', 'in', 'that', 'there', 'is', 'a', 'chance', 'for', 'abuse', 'of', 'the', 'medication', '.', 'so', ',', 'with', 'having', 'some', 'addiction', 'problems', 'in', 'the', 'past', ',', 'is', 'getting', 'on', 'add', '##eral', '##l', 'x', '##r', 'or', 'something', 'like', 'it', 'going', 'to', 'be', 'a', 'good', 'idea', 'for', 'me', '?', 'edit', ':', 'my', 'problems', 'have', 'been', 'with', 'de', '##press', '##ants', 'in', 'the', 'past', '.']\n",
      "INFO:__main__:Number of tokens: 98\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'addiction', 'problems', '.', 'i', \"'\", 've', 'been', 'working', 'on', 'getting', 'on', 'medication', 'for', 'a', 'couple', 'weeks', 'now', ',', 'but', 'i', \"'\", 'm', 'starting', 'to', 'have', 'second', 'thoughts', '.', 'everything', 'i', 'read', 'about', 'most', 'of', 'the', 'medications', 'throws', 'in', 'that', 'there', 'is', 'a', 'chance', 'for', 'abuse', 'of', 'the', 'medication', '.', 'so', ',', 'with', 'having', 'some', 'addiction', 'problems', 'in', 'the', 'past', ',', 'is', 'getting', 'on', 'add', '##eral', '##l', 'x', '##r', 'or', 'something', 'like', 'it', 'going', 'to', 'be', 'a', 'good', 'idea', 'for', 'me', '?', 'edit', ':', 'my', 'problems', 'have', 'been', 'with', 'de', '##press', '##ants', 'in', 'the', 'past', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['al', '##pr', '##az', '##ola', '##m', '(', 'x', '##ana', '##x', ')', 'and', 'add', '/', 'hd', 'has', 'anyone', 'been', 'on', 'al', '##pr', '##az', '##ola', '##m', 'for', 'add', '/', 'hd', '?', 'i', 'know', 'it', 'seems', 'like', 'an', 'odd', 'route', 'to', 'take', '.', 'i', \"'\", 've', 'been', 'on', 'it', 'for', 'a', 'few', 'months', ',', 'and', 'while', 'it', 'does', 'help', 'with', 'a', 'lot', 'of', 'my', 'problems', '(', 'before', 'i', 'just', 'used', 'trees', ')', 'i', 'still', 'lack', 'a', 'certain', 'focus', '.', 'so', 'i', 'was', 'just', 'wondering', 'if', 'anybody', 'else', 'had', 'used', 'it', ',', 'and', 'how', 'it', 'seemed', 'to', 'work', 'with', 'you', '.']\n",
      "INFO:__main__:Number of tokens: 96\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['al', '##pr', '##az', '##ola', '##m', '(', 'x', '##ana', '##x', ')', 'and', 'add', '/', 'hd', 'has', 'anyone', 'been', 'on', 'al', '##pr', '##az', '##ola', '##m', 'for', 'add', '/', 'hd', '?', 'i', 'know', 'it', 'seems', 'like', 'an', 'odd', 'route', 'to', 'take', '.', 'i', \"'\", 've', 'been', 'on', 'it', 'for', 'a', 'few', 'months', ',', 'and', 'while', 'it', 'does', 'help', 'with', 'a', 'lot', 'of', 'my', 'problems', '(', 'before', 'i', 'just', 'used', 'trees', ')', 'i', 'still', 'lack', 'a', 'certain', 'focus', '.', 'so', 'i', 'was', 'just', 'wondering', 'if', 'anybody', 'else', 'had', 'used', 'it', ',', 'and', 'how', 'it', 'seemed', 'to', 'work', 'with', 'you', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'need', 'to', 'vent', 'a', 'little', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'need', 'to', 'vent', 'a', 'little', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['speaks', 'for', 'itself', ':', ')', 'cute', 'hindi', 'song', 'by', 'kids', 'about', 'ad', '##hd', '.']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['speaks', 'for', 'itself', ':', ')', 'cute', 'hindi', 'song', 'by', 'kids', 'about', 'ad', '##hd', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['\"', 'rita', '##lin', 'gone', 'wrong', '\"', '-', 'professor', 'alan', 'sr', '##ou', '##fe', 'discusses', 'over', 'medication', 'and', 'the', 'problems', 'with', 'the', 'current', 'approach', 'to', 'understanding', 'and', 'treating', 'behavioral', 'problems']\n",
      "INFO:__main__:Number of tokens: 28\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['\"', 'rita', '##lin', 'gone', 'wrong', '\"', '-', 'professor', 'alan', 'sr', '##ou', '##fe', 'discusses', 'over', 'medication', 'and', 'the', 'problems', 'with', 'the', 'current', 'approach', 'to', 'understanding', 'and', 'treating', 'behavioral', 'problems']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'you', 'do', 'if', 'your', 'significant', 'other', 'doesn', \"'\", 't', 'understand', 'ad', '##hd', '?', 'story', '##time', ':', 'my', 'significant', 'other', 'agrees', 'that', 'i', 'have', 'ad', '##hd', ',', 'but', 'considers', 'it', 'a', 'poor', 'excuse', 'for', 'my', 'behavior', '.', 'he', 'is', 'understand', '##ably', 'frustrated', 'when', 'i', 'don', \"'\", 't', 'finish', 'all', 'of', 'my', 'share', 'of', 'the', 'chores', ',', 'and', 'when', 'i', 'interrupt', 'him', ',', 'but', 'when', 'i', 'try', 'to', 'approach', 'him', 'for', 'a', 'conversation', ',', 'it', 'always', 'turns', 'into', 'a', 'one', '-', 'sided', 'discussion', 'on', 'how', 'i', 'need', 'to', '\"', 'try', 'harder', ',', '\"', 'and', 'how', 'he', 'knew', 'two', 'people', 'with', 'ad', '##hd', 'who', 'struggled', 'but', 'still', 'managed', 'to', 'do', 'things', '(', 'both', 'with', 'access', 'to', 'medication', ')', ',', 'and', 'thus', 'i', 'have', 'no', 'excuse', '.', 'i', 'got', 'fed', 'up', 'one', 'day', 'while', 'trying', 'to', 'explain', 'it', 'to', 'him', ',', 'and', 'told', 'him', 'he', 'was', 'un', '##in', '##formed', ',', 'and', 'he', 'just', 'said', 'fuck', 'you', 'and', 'left', '.', 'of', 'course', ',', 'to', 'only', 'take', 'who', 'he', 'is', 'in', 'the', 'context', 'of', 'this', 'understand', '##ably', 'severe', 'and', 'in', '##fur', '##iating', 'error', 'would', 'be', 'to', 'ignore', 'everything', 'he', 'and', 'i', 'have', 'been', 'through', ',', 'and', 'the', 'many', 'ways', 'he', 'has', 'inspired', 'me', 'to', 'be', 'a', 'better', 'person', 'through', 'himself', '.', 'however', ',', 'i', 'cannot', 'help', 'but', 'feel', 'as', 'though', 'this', 'situation', 'must', 'be', 're', '##med', '##ied', ',', 'in', 'order', 'for', 'the', 'relationship', 'to', 'remain', 'healthy', 'and', 'fulfilling', 'for', 'the', 'both', 'of', 'us', '.']\n",
      "INFO:__main__:Number of tokens: 240\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'you', 'do', 'if', 'your', 'significant', 'other', 'doesn', \"'\", 't', 'understand', 'ad', '##hd', '?', 'story', '##time', ':', 'my', 'significant', 'other', 'agrees', 'that', 'i', 'have', 'ad', '##hd', ',', 'but', 'considers', 'it', 'a', 'poor', 'excuse', 'for', 'my', 'behavior', '.', 'he', 'is', 'understand', '##ably', 'frustrated', 'when', 'i', 'don', \"'\", 't', 'finish', 'all', 'of', 'my', 'share', 'of', 'the', 'chores', ',', 'and', 'when', 'i', 'interrupt', 'him', ',', 'but', 'when', 'i', 'try', 'to', 'approach', 'him', 'for', 'a', 'conversation', ',', 'it', 'always', 'turns', 'into', 'a', 'one', '-', 'sided', 'discussion', 'on', 'how', 'i', 'need', 'to', '\"', 'try', 'harder', ',', '\"', 'and', 'how', 'he', 'knew', 'two', 'people', 'with', 'ad', '##hd', 'who', 'struggled', 'but', 'still', 'managed', 'to', 'do', 'things', '(', 'both', 'with', 'access', 'to', 'medication', ')', ',', 'and', 'thus', 'i', 'have', 'no', 'excuse', '.', 'i', 'got', 'fed', 'up', 'one', 'day', 'while', 'trying', 'to', 'explain', 'it', 'to', 'him', ',', 'and', 'told', 'him', 'he', 'was', 'un', '##in', '##formed', ',', 'and', 'he', 'just', 'said', 'fuck', 'you', 'and', 'left', '.', 'of', 'course', ',', 'to', 'only', 'take', 'who', 'he', 'is', 'in', 'the', 'context', 'of', 'this', 'understand', '##ably', 'severe', 'and', 'in', '##fur', '##iating', 'error', 'would', 'be', 'to', 'ignore', 'everything', 'he', 'and', 'i', 'have', 'been', 'through', ',', 'and', 'the', 'many', 'ways', 'he', 'has', 'inspired', 'me', 'to', 'be', 'a', 'better', 'person', 'through', 'himself', '.', 'however', ',', 'i', 'cannot', 'help', 'but', 'feel', 'as', 'though', 'this', 'situation', 'must', 'be', 're', '##med', '##ied', ',', 'in', 'order', 'for', 'the', 'relationship', 'to', 'remain', 'healthy', 'and', 'fulfilling', 'for', 'the', 'both', 'of', 'us', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'help']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'help']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['help', 'on', 'finding', 'help', '?', 'i', 'think', 'i', 'may', 'have', 'ad', '##hd', '.', 'i', 'am', '21', 'years', 'old', ',', 'and', 'broke', 'as', 'shit', ',', 'which', 'hind', '##ers', 'getting', 'help', '.', 'but', '.', '.', '.', 'i', \"'\", 've', 'been', 'reading', 'on', 'it', ',', 'and', 'all', 'the', 'symptoms', 'are', 'there', '.', 'i', 'fi', '##dget', 'all', 'the', 'time', '.', 'i', 'can', \"'\", 't', 'be', 'still', '.', 'i', 'can', \"'\", 't', 'watch', 'tv', ',', 'because', 'i', 'become', 'bored', 'with', 'one', 'thing', '.', 'if', 'i', \"'\", 'm', 'sitting', 'down', ',', 'i', 'feel', 'i', 'have', 'to', 'stand', 'up', 'and', 'walk', 'around', ',', 'or', 'do', 'something', '.', 'my', 'mind', 'bounce', '##s', 'all', 'over', 'the', 'place', ',', 'and', 'when', 'other', 'people', 'are', 'talking', 'my', 'mind', 'races', 'all', 'over', 'the', 'place', '.', 'i', 'lose', 'my', 'train', 'of', 'thought', 'easily', ',', 'and', 'a', 'simple', 'word', 'in', 'a', 'sentence', 'during', 'a', 'conversation', 'will', 'get', 'me', 'going', 'off', 'on', 'a', 'complete', 'tangent', 'related', 'to', 'that', 'word', '.', 'i', 'can', \"'\", 't', 'sleep', 'at', 'night', 'without', 'being', 'drunk', 'off', 'my', 'ass', 'because', 'if', 'i', 'try', ',', 'my', 'mind', 'just', 'won', \"'\", 't', 'shut', 'down', 'long', 'enough', 'for', 'me', 'to', 'fall', 'asleep', '.', 'my', 'moods', 'bounce', 'from', 'extreme', 'to', 'extreme', 'with', 'every', 'breath', '.', 'i', \"'\", 'm', 'happy', ',', 'then', 'depressed', ',', 'then', 'angry', 'as', 'hell', ',', 'to', 'sorry', 'for', 'flipping', 'out', ',', 'to', 'happy', 'again', '.', 'i', 'can', \"'\", 't', 'finish', 'anything', 'i', 'start', 'because', 'i', 'get', 'bored', 'with', 'it', 'halfway', 'through', '.', 'so', ',', 'i', \"'\", 'm', 'looking', 'for', 'some', 'low', 'income', 'psychiatric', 'resources', 'in', 'the', 'kc', '##mo', 'area', ',', 'because', 'i', 'just', 'want', 'to', 'be', 'able', 'to', 'live', 'normally', 'and', 'focus', 'on', 'shit', 'long', 'enough', 'to', 'be', 'able', 'to', 'go', 'somewhere', 'besides', 'bus', '##sing', 'tables', 'at', 'a', 'restaurant', '.', 'i', 'feel', 'helpless', 'and', 'like', 'there', \"'\", 's', 'no', 'point', ',', 'since', 'i', \"'\", 'll', 'never', 'finish', 'anything', 'i', 'start', 'to', 'put', 'me', 'on', 'that', 'path', '.', 'any', 'help', 'y', \"'\", 'all', 'can', 'give', 'me', '?']\n",
      "INFO:__main__:Number of tokens: 324\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['help', 'on', 'finding', 'help', '?', 'i', 'think', 'i', 'may', 'have', 'ad', '##hd', '.', 'i', 'am', '21', 'years', 'old', ',', 'and', 'broke', 'as', 'shit', ',', 'which', 'hind', '##ers', 'getting', 'help', '.', 'but', '.', '.', '.', 'i', \"'\", 've', 'been', 'reading', 'on', 'it', ',', 'and', 'all', 'the', 'symptoms', 'are', 'there', '.', 'i', 'fi', '##dget', 'all', 'the', 'time', '.', 'i', 'can', \"'\", 't', 'be', 'still', '.', 'i', 'can', \"'\", 't', 'watch', 'tv', ',', 'because', 'i', 'become', 'bored', 'with', 'one', 'thing', '.', 'if', 'i', \"'\", 'm', 'sitting', 'down', ',', 'i', 'feel', 'i', 'have', 'to', 'stand', 'up', 'and', 'walk', 'around', ',', 'or', 'do', 'something', '.', 'my', 'mind', 'bounce', '##s', 'all', 'over', 'the', 'place', ',', 'and', 'when', 'other', 'people', 'are', 'talking', 'my', 'mind', 'races', 'all', 'over', 'the', 'place', '.', 'i', 'lose', 'my', 'train', 'of', 'thought', 'easily', ',', 'and', 'a', 'simple', 'word', 'in', 'a', 'sentence', 'during', 'a', 'conversation', 'will', 'get', 'me', 'going', 'off', 'on', 'a', 'complete', 'tangent', 'related', 'to', 'that', 'word', '.', 'i', 'can', \"'\", 't', 'sleep', 'at', 'night', 'without', 'being', 'drunk', 'off', 'my', 'ass', 'because', 'if', 'i', 'try', ',', 'my', 'mind', 'just', 'won', \"'\", 't', 'shut', 'down', 'long', 'enough', 'for', 'me', 'to', 'fall', 'asleep', '.', 'my', 'moods', 'bounce', 'from', 'extreme', 'to', 'extreme', 'with', 'every', 'breath', '.', 'i', \"'\", 'm', 'happy', ',', 'then', 'depressed', ',', 'then', 'angry', 'as', 'hell', ',', 'to', 'sorry', 'for', 'flipping', 'out', ',', 'to', 'happy', 'again', '.', 'i', 'can', \"'\", 't', 'finish', 'anything', 'i', 'start', 'because', 'i', 'get', 'bored', 'with', 'it', 'halfway', 'through', '.', 'so', ',', 'i', \"'\", 'm', 'looking', 'for', 'some', 'low', 'income', 'psychiatric', 'resources', 'in', 'the', 'kc', '##mo', 'area', ',', 'because', 'i', 'just', 'want', 'to', 'be', 'able', 'to', 'live', 'normally', 'and', 'focus', 'on', 'shit', 'long', 'enough', 'to', 'be', 'able', 'to', 'go', 'somewhere', 'besides', 'bus', '##sing', 'tables', 'at', 'a', 'restaurant', '.', 'i', 'feel', 'helpless', 'and', 'like', 'there', \"'\", 's', 'no', 'point', ',', 'since', 'i', \"'\", 'll', 'never', 'finish', 'anything', 'i', 'start', 'to', 'put', 'me', 'on', 'that', 'path', '.', 'any', 'help', 'y', \"'\", 'all', 'can', 'give', 'me', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anybody', 'ever', 'tried', 'natural', 'traditional', 'chinese', 'medicine', 'to', 'treat', 'ad', '##hd', '?', 'what', 'is', 'your', 'opinion', '.', 'i', \"'\", 've', 'been', 'taking', 'add', '##eral', '##l', 'for', 'about', '5', 'years', '.', 'my', 'wife', 'and', 'i', 'have', 'moved', 'to', 'guangzhou', ',', 'china', 'for', 'teaching', 'jobs', 'and', 'i', 'can', \"'\", 't', 'find', 'any', 'way', 'to', 'get', 'add', '##eral', '##l', 'or', 'any', 'other', 'med', '##s', 'that', 'are', 'in', 'this', 'classification', '.', 'i', 'have', 'read', 'about', 'tia', '##osh', '##en', 'liquor', 'or', 'yi', '##zhi', 'syrup', 'and', 'i', 'am', 'curious', '.', 'http', ':', '/', '/', 'www', '.', 'harry', '##hong', '.', 'com', '/', 'index', '_', 'files', '/', 'ad', '##hd', '##nat', '##ural', '##way', '.', 'h', '##tm']\n",
      "INFO:__main__:Number of tokens: 108\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anybody', 'ever', 'tried', 'natural', 'traditional', 'chinese', 'medicine', 'to', 'treat', 'ad', '##hd', '?', 'what', 'is', 'your', 'opinion', '.', 'i', \"'\", 've', 'been', 'taking', 'add', '##eral', '##l', 'for', 'about', '5', 'years', '.', 'my', 'wife', 'and', 'i', 'have', 'moved', 'to', 'guangzhou', ',', 'china', 'for', 'teaching', 'jobs', 'and', 'i', 'can', \"'\", 't', 'find', 'any', 'way', 'to', 'get', 'add', '##eral', '##l', 'or', 'any', 'other', 'med', '##s', 'that', 'are', 'in', 'this', 'classification', '.', 'i', 'have', 'read', 'about', 'tia', '##osh', '##en', 'liquor', 'or', 'yi', '##zhi', 'syrup', 'and', 'i', 'am', 'curious', '.', 'http', ':', '/', '/', 'www', '.', 'harry', '##hong', '.', 'com', '/', 'index', '_', 'files', '/', 'ad', '##hd', '##nat', '##ural', '##way', '.', 'h', '##tm']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['as', 'the', 'parent', 'of', 'a', 'boy', 'diagnosed', 'with', 'ad', '##hd', ',', 'what', 'can', 'i', 'do', 'to', 'help', 'my', 'son', '?', 'my', 'son', 'is', '8', ',', 'he', 'will', 'be', '9', 'in', 'march', '.', 'he', 'was', 'diagnosed', 'with', 'severe', 'ad', '##hd', 'and', 'as', '##per', '##ger', \"'\", 's', 'syndrome', 'when', 'he', 'was', 'nearly', '5', '.', 'i', 'held', 'off', 'on', 'medication', 'and', 'tried', 'many', 'alternative', 'behavior', 'modification', 'techniques', 'until', 'it', 'became', 'apparent', 'that', 'there', 'was', 'no', 'way', 'i', 'could', 'help', 'my', 'son', 'without', 'med', '##s', 'and', 'he', 'would', 'likely', 'kill', '/', 'mai', '##m', 'himself', 'or', 'someone', 'else', 'in', 'a', 'short', 'amount', 'of', 'time', '.', 'i', 'have', 'tried', 'so', 'damned', 'hard', 'to', 'be', 'strong', ',', 'patient', ',', 'understanding', ',', 'firm', ',', 'and', 'consistent', ',', 'but', 'we', 'don', \"'\", 't', 'seem', 'to', 'be', 'making', 'any', 'head', '##way', 'into', 'him', 'understanding', 'fundamental', 'things', 'like', ':', 'it', 'is', 'time', 'to', 'brush', 'your', 'teeth', 'so', 'do', 'it', ',', 'it', 'is', 'time', 'get', 'dressed', 'for', 'school', 'so', 'do', 'it', ',', 'put', 'your', 'dishes', 'into', 'the', 'dish', '##wash', '##er', '.', 'he', 'gets', 'away', 'with', 'leaving', 'the', 'toilet', 'seat', 'up', 'as', 'then', 'i', 'know', 'he', 'is', 'actually', 'putting', 'the', 'seat', 'up', '.', '.', '.', 'what', 'can', 'i', 'do', 'to', 'help', 'him', 'understand', 'that', 'at', 'certain', 'times', 'we', 'have', 'to', 'do', 'certain', 'things', ',', 'and', 'then', 'to', 'actually', 'do', 'them', '?', 'he', 'knows', 'that', 'he', 'has', 'to', 'brush', 'his', 'teeth', 'at', '8', ':', '00', 'every', 'night', ',', 'and', 'every', 'night', 'he', 'spends', '15', 'minutes', '\"', 'getting', 'the', 'tooth', '##brush', 'ready', '\"', 'and', 'then', 'complain', '##s', 'that', 'he', 'didn', \"'\", 't', 'get', 'any', 'time', 'to', 'play', 'just', 'before', 'bed', 'because', 'it', 'took', 'him', '30', 'minutes', 'to', 'get', 'his', 'teeth', 'clean', '.', 'he', 'is', 'currently', 'in', 'between', 'good', 'doctors', 'due', 'to', 'a', 'change', 'in', 'insurance', ',', 'and', 'the', 'doctor', 'at', 'the', 'clinic', 'that', 'he', 'is', 'forced', 'to', 'go', 'to', 'doesn', \"'\", 't', 'believe', 'in', 'ad', '##hd', ',', 'or', 'medication', 'for', 'a', 'child', ',', 'so', 'we', 'get', 'to', 'put', 'up', 'with', 'her', 'conde', '##sc', '##ending', 'bs', 'until', 'june', 'when', 'he', 'gets', 'to', 'see', 'the', 'new', 'doctor', '.', 'please', 'help', 'me', 'help', 'my', 'son', '.', 'what', 'can', 'i', 'do', 'to', 'help', 'him', 'understand', '?', 'what', 'do', 'i', 'need', 'to', 'understand', '?']\n",
      "INFO:__main__:Number of tokens: 364\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['as', 'the', 'parent', 'of', 'a', 'boy', 'diagnosed', 'with', 'ad', '##hd', ',', 'what', 'can', 'i', 'do', 'to', 'help', 'my', 'son', '?', 'my', 'son', 'is', '8', ',', 'he', 'will', 'be', '9', 'in', 'march', '.', 'he', 'was', 'diagnosed', 'with', 'severe', 'ad', '##hd', 'and', 'as', '##per', '##ger', \"'\", 's', 'syndrome', 'when', 'he', 'was', 'nearly', '5', '.', 'i', 'held', 'off', 'on', 'medication', 'and', 'tried', 'many', 'alternative', 'behavior', 'modification', 'techniques', 'until', 'it', 'became', 'apparent', 'that', 'there', 'was', 'no', 'way', 'i', 'could', 'help', 'my', 'son', 'without', 'med', '##s', 'and', 'he', 'would', 'likely', 'kill', '/', 'mai', '##m', 'himself', 'or', 'someone', 'else', 'in', 'a', 'short', 'amount', 'of', 'time', '.', 'i', 'have', 'tried', 'so', 'damned', 'hard', 'to', 'be', 'strong', ',', 'patient', ',', 'understanding', ',', 'firm', ',', 'and', 'consistent', ',', 'but', 'we', 'don', \"'\", 't', 'seem', 'to', 'be', 'making', 'any', 'head', '##way', 'into', 'him', 'understanding', 'fundamental', 'things', 'like', ':', 'it', 'is', 'time', 'to', 'brush', 'your', 'teeth', 'so', 'do', 'it', ',', 'it', 'is', 'time', 'get', 'dressed', 'for', 'school', 'so', 'do', 'it', ',', 'put', 'your', 'dishes', 'into', 'the', 'dish', '##wash', '##er', '.', 'he', 'gets', 'away', 'with', 'leaving', 'the', 'toilet', 'seat', 'up', 'as', 'then', 'i', 'know', 'he', 'is', 'actually', 'putting', 'the', 'seat', 'up', '.', '.', '.', 'what', 'can', 'i', 'do', 'to', 'help', 'him', 'understand', 'that', 'at', 'certain', 'times', 'we', 'have', 'to', 'do', 'certain', 'things', ',', 'and', 'then', 'to', 'actually', 'do', 'them', '?', 'he', 'knows', 'that', 'he', 'has', 'to', 'brush', 'his', 'teeth', 'at', '8', ':', '00', 'every', 'night', ',', 'and', 'every', 'night', 'he', 'spends', '15', 'minutes', '\"', 'getting', 'the', 'tooth', '##brush', 'ready', '\"', 'and', 'then', 'complain', '##s', 'that', 'he', 'didn', \"'\", 't', 'get', 'any', 'time', 'to', 'play', 'just', 'before', 'bed', 'because', 'it', 'took', 'him', '30', 'minutes', 'to', 'get', 'his', 'teeth', 'clean', '.', 'he', 'is', 'currently', 'in', 'between', 'good', 'doctors', 'due', 'to', 'a', 'change', 'in', 'insurance', ',', 'and', 'the', 'doctor', 'at', 'the', 'clinic', 'that', 'he', 'is', 'forced', 'to', 'go', 'to', 'doesn', \"'\", 't', 'believe', 'in', 'ad', '##hd', ',', 'or', 'medication', 'for', 'a', 'child', ',', 'so', 'we', 'get', 'to', 'put', 'up', 'with', 'her', 'conde', '##sc', '##ending', 'bs', 'until', 'june', 'when', 'he', 'gets', 'to', 'see', 'the', 'new', 'doctor', '.', 'please', 'help', 'me', 'help', 'my', 'son', '.', 'what', 'can', 'i', 'do', 'to', 'help', 'him', 'understand', '?', 'what', 'do', 'i', 'need', 'to', 'understand', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['for', 'me', ',', 'this', 'is', 'the', 'ultimate', 'super', '##power', '!', '!', '!']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['for', 'me', ',', 'this', 'is', 'the', 'ultimate', 'super', '##power', '!', '!', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['fall', 'quiz', ':', 'which', 'type', '##face', 'are', 'you', '?', '|', 'fashion', '##cl', '##ub', '.', 'com']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['fall', 'quiz', ':', 'which', 'type', '##face', 'are', 'you', '?', '|', 'fashion', '##cl', '##ub', '.', 'com']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'side', 'effects', 'hey', 'guys', ',', 'just', 'wondering', 'you', 'guys', 'notice', 'any', 'specific', 'side', '-', 'effects', 'from', 'add', '##eral', '##l', '.', 'personally', 'i', 'experience', 'loss', 'of', 'appetite', 'and', 'dry', '-', 'mouth', 'that', 'occasionally', 'causes', 'me', 'to', 'gag', 'at', 'times', '.', 'has', 'anyone', 'else', 'experienced', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 48\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'side', 'effects', 'hey', 'guys', ',', 'just', 'wondering', 'you', 'guys', 'notice', 'any', 'specific', 'side', '-', 'effects', 'from', 'add', '##eral', '##l', '.', 'personally', 'i', 'experience', 'loss', 'of', 'appetite', 'and', 'dry', '-', 'mouth', 'that', 'occasionally', 'causes', 'me', 'to', 'gag', 'at', 'times', '.', 'has', 'anyone', 'else', 'experienced', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'ready', 'to', 'give', 'up', 'on', 'medication', ',', 'but', 'i', \"'\", 'm', 'terrified', 'of', 'obesity', '.', 'i', \"'\", 've', 'been', 'on', 'add', 'med', '##s', 'since', '3rd', 'grade', ',', 'with', 'a', 'break', 'of', 'about', '5', 'years', 'between', 'high', 'school', 'and', 'college', '.', 'finishing', 'up', 'college', 'now', ',', 'and', 'none', 'of', 'the', 'med', '##s', 'i', \"'\", 've', 'been', 'trying', 'are', 'helping', '(', 'ri', '##t', '.', ',', 'add', '.', ',', 'con', '##c', ',', 'vi', '##v', '.', ')', ',', 'they', 'are', 'mildly', 'helpful', 'at', 'best', ',', 'and', 'not', 'worth', 'the', 'expense', '.', 'if', 'i', 'can', 'graduate', 'this', 'year', ',', 'i', \"'\", 'm', 'ready', 'to', 'go', 'back', 'off', 'them', ',', 'as', 'i', 'seem', 'to', 'function', 'ok', 'outside', 'of', 'academics', 'despite', 'the', 'symptoms', '.', 'however', ',', 'last', 'time', 'i', 'went', 'off', 'med', '##s', 'i', 'became', 'seriously', 'obe', '##se', ',', 'and', 'couldn', \"'\", 't', 'control', 'my', 'appetite', '(', 'maybe', 'i', 'never', 'learned', 'how', 'b', '/', 'c', 'of', 'med', '##s', '?', ')', '.', 'the', 'rebound', 'when', 'getting', 'off', 'med', '##s', 'is', 'the', 'worst', '.', 'took', 'a', 'month', 'long', 'tolerance', 'break', 'and', 'gained', '20', '##lb', '##s', 'this', 'year', '.', 'are', 'there', 'any', 'cheap', 'med', '##s', 'that', 'can', 'keep', 'my', 'appetite', 'down', ',', 'with', 'few', 'side', 'effects', '?', 'i', \"'\", 'm', 'not', 'terribly', 'thin', 'now', ',', 'but', 'i', 'know', 'i', \"'\", 'll', 'balloon', 'way', 'up', 'if', 'i', 'go', 'off', 'med', '##s', '.', ':', '(', 'i', \"'\", 'm', 'ready', 'to', 'stop', 'trying', 'to', 'change', 'me', 'to', 'fit', 'the', 'world', ',', 'and', 'look', 'for', 'environments', 'and', 'activities', 'that', 'fit', 'the', 'way', 'my', 'mind', 'works', ',', 'but', 'this', 'one', 'thing', 'is', 'what', 'i', 'can', \"'\", 't', 'deal', 'with', '.']\n",
      "INFO:__main__:Number of tokens: 268\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'ready', 'to', 'give', 'up', 'on', 'medication', ',', 'but', 'i', \"'\", 'm', 'terrified', 'of', 'obesity', '.', 'i', \"'\", 've', 'been', 'on', 'add', 'med', '##s', 'since', '3rd', 'grade', ',', 'with', 'a', 'break', 'of', 'about', '5', 'years', 'between', 'high', 'school', 'and', 'college', '.', 'finishing', 'up', 'college', 'now', ',', 'and', 'none', 'of', 'the', 'med', '##s', 'i', \"'\", 've', 'been', 'trying', 'are', 'helping', '(', 'ri', '##t', '.', ',', 'add', '.', ',', 'con', '##c', ',', 'vi', '##v', '.', ')', ',', 'they', 'are', 'mildly', 'helpful', 'at', 'best', ',', 'and', 'not', 'worth', 'the', 'expense', '.', 'if', 'i', 'can', 'graduate', 'this', 'year', ',', 'i', \"'\", 'm', 'ready', 'to', 'go', 'back', 'off', 'them', ',', 'as', 'i', 'seem', 'to', 'function', 'ok', 'outside', 'of', 'academics', 'despite', 'the', 'symptoms', '.', 'however', ',', 'last', 'time', 'i', 'went', 'off', 'med', '##s', 'i', 'became', 'seriously', 'obe', '##se', ',', 'and', 'couldn', \"'\", 't', 'control', 'my', 'appetite', '(', 'maybe', 'i', 'never', 'learned', 'how', 'b', '/', 'c', 'of', 'med', '##s', '?', ')', '.', 'the', 'rebound', 'when', 'getting', 'off', 'med', '##s', 'is', 'the', 'worst', '.', 'took', 'a', 'month', 'long', 'tolerance', 'break', 'and', 'gained', '20', '##lb', '##s', 'this', 'year', '.', 'are', 'there', 'any', 'cheap', 'med', '##s', 'that', 'can', 'keep', 'my', 'appetite', 'down', ',', 'with', 'few', 'side', 'effects', '?', 'i', \"'\", 'm', 'not', 'terribly', 'thin', 'now', ',', 'but', 'i', 'know', 'i', \"'\", 'll', 'balloon', 'way', 'up', 'if', 'i', 'go', 'off', 'med', '##s', '.', ':', '(', 'i', \"'\", 'm', 'ready', 'to', 'stop', 'trying', 'to', 'change', 'me', 'to', 'fit', 'the', 'world', ',', 'and', 'look', 'for', 'environments', 'and', 'activities', 'that', 'fit', 'the', 'way', 'my', 'mind', 'works', ',', 'but', 'this', 'one', 'thing', 'is', 'what', 'i', 'can', \"'\", 't', 'deal', 'with', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', 'some', 'encouragement', '!']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', 'some', 'encouragement', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['not', 'sure', 'what', 'to', 'make', 'of', 'this', 'song', '(', 'listen', 'to', 'the', 'lyrics', ')', '.']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['not', 'sure', 'what', 'to', 'make', 'of', 'this', 'song', '(', 'listen', 'to', 'the', 'lyrics', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'america', 'made', 'it', \"'\", 's', 'children', 'crazy', '.', 'is', 'this', 'article', 'right', 'or', 'wrong', '?', 'is', 'it', 'reliable', '?']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'america', 'made', 'it', \"'\", 's', 'children', 'crazy', '.', 'is', 'this', 'article', 'right', 'or', 'wrong', '?', 'is', 'it', 'reliable', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['magical', 'countdown', 'timer', '?']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['magical', 'countdown', 'timer', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'some', 'simple', 'things', 'that', 'make', 'your', 'life', 'with', 'add', 'easier', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'some', 'simple', 'things', 'that', 'make', 'your', 'life', 'with', 'add', 'easier', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['potent', '##iating', 'add', '##eral', '##l', 'recently', 'i', \"'\", 've', 'been', 'trying', 'to', 'get', 'the', 'most', 'out', 'of', 'my', 'add', '##eral', '##l', 'and', 'so', 'i', \"'\", 've', 'been', 'trying', 'different', 'potent', '##iating', 'tricks', '.', 'i', \"'\", 've', 'heard', 'that', 'ant', '##ac', '##ids', '(', 'tu', '##ms', ')', 'help', 'you', 'absorb', 'the', 'add', '##eral', '##l', 'better', '.', 'i', \"'\", 've', 'also', 'heard', 'that', 'the', 'supplement', 'l', '-', 'ar', '##gin', '##ine', 'potent', '##iate', '##s', 'add', '##eral', '##l', 'as', 'it', 'increases', 'your', 'blood', 'flow', 'and', 'helps', 'the', 'add', '##eral', '##l', 'to', 'be', 'more', 'effective', '.', 'have', 'you', 'guys', 'heard', 'of', 'potent', '##iating', 'supplement', 'for', 'amp', '##het', '##amine', '##s', '?']\n",
      "INFO:__main__:Number of tokens: 104\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['potent', '##iating', 'add', '##eral', '##l', 'recently', 'i', \"'\", 've', 'been', 'trying', 'to', 'get', 'the', 'most', 'out', 'of', 'my', 'add', '##eral', '##l', 'and', 'so', 'i', \"'\", 've', 'been', 'trying', 'different', 'potent', '##iating', 'tricks', '.', 'i', \"'\", 've', 'heard', 'that', 'ant', '##ac', '##ids', '(', 'tu', '##ms', ')', 'help', 'you', 'absorb', 'the', 'add', '##eral', '##l', 'better', '.', 'i', \"'\", 've', 'also', 'heard', 'that', 'the', 'supplement', 'l', '-', 'ar', '##gin', '##ine', 'potent', '##iate', '##s', 'add', '##eral', '##l', 'as', 'it', 'increases', 'your', 'blood', 'flow', 'and', 'helps', 'the', 'add', '##eral', '##l', 'to', 'be', 'more', 'effective', '.', 'have', 'you', 'guys', 'heard', 'of', 'potent', '##iating', 'supplement', 'for', 'amp', '##het', '##amine', '##s', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'something', 'that', 'you', 'think', 'everyone', 'should', 'know', 'about', 'add', '/', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'something', 'that', 'you', 'think', 'everyone', 'should', 'know', 'about', 'add', '/', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['for', 'those', 'who', 'have', 'trouble', 'with', 'add', '##eral', '##l', 'tolerance', 'and', '/', 'or', 'want', 'to', 'keep', 'the', 'en', '##er', '##gizing', '/', 'pro', '-', 'social', 'aspects', 'of', 'amp', '##het', '##amine']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['for', 'those', 'who', 'have', 'trouble', 'with', 'add', '##eral', '##l', 'tolerance', 'and', '/', 'or', 'want', 'to', 'keep', 'the', 'en', '##er', '##gizing', '/', 'pro', '-', 'social', 'aspects', 'of', 'amp', '##het', '##amine']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['less', 'wo', '##rri', '##some', '=', 'less', 'focused', 'in', 'class', '.', '.', '.', 'is', 'this', 'normal', '?', 'i', \"'\", 've', 'been', 'on', 'concert', '##a', 'for', 'about', '2', 'months', ',', 'and', 'its', 'a', 'very', 'small', '18', '##mg', 'dose', '.', 'i', \"'\", 'm', 'primarily', 'ina', '##tten', '##tive', '.', 'i', 'starting', 'taking', 'it', '3', 'weeks', 'before', 'my', 'semester', 'and', 'its', 'my', 'first', 'med', '##icated', 'semester', 'ever', '.', 'when', 'doing', 'homework', 'the', 'effects', 'are', 'still', 'not', '##ica', '##ble', '.', 'i', 'can', 'stay', 'focused', ',', 'and', 'step', 'by', 'step', 'tackle', 'what', 'i', 'need', 'to', 'do', 'with', 'some', 'self', 'discipline', '.', 'i', \"'\", 'm', 'no', 'longer', 'en', '##er', '##gi', '##zed', 'from', 'it', ',', 'but', 'that', '##s', 'probably', 'just', 'my', 'body', '##ing', 'adapting', 'to', 'the', 'medication', 'which', 'is', 'ok', '.', 'stud', '##dy', '##ing', 'my', 'textbooks', 'or', 'doing', 'homework', 'off', 'the', 'medication', 'is', 'so', 'much', 'harder', ',', 'no', 'matter', 'how', 'much', 'di', '##ci', '##plin', '##e', 'i', 'have', ',', 'if', 'i', \"'\", 'm', 'off', 'the', 'medication', 'anxiety', 'will', 'force', 'me', 'to', 'stop', 'reading', 'and', 'do', 'something', 'else', '.', '1', '.', 'however', ',', 'when', 'it', 'comes', 'to', 'class', ',', 'i', 'can', 'not', 'listen', 'to', 'the', 'teacher', '.', 'i', \"'\", 'll', 'start', 'playing', 'with', 'anything', 'around', 'me', 'even', 'a', 'string', ',', 'or', 'zone', 'out', '.', 'i', \"'\", 'm', 'even', 'in', 'class', 'now', '.', 'i', 'just', 'have', 'no', 'interest', 'in', 'what', 'some', 'teachers', 'are', 'saying', ',', 'even', 'though', 'i', 'like', 'the', 'topics', '.', 'just', 'don', \"'\", 't', 'want', 'to', 'hear', 'her', '.', 'before', ',', 'i', 'couldn', \"'\", 't', 'pay', 'attention', 'because', 'i', \"'\", 'd', 'get', 'a', 'lot', 'of', 'anxiety', 'until', 'i', 'opened', 'my', 'computer', '/', 'phone', '/', 'got', 'distracted', '.', 'now', ',', 'i', 'have', 'no', 'anxiety', ',', 'but', 'just', 'can', \"'\", 't', 'listen', ',', 'it', \"'\", 's', 'like', 'i', 'suddenly', 'have', 'no', 'interest', 'it', 'what', \"'\", 's', 'being', 'said', 'so', 'i', 'entertain', 'myself', '.', '2', '.', 'in', 'addition', 'to', 'not', 'being', 'able', 'to', 'listen', 'to', 'any', 'professors', ',', 'i', \"'\", 've', 'started', '\"', 'not', 'giving', 'a', 'f', '*', '*', '*', '\"', 'in', 'general', ',', 'about', 'things', 'i', 'usually', 'care', 'about', '.', 'i', 'do', 'not', 'mean', '\"', 'no', 'motivation', '\"', ',', 'i', 'still', 'have', 'a', 'ton', '(', 'possibly', 'more', ')', 'motivation', 'to', 'work', 'out', ',', 'do', 'homework', ',', 'study', ',', 'etc', '.', 'but', 'i', 'do', 'mean', '\"', 'just', 'not', 'caring', '\"', 'about', 'things', '.', 'ex', '.', '1', ':', 'i', \"'\", 've', 'never', 'been', 'late', 'to', 'class', 'until', 'medication', 'started', '.', 'i', 'now', 'walk', 'and', 'think', '\"', 'i', 'can', 'run', 'and', 'be', 'on', 'time', ',', 'but', 'if', 'they', 'care', 'if', 'i', \"'\", 'm', '10', 'late', 'its', 'not', 'their', 'business', '.', '.', '.', 'i', 'can', \"'\", 't', 'understand', 'the', 'lady', 'very', 'well', 'anyway', '##s', '.', '.', '.', 'we', 'aren', \"'\", 't', 'graded', 'on', 'being', 'on', 'time', ',', 'i', 'pay', 'for', 'it', ',', 'if', 'i', \"'\", 'm', 'late', 'and', '\"', 'rude', '\"', 'she', 'can', 'go', 'worry', 'about', 'something', 'else', '\"', '.', 'those', 'aren', \"'\", 't', 'thoughts', 'i', \"'\", 'd', 'normally', 'have', ',', 'i', \"'\", 'd', 'always', 'run', 'if', 'i', \"'\", 'm', 'late', '.', 'ex', '.', '2', ':', 'dishes', 'in', 'the', 'sink', 'from', 'room', '##ates', 'used', 'to', 'drive', 'me', 'up', 'a', 'wall', 'and', 'cause', 'so', 'so', 'much', 'anxiety', '.', 'now', ',', 'it', 'ann', '##oys', 'me', ',', 'but', 'i', 'can', 'do', 'work', 'without', 'having', 'the', 'thought', 'of', 'dishes', 'distract', 'me', '.', 'ex', '.', '1', 'seems', 'negative', 'and', 'ex', '##2', '.', '2', 'seems', 'positive', 'but', 'i', \"'\", 'm', 'unsure', '.', '.', '.', 'is', 'this', 'normal', '?', 'should', 'i', 'ask', 'my', 'doctor', 'to', 'try', 'a', 'different', 'st', '##im', '##ula', '##nt', '?', 'different', 'dose', '?', 'hormone', 'therapy', 'instead', '?', 'wondering', 'why', 'i', 'still', 'can', \"'\", 't', 'focus', 'in', 'a', 'classroom', '.', '.', '.', 'add', '##eral', '##l', 'ruins', 'my', 'appetite', ',', 'concert', '##a', '(', 'or', 'rita', '##lin', ')', 'gives', 'me', 'no', 'negative', 'side', 'effects', 'other', 'than', 'the', 'above', '.', '.', '.', 'thanks']\n",
      "INFO:__main__:Number of tokens: 629\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['less', 'wo', '##rri', '##some', '=', 'less', 'focused', 'in', 'class', '.', '.', '.', 'is', 'this', 'normal', '?', 'i', \"'\", 've', 'been', 'on', 'concert', '##a', 'for', 'about', '2', 'months', ',', 'and', 'its', 'a', 'very', 'small', '18', '##mg', 'dose', '.', 'i', \"'\", 'm', 'primarily', 'ina', '##tten', '##tive', '.', 'i', 'starting', 'taking', 'it', '3', 'weeks', 'before', 'my', 'semester', 'and', 'its', 'my', 'first', 'med', '##icated', 'semester', 'ever', '.', 'when', 'doing', 'homework', 'the', 'effects', 'are', 'still', 'not', '##ica', '##ble', '.', 'i', 'can', 'stay', 'focused', ',', 'and', 'step', 'by', 'step', 'tackle', 'what', 'i', 'need', 'to', 'do', 'with', 'some', 'self', 'discipline', '.', 'i', \"'\", 'm', 'no', 'longer', 'en', '##er', '##gi', '##zed', 'from', 'it', ',', 'but', 'that', '##s', 'probably', 'just', 'my', 'body', '##ing', 'adapting', 'to', 'the', 'medication', 'which', 'is', 'ok', '.', 'stud', '##dy', '##ing', 'my', 'textbooks', 'or', 'doing', 'homework', 'off', 'the', 'medication', 'is', 'so', 'much', 'harder', ',', 'no', 'matter', 'how', 'much', 'di', '##ci', '##plin', '##e', 'i', 'have', ',', 'if', 'i', \"'\", 'm', 'off', 'the', 'medication', 'anxiety', 'will', 'force', 'me', 'to', 'stop', 'reading', 'and', 'do', 'something', 'else', '.', '1', '.', 'however', ',', 'when', 'it', 'comes', 'to', 'class', ',', 'i', 'can', 'not', 'listen', 'to', 'the', 'teacher', '.', 'i', \"'\", 'll', 'start', 'playing', 'with', 'anything', 'around', 'me', 'even', 'a', 'string', ',', 'or', 'zone', 'out', '.', 'i', \"'\", 'm', 'even', 'in', 'class', 'now', '.', 'i', 'just', 'have', 'no', 'interest', 'in', 'what', 'some', 'teachers', 'are', 'saying', ',', 'even', 'though', 'i', 'like', 'the', 'topics', '.', 'just', 'don', \"'\", 't', 'want', 'to', 'hear', 'her', '.', 'before', ',', 'i', 'couldn', \"'\", 't', 'pay', 'attention', 'because', 'i', \"'\", 'd', 'get', 'a', 'lot', 'of', 'anxiety', 'until', 'i', 'opened', 'my', 'computer', '/', 'phone', '/', 'got', 'distracted', '.', 'now', ',', 'i', 'have', 'no', 'anxiety', ',', 'but', 'just', 'can', \"'\", 't', 'listen', ',', 'it', \"'\", 's', 'like', 'i', 'suddenly', 'have', 'no', 'interest', 'it', 'what', \"'\", 's', 'being', 'said', 'so', 'i', 'entertain', 'myself', '.', '2', '.', 'in', 'addition', 'to', 'not', 'being', 'able', 'to', 'listen', 'to', 'any', 'professors', ',', 'i', \"'\", 've', 'started', '\"', 'not', 'giving', 'a', 'f', '*', '*', '*', '\"', 'in', 'general', ',', 'about', 'things', 'i', 'usually', 'care', 'about', '.', 'i', 'do', 'not', 'mean', '\"', 'no', 'motivation', '\"', ',', 'i', 'still', 'have', 'a', 'ton', '(', 'possibly', 'more', ')', 'motivation', 'to', 'work', 'out', ',', 'do', 'homework', ',', 'study', ',', 'etc', '.', 'but', 'i', 'do', 'mean', '\"', 'just', 'not', 'caring', '\"', 'about', 'things', '.', 'ex', '.', '1', ':', 'i', \"'\", 've', 'never', 'been', 'late', 'to', 'class', 'until', 'medication', 'started', '.', 'i', 'now', 'walk', 'and', 'think', '\"', 'i', 'can', 'run', 'and', 'be', 'on', 'time', ',', 'but', 'if', 'they', 'care', 'if', 'i', \"'\", 'm', '10', 'late', 'its', 'not', 'their', 'business', '.', '.', '.', 'i', 'can', \"'\", 't', 'understand', 'the', 'lady', 'very', 'well', 'anyway', '##s', '.', '.', '.', 'we', 'aren', \"'\", 't', 'graded', 'on', 'being', 'on', 'time', ',', 'i', 'pay', 'for', 'it', ',', 'if', 'i', \"'\", 'm', 'late', 'and', '\"', 'rude', '\"', 'she', 'can', 'go', 'worry', 'about', 'something', 'else', '\"', '.', 'those', 'aren', \"'\", 't', 'thoughts', 'i', \"'\", 'd', 'normally', 'have', ',', 'i', \"'\", 'd', 'always', 'run', 'if', 'i', \"'\", 'm', 'late', '.', 'ex', '.', '2', ':', 'dishes', 'in', 'the', 'sink', 'from', 'room', '##ates', 'used', 'to', 'drive', 'me', 'up'], ['a', 'wall', 'and', 'cause', 'so', 'so', 'much', 'anxiety', '.', 'now', ',', 'it', 'ann', '##oys', 'me', ',', 'but', 'i', 'can', 'do', 'work', 'without', 'having', 'the', 'thought', 'of', 'dishes', 'distract', 'me', '.', 'ex', '.', '1', 'seems', 'negative', 'and', 'ex', '##2', '.', '2', 'seems', 'positive', 'but', 'i', \"'\", 'm', 'unsure', '.', '.', '.', 'is', 'this', 'normal', '?', 'should', 'i', 'ask', 'my', 'doctor', 'to', 'try', 'a', 'different', 'st', '##im', '##ula', '##nt', '?', 'different', 'dose', '?', 'hormone', 'therapy', 'instead', '?', 'wondering', 'why', 'i', 'still', 'can', \"'\", 't', 'focus', 'in', 'a', 'classroom', '.', '.', '.', 'add', '##eral', '##l', 'ruins', 'my', 'appetite', ',', 'concert', '##a', '(', 'or', 'rita', '##lin', ')', 'gives', 'me', 'no', 'negative', 'side', 'effects', 'other', 'than', 'the', 'above', '.', '.', '.', 'thanks']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'kind', 'of', 'doctor', 'can', 'dia', '##gno', '##se', 'me', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'kind', 'of', 'doctor', 'can', 'dia', '##gno', '##se', 'me', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'have', 'trouble', 'dec', '##ip', '##hering', 'lyrics', '?', 'with', 'an', 'opening', 'like', 'that', ',', 'i', 'considered', 'posting', 'it', 'into', 'the', 'da', '##e', 'sub', '##red', '##dit', 'but', 'felt', 'that', '/', 'r', '/', 'ad', '##hd', 'was', 'more', 'applicable', 'from', 'my', 'experiences', 'that', 'i', 'discuss', 'below', '.', 'anyway', '##s', '.', '.', 'for', 'a', 'few', 'years', 'now', 'i', \"'\", 've', 'had', 'a', 'lot', 'of', 'trouble', 'with', 'picking', 'out', 'lyrics', 'from', 'songs', 'or', 'performances', '.', 'i', 'can', \"'\", 't', 'really', 'pin', '##point', 'when', 'this', 'may', 'have', 'started', 'because', 'i', 'do', 'know', 'some', 'lyrics', 'to', 'various', 'songs', '.', 'bear', 'with', 'me', 'because', 'it', \"'\", 's', 'a', 'tad', 'odd', 'to', 'explain', '.', 'in', 'some', 'cases', ',', 'i', \"'\", 'm', 'able', 'to', 'hear', 'the', 'lyrics', 'in', 'songs', 'but', 'have', 'issues', 'retaining', 'them', 'and', 'understanding', 'them', '.', 'in', 'other', 'cases', ',', 'it', \"'\", 's', 'extremely', 'difficult', 'for', 'me', 'to', 'understand', 'what', 'is', 'actually', 'being', 'said', 'in', 'the', 'song', '.', 'both', 'these', 'situations', 'contribute', 'to', 'me', 'generally', 'not', 'knowing', 'the', 'lyrics', 'to', 'songs', 'and', 'not', 'understanding', 'what', 'the', 'songs', 'are', 'about', '.', 'i', \"'\", 'm', 'still', 'able', 'to', 'enjoy', 'the', 'vocals', 'in', 'a', 'song', ',', 'but', 'not', 'necessarily', 'the', 'actual', 'content', 'of', 'what', 'they', \"'\", 're', 'saying', '.', '(', 'just', 'the', 'sound', ',', 'fluctuations', ',', 'and', 'how', 'they', 'mesh', 'with', 'the', 'song', 'i', 'guess', '?', ')', '.', 'if', 'that', 'makes', 'sense', 'at', 'all', '.', 'having', 'said', 'that', ',', 'normal', 'day', '-', 'to', '-', 'day', 'conversations', 'aren', \"'\", 't', 'difficult', 'for', 'me', '.', 'i', 'understand', 'what', 'people', 'are', 'saying', 'to', 'me', 'and', 'what', \"'\", 's', 'going', 'on', 'around', 'me', '.', 'i', 'ask', 'here', 'because', 'i', 'have', 'add', 'and', 'i', 'have', 'found', 'that', 'my', 'medicine', 'helps', 'with', 'this', 'issue', '.', 'when', 'i', 'first', 'started', 'taking', 'my', 'rita', '##lin', 'and', 'was', 'listening', 'to', 'music', 'in', 'my', 'car', '-', 'i', 'was', 'actually', 'listening', 'to', 'the', 'song', '!', 'i', 'knew', 'what', 'was', 'being', 'said', 'and', 'what', 'the', 'song', 'was', 'about', '!', 'it', 'was', 'so', 'un', '##bel', '##ie', '##va', '##bly', 'amazing', 'i', 'almost', 'wanted', 'to', 'cry', '.', 'occasionally', 'i', 'have', 'moments', 'throughout', 'the', 'day', 'where', 'lyrics', 'stand', 'out', 'better', 'but', 'i', 'haven', \"'\", 't', 'had', 'much', 'figuring', 'out', 'what', 'has', 'helped', 'me', 'other', 'than', 'rita', '##lin', '.', 'has', 'anyone', 'else', 'ever', 'experienced', 'a', 'similar', 'issue', '?', 'do', 'you', 'have', 'any', 'input', 'on', 'what', 'could', 'be', 'the', 'cause', 'or', 'what', 'could', 'possibly', 'help', '?', 'the', 'only', 'real', 'information', 'i', \"'\", 've', 'found', 'online', 'has', 'pointed', 'to', 'a', 'thing', 'called', 'auditory', 'processing', 'disorder', '.', 't', '##l', ';', 'dr', ':', 'i', 'have', 'trouble', 'hearing', '/', 'understanding', 'lyrics', 'in', 'songs', '.', 'rita', '##lin', 'seems', 'to', 'help', '.']\n",
      "INFO:__main__:Number of tokens: 429\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'have', 'trouble', 'dec', '##ip', '##hering', 'lyrics', '?', 'with', 'an', 'opening', 'like', 'that', ',', 'i', 'considered', 'posting', 'it', 'into', 'the', 'da', '##e', 'sub', '##red', '##dit', 'but', 'felt', 'that', '/', 'r', '/', 'ad', '##hd', 'was', 'more', 'applicable', 'from', 'my', 'experiences', 'that', 'i', 'discuss', 'below', '.', 'anyway', '##s', '.', '.', 'for', 'a', 'few', 'years', 'now', 'i', \"'\", 've', 'had', 'a', 'lot', 'of', 'trouble', 'with', 'picking', 'out', 'lyrics', 'from', 'songs', 'or', 'performances', '.', 'i', 'can', \"'\", 't', 'really', 'pin', '##point', 'when', 'this', 'may', 'have', 'started', 'because', 'i', 'do', 'know', 'some', 'lyrics', 'to', 'various', 'songs', '.', 'bear', 'with', 'me', 'because', 'it', \"'\", 's', 'a', 'tad', 'odd', 'to', 'explain', '.', 'in', 'some', 'cases', ',', 'i', \"'\", 'm', 'able', 'to', 'hear', 'the', 'lyrics', 'in', 'songs', 'but', 'have', 'issues', 'retaining', 'them', 'and', 'understanding', 'them', '.', 'in', 'other', 'cases', ',', 'it', \"'\", 's', 'extremely', 'difficult', 'for', 'me', 'to', 'understand', 'what', 'is', 'actually', 'being', 'said', 'in', 'the', 'song', '.', 'both', 'these', 'situations', 'contribute', 'to', 'me', 'generally', 'not', 'knowing', 'the', 'lyrics', 'to', 'songs', 'and', 'not', 'understanding', 'what', 'the', 'songs', 'are', 'about', '.', 'i', \"'\", 'm', 'still', 'able', 'to', 'enjoy', 'the', 'vocals', 'in', 'a', 'song', ',', 'but', 'not', 'necessarily', 'the', 'actual', 'content', 'of', 'what', 'they', \"'\", 're', 'saying', '.', '(', 'just', 'the', 'sound', ',', 'fluctuations', ',', 'and', 'how', 'they', 'mesh', 'with', 'the', 'song', 'i', 'guess', '?', ')', '.', 'if', 'that', 'makes', 'sense', 'at', 'all', '.', 'having', 'said', 'that', ',', 'normal', 'day', '-', 'to', '-', 'day', 'conversations', 'aren', \"'\", 't', 'difficult', 'for', 'me', '.', 'i', 'understand', 'what', 'people', 'are', 'saying', 'to', 'me', 'and', 'what', \"'\", 's', 'going', 'on', 'around', 'me', '.', 'i', 'ask', 'here', 'because', 'i', 'have', 'add', 'and', 'i', 'have', 'found', 'that', 'my', 'medicine', 'helps', 'with', 'this', 'issue', '.', 'when', 'i', 'first', 'started', 'taking', 'my', 'rita', '##lin', 'and', 'was', 'listening', 'to', 'music', 'in', 'my', 'car', '-', 'i', 'was', 'actually', 'listening', 'to', 'the', 'song', '!', 'i', 'knew', 'what', 'was', 'being', 'said', 'and', 'what', 'the', 'song', 'was', 'about', '!', 'it', 'was', 'so', 'un', '##bel', '##ie', '##va', '##bly', 'amazing', 'i', 'almost', 'wanted', 'to', 'cry', '.', 'occasionally', 'i', 'have', 'moments', 'throughout', 'the', 'day', 'where', 'lyrics', 'stand', 'out', 'better', 'but', 'i', 'haven', \"'\", 't', 'had', 'much', 'figuring', 'out', 'what', 'has', 'helped', 'me', 'other', 'than', 'rita', '##lin', '.', 'has', 'anyone', 'else', 'ever', 'experienced', 'a', 'similar', 'issue', '?', 'do', 'you', 'have', 'any', 'input', 'on', 'what', 'could', 'be', 'the', 'cause', 'or', 'what', 'could', 'possibly', 'help', '?', 'the', 'only', 'real', 'information', 'i', \"'\", 've', 'found', 'online', 'has', 'pointed', 'to', 'a', 'thing', 'called', 'auditory', 'processing', 'disorder', '.', 't', '##l', ';', 'dr', ':', 'i', 'have', 'trouble', 'hearing', '/', 'understanding', 'lyrics', 'in', 'songs', '.', 'rita', '##lin', 'seems', 'to', 'help', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', 'anyone', 'else', 'never', 'remember', 'people', \"'\", 's', 'names', '?', 'i', \"'\", 'm', 'not', 'sure', 'if', 'this', 'is', 'solely', 'add', ',', 'though', 'i', 'suspect', 'it', 'does', 'because', 'this', 'issue', 'has', 'to', 'do', 'with', 'recall', '.', 'for', 'me', 'it', 'generally', 'happens', 'with', 'people', \"'\", 's', 'names', 'from', 'classes', '.', 'i', \"'\", 'll', 'run', 'into', 'them', 'at', 'a', 'party', 'and', 'know', 'exactly', 'who', 'they', 'are', 'but', 'not', 'be', 'able', 'to', 'think', 'of', 'their', 'name', '.', 'and', 'these', 'are', 'small', 'classes', ',', '5', 'to', '18', 'kids', '.']\n",
      "INFO:__main__:Number of tokens: 84\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', 'anyone', 'else', 'never', 'remember', 'people', \"'\", 's', 'names', '?', 'i', \"'\", 'm', 'not', 'sure', 'if', 'this', 'is', 'solely', 'add', ',', 'though', 'i', 'suspect', 'it', 'does', 'because', 'this', 'issue', 'has', 'to', 'do', 'with', 'recall', '.', 'for', 'me', 'it', 'generally', 'happens', 'with', 'people', \"'\", 's', 'names', 'from', 'classes', '.', 'i', \"'\", 'll', 'run', 'into', 'them', 'at', 'a', 'party', 'and', 'know', 'exactly', 'who', 'they', 'are', 'but', 'not', 'be', 'able', 'to', 'think', 'of', 'their', 'name', '.', 'and', 'these', 'are', 'small', 'classes', ',', '5', 'to', '18', 'kids', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['when', 'your', 'add', 'ad', '##hd', 'is', 'so', 'severe', 'you', 'are', 'totally', 'attention', '##less', '.']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['when', 'your', 'add', 'ad', '##hd', 'is', 'so', 'severe', 'you', 'are', 'totally', 'attention', '##less', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '(', 'dex', '##ed', '##rine', 'if', 'we', 'are', 'to', 'be', 'specific', ')', 'and', 'weed']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '(', 'dex', '##ed', '##rine', 'if', 'we', 'are', 'to', 'be', 'specific', ')', 'and', 'weed']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['canadian', 'students', 'with', 'loans', 'looking', 'to', 'get', 'assessed', 'just', 'to', 'share', 'in', 'case', 'anyone', 'comes', 'looking', 'here', 'for', 'help', 'finding', 'assessments', '.', 'when', 'i', 'was', 'assessed', 'i', 'was', 'on', 'os', '##ap', '(', 'ontario', 'loans', ')', 'and', 'because', 'of', 'that', 'my', 'entire', 'assessment', 'fees', 'were', 'covered', 'by', 'the', 'gov', \"'\", 't', '.', 'i', 'also', 'got', 'extra', 'grants', 'for', 'being', 'a', 'student', 'with', 'a', 'disability', '.', 'so', 'if', 'you', 'think', 'you', 'need', 'assessing', 'and', 'can', \"'\", 't', 'afford', 'it', ',', 'look', 'into', 'that', ':', ')']\n",
      "INFO:__main__:Number of tokens: 83\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['canadian', 'students', 'with', 'loans', 'looking', 'to', 'get', 'assessed', 'just', 'to', 'share', 'in', 'case', 'anyone', 'comes', 'looking', 'here', 'for', 'help', 'finding', 'assessments', '.', 'when', 'i', 'was', 'assessed', 'i', 'was', 'on', 'os', '##ap', '(', 'ontario', 'loans', ')', 'and', 'because', 'of', 'that', 'my', 'entire', 'assessment', 'fees', 'were', 'covered', 'by', 'the', 'gov', \"'\", 't', '.', 'i', 'also', 'got', 'extra', 'grants', 'for', 'being', 'a', 'student', 'with', 'a', 'disability', '.', 'so', 'if', 'you', 'think', 'you', 'need', 'assessing', 'and', 'can', \"'\", 't', 'afford', 'it', ',', 'look', 'into', 'that', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['special', 'accommodations', 'for', 'a', 'national', 'exam']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['special', 'accommodations', 'for', 'a', 'national', 'exam']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dr', '.', 'hall', '##owe', '##ll', '’', 's', 'response', 'to', 'ny', 'times', 'piece', '“', 'rita', '##lin', 'gone', 'wrong', '.', '”']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dr', '.', 'hall', '##owe', '##ll', '’', 's', 'response', 'to', 'ny', 'times', 'piece', '“', 'rita', '##lin', 'gone', 'wrong', '.', '”']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', '\"', 'rita', '##lin', 'gone', 'wrong', '\"', 'is', 'wrong']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', '\"', 'rita', '##lin', 'gone', 'wrong', '\"', 'is', 'wrong']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', '“', 'rita', '##lin', 'gone', 'wrong', '”', 'is', 'wrong', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', '“', 'rita', '##lin', 'gone', 'wrong', '”', 'is', 'wrong', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', '“', 'rita', '##lin', 'gone', 'wrong', '”', 'is', 'wrong', '?', '(', 'huffington', 'post', ')']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', '“', 'rita', '##lin', 'gone', 'wrong', '”', 'is', 'wrong', '?', '(', 'huffington', 'post', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'the', 'signs', 'i', 'should', 'look', 'for', 'to', 'know', 'if', 'i', 'have', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'the', 'signs', 'i', 'should', 'look', 'for', 'to', 'know', 'if', 'i', 'have', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'make', 'lists', '?', 'hello', 'everyone', '!', 'i', 'recently', 'returned', 'from', 'an', 'appointment', 'where', 'my', 'doctor', 'and', 'i', 'discussed', 'the', 'tricks', 'i', 'used', '(', 'se', '##gre', '##gating', 'work', '/', 'play', 'areas', ',', 'incentives', 'for', 'finishing', 'a', 'task', ',', 'etc', '.', ')', 'one', 'of', 'the', 'things', 'that', 'was', 'missing', 'that', 'i', 'heard', 'was', 'effective', 'was', 'making', 'lists', '.', 'i', 'want', 'to', 'try', 'my', 'hand', 'at', 'it', ',', 'but', 'i', 'think', 'it', 'would', 'be', 'best', 'if', 'i', 'heard', 'the', 'way', 'that', 'you', 'guys', 'do', 'it', '.', 'what', 'goes', 'into', 'making', 'your', 'lists', '?', 'what', 'is', 'the', 'process', 'like', '?', 'tips', 'and', 'tricks', 'you', 'found', 'after', 'making', 'them', 'for', 'a', 'while', '?', 'i', 'think', 'that', 'many', 'of', 'us', 'here', 'can', 'benefit', 'from', 'such', 'an', 'a', 'discussion', '.']\n",
      "INFO:__main__:Number of tokens: 126\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'make', 'lists', '?', 'hello', 'everyone', '!', 'i', 'recently', 'returned', 'from', 'an', 'appointment', 'where', 'my', 'doctor', 'and', 'i', 'discussed', 'the', 'tricks', 'i', 'used', '(', 'se', '##gre', '##gating', 'work', '/', 'play', 'areas', ',', 'incentives', 'for', 'finishing', 'a', 'task', ',', 'etc', '.', ')', 'one', 'of', 'the', 'things', 'that', 'was', 'missing', 'that', 'i', 'heard', 'was', 'effective', 'was', 'making', 'lists', '.', 'i', 'want', 'to', 'try', 'my', 'hand', 'at', 'it', ',', 'but', 'i', 'think', 'it', 'would', 'be', 'best', 'if', 'i', 'heard', 'the', 'way', 'that', 'you', 'guys', 'do', 'it', '.', 'what', 'goes', 'into', 'making', 'your', 'lists', '?', 'what', 'is', 'the', 'process', 'like', '?', 'tips', 'and', 'tricks', 'you', 'found', 'after', 'making', 'them', 'for', 'a', 'while', '?', 'i', 'think', 'that', 'many', 'of', 'us', 'here', 'can', 'benefit', 'from', 'such', 'an', 'a', 'discussion', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'got', 'diagnosed', 'with', 'add', 'this', 'week', '.']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'got', 'diagnosed', 'with', 'add', 'this', 'week', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'thought', 'like', 'this', 'for', 'years', '(', 'x', '-', 'post', 'from', '/', 'quotes', '##por', '##n']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'thought', 'like', 'this', 'for', 'years', '(', 'x', '-', 'post', 'from', '/', 'quotes', '##por', '##n']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['conflict', '##ed', 'about', 'ad', '##hd', '-', 'kinda', 'high', 'functioning', 'but', 'not', 'really']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['conflict', '##ed', 'about', 'ad', '##hd', '-', 'kinda', 'high', 'functioning', 'but', 'not', 'really']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'in', 'the', 'uk', '.', 'i', \"'\", 'm', 'a', '24', 'year', 'old', 'male', 'and', 'in', 'a', 'nuts', '##hell', 'i', \"'\", 've', 'always', 'have', 'major', 'problems', 'with', 'concentration', ',', 'pro', '##cr', '##ast', '##ination', 'and', 'other', 'issues', 'i', 'recognise', 'as', 'possible', 'symptoms', 'to', 'add', '.', 'after', 'a', 'long', 'talk', 'with', 'my', 'friend', 'who', \"'\", 's', 'brother', 'was', 'diagnosed', ',', 'reflecting', 'and', 'doing', 'research', 'i', 'really', 'think', 'i', 'may', 'have', 'it', '.', 'my', 'question', 'is', 'wet', '##her', 'anyone', 'knows', 'about', 'diagnosis', 'and', 'treatment', 'in', 'the', 'uk', '?', 'i', 'have', 'been', 'thinking', 'about', 'going', 'to', 'the', 'doctors', 'for', 'a', 'whole', '(', 'read', ':', '3', '+', 'months', ')', 'but', 'don', \"'\", 't', 'know', 'what', 'to', 'say', 'and', 'feel', 'like', 'i', 'wouldn', \"'\", 't', 'know', 'how', 'to', 'get', 'what', 'i', \"'\", 'm', 'thinking', 'out', 'properly', ',', 'if', 'that', 'makes', 'sense', '?', 'any', 'help', 'or', 'advice', 'will', 'be', 'greatly', 'appreciated']\n",
      "INFO:__main__:Number of tokens: 144\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'in', 'the', 'uk', '.', 'i', \"'\", 'm', 'a', '24', 'year', 'old', 'male', 'and', 'in', 'a', 'nuts', '##hell', 'i', \"'\", 've', 'always', 'have', 'major', 'problems', 'with', 'concentration', ',', 'pro', '##cr', '##ast', '##ination', 'and', 'other', 'issues', 'i', 'recognise', 'as', 'possible', 'symptoms', 'to', 'add', '.', 'after', 'a', 'long', 'talk', 'with', 'my', 'friend', 'who', \"'\", 's', 'brother', 'was', 'diagnosed', ',', 'reflecting', 'and', 'doing', 'research', 'i', 'really', 'think', 'i', 'may', 'have', 'it', '.', 'my', 'question', 'is', 'wet', '##her', 'anyone', 'knows', 'about', 'diagnosis', 'and', 'treatment', 'in', 'the', 'uk', '?', 'i', 'have', 'been', 'thinking', 'about', 'going', 'to', 'the', 'doctors', 'for', 'a', 'whole', '(', 'read', ':', '3', '+', 'months', ')', 'but', 'don', \"'\", 't', 'know', 'what', 'to', 'say', 'and', 'feel', 'like', 'i', 'wouldn', \"'\", 't', 'know', 'how', 'to', 'get', 'what', 'i', \"'\", 'm', 'thinking', 'out', 'properly', ',', 'if', 'that', 'makes', 'sense', '?', 'any', 'help', 'or', 'advice', 'will', 'be', 'greatly', 'appreciated']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['went', 'to', 'f', '##du', \"'\", 's', 'adult', 'ad', '##hd', 'testing', ',', 'got', 'back', 'results', '.', 'so', 'i', 'recently', 'went', 'to', 'f', '##du', '(', 'fair', '##leigh', 'dickinson', 'university', ')', 'adult', 'ad', '##hd', 'clinic', 'where', 'they', 'administered', 'these', 'tests', ':', '*', 'california', 'verbal', 'learning', 'test', '-', 'ii', '*', 'conner', \"'\", 's', 'adult', 'ad', '##hd', 'rating', 'scales', 'self', '-', 'report', ':', 'long', 'version', '*', 'conner', \"'\", 's', 'adult', 'ad', '##hd', 'rating', 'scales', 'observer', ':', 'long', 'version', '*', 'conner', \"'\", 's', 'continues', 'performance', 'test', '-', 'ii', '*', 'del', '##is', '-', 'kaplan', 'executive', 'functioning', 'system', ':', 'trail', 'making', ',', 'verbal', 'flu', '##ency', ',', 'color', 'word', 'interference', ',', 'and', 'tower', 'test', '*', 'minnesota', 'multi', '##pha', '##sic', 'personality', 'inventory', '2', '*', 'paced', 'auditory', 'serial', 'addition', 'test', '(', 'this', 'test', 'sucks', 'bt', '##w', ')', '*', 'rey', '-', 'os', '##ter', '##rie', '##th', 'complex', 'figure', ':', 'copy', ',', 'immediate', ',', 'and', 'delay', '*', 'social', 'adjustment', 'scale', 'self', '-', 'report', '*', 'we', '##chs', '##ler', 'adult', 'intelligence', 'scale', '-', 'forth', 'edition', '*', 'wood', '##cock', '-', 'johnson', 'iii', 'test', 'of', 'achievement', 'form', 'a', ':', 'sub', '##test', '##s', '1', '-', '12', 'it', 'was', 'hours', 'to', 'complete', 'these', 'test', 'and', 'very', 'through', 'i', 'also', 'allowed', 'them', 'to', 'use', 'my', 'information', 'in', 'studies', 'and', 'statistics', 'to', 'help', 'figure', 'out', 'ad', '##hd', 'in', 'adults', '.', 'bottom', '-', 'line', 'i', 'apparently', 'suffer', 'from', ':', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', ',', 'pre', '##dom', '##inate', '##ly', 'ina', '##tten', '##tive', 'type', '.', 'i', 'highly', 'recommend', 'anybody', 'in', 'the', 'the', 'area', 'surrounding', 'this', 'place', 'to', 'get', 'tested', 'or', 'find', 'a', 'local', 'school', 'who', 'administer', '##s', 'similar', 'testing', '.', 'it', 'helps', 'really', 'pin', '##point', 'what', \"'\", 's', 'going', 'on', 'with', 'you', 'much', 'better', 'than', 'i', 'have', 'ever', 'had', 'before', '.', 'i', 'now', 'have', 'a', 'list', 'of', 'recommendations', 'and', 'will', 'be', 'looking', 'for', 'additional', 'help', 'beyond', 'that', 'of', 'mere', 'medicine', '.']\n",
      "INFO:__main__:Number of tokens: 300\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['went', 'to', 'f', '##du', \"'\", 's', 'adult', 'ad', '##hd', 'testing', ',', 'got', 'back', 'results', '.', 'so', 'i', 'recently', 'went', 'to', 'f', '##du', '(', 'fair', '##leigh', 'dickinson', 'university', ')', 'adult', 'ad', '##hd', 'clinic', 'where', 'they', 'administered', 'these', 'tests', ':', '*', 'california', 'verbal', 'learning', 'test', '-', 'ii', '*', 'conner', \"'\", 's', 'adult', 'ad', '##hd', 'rating', 'scales', 'self', '-', 'report', ':', 'long', 'version', '*', 'conner', \"'\", 's', 'adult', 'ad', '##hd', 'rating', 'scales', 'observer', ':', 'long', 'version', '*', 'conner', \"'\", 's', 'continues', 'performance', 'test', '-', 'ii', '*', 'del', '##is', '-', 'kaplan', 'executive', 'functioning', 'system', ':', 'trail', 'making', ',', 'verbal', 'flu', '##ency', ',', 'color', 'word', 'interference', ',', 'and', 'tower', 'test', '*', 'minnesota', 'multi', '##pha', '##sic', 'personality', 'inventory', '2', '*', 'paced', 'auditory', 'serial', 'addition', 'test', '(', 'this', 'test', 'sucks', 'bt', '##w', ')', '*', 'rey', '-', 'os', '##ter', '##rie', '##th', 'complex', 'figure', ':', 'copy', ',', 'immediate', ',', 'and', 'delay', '*', 'social', 'adjustment', 'scale', 'self', '-', 'report', '*', 'we', '##chs', '##ler', 'adult', 'intelligence', 'scale', '-', 'forth', 'edition', '*', 'wood', '##cock', '-', 'johnson', 'iii', 'test', 'of', 'achievement', 'form', 'a', ':', 'sub', '##test', '##s', '1', '-', '12', 'it', 'was', 'hours', 'to', 'complete', 'these', 'test', 'and', 'very', 'through', 'i', 'also', 'allowed', 'them', 'to', 'use', 'my', 'information', 'in', 'studies', 'and', 'statistics', 'to', 'help', 'figure', 'out', 'ad', '##hd', 'in', 'adults', '.', 'bottom', '-', 'line', 'i', 'apparently', 'suffer', 'from', ':', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', ',', 'pre', '##dom', '##inate', '##ly', 'ina', '##tten', '##tive', 'type', '.', 'i', 'highly', 'recommend', 'anybody', 'in', 'the', 'the', 'area', 'surrounding', 'this', 'place', 'to', 'get', 'tested', 'or', 'find', 'a', 'local', 'school', 'who', 'administer', '##s', 'similar', 'testing', '.', 'it', 'helps', 'really', 'pin', '##point', 'what', \"'\", 's', 'going', 'on', 'with', 'you', 'much', 'better', 'than', 'i', 'have', 'ever', 'had', 'before', '.', 'i', 'now', 'have', 'a', 'list', 'of', 'recommendations', 'and', 'will', 'be', 'looking', 'for', 'additional', 'help', 'beyond', 'that', 'of', 'mere', 'medicine', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', 'anyone', 'relate', 'to', 'this', '?', 'ca', '##co', '##phon', '##ous', 'noise', 'so', 'i', \"'\", 'm', 'sitting', 'in', 'the', 'campus', 'food', 'hall', 'and', 'the', 'local', 'middle', 'school', 'just', 'came', 'in', 'to', 'eat', 'for', 'some', 'reason', '.', 'we', 'all', 'know', 'how', 'chaotic', '##ally', 'loud', 'these', 'bug', '##gers', 'are', 'when', 'left', 'to', 'their', 'own', 'devices', '.', 'my', 'add', '-', 'pi', 'makes', 'ca', '##co', '##phon', '##ous', 'noises', 'like', 'this', 'nearly', 'unbearable', 'not', 'as', 'in', 'too', 'loud', ',', 'but', 'they', 'make', 'me', 'feel', 'like', 'i', 'am', 'going', 'insane', ',', 'like', 'one', 'tiny', 'little', 'int', '##rus', '##ive', 'thought', '-', 'bullet', 'is', 'being', 'shot', 'into', 'my', 'brain', 'with', 'each', 'little', 'fuck', '##tar', '##d', 'that', 'is', 'opening', 'their', 'head', '##hole', 'and', 'adding', 'to', 'the', 'st', '##if', '##ling', 'mass', 'of', 'noise', '-', 'thought', '-', 'locus', '##ts', 'cloud', '##ing', 'the', 'entire', 'food', 'hall', 'when', 'i', 'was', 'in', 'middle', 'and', 'high', 'school', 'sometimes', 'i', 'would', 'have', 'to', 'go', 'to', 'the', 'bathroom', 'and', 'curl', 'up', 'in', 'a', 'stall', 'to', 'make', 'it', 'stop', '.', 'this', 'reminds', 'me', 'of', 'my', 'cousin', 'with', 'as', '##per', '##gers', 'that', 'can', \"'\", 't', 'deal', 'with', 'noise', 'either', '.', 'does', 'anyone', 'else', 'feel', 'like', 'this', '?', 'oh', ',', 'and', 'they', 'ate', 'all', 'the', 'pizza', 'and', 'ice', 'cream', '.']\n",
      "INFO:__main__:Number of tokens: 199\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', 'anyone', 'relate', 'to', 'this', '?', 'ca', '##co', '##phon', '##ous', 'noise', 'so', 'i', \"'\", 'm', 'sitting', 'in', 'the', 'campus', 'food', 'hall', 'and', 'the', 'local', 'middle', 'school', 'just', 'came', 'in', 'to', 'eat', 'for', 'some', 'reason', '.', 'we', 'all', 'know', 'how', 'chaotic', '##ally', 'loud', 'these', 'bug', '##gers', 'are', 'when', 'left', 'to', 'their', 'own', 'devices', '.', 'my', 'add', '-', 'pi', 'makes', 'ca', '##co', '##phon', '##ous', 'noises', 'like', 'this', 'nearly', 'unbearable', 'not', 'as', 'in', 'too', 'loud', ',', 'but', 'they', 'make', 'me', 'feel', 'like', 'i', 'am', 'going', 'insane', ',', 'like', 'one', 'tiny', 'little', 'int', '##rus', '##ive', 'thought', '-', 'bullet', 'is', 'being', 'shot', 'into', 'my', 'brain', 'with', 'each', 'little', 'fuck', '##tar', '##d', 'that', 'is', 'opening', 'their', 'head', '##hole', 'and', 'adding', 'to', 'the', 'st', '##if', '##ling', 'mass', 'of', 'noise', '-', 'thought', '-', 'locus', '##ts', 'cloud', '##ing', 'the', 'entire', 'food', 'hall', 'when', 'i', 'was', 'in', 'middle', 'and', 'high', 'school', 'sometimes', 'i', 'would', 'have', 'to', 'go', 'to', 'the', 'bathroom', 'and', 'curl', 'up', 'in', 'a', 'stall', 'to', 'make', 'it', 'stop', '.', 'this', 'reminds', 'me', 'of', 'my', 'cousin', 'with', 'as', '##per', '##gers', 'that', 'can', \"'\", 't', 'deal', 'with', 'noise', 'either', '.', 'does', 'anyone', 'else', 'feel', 'like', 'this', '?', 'oh', ',', 'and', 'they', 'ate', 'all', 'the', 'pizza', 'and', 'ice', 'cream', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'prescribed', 'v', '##y', '##van', '##se', 'and', 'well', '##bu', '##tri', '##n', '?', 'i', \"'\", 've', 'been', 'on', 'v', '##y', '##van', '##se', '(', '30', '##mg', 'for', '2', 'weeks', 'and', 'then', '40', '##mg', 'for', 'the', 'last', 'three', 'months', ')', 'and', 'it', 'has', 'helped', 'a', 'lot', 'with', 'my', 'symptoms', '.', 'everything', 'has', 'been', 'going', 'fairly', 'well', 'but', 'i', \"'\", 've', 'been', 'more', 'and', 'more', 'stressed', 'recently', '.', 'i', 'relay', '##ed', 'this', 'at', 'my', 'appointment', 'today', 'and', 'he', 'seems', 'to', 'think', 'well', '##bu', '##tri', '##n', 'is', 'in', 'order', '.', 'i', \"'\", 'm', 'just', 'stressed', 'because', 'of', 'financial', 'issues', ',', 'having', 'young', 'children', 'and', 'the', 'fact', 'that', 'it', 'is', 'likely', 'i', 'have', 'cr', '##oh', '##n', \"'\", 's', '(', 'find', 'out', 'for', 'sure', 'on', 'monday', ')', '.', 'i', 'have', 'always', 'been', 'a', 'little', 'skeptical', 'of', 'taking', 'drugs', 'without', 'knowing', 'a', 'whole', 'lot', 'about', 'them', '.', 'i', \"'\", 've', 'started', 'researching', 'but', 'i', 'want', 'to', 'know', 'if', 'anyone', 'here', 'has', 'used', 'it', ',', 'specifically', 'in', 'conjunction', 'with', 'v', '##y', '##van', '##se', 'or', 'dex', '##ed', '##rine', '.', 'thanks', 'in', 'advance', '.']\n",
      "INFO:__main__:Number of tokens: 174\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'prescribed', 'v', '##y', '##van', '##se', 'and', 'well', '##bu', '##tri', '##n', '?', 'i', \"'\", 've', 'been', 'on', 'v', '##y', '##van', '##se', '(', '30', '##mg', 'for', '2', 'weeks', 'and', 'then', '40', '##mg', 'for', 'the', 'last', 'three', 'months', ')', 'and', 'it', 'has', 'helped', 'a', 'lot', 'with', 'my', 'symptoms', '.', 'everything', 'has', 'been', 'going', 'fairly', 'well', 'but', 'i', \"'\", 've', 'been', 'more', 'and', 'more', 'stressed', 'recently', '.', 'i', 'relay', '##ed', 'this', 'at', 'my', 'appointment', 'today', 'and', 'he', 'seems', 'to', 'think', 'well', '##bu', '##tri', '##n', 'is', 'in', 'order', '.', 'i', \"'\", 'm', 'just', 'stressed', 'because', 'of', 'financial', 'issues', ',', 'having', 'young', 'children', 'and', 'the', 'fact', 'that', 'it', 'is', 'likely', 'i', 'have', 'cr', '##oh', '##n', \"'\", 's', '(', 'find', 'out', 'for', 'sure', 'on', 'monday', ')', '.', 'i', 'have', 'always', 'been', 'a', 'little', 'skeptical', 'of', 'taking', 'drugs', 'without', 'knowing', 'a', 'whole', 'lot', 'about', 'them', '.', 'i', \"'\", 've', 'started', 'researching', 'but', 'i', 'want', 'to', 'know', 'if', 'anyone', 'here', 'has', 'used', 'it', ',', 'specifically', 'in', 'conjunction', 'with', 'v', '##y', '##van', '##se', 'or', 'dex', '##ed', '##rine', '.', 'thanks', 'in', 'advance', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['evolutionary', 'psychiatry', ';', 'ad', '##hd', ',', 'food', 'additive', '##s', ',', 'and', 'his', '##tam', '##ine']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['evolutionary', 'psychiatry', ';', 'ad', '##hd', ',', 'food', 'additive', '##s', ',', 'and', 'his', '##tam', '##ine']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hello', '!', 'would', 'anybody', 'like', 'to', 'make', 'a', 'friend', '?', ':', '-', ')', 'this', 'might', 'come', 'off', 'as', 'an', 'odd', 'request', ',', 'but', '.', '.', '.', 'would', 'anyone', 'like', 'to', 'make', 'internet', 'friends', 'with', 'someone', 'who', 'has', 'ad', '##hd', '?', 'i', 'have', 'no', 'friends', 'with', 'it', ',', 'and', 'i', 'feel', 'i', 'need', 'to', 'talk', 'to', 'somebody', 'in', 'the', 'same', 'situation', 'as', 'me', '.', 'i', 'was', 'self', '-', 'referred', 'to', 'a', 'ne', '##uro', '##psy', '##cho', '##logist', 'and', 'was', 'told', 'i', 'most', 'likely', 'have', 'ad', '##hd', '-', 'pi', '.', 'i', \"'\", 'm', 'in', 'the', 'process', 'of', 'trying', 'to', 'find', 'a', 'psychiatrist', 'who', 'understands', 'the', 'disorder', ',', 'but', 'i', \"'\", 'm', 'not', 'having', 'much', 'luck', 'yet', '.', 'since', 'this', 'is', 'a', 'small', 'sub', '##red', '##dit', ',', 'and', 'i', 'don', \"'\", 't', 'have', 'any', 'qu', '##al', '##ms', 'about', 'sharing', 'a', 'few', 'im', 'addresses', ',', 'i', \"'\", 'll', 'just', 'list', 'them', ':', '*', '*', 'sky', '##pe', ':', '*', '*', 'thor', '##the', '##nor', '##se', '*', '*', 'aim', ':', '*', '*', 'thor', '##the', '##nor', '##se', '*', '*', 'ms', '##n', '/', 'yi', '##m', ':', '*', '*', 'thor', '@', 'ninja', '##fr', '##og', '##s', '.', 'com', 'i', \"'\", 'm', '29', 'a', 'year', 'old', 'guy', 'from', 'norway', '.', 'my', 'interests', 'include', 'music', 'making', ',', 'drawing', ',', 'computer', 'science', 'and', 'electronic', 'circuits', ',', 'in', 'no', 'particular', 'order', '.', 'maybe', 'we', 'can', 'discuss', 'and', 'help', 'each', 'other', 'with', 'projects', '?', 'if', 'you', \"'\", 're', 'going', 'to', 'contact', 'me', 'on', 'sky', '##pe', ',', 'chat', 'to', 'me', 'first', '.', 'i', 'can', \"'\", 't', 'answer', 'calls', 'from', 'people', 'i', 'haven', \"'\", 't', 'chat', '##ted', 'with', 'first', '.', ':', '-', ')', '*', 'if', 'anybody', 'contacts', 'me', ',', 'i', 'might', 'put', 'everyone', 'in', 'touch', 'with', 'each', 'other', 'in', 'some', 'way', '.', 'i', 'run', 'a', 'few', 'websites', 'off', 'my', 'dedicated', 'server', ',', 'and', 'could', 'set', 'some', 'stuff', 'up', 'there', ',', 'maybe', '?', '*']\n",
      "INFO:__main__:Number of tokens: 304\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hello', '!', 'would', 'anybody', 'like', 'to', 'make', 'a', 'friend', '?', ':', '-', ')', 'this', 'might', 'come', 'off', 'as', 'an', 'odd', 'request', ',', 'but', '.', '.', '.', 'would', 'anyone', 'like', 'to', 'make', 'internet', 'friends', 'with', 'someone', 'who', 'has', 'ad', '##hd', '?', 'i', 'have', 'no', 'friends', 'with', 'it', ',', 'and', 'i', 'feel', 'i', 'need', 'to', 'talk', 'to', 'somebody', 'in', 'the', 'same', 'situation', 'as', 'me', '.', 'i', 'was', 'self', '-', 'referred', 'to', 'a', 'ne', '##uro', '##psy', '##cho', '##logist', 'and', 'was', 'told', 'i', 'most', 'likely', 'have', 'ad', '##hd', '-', 'pi', '.', 'i', \"'\", 'm', 'in', 'the', 'process', 'of', 'trying', 'to', 'find', 'a', 'psychiatrist', 'who', 'understands', 'the', 'disorder', ',', 'but', 'i', \"'\", 'm', 'not', 'having', 'much', 'luck', 'yet', '.', 'since', 'this', 'is', 'a', 'small', 'sub', '##red', '##dit', ',', 'and', 'i', 'don', \"'\", 't', 'have', 'any', 'qu', '##al', '##ms', 'about', 'sharing', 'a', 'few', 'im', 'addresses', ',', 'i', \"'\", 'll', 'just', 'list', 'them', ':', '*', '*', 'sky', '##pe', ':', '*', '*', 'thor', '##the', '##nor', '##se', '*', '*', 'aim', ':', '*', '*', 'thor', '##the', '##nor', '##se', '*', '*', 'ms', '##n', '/', 'yi', '##m', ':', '*', '*', 'thor', '@', 'ninja', '##fr', '##og', '##s', '.', 'com', 'i', \"'\", 'm', '29', 'a', 'year', 'old', 'guy', 'from', 'norway', '.', 'my', 'interests', 'include', 'music', 'making', ',', 'drawing', ',', 'computer', 'science', 'and', 'electronic', 'circuits', ',', 'in', 'no', 'particular', 'order', '.', 'maybe', 'we', 'can', 'discuss', 'and', 'help', 'each', 'other', 'with', 'projects', '?', 'if', 'you', \"'\", 're', 'going', 'to', 'contact', 'me', 'on', 'sky', '##pe', ',', 'chat', 'to', 'me', 'first', '.', 'i', 'can', \"'\", 't', 'answer', 'calls', 'from', 'people', 'i', 'haven', \"'\", 't', 'chat', '##ted', 'with', 'first', '.', ':', '-', ')', '*', 'if', 'anybody', 'contacts', 'me', ',', 'i', 'might', 'put', 'everyone', 'in', 'touch', 'with', 'each', 'other', 'in', 'some', 'way', '.', 'i', 'run', 'a', 'few', 'websites', 'off', 'my', 'dedicated', 'server', ',', 'and', 'could', 'set', 'some', 'stuff', 'up', 'there', ',', 'maybe', '?', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['success', 'stories', 'hey', ',', 'i', 'was', 'just', 'diagnosed', 'with', 'ad', '##hd', 'yesterday', '(', '1', '/', '31', '/', '12', ')', '.', 'i', 'have', 'been', 'battling', 'this', 'my', 'entire', 'life', '.', 'the', 'doctor', 'i', 'saw', 'put', 'me', 'on', 'we', '##lb', '##ut', '##rin', '.', 'i', 'am', 'curious', 'to', 'hear', 'from', 'people', 'that', 'have', 'overcome', 'this', 'set', '##back', '.', 'mostly', 'i', 'just', 'am', 'curious', 'as', 'to', 'what', 'life', 'will', 'be', 'like', 'once', 'the', 'symptoms', 'are', 'under', 'control', '.', 'thanks', '-', '-', 'revolver']\n",
      "INFO:__main__:Number of tokens: 78\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['success', 'stories', 'hey', ',', 'i', 'was', 'just', 'diagnosed', 'with', 'ad', '##hd', 'yesterday', '(', '1', '/', '31', '/', '12', ')', '.', 'i', 'have', 'been', 'battling', 'this', 'my', 'entire', 'life', '.', 'the', 'doctor', 'i', 'saw', 'put', 'me', 'on', 'we', '##lb', '##ut', '##rin', '.', 'i', 'am', 'curious', 'to', 'hear', 'from', 'people', 'that', 'have', 'overcome', 'this', 'set', '##back', '.', 'mostly', 'i', 'just', 'am', 'curious', 'as', 'to', 'what', 'life', 'will', 'be', 'like', 'once', 'the', 'symptoms', 'are', 'under', 'control', '.', 'thanks', '-', '-', 'revolver']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'meet', '##up', '?', 'so', '[', 'th', '##j', \"'\", 's', 'post', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'p', '##85', '##l', '##1', '/', 'hello', '_', 'would', '_', 'anybody', '_', 'like', '_', 'to', '_', 'make', '_', 'a', '_', 'friend', '/', ')', 'got', 'me', 'thinking', '.', 'we', 'all', 'feel', 'misunderstood', 'here', 'and', 'often', 'people', 'say', 'they', \"'\", 're', 'lonely', '.', 'so', 'why', 'not', 'see', 'if', 'there', \"'\", 's', 'anywhere', 'at', 'least', 'a', 'few', 'of', 'us', 'are', 'near', 'and', 'maybe', 'consider', 'a', 'casual', 'meet', '##up', '?', 'i', \"'\", 'd', 'love', 'to', 'know', 'some', 'people', 'who', 'think', 'like', 'me', '.', 'so', 'everyone', 'say', 'where', 'you', \"'\", 're', 'from', '!', 'i', \"'\", 'm', 'from', 'central', 'virginia', '.']\n",
      "INFO:__main__:Number of tokens: 126\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'meet', '##up', '?', 'so', '[', 'th', '##j', \"'\", 's', 'post', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'p', '##85', '##l', '##1', '/', 'hello', '_', 'would', '_', 'anybody', '_', 'like', '_', 'to', '_', 'make', '_', 'a', '_', 'friend', '/', ')', 'got', 'me', 'thinking', '.', 'we', 'all', 'feel', 'misunderstood', 'here', 'and', 'often', 'people', 'say', 'they', \"'\", 're', 'lonely', '.', 'so', 'why', 'not', 'see', 'if', 'there', \"'\", 's', 'anywhere', 'at', 'least', 'a', 'few', 'of', 'us', 'are', 'near', 'and', 'maybe', 'consider', 'a', 'casual', 'meet', '##up', '?', 'i', \"'\", 'd', 'love', 'to', 'know', 'some', 'people', 'who', 'think', 'like', 'me', '.', 'so', 'everyone', 'say', 'where', 'you', \"'\", 're', 'from', '!', 'i', \"'\", 'm', 'from', 'central', 'virginia', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'take', 'supplements', 'along', 'with', 'their', 'med', '##s', 'that', 'seem', 'to', 'help', '?', 'i', \"'\", 've', 'seen', 'around', 'the', 'internet', 'people', 'posting', 'about', 'taking', 'certain', 'supplements', 'along', 'with', 'their', 'medications', '(', 'things', 'like', 'omega', '3', \"'\", 's', 'and', 'fish', 'oils', ')', 'do', 'you', 'guys', 'find', 'anything', 'that', 'helps', 'you', 'out', '?', 'i', \"'\", 'm', 'actually', 'a', 'big', 'fan', 'of', 'drinking', 'a', 'cup', 'of', 'coffee', 'in', 'the', 'afternoon', 'to', 'prevent', 'a', 'crash', '.', '.', '.', 'and', 'for', 'some', 'reason', 'it', 'helps', 'me', 'sleep', 'later', 'in', 'the', 'evening', '.']\n",
      "INFO:__main__:Number of tokens: 88\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'take', 'supplements', 'along', 'with', 'their', 'med', '##s', 'that', 'seem', 'to', 'help', '?', 'i', \"'\", 've', 'seen', 'around', 'the', 'internet', 'people', 'posting', 'about', 'taking', 'certain', 'supplements', 'along', 'with', 'their', 'medications', '(', 'things', 'like', 'omega', '3', \"'\", 's', 'and', 'fish', 'oils', ')', 'do', 'you', 'guys', 'find', 'anything', 'that', 'helps', 'you', 'out', '?', 'i', \"'\", 'm', 'actually', 'a', 'big', 'fan', 'of', 'drinking', 'a', 'cup', 'of', 'coffee', 'in', 'the', 'afternoon', 'to', 'prevent', 'a', 'crash', '.', '.', '.', 'and', 'for', 'some', 'reason', 'it', 'helps', 'me', 'sleep', 'later', 'in', 'the', 'evening', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'sense', 'of', 'touch', 'i', \"'\", 'm', 'not', 'always', 'comfortable', 'with', 'being', 'touched', '.', 'it', \"'\", 's', 'got', 'nothing', 'to', 'do', 'with', 'intimacy', 'or', 'something', 'like', 'that', ',', 'just', 'the', 'feeling', 'of', 'being', 'touched', '.', '.', '.', 'i', \"'\", 'm', 'just', 'very', 'sensitive', '.', 'when', 'my', 'g', '##f', 'suddenly', 'embrace', '##s', 'me', 'from', 'behind', ',', 'i', 'really', 'can', \"'\", 't', 'stand', 'that', '.', 'i', 'lo', '##ath', '##e', 'massage', '##s', ',', 'though', 'i', 'can', 'give', 'a', 'massage', 'no', 'problem', '.', 'i', 'never', 'enjoyed', 'play', '##fighting', ',', 'with', 'a', 'lot', 'of', 'physical', 'contact', '.', 'it', \"'\", 's', 'okay', 'when', 'i', \"'\", 'm', 'really', 'relaxed', ',', 'which', 'i', \"'\", 'm', 'not', 'often', '.', 'does', 'anyone', 'recognize', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 115\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'sense', 'of', 'touch', 'i', \"'\", 'm', 'not', 'always', 'comfortable', 'with', 'being', 'touched', '.', 'it', \"'\", 's', 'got', 'nothing', 'to', 'do', 'with', 'intimacy', 'or', 'something', 'like', 'that', ',', 'just', 'the', 'feeling', 'of', 'being', 'touched', '.', '.', '.', 'i', \"'\", 'm', 'just', 'very', 'sensitive', '.', 'when', 'my', 'g', '##f', 'suddenly', 'embrace', '##s', 'me', 'from', 'behind', ',', 'i', 'really', 'can', \"'\", 't', 'stand', 'that', '.', 'i', 'lo', '##ath', '##e', 'massage', '##s', ',', 'though', 'i', 'can', 'give', 'a', 'massage', 'no', 'problem', '.', 'i', 'never', 'enjoyed', 'play', '##fighting', ',', 'with', 'a', 'lot', 'of', 'physical', 'contact', '.', 'it', \"'\", 's', 'okay', 'when', 'i', \"'\", 'm', 'really', 'relaxed', ',', 'which', 'i', \"'\", 'm', 'not', 'often', '.', 'does', 'anyone', 'recognize', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'a', 'personal', 'experience', 're', ':', 'add', '##eral', '##l', 'vs', '.', 'v', '##y', '##van', '##se', '.', 'hey', 'r', '/', 'ad', '##hd', ':', ')', 'on', 'add', '##eral', '##l', ',', 'i', 'start', 'to', 'get', 'throbbing', 'headache', '##s', ',', 'it', 'con', '##st', '##ric', '##ts', 'my', 'muscles', '(', 'especially', 'in', 'my', 'neck', ')', ',', 'and', 'coming', 'down', 'from', 'it', '(', 'when', 'the', 'dos', '##age', 'starts', 'clearing', 'out', 'of', 'my', 'body', ')', 'causes', 'anxiety', '.', 'on', 'v', '##y', '##van', '##se', ',', 'i', 'feel', 'focused', '.', 'that', \"'\", 's', 'it', '.', 'i', 'also', 'tried', 'rita', '##lin', 'and', 'dex', '##ad', '##rin', 'and', 'both', 'caused', 'me', 'anxiety', '.', 'so', ',', 'at', 'this', 'time', 'v', '##y', '##van', '##se', 'is', 'working', 'really', 'well', 'for', 'me', '.', 'your', 'experience', 'may', 'be', 'different', ',', 'but', 'i', 'just', 'wanted', 'to', 'share', ',', 'for', 'those', 'who', 'are', 'currently', 'experiencing', 'those', 'symptoms', 'with', 'add', '##eral', '##l', 'and', 'want', 'to', 'consider', 'v', '##y', '##van', '##se', '.', 'for', 'context', ':', 'i', \"'\", 'm', '29', '&', 'have', 'ad', '##hd', '-', 'pi', '.']\n",
      "INFO:__main__:Number of tokens: 164\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'a', 'personal', 'experience', 're', ':', 'add', '##eral', '##l', 'vs', '.', 'v', '##y', '##van', '##se', '.', 'hey', 'r', '/', 'ad', '##hd', ':', ')', 'on', 'add', '##eral', '##l', ',', 'i', 'start', 'to', 'get', 'throbbing', 'headache', '##s', ',', 'it', 'con', '##st', '##ric', '##ts', 'my', 'muscles', '(', 'especially', 'in', 'my', 'neck', ')', ',', 'and', 'coming', 'down', 'from', 'it', '(', 'when', 'the', 'dos', '##age', 'starts', 'clearing', 'out', 'of', 'my', 'body', ')', 'causes', 'anxiety', '.', 'on', 'v', '##y', '##van', '##se', ',', 'i', 'feel', 'focused', '.', 'that', \"'\", 's', 'it', '.', 'i', 'also', 'tried', 'rita', '##lin', 'and', 'dex', '##ad', '##rin', 'and', 'both', 'caused', 'me', 'anxiety', '.', 'so', ',', 'at', 'this', 'time', 'v', '##y', '##van', '##se', 'is', 'working', 'really', 'well', 'for', 'me', '.', 'your', 'experience', 'may', 'be', 'different', ',', 'but', 'i', 'just', 'wanted', 'to', 'share', ',', 'for', 'those', 'who', 'are', 'currently', 'experiencing', 'those', 'symptoms', 'with', 'add', '##eral', '##l', 'and', 'want', 'to', 'consider', 'v', '##y', '##van', '##se', '.', 'for', 'context', ':', 'i', \"'\", 'm', '29', '&', 'have', 'ad', '##hd', '-', 'pi', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mapping', 'ad', '##hd', 'meet', '##ups']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mapping', 'ad', '##hd', 'meet', '##ups']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['decided', 'to', 'stop', 'taking', 'my', 'current', 'medication', ',', 'any', 'advice', 'for', 'being', 'productive', 'without', 'it', '?', 'i', 'take', 'dex', '##ed', '##rine', ',', 'and', 'have', 'for', 'about', '4', 'years', '.', 'the', 'last', '6', 'months', 'though', ',', 'it', \"'\", 's', 'steadily', 'caused', 'me', 'worse', 'and', 'worse', 'social', 'anxiety', ',', 'to', 'the', 'point', 'where', 'it', \"'\", 's', 'started', 'to', 'have', 'a', 'real', 'negative', 'impact', '.', 'i', 'really', 'can', \"'\", 't', 'stand', 'taking', 'it', 'anymore', '.', 'i', \"'\", 've', 'talked', 'to', 'my', 'doctor', ',', 'but', 'the', 'problem', 'is', 'that', 'i', \"'\", 'm', '600', '##km', 'away', 'at', 'school', ',', 'and', 'can', \"'\", 't', 'go', 'to', 'see', 'him', 'for', 'almost', 'a', 'month', '.', 'so', 'i', 'need', 'to', 'find', 'some', 'way', 'to', 'get', 'through', 'almost', 'a', 'month', 'of', 'university', 'without', 'ad', '##hd', 'medication', '.', 'i', 'don', \"'\", 't', 'really', 'see', 'any', 'alternative', 'at', 'this', 'point', ',', 'so', 'any', 'advice', 'would', 'be', 'much', 'appreciated', '.']\n",
      "INFO:__main__:Number of tokens: 147\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['decided', 'to', 'stop', 'taking', 'my', 'current', 'medication', ',', 'any', 'advice', 'for', 'being', 'productive', 'without', 'it', '?', 'i', 'take', 'dex', '##ed', '##rine', ',', 'and', 'have', 'for', 'about', '4', 'years', '.', 'the', 'last', '6', 'months', 'though', ',', 'it', \"'\", 's', 'steadily', 'caused', 'me', 'worse', 'and', 'worse', 'social', 'anxiety', ',', 'to', 'the', 'point', 'where', 'it', \"'\", 's', 'started', 'to', 'have', 'a', 'real', 'negative', 'impact', '.', 'i', 'really', 'can', \"'\", 't', 'stand', 'taking', 'it', 'anymore', '.', 'i', \"'\", 've', 'talked', 'to', 'my', 'doctor', ',', 'but', 'the', 'problem', 'is', 'that', 'i', \"'\", 'm', '600', '##km', 'away', 'at', 'school', ',', 'and', 'can', \"'\", 't', 'go', 'to', 'see', 'him', 'for', 'almost', 'a', 'month', '.', 'so', 'i', 'need', 'to', 'find', 'some', 'way', 'to', 'get', 'through', 'almost', 'a', 'month', 'of', 'university', 'without', 'ad', '##hd', 'medication', '.', 'i', 'don', \"'\", 't', 'really', 'see', 'any', 'alternative', 'at', 'this', 'point', ',', 'so', 'any', 'advice', 'would', 'be', 'much', 'appreciated', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['w', '##tf', '?', 'is', 'there', 'a', 'drug', '##less', 'cure', '?', 'hey', 'guys', ',', 'i', \"'\", 've', 'had', 'add', 'all', 'my', 'life', ',', 'was', 'diagnosed', 'not', 'that', 'long', 'ago', 'like', 'two', 'years', 'ago', '.', 'i', 'wanted', 'add', '##eral', '##l', 'so', 'i', 'could', 'get', 'high', 'and', 'work', '.', 'now', 'i', 'am', 'trying', 'to', 'not', 'beat', 'add', 'but', 'more', 'like', 'do', 'stuff', 'without', 'drugs', '.', 'taking', 'speed', 'everyday', 'seems', 'a', 'rather', 'crude', 'fix', '.', 'is', 'there', 'a', 'way', 'to', 'do', 'this', 'without', 'drugs', '?', 'is', 'there', 'a', 'way', 'to', 'fix', 'it', '?', 'i', 'found', 'out', 'that', 'i', 'prefer', 'to', 'have', 'ear', '##pl', '##ug', '##s', 'in', 'most', 'of', 'the', 'time', 'since', 'sounds', 'bother', 'me', 'a', 'lot', '.', 'drowning', 'it', 'out', 'in', 'music', 'helps', ',', 'but', 'i', 'don', \"'\", 't', 'prefer', 'it', '.', 'i', 'just', 'mentioned', 'this', 'because', 'it', 'helped', 'me', 'and', 'perhaps', 'someone', 'else', 'will', 'realize', 'that', 'they', 'don', \"'\", 't', 'like', 'exterior', 'sounds', 'as', 'well', '.', 'i', 'know', 'diet', 'and', 'exercise', 'are', 'good', 'ways', 'to', 'get', 'rid', 'of', 'it', '.', 'any', 'suggestions', 'on', 'what', 'to', 'avoid', 'and', 'what', 'exercises', '?']\n",
      "INFO:__main__:Number of tokens: 177\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['w', '##tf', '?', 'is', 'there', 'a', 'drug', '##less', 'cure', '?', 'hey', 'guys', ',', 'i', \"'\", 've', 'had', 'add', 'all', 'my', 'life', ',', 'was', 'diagnosed', 'not', 'that', 'long', 'ago', 'like', 'two', 'years', 'ago', '.', 'i', 'wanted', 'add', '##eral', '##l', 'so', 'i', 'could', 'get', 'high', 'and', 'work', '.', 'now', 'i', 'am', 'trying', 'to', 'not', 'beat', 'add', 'but', 'more', 'like', 'do', 'stuff', 'without', 'drugs', '.', 'taking', 'speed', 'everyday', 'seems', 'a', 'rather', 'crude', 'fix', '.', 'is', 'there', 'a', 'way', 'to', 'do', 'this', 'without', 'drugs', '?', 'is', 'there', 'a', 'way', 'to', 'fix', 'it', '?', 'i', 'found', 'out', 'that', 'i', 'prefer', 'to', 'have', 'ear', '##pl', '##ug', '##s', 'in', 'most', 'of', 'the', 'time', 'since', 'sounds', 'bother', 'me', 'a', 'lot', '.', 'drowning', 'it', 'out', 'in', 'music', 'helps', ',', 'but', 'i', 'don', \"'\", 't', 'prefer', 'it', '.', 'i', 'just', 'mentioned', 'this', 'because', 'it', 'helped', 'me', 'and', 'perhaps', 'someone', 'else', 'will', 'realize', 'that', 'they', 'don', \"'\", 't', 'like', 'exterior', 'sounds', 'as', 'well', '.', 'i', 'know', 'diet', 'and', 'exercise', 'are', 'good', 'ways', 'to', 'get', 'rid', 'of', 'it', '.', 'any', 'suggestions', 'on', 'what', 'to', 'avoid', 'and', 'what', 'exercises', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'careers', 'would', 'be', 'good', 'for', 'me', 'that', 'would', 'also', 'work', 'with', 'someone', 'who', 'has', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'careers', 'would', 'be', 'good', 'for', 'me', 'that', 'would', 'also', 'work', 'with', 'someone', 'who', 'has', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', ',', 'i', 'started', 'concert', '##a', 'not', 'too', 'long', 'ago', '.', 'the', 'best', 'way', 'i', 'can', 'think', 'to', 'describe', 'it', 'is', 'like', 'putting', 'glasses', 'on', 'after', 'seeing', 'the', 'eye', 'doctor', 'for', 'the', 'first', 'time', '.', 'like', ',', 'when', 'things', 'are', 'blur', '##ry', 'from', 'bad', 'eye', 'sight', ',', 'you', 'can', 'tell', 'what', 'they', 'are', 'because', 'you', 'know', 'what', \"'\", 's', 'supposed', 'to', 'be', 'there', ',', 'like', 'leaves', 'on', 'trees', '.', 'you', \"'\", 'll', 'see', 'a', 'green', 'b', '##lo', '##b', 'and', 'know', 'it', \"'\", 's', 'a', 'tree', ',', 'but', 'then', 'you', 'put', 'glasses', 'on', 'and', 'for', 'once', ',', 'you', 'actually', 'see', 'each', 'leaf', ',', 'and', 'the', 'details', 'like', 'that', '.', 'another', 'example', 'is', 'strands', 'of', 'hairs', '.', 'you', 'see', 'it', 'as', 'a', 'b', '##lo', '##b', ',', 'but', 'then', 'with', 'help', 'it', \"'\", 's', 'thousands', 'of', 'strands', 'together', 'to', 'make', 'hair', '.', 'that', \"'\", 's', 'what', 'it', 'feels', 'like', 'to', 'be', 'on', 'concert', '##a', 'for', 'me', '.', 'it', \"'\", 's', 'like', 'living', 'in', 'a', 'world', 'where', 'everyone', 'has', '20', '/', '20', 'vision', 'and', 'having', '20', '/', '100', 'vision', '.', 'i', 'feel', 'like', 'i', 'can', 'see', ',', 'and', 'i', 'feel', 'like', 'i', 'can', 'understand', '.', 'the', 'only', 'downs', '##ide', 'is', 'the', 'fact', 'that', 'once', 'i', 'come', 'down', ',', 'i', 'just', 'feel', 'dull', 'and', 'i', 'things', 'i', 'used', 'to', 'force', 'myself', 'to', 'do', 'before', 'medication', ',', 'now', 'seem', 'so', 'difficult', 'that', 'i', 'don', \"'\", 't', 'want', 'to', 'do', 'them', '.', 'i', 'don', \"'\", 't', 'know', ',', 'i', 'can', 'handle', 'the', 'difficulty', 'with', 'sleep', ',', 'but', 'the', 'part', 'i', 'struggle', 'with', 'most', 'is', 'being', 'back', 'to', 'that', 'below', 'average', 'feeling', '.']\n",
      "INFO:__main__:Number of tokens: 265\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', ',', 'i', 'started', 'concert', '##a', 'not', 'too', 'long', 'ago', '.', 'the', 'best', 'way', 'i', 'can', 'think', 'to', 'describe', 'it', 'is', 'like', 'putting', 'glasses', 'on', 'after', 'seeing', 'the', 'eye', 'doctor', 'for', 'the', 'first', 'time', '.', 'like', ',', 'when', 'things', 'are', 'blur', '##ry', 'from', 'bad', 'eye', 'sight', ',', 'you', 'can', 'tell', 'what', 'they', 'are', 'because', 'you', 'know', 'what', \"'\", 's', 'supposed', 'to', 'be', 'there', ',', 'like', 'leaves', 'on', 'trees', '.', 'you', \"'\", 'll', 'see', 'a', 'green', 'b', '##lo', '##b', 'and', 'know', 'it', \"'\", 's', 'a', 'tree', ',', 'but', 'then', 'you', 'put', 'glasses', 'on', 'and', 'for', 'once', ',', 'you', 'actually', 'see', 'each', 'leaf', ',', 'and', 'the', 'details', 'like', 'that', '.', 'another', 'example', 'is', 'strands', 'of', 'hairs', '.', 'you', 'see', 'it', 'as', 'a', 'b', '##lo', '##b', ',', 'but', 'then', 'with', 'help', 'it', \"'\", 's', 'thousands', 'of', 'strands', 'together', 'to', 'make', 'hair', '.', 'that', \"'\", 's', 'what', 'it', 'feels', 'like', 'to', 'be', 'on', 'concert', '##a', 'for', 'me', '.', 'it', \"'\", 's', 'like', 'living', 'in', 'a', 'world', 'where', 'everyone', 'has', '20', '/', '20', 'vision', 'and', 'having', '20', '/', '100', 'vision', '.', 'i', 'feel', 'like', 'i', 'can', 'see', ',', 'and', 'i', 'feel', 'like', 'i', 'can', 'understand', '.', 'the', 'only', 'downs', '##ide', 'is', 'the', 'fact', 'that', 'once', 'i', 'come', 'down', ',', 'i', 'just', 'feel', 'dull', 'and', 'i', 'things', 'i', 'used', 'to', 'force', 'myself', 'to', 'do', 'before', 'medication', ',', 'now', 'seem', 'so', 'difficult', 'that', 'i', 'don', \"'\", 't', 'want', 'to', 'do', 'them', '.', 'i', 'don', \"'\", 't', 'know', ',', 'i', 'can', 'handle', 'the', 'difficulty', 'with', 'sleep', ',', 'but', 'the', 'part', 'i', 'struggle', 'with', 'most', 'is', 'being', 'back', 'to', 'that', 'below', 'average', 'feeling', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '/', 'hd', 'in', 'a', 'nuts', '##hell']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '/', 'hd', 'in', 'a', 'nuts', '##hell']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['six', 'fundamental', 'steps', 'to', 'effectively', 'managing', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['six', 'fundamental', 'steps', 'to', 'effectively', 'managing', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['counter', '##ing', 'early', 'waking', '/', 'ins', '##om', '##nia', 'caused', 'by', 'st', '##im', '##ula', '##nts', '(', 'add', '##eral', '##l', 'ir', ')', '?']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['counter', '##ing', 'early', 'waking', '/', 'ins', '##om', '##nia', 'caused', 'by', 'st', '##im', '##ula', '##nts', '(', 'add', '##eral', '##l', 'ir', ')', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'reading', 'sex', 'at', 'dawn', 'and', 'came', 'across', 'this', 'passage', 'that', 'i', 'feel', 'could', 'apply', 'to', 'ad', '##hd', '.', 'thoughts', '?', '\"', '.', '.', '.', 'as', 'we', '’', 've', 'seen', ',', 'researchers', 'trying', 'to', 'describe', 'human', 'nature', 'are', 'highly', 'susceptible', 'to', 'flint', '##ston', '##ization', ':', 'unconscious', '##ly', 'tending', 'to', '“', 'discover', '”', 'features', 'that', 'look', 'familiar', ',', 'and', 'thereby', 'universal', '##izing', 'contemporary', 'social', 'configurations', 'while', 'inadvertently', 'blocking', 'insight', 'into', 'the', 'truth', '.', 'journalist', 'louis', 'men', '##and', 'noted', 'this', 'tendency', 'in', 'a', 'piece', 'in', 'the', 'new', 'yorker', ',', 'writing', ',', '“', 'the', 'sciences', 'of', 'human', 'nature', 'tend', 'to', 'valid', '##ate', 'the', 'practices', 'and', 'preferences', 'of', 'whatever', 'regime', 'happens', 'to', 'be', 'sponsoring', 'them', '.', 'in', 'total', '##itarian', 'regimes', ',', 'di', '##ssi', '##den', '##ce', 'is', 'treated', 'as', 'a', 'mental', 'illness', '.', 'in', 'apartheid', 'regimes', ',', 'inter', '##rac', '##ial', 'contact', 'is', 'treated', 'as', 'unnatural', '.', 'in', 'free', '-', 'market', 'regimes', ',', 'self', '-', 'interest', 'is', 'treated', 'as', 'hard', '##wire', '##d', '.', '”', 'paradox', '##ically', ',', 'in', 'each', 'of', 'these', 'cases', ',', 'so', '-', 'called', 'natural', 'behavior', 'has', 'to', 'be', 'encouraged', 'and', 'unnatural', 'abe', '##rra', '##tions', 'punished', '.', 'the', 'now', '-', 'forgotten', 'diseases', 'dr', '##ape', '##tom', '##ania', 'and', 'd', '##ys', '##ae', '##thes', '##ia', 'ae', '##thi', '##op', '##ica', 'illustrate', 'this', 'point', '.', 'both', 'were', 'described', 'in', '1851', 'by', 'dr', '.', 'samuel', 'cartwright', ',', 'a', 'leading', 'authority', 'on', 'the', 'medical', 'care', 'of', '“', 'negro', '##es', '”', 'in', 'louisiana', 'and', 'a', 'leading', 'think', '##er', 'in', 'the', 'pro', '-', 'slavery', 'movement', '.', 'in', 'his', 'article', '“', 'diseases', 'and', 'peculiar', '##ities', 'of', 'the', 'negro', 'race', ',', '”', 'dr', '.', 'cartwright', 'explained', 'that', 'dr', '##ape', '##tom', '##ania', 'was', 'the', 'disease', '“', 'causing', 'negro', '##es', 'to', 'run', 'away', '…', 'the', 'abs', '##con', '##ding', 'from', 'service', '”', 'to', 'their', 'white', 'owners', ',', 'while', 'd', '##ys', '##ae', '##thes', '##ia', 'ae', '##thi', '##op', '##ica', 'was', 'characterized', 'by', '“', 'he', '##bet', '##ude', 'of', 'and', 'ob', '##tus', '##e', 'sen', '##sibility', 'of', 'the', 'body', '.', '”', 'he', 'noted', 'that', 'slave', 'oversee', '##rs', 'often', 'referred', 'to', 'this', 'disease', ',', 'more', 'simply', ',', 'as', '“', 'ras', '##cal', '##ity', '.', '”', '\"', '[', 'sex', 'at', 'dawn', ']', '(', 'http', ':', '/', '/', 'lu', '##pta', '##ant', '##ica', '##pit', '##alis', '##ta', '.', 'files', '.', 'word', '##press', '.', 'com', '/', '2010', '/', '06', '/', 'sex', '-', 'at', '-', 'dawn', '-', 'the', '-', 'prehistoric', '-', 'origins', '-', 'of', '-', 'modern', '-', 'sexuality', '.', 'pdf', ')', ',', 'chapter', '8', 'i', 'have', 'been', 'diagnosed', 'with', 'ad', '##hd', 'and', 'totally', 'think', 'the', 'symptoms', 'are', 'real', ',', 'but', 'i', 'think', 'its', 'classification', 'as', 'a', 'disorder', 'is', 'the', 'product', 'of', 'a', 'culture', 'the', 'emphasizes', 'productivity', 'over', 'humanity', '.', 'as', 'a', 'human', 'being', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'sit', 'still', 'and', 'be', 'lectured', 'to', 'until', 'i', \"'\", 'm', 'sufficiently', 'indo', '##ct', '##rina', '##ted', 'to', 'participate', 'in', 'life', '.', 'this', 'configuration', 'goes', 'against', 'our', 'biology', 'and', 'i', 'think', 'a', 'large', 'portion', 'of', 'people', 'diagnosed', 'with', 'ad', '##hd', 'have', 'subconscious', '##ly', 'realized', 'this', 'and', 'therefore', 'find', 'no', 'motivation', 'to', 'go', 'through', 'the', 'repetitive', 'motions', 'that', 'character', '##ize', 'our', 'education', 'system', '.', 'if', 'you', 'don', \"'\", 't', 'take', 'a', 'break', 'after', 'highs', '##cho', '##ol', ',', 'you', \"'\", 'll', 'have', 'sat', 'and', 'listened', 'to', 'teachers', 'and', 'been', 'submitted', 'to', 'their', 'valuation', '##s', 'of', 'your', 'worth', '(', 'grades', ')', 'for', 'a', 'total', 'of', '17', 'years', '!', 'that', 'shit', 'is', 'wa', '##ck', '.', 'life', 'is', 'way', 'bigger', 'than', 'institutions', ',', 'whose', 'boring', 'rituals', 'are', 'a', 'waste', 'of', 'something', 'so', 'precious', 'and', 'fleeting', 'as', 'life', '.', 'i', 'think', 'part', 'of', 'ad', '##hd', 'is', 'the', 'subconscious', 'realization', 'of', 'this', '.', 'what', 'do', 'you', 'think', '?', 't', '##l', ';', 'dr', 'ad', '##hd', 'symptoms', 'are', 'real', ',', 'but', 'it', 'isn', \"'\", 't', 'a', 'disorder', '.', 'it', 'is', 'a', 'reaction', 'to', 'the', 'culture', 'of', 'production', 'and', 'the', 'de', '##hum', '##ani', '##zation', 'that', 'culture', 'en', '##tails', '.', 'update', ':', 'here', \"'\", 's', 'a', 'link', 'to', 'a', 'journal', 'article', 'regarding', 'an', '[', 'evolutionary', 'approach', 'to', 'education', ']', '(', 'http', ':', '/', '/', 'www', '.', 'pdf', '##hos', '##t', '.', 'net', '/', 'index', '.', 'php', '?', 'action', '=', 'download', '&', 'file', '=', '6', '##a', '##40', '##e', '##8', '##d', '##11', '##a', '##40', '##ac', '##0', '##a', '##0', '##55', '##5', '##d', '##49', '##01', '##9', '##ec', '##56', '##bc', ')']\n",
      "INFO:__main__:Number of tokens: 693\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'reading', 'sex', 'at', 'dawn', 'and', 'came', 'across', 'this', 'passage', 'that', 'i', 'feel', 'could', 'apply', 'to', 'ad', '##hd', '.', 'thoughts', '?', '\"', '.', '.', '.', 'as', 'we', '’', 've', 'seen', ',', 'researchers', 'trying', 'to', 'describe', 'human', 'nature', 'are', 'highly', 'susceptible', 'to', 'flint', '##ston', '##ization', ':', 'unconscious', '##ly', 'tending', 'to', '“', 'discover', '”', 'features', 'that', 'look', 'familiar', ',', 'and', 'thereby', 'universal', '##izing', 'contemporary', 'social', 'configurations', 'while', 'inadvertently', 'blocking', 'insight', 'into', 'the', 'truth', '.', 'journalist', 'louis', 'men', '##and', 'noted', 'this', 'tendency', 'in', 'a', 'piece', 'in', 'the', 'new', 'yorker', ',', 'writing', ',', '“', 'the', 'sciences', 'of', 'human', 'nature', 'tend', 'to', 'valid', '##ate', 'the', 'practices', 'and', 'preferences', 'of', 'whatever', 'regime', 'happens', 'to', 'be', 'sponsoring', 'them', '.', 'in', 'total', '##itarian', 'regimes', ',', 'di', '##ssi', '##den', '##ce', 'is', 'treated', 'as', 'a', 'mental', 'illness', '.', 'in', 'apartheid', 'regimes', ',', 'inter', '##rac', '##ial', 'contact', 'is', 'treated', 'as', 'unnatural', '.', 'in', 'free', '-', 'market', 'regimes', ',', 'self', '-', 'interest', 'is', 'treated', 'as', 'hard', '##wire', '##d', '.', '”', 'paradox', '##ically', ',', 'in', 'each', 'of', 'these', 'cases', ',', 'so', '-', 'called', 'natural', 'behavior', 'has', 'to', 'be', 'encouraged', 'and', 'unnatural', 'abe', '##rra', '##tions', 'punished', '.', 'the', 'now', '-', 'forgotten', 'diseases', 'dr', '##ape', '##tom', '##ania', 'and', 'd', '##ys', '##ae', '##thes', '##ia', 'ae', '##thi', '##op', '##ica', 'illustrate', 'this', 'point', '.', 'both', 'were', 'described', 'in', '1851', 'by', 'dr', '.', 'samuel', 'cartwright', ',', 'a', 'leading', 'authority', 'on', 'the', 'medical', 'care', 'of', '“', 'negro', '##es', '”', 'in', 'louisiana', 'and', 'a', 'leading', 'think', '##er', 'in', 'the', 'pro', '-', 'slavery', 'movement', '.', 'in', 'his', 'article', '“', 'diseases', 'and', 'peculiar', '##ities', 'of', 'the', 'negro', 'race', ',', '”', 'dr', '.', 'cartwright', 'explained', 'that', 'dr', '##ape', '##tom', '##ania', 'was', 'the', 'disease', '“', 'causing', 'negro', '##es', 'to', 'run', 'away', '…', 'the', 'abs', '##con', '##ding', 'from', 'service', '”', 'to', 'their', 'white', 'owners', ',', 'while', 'd', '##ys', '##ae', '##thes', '##ia', 'ae', '##thi', '##op', '##ica', 'was', 'characterized', 'by', '“', 'he', '##bet', '##ude', 'of', 'and', 'ob', '##tus', '##e', 'sen', '##sibility', 'of', 'the', 'body', '.', '”', 'he', 'noted', 'that', 'slave', 'oversee', '##rs', 'often', 'referred', 'to', 'this', 'disease', ',', 'more', 'simply', ',', 'as', '“', 'ras', '##cal', '##ity', '.', '”', '\"', '[', 'sex', 'at', 'dawn', ']', '(', 'http', ':', '/', '/', 'lu', '##pta', '##ant', '##ica', '##pit', '##alis', '##ta', '.', 'files', '.', 'word', '##press', '.', 'com', '/', '2010', '/', '06', '/', 'sex', '-', 'at', '-', 'dawn', '-', 'the', '-', 'prehistoric', '-', 'origins', '-', 'of', '-', 'modern', '-', 'sexuality', '.', 'pdf', ')', ',', 'chapter', '8', 'i', 'have', 'been', 'diagnosed', 'with', 'ad', '##hd', 'and', 'totally', 'think', 'the', 'symptoms', 'are', 'real', ',', 'but', 'i', 'think', 'its', 'classification', 'as', 'a', 'disorder', 'is', 'the', 'product', 'of', 'a', 'culture', 'the', 'emphasizes', 'productivity', 'over', 'humanity', '.', 'as', 'a', 'human', 'being', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'sit', 'still', 'and', 'be', 'lectured', 'to', 'until', 'i', \"'\", 'm', 'sufficiently', 'indo', '##ct', '##rina', '##ted', 'to', 'participate', 'in', 'life', '.', 'this', 'configuration', 'goes', 'against', 'our', 'biology', 'and', 'i', 'think', 'a', 'large', 'portion', 'of', 'people', 'diagnosed', 'with', 'ad', '##hd', 'have', 'subconscious', '##ly', 'realized', 'this', 'and', 'therefore', 'find', 'no', 'motivation', 'to', 'go', 'through', 'the', 'repetitive', 'motions', 'that', 'character', '##ize', 'our', 'education', 'system', '.', 'if', 'you', 'don', \"'\", 't', 'take', 'a', 'break', 'after', 'highs'], ['##cho', '##ol', ',', 'you', \"'\", 'll', 'have', 'sat', 'and', 'listened', 'to', 'teachers', 'and', 'been', 'submitted', 'to', 'their', 'valuation', '##s', 'of', 'your', 'worth', '(', 'grades', ')', 'for', 'a', 'total', 'of', '17', 'years', '!', 'that', 'shit', 'is', 'wa', '##ck', '.', 'life', 'is', 'way', 'bigger', 'than', 'institutions', ',', 'whose', 'boring', 'rituals', 'are', 'a', 'waste', 'of', 'something', 'so', 'precious', 'and', 'fleeting', 'as', 'life', '.', 'i', 'think', 'part', 'of', 'ad', '##hd', 'is', 'the', 'subconscious', 'realization', 'of', 'this', '.', 'what', 'do', 'you', 'think', '?', 't', '##l', ';', 'dr', 'ad', '##hd', 'symptoms', 'are', 'real', ',', 'but', 'it', 'isn', \"'\", 't', 'a', 'disorder', '.', 'it', 'is', 'a', 'reaction', 'to', 'the', 'culture', 'of', 'production', 'and', 'the', 'de', '##hum', '##ani', '##zation', 'that', 'culture', 'en', '##tails', '.', 'update', ':', 'here', \"'\", 's', 'a', 'link', 'to', 'a', 'journal', 'article', 'regarding', 'an', '[', 'evolutionary', 'approach', 'to', 'education', ']', '(', 'http', ':', '/', '/', 'www', '.', 'pdf', '##hos', '##t', '.', 'net', '/', 'index', '.', 'php', '?', 'action', '=', 'download', '&', 'file', '=', '6', '##a', '##40', '##e', '##8', '##d', '##11', '##a', '##40', '##ac', '##0', '##a', '##0', '##55', '##5', '##d', '##49', '##01', '##9', '##ec', '##56', '##bc', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['organizational', 'aid', '!', '!', '!']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['organizational', 'aid', '!', '!', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['this', 'book', 'is', 'the', 'best', 'tool', 'any', 'person', 'with', 'ad', '/', 'hd', 'can', 'have', '.', 'this', 'book', 'changed', 'my', 'life', 'so', 'much', '.', 'i', 'still', 're', '##rea', '##d', 'and', 'reference', 'it', 'often', ',', 'definitely', 'a', 'must', '-', 'have', '.', 'edit', ':', '.', '.', '.', 'o', '##ops', '.', 'so', 'thank', 'you', 'sum', '##thi', '##n', 'for', 'the', 'help', '.', 'the', 'book', 'is', 'called', 'delivered', 'from', 'distraction', '.']\n",
      "INFO:__main__:Number of tokens: 65\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['this', 'book', 'is', 'the', 'best', 'tool', 'any', 'person', 'with', 'ad', '/', 'hd', 'can', 'have', '.', 'this', 'book', 'changed', 'my', 'life', 'so', 'much', '.', 'i', 'still', 're', '##rea', '##d', 'and', 'reference', 'it', 'often', ',', 'definitely', 'a', 'must', '-', 'have', '.', 'edit', ':', '.', '.', '.', 'o', '##ops', '.', 'so', 'thank', 'you', 'sum', '##thi', '##n', 'for', 'the', 'help', '.', 'the', 'book', 'is', 'called', 'delivered', 'from', 'distraction', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'working', 'at', 'night', 'easier', '?', 'i', 'am', 'a', 'student', ',', 'so', 'besides', 'making', 'it', 'to', 'class', ',', 'i', 'can', 'pick', 'whenever', 'to', 'study', '.', 'i', \"'\", 've', 'only', 'been', 'trying', 'for', 'a', 'couple', 'days', 'now', ',', 'but', 'it', 'seems', 'that', 'studying', 'after', '10', 'pm', ',', 'till', 'something', 'like', '1', 'or', '2', 'am', ',', 'is', 'more', 'productive', 'for', 'me', 'than', 'the', 'whole', 'rest', 'of', 'the', 'day', '.', 'does', 'this', 'work', 'for', 'anyone', 'else', '?']\n",
      "INFO:__main__:Number of tokens: 74\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'working', 'at', 'night', 'easier', '?', 'i', 'am', 'a', 'student', ',', 'so', 'besides', 'making', 'it', 'to', 'class', ',', 'i', 'can', 'pick', 'whenever', 'to', 'study', '.', 'i', \"'\", 've', 'only', 'been', 'trying', 'for', 'a', 'couple', 'days', 'now', ',', 'but', 'it', 'seems', 'that', 'studying', 'after', '10', 'pm', ',', 'till', 'something', 'like', '1', 'or', '2', 'am', ',', 'is', 'more', 'productive', 'for', 'me', 'than', 'the', 'whole', 'rest', 'of', 'the', 'day', '.', 'does', 'this', 'work', 'for', 'anyone', 'else', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'idea', 'of', 'an', '/', 'r', '/', 'ad', '##hd', 'ir', '##c', 'channel', 'was', 'briefly', 'tossed', 'around', '.', 'i', 'think', 'it', \"'\", 's', 'a', 'good', 'idea', '.', 'anyone', 'interested', '?', 'with', 'th', '##j', \"'\", 's', '[', 'recent', 'request', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'p', '##85', '##l', '##1', '/', 'hello', '_', 'would', '_', 'anybody', '_', 'like', '_', 'to', '_', 'make', '_', 'a', '_', 'friend', '/', ')', 'to', 'make', 'friends', ',', 'it', 'seems', 'there', \"'\", 's', 'some', 'interest', 'in', 'this', 'community', 'for', 'a', 'more', 'real', '-', 'time', 'form', 'of', 'communication', '.', 'and', 'as', 'per', 'b', '##lee', '##p', '##00', '##00', '##bl', '##oop', \"'\", 's', '[', 'comment', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'p', '##85', '##l', '##1', '/', 'hello', '_', 'would', '_', 'anybody', '_', 'like', '_', 'to', '_', 'make', '_', 'a', '_', 'friend', '/', 'c', '##3', '##nd', '##9', '##lz', ')', ',', 'it', 'also', 'appears', 'that', 'ir', '##c', 'may', 'be', 'favored', 'by', 'a', 'portion', 'of', 'the', 'user', '##base', '.', 'is', 'this', 'something', 'the', 'community', 'as', 'a', 'whole', 'might', 'be', 'interested', 'in', '?', 'personally', ',', 'i', 'think', 'it', 'would', 'be', 'fantastic', 'to', 'have', 'discussions', 'in', 'real', '-', 'time', 'with', 'other', 'people', 'who', 'experience', 'similar', 'issues', '.', 'and', 'it', \"'\", 's', 'not', 'like', 'the', 'channel', 'has', 'to', 'be', '100', '%', 'ad', '##hd', 'all', 'the', 'time', '(', 'unless', 'of', 'course', 'the', 'community', 'dee', '##ms', 'it', 'so', '?', ')', ',', 'but', 'just', 'a', 'place', 'for', 'similar', 'individuals', 'to', 'cong', '##re', '##gate', 'and', 'enjoy', 'each', 'others', \"'\", 'company', '.', 'it', 'looks', 'like', '#', 'ad', '##hd', 'on', 'free', '##no', '##de', 'is', 'already', 'taken', ',', 'and', 'it', \"'\", 's', 'invite', 'only', '.', 'i', \"'\", 've', 'currently', 'got', 'a', 'bot', 'holding', '#', '/', 'r', '/', 'ad', '##hd', 'open', ',', 'but', 'am', 'hesitant', 'to', 'register', 'it', 'with', 'the', 'chan', '##ser', '##v', 'until', 'we', 'get', 'some', 'pretty', 'good', 'support', 'for', 'the', 'idea', '.', 'mod', '##s', ':', 'if', 'this', 'idea', 'takes', 'off', ',', 'is', 'this', 'something', 'we', 'can', 'stick', 'in', 'the', 'side', 'bar', '?', 'what', \"'\", 's', 'the', 'process', 'for', 'getting', 'this', 'sanctioned', 'and', 'official', '?', 'everyone', ':', 'what', 'do', 'you', 'say', '?', 'suggestions', '?', 'concerns', '?', '*', '*', 'edit', ':', '*', '*', 'we', 'will', 'probably', 'want', 'more', 'than', 'one', 'channel', 'op', 'too', ',', 'just', 'in', 'case', 'something', 'happens', '?', '*', '*', 'edit', '2', ':', '*', '*', 'here', \"'\", 's', 'the', 'info', 'to', 'connect', ',', 'sorry', 'guys', ',', 'i', 'should', 'have', 'included', 'this', 'originally', ':', 'server', ':', 'chat', '.', 'free', '##no', '##de', '.', 'net', 'port', ':', '66', '##6', '##7', 'channel', ':', '#', '/', 'r', '/', 'ad', '##hd', 'popular', 'ir', '##c', 'clients', 'include', ':', '*', '[', 'pi', '##d', '##gin', ']', '(', 'http', ':', '/', '/', 'pi', '##d', '##gin', '.', 'im', '/', ')', '-', 'windows', ',', 'linux', ',', 'os', '##x', '*', '[', 'mir', '##c', ']', '(', 'http', ':', '/', '/', 'www', '.', 'mir', '##c', '.', 'com', '/', ')', '-', 'windows', '*', '[', 'x', '##cha', '##t', ']', '(', 'http', ':', '/', '/', 'x', '##cha', '##t', '.', 'org', '/', ')', '-', 'windows', ',', 'linux', '(', 'and', 'possibly', 'os', '##x', '?', 'i', \"'\", 'm', 'getting', 'conflicting', 'reports', '.', ')', '*', '[', 'irs', '##si', ']', '(', 'http', ':', '/', '/', 'irs', '##si', '.', 'org', '/', ')', '-', 'linux', '*', '[', 'wee', '##cha', '##t', ']', '(', 'http', ':', '/', '/', 'www', '.', 'wee', '##cha', '##t', '.', 'org', '/', ')', '-', 'linux', 'there', \"'\", 's', 'a', 'larger', 'list', '[', 'here', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'comparison', '_', 'of', '_', 'internet', '_', 'relay', '_', 'chat', '_', 'clients', ')', '.']\n",
      "INFO:__main__:Number of tokens: 603\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['the', 'idea', 'of', 'an', '/', 'r', '/', 'ad', '##hd', 'ir', '##c', 'channel', 'was', 'briefly', 'tossed', 'around', '.', 'i', 'think', 'it', \"'\", 's', 'a', 'good', 'idea', '.', 'anyone', 'interested', '?', 'with', 'th', '##j', \"'\", 's', '[', 'recent', 'request', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'p', '##85', '##l', '##1', '/', 'hello', '_', 'would', '_', 'anybody', '_', 'like', '_', 'to', '_', 'make', '_', 'a', '_', 'friend', '/', ')', 'to', 'make', 'friends', ',', 'it', 'seems', 'there', \"'\", 's', 'some', 'interest', 'in', 'this', 'community', 'for', 'a', 'more', 'real', '-', 'time', 'form', 'of', 'communication', '.', 'and', 'as', 'per', 'b', '##lee', '##p', '##00', '##00', '##bl', '##oop', \"'\", 's', '[', 'comment', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'p', '##85', '##l', '##1', '/', 'hello', '_', 'would', '_', 'anybody', '_', 'like', '_', 'to', '_', 'make', '_', 'a', '_', 'friend', '/', 'c', '##3', '##nd', '##9', '##lz', ')', ',', 'it', 'also', 'appears', 'that', 'ir', '##c', 'may', 'be', 'favored', 'by', 'a', 'portion', 'of', 'the', 'user', '##base', '.', 'is', 'this', 'something', 'the', 'community', 'as', 'a', 'whole', 'might', 'be', 'interested', 'in', '?', 'personally', ',', 'i', 'think', 'it', 'would', 'be', 'fantastic', 'to', 'have', 'discussions', 'in', 'real', '-', 'time', 'with', 'other', 'people', 'who', 'experience', 'similar', 'issues', '.', 'and', 'it', \"'\", 's', 'not', 'like', 'the', 'channel', 'has', 'to', 'be', '100', '%', 'ad', '##hd', 'all', 'the', 'time', '(', 'unless', 'of', 'course', 'the', 'community', 'dee', '##ms', 'it', 'so', '?', ')', ',', 'but', 'just', 'a', 'place', 'for', 'similar', 'individuals', 'to', 'cong', '##re', '##gate', 'and', 'enjoy', 'each', 'others', \"'\", 'company', '.', 'it', 'looks', 'like', '#', 'ad', '##hd', 'on', 'free', '##no', '##de', 'is', 'already', 'taken', ',', 'and', 'it', \"'\", 's', 'invite', 'only', '.', 'i', \"'\", 've', 'currently', 'got', 'a', 'bot', 'holding', '#', '/', 'r', '/', 'ad', '##hd', 'open', ',', 'but', 'am', 'hesitant', 'to', 'register', 'it', 'with', 'the', 'chan', '##ser', '##v', 'until', 'we', 'get', 'some', 'pretty', 'good', 'support', 'for', 'the', 'idea', '.', 'mod', '##s', ':', 'if', 'this', 'idea', 'takes', 'off', ',', 'is', 'this', 'something', 'we', 'can', 'stick', 'in', 'the', 'side', 'bar', '?', 'what', \"'\", 's', 'the', 'process', 'for', 'getting', 'this', 'sanctioned', 'and', 'official', '?', 'everyone', ':', 'what', 'do', 'you', 'say', '?', 'suggestions', '?', 'concerns', '?', '*', '*', 'edit', ':', '*', '*', 'we', 'will', 'probably', 'want', 'more', 'than', 'one', 'channel', 'op', 'too', ',', 'just', 'in', 'case', 'something', 'happens', '?', '*', '*', 'edit', '2', ':', '*', '*', 'here', \"'\", 's', 'the', 'info', 'to', 'connect', ',', 'sorry', 'guys', ',', 'i', 'should', 'have', 'included', 'this', 'originally', ':', 'server', ':', 'chat', '.', 'free', '##no', '##de', '.', 'net', 'port', ':', '66', '##6', '##7', 'channel', ':', '#', '/', 'r', '/', 'ad', '##hd', 'popular', 'ir', '##c', 'clients', 'include', ':', '*', '[', 'pi', '##d', '##gin', ']', '(', 'http', ':', '/', '/', 'pi', '##d', '##gin', '.', 'im', '/', ')', '-', 'windows', ',', 'linux', ',', 'os', '##x', '*', '[', 'mir', '##c', ']', '(', 'http', ':', '/', '/', 'www', '.', 'mir', '##c', '.', 'com', '/', ')', '-', 'windows', '*', '[', 'x', '##cha', '##t', ']', '(', 'http', ':', '/', '/', 'x', '##cha', '##t', '.', 'org', '/', ')', '-', 'windows', ','], ['linux', '(', 'and', 'possibly', 'os', '##x', '?', 'i', \"'\", 'm', 'getting', 'conflicting', 'reports', '.', ')', '*', '[', 'irs', '##si', ']', '(', 'http', ':', '/', '/', 'irs', '##si', '.', 'org', '/', ')', '-', 'linux', '*', '[', 'wee', '##cha', '##t', ']', '(', 'http', ':', '/', '/', 'www', '.', 'wee', '##cha', '##t', '.', 'org', '/', ')', '-', 'linux', 'there', \"'\", 's', 'a', 'larger', 'list', '[', 'here', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'comparison', '_', 'of', '_', 'internet', '_', 'relay', '_', 'chat', '_', 'clients', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'it', 'possible', 'to', 'have', 'a', 'happy', 'relationship', 'with', 'add', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'it', 'possible', 'to', 'have', 'a', 'happy', 'relationship', 'with', 'add', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'doctor', 'told', 'me', 'that', 'going', 'off', 'my', 'med', '##s', 'for', 'a', 'few', 'days', 'will', 'help', 'keep', 'my', 'tolerance', 'low', '.']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'doctor', 'told', 'me', 'that', 'going', 'off', 'my', 'med', '##s', 'for', 'a', 'few', 'days', 'will', 'help', 'keep', 'my', 'tolerance', 'low', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'people', 'without', 'ad', '(', 'h', ')', 'd', 'feel', 'like', 'we', 'do', 'when', 'we', 'take', 'our', 'medication', '?']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'people', 'without', 'ad', '(', 'h', ')', 'd', 'feel', 'like', 'we', 'do', 'when', 'we', 'take', 'our', 'medication', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['off', 'med', '##s', 'for', 'years', ',', 'considering', 'going', 'back', '.', 'i', \"'\", 'm', 'wondering', 'what', ',', 'specifically', ',', 'the', 'med', '##s', 'do', '?', 'around', 'the', 'same', 'time', 'that', 'i', 'got', 'off', 'the', 'med', '##s', '(', 'beginning', 'of', 'high', 'school', ',', 'end', 'of', 'middle', 'school', ')', 'my', 'grades', 'dropped', ',', 'but', 'i', 'became', 'more', 'social', ',', 'i', 'suppose', '.', 'i', \"'\", 'm', 'wondering', 'if', 'getting', 'back', 'on', 'the', 'med', '##s', 'will', 'honestly', 'help', '(', 'or', 'if', 'there', 'are', 'other', ',', 'superior', 'methods', ')', ',', 'and', 'i', \"'\", 'm', 'also', 'wondering', 'about', 'side', 'effects', '.', 'i', 'associate', 'the', 'med', '##s', 'with', 'poor', 'social', 'skills', 'and', 'more', 'closet', '##ed', 'behavior', ';', 'was', 'that', 'the', 'med', '##s', ',', 'or', 'simply', 'a', 'transition', 'in', 'my', 'life', '?', 'furthermore', ',', 'i', \"'\", 'm', 'reading', 'about', 'other', 'effects', 'of', 'ad', '##hd', '/', 'add', 'and', 'seeing', 'they', 'impact', 'relationships', 'and', 'more', ',', 'it', \"'\", 's', 'not', 'just', 'about', 'school', '.', 'would', 'any', 'sort', 'of', 'medication', 'deal', 'with', 'that', ',', 'or', 'is', 'that', 'simply', 'a', 'counseling', 'issue', '?', 'really', ',', 'i', \"'\", 'm', 'unhappy', 'with', 'my', 'ability', 'to', 'focus', 'on', 'school', '##work', '.', 'thoughts', '?', '*', 'edit', 'ha', '##ha', ',', 'i', \"'\", 'm', 'at', 'a', 'bad', 'place', 'right', 'now', ',', 'and', 'i', 'made', 'the', 'original', 'post', 'while', 'somewhat', 'emotional', ';', 'apologies', '.', 'it', \"'\", 's', 'been', 'altered', 'to', 'be', 'more', 'accurate', '.']\n",
      "INFO:__main__:Number of tokens: 223\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['off', 'med', '##s', 'for', 'years', ',', 'considering', 'going', 'back', '.', 'i', \"'\", 'm', 'wondering', 'what', ',', 'specifically', ',', 'the', 'med', '##s', 'do', '?', 'around', 'the', 'same', 'time', 'that', 'i', 'got', 'off', 'the', 'med', '##s', '(', 'beginning', 'of', 'high', 'school', ',', 'end', 'of', 'middle', 'school', ')', 'my', 'grades', 'dropped', ',', 'but', 'i', 'became', 'more', 'social', ',', 'i', 'suppose', '.', 'i', \"'\", 'm', 'wondering', 'if', 'getting', 'back', 'on', 'the', 'med', '##s', 'will', 'honestly', 'help', '(', 'or', 'if', 'there', 'are', 'other', ',', 'superior', 'methods', ')', ',', 'and', 'i', \"'\", 'm', 'also', 'wondering', 'about', 'side', 'effects', '.', 'i', 'associate', 'the', 'med', '##s', 'with', 'poor', 'social', 'skills', 'and', 'more', 'closet', '##ed', 'behavior', ';', 'was', 'that', 'the', 'med', '##s', ',', 'or', 'simply', 'a', 'transition', 'in', 'my', 'life', '?', 'furthermore', ',', 'i', \"'\", 'm', 'reading', 'about', 'other', 'effects', 'of', 'ad', '##hd', '/', 'add', 'and', 'seeing', 'they', 'impact', 'relationships', 'and', 'more', ',', 'it', \"'\", 's', 'not', 'just', 'about', 'school', '.', 'would', 'any', 'sort', 'of', 'medication', 'deal', 'with', 'that', ',', 'or', 'is', 'that', 'simply', 'a', 'counseling', 'issue', '?', 'really', ',', 'i', \"'\", 'm', 'unhappy', 'with', 'my', 'ability', 'to', 'focus', 'on', 'school', '##work', '.', 'thoughts', '?', '*', 'edit', 'ha', '##ha', ',', 'i', \"'\", 'm', 'at', 'a', 'bad', 'place', 'right', 'now', ',', 'and', 'i', 'made', 'the', 'original', 'post', 'while', 'somewhat', 'emotional', ';', 'apologies', '.', 'it', \"'\", 's', 'been', 'altered', 'to', 'be', 'more', 'accurate', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'son', 'is', 'soo', '##oo', '##oo', 'slow']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'son', 'is', 'soo', '##oo', '##oo', 'slow']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'it', 'worth', 'it', 'to', 'be', 'med', '##icated', '?', 'i', 'was', 'just', 'diagnosed', 'with', 'add', 'from', 'my', 'doctor', ',', 'and', 'i', 'have', 'a', 'visit', 'coming', 'up', 'to', 'discuss', 'options', '.', 'my', 'grades', 'aren', \"'\", 't', 'the', 'best', 'and', 'i', 'know', 'if', 'i', 'can', 'just', 'concentrate', 'i', 'would', 'be', 'able', 'to', 'do', 'really', 'great', '.', 'i', \"'\", 've', 'just', 'heard', 'so', 'many', 'bad', 'things', 'about', 'add', '##eral', '##l', 'and', 'other', 'amp', '##het', '##amine', '##s', '.', 'a', 'majority', 'of', 'the', 'stories', 'i', 'have', 'read', 'online', 'talk', 'about', 'how', 'the', 'medication', 'had', 'ruined', 'their', 'lives', 'and', 'how', 'their', 'tolerance', 'just', 'keeps', 'rising', 'to', 'un', '##hea', '##lth', '##y', 'levels', '.', 'also', 'requiem', 'for', 'a', 'dream', 'really', 'creep', '##ed', 'me', 'out', 'as', 'well', '.', 'so', 'from', 'your', 'experience', 'is', 'it', 'worth', 'it', '?', 'bonus', 'question', ':', 'how', 'did', 'you', 'deal', 'with', 'the', 'tolerance', '?', 'edit', '##1', ':', 'thanks', 'a', 'lot', 'guys', '-', 'it', 'just', 'really', 'freaked', 'me', 'out', 'when', 'i', 'read', 'all', 'those', 'stories', 'online', 'and', 'was', 'just', 'scared', 'about', 'if', 'the', 'doctor', 'might', 'have', 'just', 'over', 'diagnosed', 'me', 'or', 'something', '.', 'i', 'guess', 'i', \"'\", 'll', 'go', 'for', 'a', 'trial', 'run', '-', 'any', 'idea', \"'\", 's', 'on', 'what', 'to', 'take', 'and', 'at', 'what', 'dos', '##age', '?']\n",
      "INFO:__main__:Number of tokens: 202\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'it', 'worth', 'it', 'to', 'be', 'med', '##icated', '?', 'i', 'was', 'just', 'diagnosed', 'with', 'add', 'from', 'my', 'doctor', ',', 'and', 'i', 'have', 'a', 'visit', 'coming', 'up', 'to', 'discuss', 'options', '.', 'my', 'grades', 'aren', \"'\", 't', 'the', 'best', 'and', 'i', 'know', 'if', 'i', 'can', 'just', 'concentrate', 'i', 'would', 'be', 'able', 'to', 'do', 'really', 'great', '.', 'i', \"'\", 've', 'just', 'heard', 'so', 'many', 'bad', 'things', 'about', 'add', '##eral', '##l', 'and', 'other', 'amp', '##het', '##amine', '##s', '.', 'a', 'majority', 'of', 'the', 'stories', 'i', 'have', 'read', 'online', 'talk', 'about', 'how', 'the', 'medication', 'had', 'ruined', 'their', 'lives', 'and', 'how', 'their', 'tolerance', 'just', 'keeps', 'rising', 'to', 'un', '##hea', '##lth', '##y', 'levels', '.', 'also', 'requiem', 'for', 'a', 'dream', 'really', 'creep', '##ed', 'me', 'out', 'as', 'well', '.', 'so', 'from', 'your', 'experience', 'is', 'it', 'worth', 'it', '?', 'bonus', 'question', ':', 'how', 'did', 'you', 'deal', 'with', 'the', 'tolerance', '?', 'edit', '##1', ':', 'thanks', 'a', 'lot', 'guys', '-', 'it', 'just', 'really', 'freaked', 'me', 'out', 'when', 'i', 'read', 'all', 'those', 'stories', 'online', 'and', 'was', 'just', 'scared', 'about', 'if', 'the', 'doctor', 'might', 'have', 'just', 'over', 'diagnosed', 'me', 'or', 'something', '.', 'i', 'guess', 'i', \"'\", 'll', 'go', 'for', 'a', 'trial', 'run', '-', 'any', 'idea', \"'\", 's', 'on', 'what', 'to', 'take', 'and', 'at', 'what', 'dos', '##age', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'mini', 'ran', '##t', 'stupid', '##ly', 'way', 't', '##l', ';', 'dr', ':', 'i', 'feel', 'like', 'i', \"'\", 'm', 'lying', 'about', 'having', 'ad', '##hd', 'even', 'though', 'i', 'know', 'i', 'have', 'it', '.', 'i', 'don', \"'\", 't', 'feel', 'supported', 'in', 'my', 'diagnosis', 'and', 'decision', 'to', 'take', 'med', '##s', 'for', 'ad', '##hd', '.', 'i', 'have', 'ad', '##hd', 'and', 'social', 'anxiety', 'disorder', '.', 'i', 'just', 'had', 'a', 'breakdown', 'on', 'friday', 'night', 'because', 'of', 'ad', '##hd', 'and', 'social', 'anxiety', '(', 'pm', '##s', 'was', 'also', 'a', 'factor', '-', 'however', 'we', 'shall', 'choose', 'to', 'overlook', 'this', 'a', 'slight', 'bit', 'please', ')', 'i', 'was', 'given', 'med', '##s', 'for', 'the', 'first', 'time', 'on', 'monday', '-', 'i', 'took', 'them', 'and', 'they', 'made', 'my', 'head', 'feel', 'clear', ',', 'increased', 'my', 'focusing', 'to', 'kind', 'of', 'absurd', '/', 'unnecessary', 'levels', ',', 'my', 'sister', 'said', 'she', 'notices', 'i', 'can', 'speak', 'more', 'smoothly', ',', 'and', 'i', 'can', 'actually', 'carry', 'on', 'conversations', 'for', 'the', 'first', 'time', 'in', 'my', 'life', '-', 'without', 'getting', 'bored', 'and', 'tuning', 'out', '.', 'however', 'i', 'feel', 'my', 'only', 'support', 'is', 'from', 'my', 'twin', 'and', 'a', 'strange', 'type', 'of', 'support', 'from', 'my', 'bf', '.', 'my', 'parents', 'do', 'not', 'want', 'me', 'med', '##icated', '(', 'long', 'term', 'effects', ',', 'being', 'do', '##ped', 'up', '-', 'yes', 'do', '##ped', 'up', ',', 'because', 'it', 'is', 'a', 'de', '##press', '##ant', 'right', ',', 'i', 'don', \"'\", 't', 'want', 'you', 'on', 'med', '##s', ',', 'look', 'how', 'far', 'you', \"'\", 've', 'gone', 'without', 'med', '##s', '.', '.', '.', ')', '.', 'for', 'some', 'reason', 'i', 'know', 'i', 'have', 'ad', '##hd', ',', 'but', 'it', 'is', 'so', 'hard', 'for', 'me', 'to', 'admit', 'it', 'because', 'i', 'feel', 'like', 'a', 'liar', '.', 'i', 'feel', 'that', 'the', 'lack', 'of', 'support', 'from', 'my', 'parents', 'does', 'not', 'help', '(', 'even', 'though', 'they', 'can', 'accept', 'the', 'idea', 'of', 'me', 'having', 'ad', '##hd', 'i', 'think', ')', 'when', 'i', 'went', 'to', 'get', 'tested', 'my', 'mom', 'told', 'me', 'she', 'doubted', 'i', 'had', 'it', ',', 'i', 'wasn', \"'\", 't', 'hyper', '##active', 'as', 'a', 'kid', 'or', 'anything', '.', 'in', 'part', 'i', 'think', 'being', '21', 'and', 'getting', 'diagnosed', 'makes', 'me', 'feel', 'this', 'way', '.', 'even', 'when', 'i', 'started', 'i', 'always', 'felt', 'like', 'i', 'was', 'lying', 'even', 'though', 'i', 'wasn', \"'\", 't', '.']\n",
      "INFO:__main__:Number of tokens: 355\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'mini', 'ran', '##t', 'stupid', '##ly', 'way', 't', '##l', ';', 'dr', ':', 'i', 'feel', 'like', 'i', \"'\", 'm', 'lying', 'about', 'having', 'ad', '##hd', 'even', 'though', 'i', 'know', 'i', 'have', 'it', '.', 'i', 'don', \"'\", 't', 'feel', 'supported', 'in', 'my', 'diagnosis', 'and', 'decision', 'to', 'take', 'med', '##s', 'for', 'ad', '##hd', '.', 'i', 'have', 'ad', '##hd', 'and', 'social', 'anxiety', 'disorder', '.', 'i', 'just', 'had', 'a', 'breakdown', 'on', 'friday', 'night', 'because', 'of', 'ad', '##hd', 'and', 'social', 'anxiety', '(', 'pm', '##s', 'was', 'also', 'a', 'factor', '-', 'however', 'we', 'shall', 'choose', 'to', 'overlook', 'this', 'a', 'slight', 'bit', 'please', ')', 'i', 'was', 'given', 'med', '##s', 'for', 'the', 'first', 'time', 'on', 'monday', '-', 'i', 'took', 'them', 'and', 'they', 'made', 'my', 'head', 'feel', 'clear', ',', 'increased', 'my', 'focusing', 'to', 'kind', 'of', 'absurd', '/', 'unnecessary', 'levels', ',', 'my', 'sister', 'said', 'she', 'notices', 'i', 'can', 'speak', 'more', 'smoothly', ',', 'and', 'i', 'can', 'actually', 'carry', 'on', 'conversations', 'for', 'the', 'first', 'time', 'in', 'my', 'life', '-', 'without', 'getting', 'bored', 'and', 'tuning', 'out', '.', 'however', 'i', 'feel', 'my', 'only', 'support', 'is', 'from', 'my', 'twin', 'and', 'a', 'strange', 'type', 'of', 'support', 'from', 'my', 'bf', '.', 'my', 'parents', 'do', 'not', 'want', 'me', 'med', '##icated', '(', 'long', 'term', 'effects', ',', 'being', 'do', '##ped', 'up', '-', 'yes', 'do', '##ped', 'up', ',', 'because', 'it', 'is', 'a', 'de', '##press', '##ant', 'right', ',', 'i', 'don', \"'\", 't', 'want', 'you', 'on', 'med', '##s', ',', 'look', 'how', 'far', 'you', \"'\", 've', 'gone', 'without', 'med', '##s', '.', '.', '.', ')', '.', 'for', 'some', 'reason', 'i', 'know', 'i', 'have', 'ad', '##hd', ',', 'but', 'it', 'is', 'so', 'hard', 'for', 'me', 'to', 'admit', 'it', 'because', 'i', 'feel', 'like', 'a', 'liar', '.', 'i', 'feel', 'that', 'the', 'lack', 'of', 'support', 'from', 'my', 'parents', 'does', 'not', 'help', '(', 'even', 'though', 'they', 'can', 'accept', 'the', 'idea', 'of', 'me', 'having', 'ad', '##hd', 'i', 'think', ')', 'when', 'i', 'went', 'to', 'get', 'tested', 'my', 'mom', 'told', 'me', 'she', 'doubted', 'i', 'had', 'it', ',', 'i', 'wasn', \"'\", 't', 'hyper', '##active', 'as', 'a', 'kid', 'or', 'anything', '.', 'in', 'part', 'i', 'think', 'being', '21', 'and', 'getting', 'diagnosed', 'makes', 'me', 'feel', 'this', 'way', '.', 'even', 'when', 'i', 'started', 'i', 'always', 'felt', 'like', 'i', 'was', 'lying', 'even', 'though', 'i', 'wasn', \"'\", 't', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'don', \"'\", 't', 'know', 'if', 'i', 'should', 'pursue', 'med', '##s', 'again', '.']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'don', \"'\", 't', 'know', 'if', 'i', 'should', 'pursue', 'med', '##s', 'again', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['clip', 'from', 'the', 'weather', 'man', 'portraying', 'exactly', 'how', 'my', 'ad', '##hd', 'thoughts', 'progress', '.', 'i', \"'\", 'm', 'sure', 'some', 'of', 'you', 'can', 'relate', '.', 'ns', '##f', '##w', 'language', '.']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['clip', 'from', 'the', 'weather', 'man', 'portraying', 'exactly', 'how', 'my', 'ad', '##hd', 'thoughts', 'progress', '.', 'i', \"'\", 'm', 'sure', 'some', 'of', 'you', 'can', 'relate', '.', 'ns', '##f', '##w', 'language', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['smoking', 'and', 'add', '##eral', '##l', '?', 'iv', '##e', 'taken', 'various', 'st', '##im', '##ula', '##nts', 'to', 'treat', 'my', 'add', 'from', 'age', '14', 'to', 'current', '(', '21', ')', '.', 'i', 'started', 'smoking', 'cigarettes', 'when', 'i', 'turned', '18', 'and', 'got', 'addicted', 'pretty', 'quickly', '.', 'i', 'can', 'turn', 'them', 'down', 'when', 'im', 'my', 'usual', 'add', 'self', ',', 'but', 'if', 'i', 'take', 'my', 'add', '##eral', '##l', ',', 'i', 'end', 'up', 'chain', 'smoking', 'all', 'day', 'and', 'feeling', 'horrible', '.', 'anybody', 'else', 'notice', 'this', 'on', 'st', '##im', '##ula', '##nts', '?', 'is', 'this', 'due', 'to', 'the', 'increased', 'do', '##pa', '##mine', 'on', 'add', '##ie', '?', 'thanks', 'for', 'any', 'help', ',', 'im', 'new', 'to', 'red', '##dit', 'right', 'now', ',', 'but', 'im', 'really', 'starting', 'to', 'dig', 'this', 'community', '.']\n",
      "INFO:__main__:Number of tokens: 118\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['smoking', 'and', 'add', '##eral', '##l', '?', 'iv', '##e', 'taken', 'various', 'st', '##im', '##ula', '##nts', 'to', 'treat', 'my', 'add', 'from', 'age', '14', 'to', 'current', '(', '21', ')', '.', 'i', 'started', 'smoking', 'cigarettes', 'when', 'i', 'turned', '18', 'and', 'got', 'addicted', 'pretty', 'quickly', '.', 'i', 'can', 'turn', 'them', 'down', 'when', 'im', 'my', 'usual', 'add', 'self', ',', 'but', 'if', 'i', 'take', 'my', 'add', '##eral', '##l', ',', 'i', 'end', 'up', 'chain', 'smoking', 'all', 'day', 'and', 'feeling', 'horrible', '.', 'anybody', 'else', 'notice', 'this', 'on', 'st', '##im', '##ula', '##nts', '?', 'is', 'this', 'due', 'to', 'the', 'increased', 'do', '##pa', '##mine', 'on', 'add', '##ie', '?', 'thanks', 'for', 'any', 'help', ',', 'im', 'new', 'to', 'red', '##dit', 'right', 'now', ',', 'but', 'im', 'really', 'starting', 'to', 'dig', 'this', 'community', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'think', 'i', 'may', 'have', 'add', 'i', 'have', 'been', 'playing', 'with', 'the', 'idea', 'for', 'a', 'month', 'or', 'two', 'now', ',', 'and', 'it', 'just', 'makes', 'so', 'much', 'sense', 'to', 'me', '.', 'i', 'have', 'had', 'trouble', 'in', 'school', 'in', 'the', 'past', ',', 'and', 'that', 'is', 'my', 'main', 'concern', 'as', 'i', 'am', 'currently', 'back', 'in', 'college', '.', 'if', 'i', 'try', 'to', 'sit', 'down', 'and', 'study', ',', 'i', 'can', 'read', 'maybe', 'a', 'single', 'page', 'out', 'of', 'my', 'reading', 'before', 'my', 'mind', 'starts', 'to', 'wander', ',', 'and', 'then', 'a', 'few', 'minutes', 'later', 'i', 'realize', 'i', 'don', \"'\", 't', 'even', 'remember', 'what', 'i', 'read', '.', 'it', 'is', 'the', 'most', 'frustrating', 'thing', 'i', 'have', 'ever', 'experienced', '.', 'sometimes', 'i', 'forgot', 'important', 'things', 'that', 'my', 'girlfriend', 'wants', 'me', 'to', 'remember', ',', 'and', 'that', 'sucks', '.', 'i', 'used', 'to', 'study', 'for', '10', 'hours', 'on', 'a', 'single', 'subject', 'and', 'not', 'be', 'able', 'to', 'finish', 'my', 'online', 'homework', '.', 'i', 'feel', 'like', 'there', 'are', 'just', 'thoughts', 'that', 'don', \"'\", 't', 'stop', 'popping', 'into', 'my', 'head', 'among', 'other', 'things', '.', 'i', 'feel', 'that', 'a', 'diagnosis', 'would', 'first', 'get', 'me', 'some', 'relief', 'in', 'knowing', 'that', 'what', 'i', 'have', 'tried', 'to', 'do', 'for', 'years', 'is', 'not', 'fully', 'my', 'fault', 'when', 'i', 'try', 'so', 'hard', '.', 'is', 'there', 'any', 'advice', 'that', 'you', 'guys', 'might', 'have', 'for', 'me', '?', 'i', 'am', 'going', 'to', 'be', 'calling', 'my', 'local', 'medical', 'center', 'to', 'set', 'up', 'an', 'appointment', ',', 'but', 'it', 'even', 'feels', 'nerve', '-', 'rack', '##ing', 'just', 'doing', 'this', '.']\n",
      "INFO:__main__:Number of tokens: 241\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'think', 'i', 'may', 'have', 'add', 'i', 'have', 'been', 'playing', 'with', 'the', 'idea', 'for', 'a', 'month', 'or', 'two', 'now', ',', 'and', 'it', 'just', 'makes', 'so', 'much', 'sense', 'to', 'me', '.', 'i', 'have', 'had', 'trouble', 'in', 'school', 'in', 'the', 'past', ',', 'and', 'that', 'is', 'my', 'main', 'concern', 'as', 'i', 'am', 'currently', 'back', 'in', 'college', '.', 'if', 'i', 'try', 'to', 'sit', 'down', 'and', 'study', ',', 'i', 'can', 'read', 'maybe', 'a', 'single', 'page', 'out', 'of', 'my', 'reading', 'before', 'my', 'mind', 'starts', 'to', 'wander', ',', 'and', 'then', 'a', 'few', 'minutes', 'later', 'i', 'realize', 'i', 'don', \"'\", 't', 'even', 'remember', 'what', 'i', 'read', '.', 'it', 'is', 'the', 'most', 'frustrating', 'thing', 'i', 'have', 'ever', 'experienced', '.', 'sometimes', 'i', 'forgot', 'important', 'things', 'that', 'my', 'girlfriend', 'wants', 'me', 'to', 'remember', ',', 'and', 'that', 'sucks', '.', 'i', 'used', 'to', 'study', 'for', '10', 'hours', 'on', 'a', 'single', 'subject', 'and', 'not', 'be', 'able', 'to', 'finish', 'my', 'online', 'homework', '.', 'i', 'feel', 'like', 'there', 'are', 'just', 'thoughts', 'that', 'don', \"'\", 't', 'stop', 'popping', 'into', 'my', 'head', 'among', 'other', 'things', '.', 'i', 'feel', 'that', 'a', 'diagnosis', 'would', 'first', 'get', 'me', 'some', 'relief', 'in', 'knowing', 'that', 'what', 'i', 'have', 'tried', 'to', 'do', 'for', 'years', 'is', 'not', 'fully', 'my', 'fault', 'when', 'i', 'try', 'so', 'hard', '.', 'is', 'there', 'any', 'advice', 'that', 'you', 'guys', 'might', 'have', 'for', 'me', '?', 'i', 'am', 'going', 'to', 'be', 'calling', 'my', 'local', 'medical', 'center', 'to', 'set', 'up', 'an', 'appointment', ',', 'but', 'it', 'even', 'feels', 'nerve', '-', 'rack', '##ing', 'just', 'doing', 'this', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'doctor', 'says', 'because', 'i', \"'\", 'm', 'doing', 'an', 'economics', 'degree', 'i', 'can', \"'\", 't', 'have', 'add', '.', 'i', 'spoke', 'with', 'him', 'recently', 'requesting', 'a', 'refer', '##ral', 'to', 'a', 'specialist', 'for', 'add', 'assessment', ',', 'and', 'he', 'told', 'me', 'i', 'wouldn', \"'\", 't', 'even', 'be', 'considered', 'on', 'the', 'basis', 'that', ',', 'because', 'i', \"'\", 'm', 'achieving', 'at', 'an', 'academic', 'level', 'higher', 'than', 'the', 'average', 'person', ',', 'i', \"'\", 'm', 'therefore', 'not', 'exhibiting', 'a', 'sufficient', 'level', 'of', 'impairment', 'to', 'justify', 'diagnosis', 'or', 'even', 'assessment', 'by', 'a', 'specialist', '.', '(', 'i', \"'\", 'm', '21', 'and', 'live', 'in', 'the', 'uk', ',', 'bt', '##w', ')', '.', 'is', 'he', 'right', 'in', 'saying', 'that', 'a', 'person', 'truly', 'af', '##flict', '##ed', 'with', 'add', 'would', 'be', 'incapable', 'of', 'even', 'attaining', 'acceptance', 'into', 'university', '?', 'i', 'feel', 'like', 'i', \"'\", 've', 'experienced', 'many', 'of', 'the', 'supposed', 'hallmark', 'symptoms', 'of', 'add', 'since', 'childhood', ',', 'such', 'as', 'personal', 'time', '##keeping', 'issues', ',', 'distract', '##ibility', ',', 'severe', 'pro', '##cr', '##ast', '##ination', ',', 'an', 'ins', '##ur', '##mount', '##able', 'ave', '##rs', '##ion', 'to', 'study', 'and', 'mentally', 'demanding', 'tasks', ',', 'have', 'lost', 'a', 'job', 'for', 'making', 'too', 'many', 'mistakes', ',', 'etc', ',', 'etc', '.', 'what', 'degree', '(', 'if', 'any', ')', 'of', 'societal', 'functionality', 'eliminate', '##s', 'the', 'possibility', 'of', 'a', 'person', 'having', 'add', '?']\n",
      "INFO:__main__:Number of tokens: 207\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'doctor', 'says', 'because', 'i', \"'\", 'm', 'doing', 'an', 'economics', 'degree', 'i', 'can', \"'\", 't', 'have', 'add', '.', 'i', 'spoke', 'with', 'him', 'recently', 'requesting', 'a', 'refer', '##ral', 'to', 'a', 'specialist', 'for', 'add', 'assessment', ',', 'and', 'he', 'told', 'me', 'i', 'wouldn', \"'\", 't', 'even', 'be', 'considered', 'on', 'the', 'basis', 'that', ',', 'because', 'i', \"'\", 'm', 'achieving', 'at', 'an', 'academic', 'level', 'higher', 'than', 'the', 'average', 'person', ',', 'i', \"'\", 'm', 'therefore', 'not', 'exhibiting', 'a', 'sufficient', 'level', 'of', 'impairment', 'to', 'justify', 'diagnosis', 'or', 'even', 'assessment', 'by', 'a', 'specialist', '.', '(', 'i', \"'\", 'm', '21', 'and', 'live', 'in', 'the', 'uk', ',', 'bt', '##w', ')', '.', 'is', 'he', 'right', 'in', 'saying', 'that', 'a', 'person', 'truly', 'af', '##flict', '##ed', 'with', 'add', 'would', 'be', 'incapable', 'of', 'even', 'attaining', 'acceptance', 'into', 'university', '?', 'i', 'feel', 'like', 'i', \"'\", 've', 'experienced', 'many', 'of', 'the', 'supposed', 'hallmark', 'symptoms', 'of', 'add', 'since', 'childhood', ',', 'such', 'as', 'personal', 'time', '##keeping', 'issues', ',', 'distract', '##ibility', ',', 'severe', 'pro', '##cr', '##ast', '##ination', ',', 'an', 'ins', '##ur', '##mount', '##able', 'ave', '##rs', '##ion', 'to', 'study', 'and', 'mentally', 'demanding', 'tasks', ',', 'have', 'lost', 'a', 'job', 'for', 'making', 'too', 'many', 'mistakes', ',', 'etc', ',', 'etc', '.', 'what', 'degree', '(', 'if', 'any', ')', 'of', 'societal', 'functionality', 'eliminate', '##s', 'the', 'possibility', 'of', 'a', 'person', 'having', 'add', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'd', 'advise', 'you', 'to', 'watch', 'as', 'many', 'of', 'this', 'guy', \"'\", 's', 'vi', '##ds', '/', 'lectures', 'to', 'understand', 'our', 'situation', 'better']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'd', 'advise', 'you', 'to', 'watch', 'as', 'many', 'of', 'this', 'guy', \"'\", 's', 'vi', '##ds', '/', 'lectures', 'to', 'understand', 'our', 'situation', 'better']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'meditation', ':', 'my', 'experience']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'meditation', ':', 'my', 'experience']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['are', 'there', 'good', 'documentaries', 'about', 'add', '/', 'ad', '##hd', 'this', 'has', 'probably', 'been', 'asked', 'before', 'but', 'my', 'girlfriend', 'and', 'i', 'are', 'looking', 'for', 'some', 'good', 'documentaries', 'we', 'could', 'watch', 'together', '.', 'so', 'she', 'can', 'get', 'a', 'bit', 'of', 'a', 'glimpse', 'into', \"'\", 'my', 'world', \"'\", '.', 'iv', '##e', 'been', 'trying', 'to', 'look', 'for', 'documentaries', 'but', 'i', 'can', 'only', 'find', '\"', 'living', 'with', 'add', '\"', 'and', '\"', 'struggle', 'for', 'control', '\"', 'but', 'i', 'can', \"'\", 't', 'find', 'the', 'proper', 'torre', '##nts', 'to', 'download', 'them', '.']\n",
      "INFO:__main__:Number of tokens: 84\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['are', 'there', 'good', 'documentaries', 'about', 'add', '/', 'ad', '##hd', 'this', 'has', 'probably', 'been', 'asked', 'before', 'but', 'my', 'girlfriend', 'and', 'i', 'are', 'looking', 'for', 'some', 'good', 'documentaries', 'we', 'could', 'watch', 'together', '.', 'so', 'she', 'can', 'get', 'a', 'bit', 'of', 'a', 'glimpse', 'into', \"'\", 'my', 'world', \"'\", '.', 'iv', '##e', 'been', 'trying', 'to', 'look', 'for', 'documentaries', 'but', 'i', 'can', 'only', 'find', '\"', 'living', 'with', 'add', '\"', 'and', '\"', 'struggle', 'for', 'control', '\"', 'but', 'i', 'can', \"'\", 't', 'find', 'the', 'proper', 'torre', '##nts', 'to', 'download', 'them', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['questions', 'from', 'a', 'parent', 'of', 'a', '4', 'year', 'old', '.', 'we', 'saw', 'a', 'psychologist', 'for', 'our', '4', 'year', 'old', 'and', 'they', 'think', 'it', 'is', 'ad', '##hd', '.', 'they', 'have', 'asked', 'us', 'to', 'put', 'him', 'on', 'a', 'sugar', 'free', '/', 'g', '##lu', '##ten', 'free', 'diet', 'to', 'see', 'if', 'he', 'calm', '##s', 'down', '(', 'seems', 'to', 'slow', 'him', 'down', 'a', 'bit', ')', '.', 'for', 'any', 'experts', 'out', 'there', 'the', 'symptoms', 'are', ':', '1', '.', 'in', 'ability', 'to', 'question', 'things', '.', 'he', 'does', 'not', 'ask', 'questions', '.', '2', '.', 'wants', 'to', 'be', 'physically', 'active', '(', 'climb', '/', 'jump', '/', 'run', ')', 'etc', '.', 'my', 'question', '(', 's', ')', ':', 'when', 'we', 'were', 'in', 'the', 'us', '(', 'we', 'are', 'in', 'india', 'for', 'a', 'vacation', ')', ',', 'i', 'thought', 'that', 'his', 'speech', '/', 'interaction', 'was', 'slow', 'because', 'he', 'is', 'confused', 'with', 'english', 'and', 'our', 'mother', 'tongue', '.', 'back', 'in', 'india', ',', 'he', 'seems', 'genuinely', 'happy', 'with', 'his', 'grand', 'parents', 'and', 'cousins', '.', 'we', 'are', 'beginning', 'to', 'wonder', 'if', 'being', 'in', 'a', 'larger', 'community', 'helps', 'him', 'to', 'build', 'his', 'speech', 'etc', '.', 'does', 'any', 'one', 'have', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 182\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['questions', 'from', 'a', 'parent', 'of', 'a', '4', 'year', 'old', '.', 'we', 'saw', 'a', 'psychologist', 'for', 'our', '4', 'year', 'old', 'and', 'they', 'think', 'it', 'is', 'ad', '##hd', '.', 'they', 'have', 'asked', 'us', 'to', 'put', 'him', 'on', 'a', 'sugar', 'free', '/', 'g', '##lu', '##ten', 'free', 'diet', 'to', 'see', 'if', 'he', 'calm', '##s', 'down', '(', 'seems', 'to', 'slow', 'him', 'down', 'a', 'bit', ')', '.', 'for', 'any', 'experts', 'out', 'there', 'the', 'symptoms', 'are', ':', '1', '.', 'in', 'ability', 'to', 'question', 'things', '.', 'he', 'does', 'not', 'ask', 'questions', '.', '2', '.', 'wants', 'to', 'be', 'physically', 'active', '(', 'climb', '/', 'jump', '/', 'run', ')', 'etc', '.', 'my', 'question', '(', 's', ')', ':', 'when', 'we', 'were', 'in', 'the', 'us', '(', 'we', 'are', 'in', 'india', 'for', 'a', 'vacation', ')', ',', 'i', 'thought', 'that', 'his', 'speech', '/', 'interaction', 'was', 'slow', 'because', 'he', 'is', 'confused', 'with', 'english', 'and', 'our', 'mother', 'tongue', '.', 'back', 'in', 'india', ',', 'he', 'seems', 'genuinely', 'happy', 'with', 'his', 'grand', 'parents', 'and', 'cousins', '.', 'we', 'are', 'beginning', 'to', 'wonder', 'if', 'being', 'in', 'a', 'larger', 'community', 'helps', 'him', 'to', 'build', 'his', 'speech', 'etc', '.', 'does', 'any', 'one', 'have', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['maintaining', 'long', '-', 'term', 'interests', 'with', 'short', '-', 'acting', 'drugs', 'one', 'of', 'the', 'most', 'cr', '##ip', '##pling', 'symptoms', 'of', 'my', 'ad', '##hd', 'is', 'an', 'inability', 'to', 'maintain', 'an', 'interest', 'in', 'anything', 'for', 'very', 'long', '.', 'iv', '##e', 'had', 'several', 'careers', ',', 'ho', '##bbies', ',', 'sports', ',', 'languages', ',', 'instruments', ',', 'etc', '.', 'how', 'does', 'a', 'drug', 'that', 'works', 'for', 'a', 'few', 'hours', 'at', 'a', 'time', 'treat', 'help', 'maintain', 'an', 'interest', 'that', 'should', 'last', 'years', '?', 'what', 'has', 'your', 'experience', 'been', '?']\n",
      "INFO:__main__:Number of tokens: 81\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['maintaining', 'long', '-', 'term', 'interests', 'with', 'short', '-', 'acting', 'drugs', 'one', 'of', 'the', 'most', 'cr', '##ip', '##pling', 'symptoms', 'of', 'my', 'ad', '##hd', 'is', 'an', 'inability', 'to', 'maintain', 'an', 'interest', 'in', 'anything', 'for', 'very', 'long', '.', 'iv', '##e', 'had', 'several', 'careers', ',', 'ho', '##bbies', ',', 'sports', ',', 'languages', ',', 'instruments', ',', 'etc', '.', 'how', 'does', 'a', 'drug', 'that', 'works', 'for', 'a', 'few', 'hours', 'at', 'a', 'time', 'treat', 'help', 'maintain', 'an', 'interest', 'that', 'should', 'last', 'years', '?', 'what', 'has', 'your', 'experience', 'been', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', 'some', 'guidance', ',', 'just', 'had', 'my', 'first', 'meeting', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', 'some', 'guidance', ',', 'just', 'had', 'my', 'first', 'meeting', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['za', '##p', 'your', 'brain', 'into', 'the', 'zone', ':', 'fast', 'track', 'to', 'pure', 'focus', '(', 'x', '-', 'post', 'from', '/', 'r', '/', 'science', ')']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['za', '##p', 'your', 'brain', 'into', 'the', 'zone', ':', 'fast', 'track', 'to', 'pure', 'focus', '(', 'x', '-', 'post', 'from', '/', 'r', '/', 'science', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['pd', '##a', 'in', 'relationships', '?']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['pd', '##a', 'in', 'relationships', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['could', 'i', 'have', 'ad', '##hd', '?', 'i', 'know', 'i', 'should', 'see', 'a', 'counselor', 'i', 'made', 'like', 'an', 'appointment', 'before', 'but', 'forgot', 'about', 'it', 'so', 'i', \"'\", 'm', 'going', 'tomorrow', '.', 'i', \"'\", 'm', '18', 'at', 'university', '.', 'okay', 'so', 'here', \"'\", 's', 'what', 'i', 'think', 'i', 'have', ':', '-', 'during', 'class', 'i', 'noticed', 'that', 'i', 'constantly', 'fi', '##dget', 'with', 'my', 'hands', '.', 'i', 'have', 'a', 'habit', 'of', 'rubbing', 'my', 'chin', 'with', 'my', 'hands', '.', '-', 'i', 'also', 'have', 'a', 'leg', 'tapping', 'problem', 'i', 'do', 'this', 'un', '##con', '##tro', '##lla', '##bly', 'and', 'some', 'students', 'get', 'bothered', '(', 'in', 'high', 'school', ')', '.', '-', 'i', 'feel', 'sometimes', 'a', 'lot', 'with', 'energy', 'and', 'i', 'can', 'concentrate', 'a', 'little', 'then', 'i', 'start', 'going', 'off', 'and', 'day', 'dream', ',', 'go', 'on', 'my', 'phone', 'during', 'class', '-', 'i', 'also', 'feel', 'ir', '##rita', '##ble', 'like', 'i', 'wanna', 'get', 'up', 'and', 'go', 'somewhere', 'else', 'even', 'when', 'i', 'try', 'to', 'study', 'a', 'lone', 'on', 'a', 'comfortable', 'chair', '.', '-', 'this', 'energy', 'then', 'makes', 'me', 'tired', 'in', 'the', 'day', 'and', 'i', 'sleep', 'in', 'class', 'even', 'though', 'i', 'get', '8', '-', '9', 'hour', 'sleep', '-', 'its', 'so', 'hard', 'to', 'follow', 'the', 'teacher', 'i', 'just', 'can', \"'\", 't', 'do', 'it', '-', 'i', 'forget', 'a', 'lot', 'o', 'things', 'like', 'bringing', 'my', 'lunch', 'from', 'work', '(', 'all', 'the', 'time', ')', 'and', 'i', 'leave', 'for', 'home', 'leaving', 'my', 'left', 'over', 'lunch', 'at', 'work', '-', 'i', 'get', 'a', 'weird', 'feeling', 'in', 'my', 'eyes', 'and', 'forehead', 'during', 'class', '-', 'when', 'i', 'was', 'little', 'i', 'used', 'to', 'say', 'and', 'do', 'stuff', 'that', 'caused', 'me', 'embarrassment', 'or', 'made', 'fun', 'of', 'someone', '.', 'without', 'thinking', '-', 'when', 'i', 'star', '##ve', 'myself', 'it', \"'\", 's', 'like', 'i', \"'\", 'm', 'with', 'a', 'lot', 'of', 'energy', 'and', 'i', 'constantly', 'do', 'stuff', '-', 'during', 'class', 'i', 'always', 'mis', '##rea', '##d', 'a', 'question', 'and', 'do', 'something', 'accident', '##lyn', 'on', 'my', 'answer', '-', 'i', 'never', 'was', 'diagnosed', '-', 'i', 'was', 'above', 'average', 'in', 'middle', 'school', 'and', 'now', 'i', \"'\", 'm', 'doing', 'horribly', '-', 'i', 'lack', 'a', 'lot', 'of', 'motivation', 'sometimes', 'i', 'would', 'just', 'lay', 'in', 'my', 'room', 'do', 'nothing', 'how', 'can', 'i', 'make', 'sure', 'of', 'i', 'have', 'it', 'also', 'how', 'can', 'i', 'make', 'sure', 'the', 'counselor', 'doesn', \"'\", 't', 'mis', '##dia', '##gno', '##se', 'or', 'give', 'me', 'a', 'wrong', 'treatment', '.', 'what', 'should', 'i', 'also', 'look', 'forward', 'too', '?', 'thanks', 'red', '##dit', '!']\n",
      "INFO:__main__:Number of tokens: 386\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['could', 'i', 'have', 'ad', '##hd', '?', 'i', 'know', 'i', 'should', 'see', 'a', 'counselor', 'i', 'made', 'like', 'an', 'appointment', 'before', 'but', 'forgot', 'about', 'it', 'so', 'i', \"'\", 'm', 'going', 'tomorrow', '.', 'i', \"'\", 'm', '18', 'at', 'university', '.', 'okay', 'so', 'here', \"'\", 's', 'what', 'i', 'think', 'i', 'have', ':', '-', 'during', 'class', 'i', 'noticed', 'that', 'i', 'constantly', 'fi', '##dget', 'with', 'my', 'hands', '.', 'i', 'have', 'a', 'habit', 'of', 'rubbing', 'my', 'chin', 'with', 'my', 'hands', '.', '-', 'i', 'also', 'have', 'a', 'leg', 'tapping', 'problem', 'i', 'do', 'this', 'un', '##con', '##tro', '##lla', '##bly', 'and', 'some', 'students', 'get', 'bothered', '(', 'in', 'high', 'school', ')', '.', '-', 'i', 'feel', 'sometimes', 'a', 'lot', 'with', 'energy', 'and', 'i', 'can', 'concentrate', 'a', 'little', 'then', 'i', 'start', 'going', 'off', 'and', 'day', 'dream', ',', 'go', 'on', 'my', 'phone', 'during', 'class', '-', 'i', 'also', 'feel', 'ir', '##rita', '##ble', 'like', 'i', 'wanna', 'get', 'up', 'and', 'go', 'somewhere', 'else', 'even', 'when', 'i', 'try', 'to', 'study', 'a', 'lone', 'on', 'a', 'comfortable', 'chair', '.', '-', 'this', 'energy', 'then', 'makes', 'me', 'tired', 'in', 'the', 'day', 'and', 'i', 'sleep', 'in', 'class', 'even', 'though', 'i', 'get', '8', '-', '9', 'hour', 'sleep', '-', 'its', 'so', 'hard', 'to', 'follow', 'the', 'teacher', 'i', 'just', 'can', \"'\", 't', 'do', 'it', '-', 'i', 'forget', 'a', 'lot', 'o', 'things', 'like', 'bringing', 'my', 'lunch', 'from', 'work', '(', 'all', 'the', 'time', ')', 'and', 'i', 'leave', 'for', 'home', 'leaving', 'my', 'left', 'over', 'lunch', 'at', 'work', '-', 'i', 'get', 'a', 'weird', 'feeling', 'in', 'my', 'eyes', 'and', 'forehead', 'during', 'class', '-', 'when', 'i', 'was', 'little', 'i', 'used', 'to', 'say', 'and', 'do', 'stuff', 'that', 'caused', 'me', 'embarrassment', 'or', 'made', 'fun', 'of', 'someone', '.', 'without', 'thinking', '-', 'when', 'i', 'star', '##ve', 'myself', 'it', \"'\", 's', 'like', 'i', \"'\", 'm', 'with', 'a', 'lot', 'of', 'energy', 'and', 'i', 'constantly', 'do', 'stuff', '-', 'during', 'class', 'i', 'always', 'mis', '##rea', '##d', 'a', 'question', 'and', 'do', 'something', 'accident', '##lyn', 'on', 'my', 'answer', '-', 'i', 'never', 'was', 'diagnosed', '-', 'i', 'was', 'above', 'average', 'in', 'middle', 'school', 'and', 'now', 'i', \"'\", 'm', 'doing', 'horribly', '-', 'i', 'lack', 'a', 'lot', 'of', 'motivation', 'sometimes', 'i', 'would', 'just', 'lay', 'in', 'my', 'room', 'do', 'nothing', 'how', 'can', 'i', 'make', 'sure', 'of', 'i', 'have', 'it', 'also', 'how', 'can', 'i', 'make', 'sure', 'the', 'counselor', 'doesn', \"'\", 't', 'mis', '##dia', '##gno', '##se', 'or', 'give', 'me', 'a', 'wrong', 'treatment', '.', 'what', 'should', 'i', 'also', 'look', 'forward', 'too', '?', 'thanks', 'red', '##dit', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'had', 'a', 'similar', 'experience', 'as', 'er', '##go', '##34', '##5', 'edit', ':', 'er', '##go', '##45', '##6', ',', 'sorry', 'wrong', 'user', '##name', 'saw', 'a', 'psychologist', 'in', 'early', 'december', '.', 'expressed', 'my', 'concerns', 'with', 'concentration', ',', 'lack', 'of', 'focus', ',', 'boredom', ',', 'day', '##dre', '##ami', '##ng', ',', 'scattered', 'thoughts', ',', 'struggling', 'with', 'work', ',', 'etc', '.', 'explained', 'that', 'i', 'have', 'someone', 'at', 'work', 'who', 'i', 'con', '##fide', '##d', 'in', ',', 'and', 'they', 'suggested', 'i', 'look', 'into', 'getting', 'screened', 'since', 'they', 'shared', 'a', 'lot', 'of', 'these', 'problems', 'before', 'getting', 'treated', 'for', 'add', '/', 'ad', '##hd', '.', 'the', 'psychologist', 'was', 'nice', ',', 'friendly', ',', 'not', 'rushed', ',', 'recommended', 'a', 'book', 'to', 'read', 'in', 'regards', 'to', 'these', 'issues', ',', 'and', 'set', 'me', 'up', 'for', 'a', 'follow', '-', 'up', 'appointment', 'with', 'a', 'psychiatrist', '.', 'unfortunately', ',', 'all', 'the', 'doctors', 'in', 'this', 'practice', 'were', 'booked', 'solid', 'for', 'the', 'next', 'two', 'months', 'and', 'only', 'recently', 'was', 'i', 'able', 'to', 'be', 'seen', '.', 'i', 'had', 'to', 'take', 'time', 'off', 'work', 'for', 'the', 'visit', 'and', 'got', 'to', 'the', 'office', '20', 'min', '##s', 'early', '.', 'the', 'doctor', 'was', '20', 'minutes', 'late', 'for', 'the', 'visit', '(', 'was', 'coming', 'back', 'from', 'lunch', 'i', 'assume', ',', 'so', 'not', 'like', 'she', 'was', 'with', 'a', 'patient', 'before', ')', '.', 'i', 'shared', 'pretty', 'much', 'everything', 'i', 'did', 'with', 'the', 'ps', '##ych', 'at', 'the', 'past', 'visit', 'with', 'her', ',', 'though', 'it', 'did', 'not', 'seem', 'nearly', 'as', 'comfortable', 'to', 'talk', 'with', 'her', '.', 'she', 'spoke', 'quickly', 'and', 'it', 'almost', 'felt', 'more', 'like', 'an', 'interrogation', 'than', 'a', 'patient', 'interview', '.', 'she', 'was', 'also', 'hard', 'to', 'understand', 'at', 'times', 'due', 'to', 'her', 'accent', '(', 'she', 'had', 'a', 'somewhat', 'thick', 'indian', 'accent', ')', '.', 'she', 'focused', 'a', 'lot', 'of', 'the', 'questions', 'on', 'my', 'early', 'life', '/', 'grade', 'school', '/', 'high', 'school', 'and', 'did', 'not', 'really', 'spend', 'much', 'time', 'talking', 'about', 'my', 'current', 'life', '.', 'basically', 'said', 'that', 'people', 'with', 'add', 'don', \"'\", 't', 'suddenly', 'develop', 'the', 'condition', ',', 'and', 'that', 'because', 'i', 'passed', 'school', '/', 'college', 'without', 'extreme', 'problems', '/', 'got', 'fired', ',', 'she', 'was', \"'\", 'not', 'concerned', \"'\", 'me', 'having', 'any', 'add', 'symptoms', '.', 'she', 'said', 'that', 'people', 'with', 'add', 'have', 'problems', 'every', 'day', 'with', 'their', 'condition', 'and', 'have', 'more', 'issues', 'with', 'finishing', 'school', '/', 'holding', 'a', 'job', '/', 'getting', 'in', 'trouble', 'with', 'the', 'law', '/', 'etc', '.', 'she', 'said', 'that', 'i', 'should', 'just', 'make', 'lists', 'and', 'ask', 'for', 'help', 'at', 'work', 'and', 'that', 'being', 'new', 'at', 'work', 'is', 'a', 'reason', 'for', 'my', 'anxiety', '/', 'difficulty', 'with', 'work', '.', 'she', 'said', 'she', 'feels', 'the', 'same', 'sort', 'of', 'frustration', 'when', 'she', 'gets', 'into', 'the', 'office', 'and', 'sees', 'a', 'huge', 'stack', 'of', 'patient', 'charts', 'to', 'go', 'through', '.', 'she', 'said', 'that', 'on', 'the', 'screening', 'question', '##naire', 'i', 'completed', 'with', 'the', 'psychologist', 'everyone', 'can', 'at', 'times', 'feels', 'the', 'same', 'extreme', 'symptoms', 'indicative', 'of', 'add', '.', 'said', 'that', 'i', 'could', 'return', 'for', 'a', 'follow', 'up', 'visit', 'with', 'my', 'parents', 'for', 'a', 're', '-', 'evaluation', '(', 'i', \"'\", 'm', '25', ')', '.', 'i', 'don', \"'\", 't', 'necessarily', 'disagree', 'with', 'any', 'of', 'her', 'assessments', ',', 'but', 'i', 'still', 'left', 'feeling', 'somewhat', 'de', '##jected', 'and', 'stupid', 'for', 'even', 'going', 'to', 'be', 'seen', '.', 'and', 'yet', 'the', 'issues', 'continue', 'and', 'i', 'can', \"'\", 't', 'help', 'but', 'feel', 'like', 'maybe', 'she', 'was', 'wrong', 'and', 'quick', 'to', 'assume', 'that', 'i', 'was', 'just', 'lazy', 'and', '/', 'or', 'drug', '-', 'seeking', '.', '(', 'she', 'probably', 'does', 'see', 'a', 'lot', 'of', 'that', 'since', 'this', 'was', 'in', 'a', 'college', 'area', 'and', 'there', 'probably', 'are', 'a', 'decent', 'number', 'of', 'students', 'who', 'go', \"'\", 'fishing', \"'\", 'for', 'medication', '.', 'it', 'probably', 'doesn', \"'\", 't', 'help', 'that', 'i', 'look', 'young', 'for', 'my', 'age', '.', ')', 'should', 'i', 'seek', 'a', 'different', 'opinion', '?', 'and', 'if', 'so', 'should', 'i', 'relate', 'this', 'experience', 'to', 'the', 'other', 'ps', '##ych', '?', 'am', 'i', 'just', 'a', 'lazy', ',', 'un', '##mot', '##ivated', '20', '-', 'something', '?', 'finally', ',', 'in', 'the', 'off', '-', 'chance', 'anyone', 'is', 'in', 'my', 'area', ',', 'does', 'anyone', 'have', 'a', 'doctor', 'recommendation', 'in', 'pittsburgh', ',', 'pa', '?', 'prefer', '##ably', 'private', 'practice', 'and', 'not', 'part', 'of', 'a', 'huge', 'group', ',', 'since', 'it', 'seems', 'like', 'these', 'doctors', 'are', 'over', '##work', '##ed', 'and', 'just', 'want', 'to', 'get', 'patients', 'in', 'and', 'out', 'as', 'fast', '/', 'efficiently', 'as', 'possible', '.']\n",
      "INFO:__main__:Number of tokens: 693\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['i', 'had', 'a', 'similar', 'experience', 'as', 'er', '##go', '##34', '##5', 'edit', ':', 'er', '##go', '##45', '##6', ',', 'sorry', 'wrong', 'user', '##name', 'saw', 'a', 'psychologist', 'in', 'early', 'december', '.', 'expressed', 'my', 'concerns', 'with', 'concentration', ',', 'lack', 'of', 'focus', ',', 'boredom', ',', 'day', '##dre', '##ami', '##ng', ',', 'scattered', 'thoughts', ',', 'struggling', 'with', 'work', ',', 'etc', '.', 'explained', 'that', 'i', 'have', 'someone', 'at', 'work', 'who', 'i', 'con', '##fide', '##d', 'in', ',', 'and', 'they', 'suggested', 'i', 'look', 'into', 'getting', 'screened', 'since', 'they', 'shared', 'a', 'lot', 'of', 'these', 'problems', 'before', 'getting', 'treated', 'for', 'add', '/', 'ad', '##hd', '.', 'the', 'psychologist', 'was', 'nice', ',', 'friendly', ',', 'not', 'rushed', ',', 'recommended', 'a', 'book', 'to', 'read', 'in', 'regards', 'to', 'these', 'issues', ',', 'and', 'set', 'me', 'up', 'for', 'a', 'follow', '-', 'up', 'appointment', 'with', 'a', 'psychiatrist', '.', 'unfortunately', ',', 'all', 'the', 'doctors', 'in', 'this', 'practice', 'were', 'booked', 'solid', 'for', 'the', 'next', 'two', 'months', 'and', 'only', 'recently', 'was', 'i', 'able', 'to', 'be', 'seen', '.', 'i', 'had', 'to', 'take', 'time', 'off', 'work', 'for', 'the', 'visit', 'and', 'got', 'to', 'the', 'office', '20', 'min', '##s', 'early', '.', 'the', 'doctor', 'was', '20', 'minutes', 'late', 'for', 'the', 'visit', '(', 'was', 'coming', 'back', 'from', 'lunch', 'i', 'assume', ',', 'so', 'not', 'like', 'she', 'was', 'with', 'a', 'patient', 'before', ')', '.', 'i', 'shared', 'pretty', 'much', 'everything', 'i', 'did', 'with', 'the', 'ps', '##ych', 'at', 'the', 'past', 'visit', 'with', 'her', ',', 'though', 'it', 'did', 'not', 'seem', 'nearly', 'as', 'comfortable', 'to', 'talk', 'with', 'her', '.', 'she', 'spoke', 'quickly', 'and', 'it', 'almost', 'felt', 'more', 'like', 'an', 'interrogation', 'than', 'a', 'patient', 'interview', '.', 'she', 'was', 'also', 'hard', 'to', 'understand', 'at', 'times', 'due', 'to', 'her', 'accent', '(', 'she', 'had', 'a', 'somewhat', 'thick', 'indian', 'accent', ')', '.', 'she', 'focused', 'a', 'lot', 'of', 'the', 'questions', 'on', 'my', 'early', 'life', '/', 'grade', 'school', '/', 'high', 'school', 'and', 'did', 'not', 'really', 'spend', 'much', 'time', 'talking', 'about', 'my', 'current', 'life', '.', 'basically', 'said', 'that', 'people', 'with', 'add', 'don', \"'\", 't', 'suddenly', 'develop', 'the', 'condition', ',', 'and', 'that', 'because', 'i', 'passed', 'school', '/', 'college', 'without', 'extreme', 'problems', '/', 'got', 'fired', ',', 'she', 'was', \"'\", 'not', 'concerned', \"'\", 'me', 'having', 'any', 'add', 'symptoms', '.', 'she', 'said', 'that', 'people', 'with', 'add', 'have', 'problems', 'every', 'day', 'with', 'their', 'condition', 'and', 'have', 'more', 'issues', 'with', 'finishing', 'school', '/', 'holding', 'a', 'job', '/', 'getting', 'in', 'trouble', 'with', 'the', 'law', '/', 'etc', '.', 'she', 'said', 'that', 'i', 'should', 'just', 'make', 'lists', 'and', 'ask', 'for', 'help', 'at', 'work', 'and', 'that', 'being', 'new', 'at', 'work', 'is', 'a', 'reason', 'for', 'my', 'anxiety', '/', 'difficulty', 'with', 'work', '.', 'she', 'said', 'she', 'feels', 'the', 'same', 'sort', 'of', 'frustration', 'when', 'she', 'gets', 'into', 'the', 'office', 'and', 'sees', 'a', 'huge', 'stack', 'of', 'patient', 'charts', 'to', 'go', 'through', '.', 'she', 'said', 'that', 'on', 'the', 'screening', 'question', '##naire', 'i', 'completed', 'with', 'the', 'psychologist', 'everyone', 'can', 'at', 'times', 'feels', 'the', 'same', 'extreme', 'symptoms', 'indicative', 'of', 'add', '.', 'said', 'that', 'i', 'could', 'return', 'for', 'a', 'follow', 'up', 'visit', 'with', 'my', 'parents', 'for', 'a', 're', '-', 'evaluation', '(', 'i', \"'\", 'm', '25', ')', '.', 'i', 'don', \"'\", 't', 'necessarily', 'disagree', 'with', 'any', 'of', 'her', 'assessments', ',', 'but', 'i', 'still', 'left', 'feeling', 'somewhat', 'de', '##jected', 'and', 'stupid', 'for', 'even', 'going', 'to'], ['be', 'seen', '.', 'and', 'yet', 'the', 'issues', 'continue', 'and', 'i', 'can', \"'\", 't', 'help', 'but', 'feel', 'like', 'maybe', 'she', 'was', 'wrong', 'and', 'quick', 'to', 'assume', 'that', 'i', 'was', 'just', 'lazy', 'and', '/', 'or', 'drug', '-', 'seeking', '.', '(', 'she', 'probably', 'does', 'see', 'a', 'lot', 'of', 'that', 'since', 'this', 'was', 'in', 'a', 'college', 'area', 'and', 'there', 'probably', 'are', 'a', 'decent', 'number', 'of', 'students', 'who', 'go', \"'\", 'fishing', \"'\", 'for', 'medication', '.', 'it', 'probably', 'doesn', \"'\", 't', 'help', 'that', 'i', 'look', 'young', 'for', 'my', 'age', '.', ')', 'should', 'i', 'seek', 'a', 'different', 'opinion', '?', 'and', 'if', 'so', 'should', 'i', 'relate', 'this', 'experience', 'to', 'the', 'other', 'ps', '##ych', '?', 'am', 'i', 'just', 'a', 'lazy', ',', 'un', '##mot', '##ivated', '20', '-', 'something', '?', 'finally', ',', 'in', 'the', 'off', '-', 'chance', 'anyone', 'is', 'in', 'my', 'area', ',', 'does', 'anyone', 'have', 'a', 'doctor', 'recommendation', 'in', 'pittsburgh', ',', 'pa', '?', 'prefer', '##ably', 'private', 'practice', 'and', 'not', 'part', 'of', 'a', 'huge', 'group', ',', 'since', 'it', 'seems', 'like', 'these', 'doctors', 'are', 'over', '##work', '##ed', 'and', 'just', 'want', 'to', 'get', 'patients', 'in', 'and', 'out', 'as', 'fast', '/', 'efficiently', 'as', 'possible', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '/', 'ad', '##hd', 'and', 'low', 'testosterone', 'levels', '?', 'let', 'me', 'introduce', 'myself', '.', 'i', \"'\", 'm', 'a', '19', 'year', 'old', 'male', 'living', 'in', 'the', 'netherlands', 'and', 'i', 'have', 'been', 'diagnosed', 'with', 'add', '.', 'today', 'i', 'was', 'thinking', 'about', 'my', 'add', 'and', 'testosterone', 'suddenly', 'popped', 'up', 'into', 'my', 'head', '.', 'i', 'started', 'to', 'investigate', 'low', 'testosterone', 'levels', 'and', 'add', '/', 'ad', '##hd', '.', 'but', 'i', 'don', \"'\", 't', 'have', 'enough', 'knowledge', 'on', 'this', 'knowledge', '.', 'what', 'i', \"'\", 've', 'seen', 'is', 'that', 'low', 'testosterone', 'levels', 'can', 'ag', '##gra', '##vate', 'add', '/', 'ad', '##hd', '.', 'anyone', 'out', 'there', 'with', 'more', 'knowledge', 'on', 'this', 'subject', '?', 'ps', 'sorry', 'for', 'my', 'poor', 'english', '.']\n",
      "INFO:__main__:Number of tokens: 111\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '/', 'ad', '##hd', 'and', 'low', 'testosterone', 'levels', '?', 'let', 'me', 'introduce', 'myself', '.', 'i', \"'\", 'm', 'a', '19', 'year', 'old', 'male', 'living', 'in', 'the', 'netherlands', 'and', 'i', 'have', 'been', 'diagnosed', 'with', 'add', '.', 'today', 'i', 'was', 'thinking', 'about', 'my', 'add', 'and', 'testosterone', 'suddenly', 'popped', 'up', 'into', 'my', 'head', '.', 'i', 'started', 'to', 'investigate', 'low', 'testosterone', 'levels', 'and', 'add', '/', 'ad', '##hd', '.', 'but', 'i', 'don', \"'\", 't', 'have', 'enough', 'knowledge', 'on', 'this', 'knowledge', '.', 'what', 'i', \"'\", 've', 'seen', 'is', 'that', 'low', 'testosterone', 'levels', 'can', 'ag', '##gra', '##vate', 'add', '/', 'ad', '##hd', '.', 'anyone', 'out', 'there', 'with', 'more', 'knowledge', 'on', 'this', 'subject', '?', 'ps', 'sorry', 'for', 'my', 'poor', 'english', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['concert', '##a', '54', '##mg', ',', 'worked', 'for', 'one', 'week', '.', '.', '.', 'i', \"'\", 've', 'just', 'upgraded', 'my', 'dos', '##age', 'to', '54', '##mg', 'of', 'concert', '##a', '.', 'the', 'first', 'week', 'i', 'was', 'happy', ',', 'noticed', 'a', 'substantial', 'effect', 'in', 'comparison', 'to', 'the', '36', '##mg', 'dos', '##age', '.', 'now', 'it', \"'\", 's', 'fading', ',', 'it', 'seems', 'to', 'work', 'on', 'and', 'off', 'depending', 'on', 'the', 'day', '.', 'when', 'it', 'works', ',', 'everything', 'is', 'good', ',', 'but', 'when', 'it', 'doesn', \"'\", 't', 'why', 'bother', 'even', 'taking', 'it', '.', 'also', ',', 'i', 'still', 'have', 'the', 'issue', 'of', 'the', 'medication', 'lasting', 'no', 'more', 'than', '6', '-', '8', 'hours', ',', 'leaving', 'useless', 'in', 'the', 'evening', '.', 'i', \"'\", 'm', 'starting', 'to', 'think', 'that', 'i', 'may', 'need', 'to', 'switch', 'to', 'something', 'else', '.', 'any', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 129\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['concert', '##a', '54', '##mg', ',', 'worked', 'for', 'one', 'week', '.', '.', '.', 'i', \"'\", 've', 'just', 'upgraded', 'my', 'dos', '##age', 'to', '54', '##mg', 'of', 'concert', '##a', '.', 'the', 'first', 'week', 'i', 'was', 'happy', ',', 'noticed', 'a', 'substantial', 'effect', 'in', 'comparison', 'to', 'the', '36', '##mg', 'dos', '##age', '.', 'now', 'it', \"'\", 's', 'fading', ',', 'it', 'seems', 'to', 'work', 'on', 'and', 'off', 'depending', 'on', 'the', 'day', '.', 'when', 'it', 'works', ',', 'everything', 'is', 'good', ',', 'but', 'when', 'it', 'doesn', \"'\", 't', 'why', 'bother', 'even', 'taking', 'it', '.', 'also', ',', 'i', 'still', 'have', 'the', 'issue', 'of', 'the', 'medication', 'lasting', 'no', 'more', 'than', '6', '-', '8', 'hours', ',', 'leaving', 'useless', 'in', 'the', 'evening', '.', 'i', \"'\", 'm', 'starting', 'to', 'think', 'that', 'i', 'may', 'need', 'to', 'switch', 'to', 'something', 'else', '.', 'any', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'side', 'projects', '?', 'i', \"'\", 'm', 'sure', 'i', \"'\", 'm', 'not', 'the', 'only', 'one', 'here', 'who', 'has', 'gone', 'through', 'the', 'following', 'cycle', 'dozens', 'of', 'times', ':', '(', '1', ')', 'great', 'idea', 'for', 'a', 'small', ',', 'evenings', 'and', 'weekends', 'project', ';', '(', '2', ')', 'intense', 'study', 'and', 'preparation', ';', '(', '3', ')', 'slow', 'progress', ',', 'realization', 'that', 'even', 'this', '\"', 'small', '\"', 'project', 'will', 'require', 'a', 'lot', 'more', 'work', 'than', 'expected', ';', '(', '4', ')', 'di', '##sil', '##lusion', 'and', 'abandonment', '.', 'has', 'anyone', 'here', 'ever', 'escaped', 'this', 'cycle', '?', 'what', 'did', 'you', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 96\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'side', 'projects', '?', 'i', \"'\", 'm', 'sure', 'i', \"'\", 'm', 'not', 'the', 'only', 'one', 'here', 'who', 'has', 'gone', 'through', 'the', 'following', 'cycle', 'dozens', 'of', 'times', ':', '(', '1', ')', 'great', 'idea', 'for', 'a', 'small', ',', 'evenings', 'and', 'weekends', 'project', ';', '(', '2', ')', 'intense', 'study', 'and', 'preparation', ';', '(', '3', ')', 'slow', 'progress', ',', 'realization', 'that', 'even', 'this', '\"', 'small', '\"', 'project', 'will', 'require', 'a', 'lot', 'more', 'work', 'than', 'expected', ';', '(', '4', ')', 'di', '##sil', '##lusion', 'and', 'abandonment', '.', 'has', 'anyone', 'here', 'ever', 'escaped', 'this', 'cycle', '?', 'what', 'did', 'you', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'advice']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'advice']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['insurance', 'company', 'charging', '$', '950', 'a', 'month', 'because', 'of', 'ad', '##hd', '.', 'help', '!', 'hi', 'ad', '##hd', ',', 'i', 'just', 'found', 'this', 'sub', '##red', '##dit', 'and', 'i', \"'\", 'm', 'hoping', 'you', 'can', 'help', 'me', 'out', 'or', 'share', 'your', 'experiences', '.', 'i', 'just', 'graduated', 'in', 'december', 'and', 'am', 'in', 'the', 'process', 'of', 'looking', 'for', 'a', 'job', '.', 'my', 'school', 'insurance', 'expired', 'this', 'month', ',', 'and', 'i', 'applied', 'for', 'an', 'independent', 'plan', 'through', 'anthem', 'blue', 'cross', '.', 'after', 'waiting', 'weeks', 'to', 'hear', 'back', ',', 'i', 'finally', 'called', 'today', 'to', 'find', 'out', 'that', 'my', 'application', 'was', 'declined', ',', 'and', 'they', \"'\", 're', 'offering', 'me', 'two', 'other', 'plan', 'options', '.', 'one', 'costs', '$', '950', 'a', 'month', 'and', 'the', 'other', 'costs', '$', '97', '##0', '.', 'reason', 'for', 'decline', 'and', 'price', '##y', 'plans', ':', 'treatment', 'and', 'medication', 'for', 'my', 'ad', '##hd', '.', 'i', 'am', 'fl', '##ab', '##berg', '##ast', '##ed', '.', 'i', 'just', 'want', 'to', 'cry', ',', 'there', 'is', 'no', 'way', 'that', 'i', 'can', 'afford', '$', '950', 'a', 'month', 'as', 'i', 'am', 'unemployed', 'and', 'spent', 'the', 'last', '3', 'years', 'as', 'a', 'student', '.', 'i', 'can', \"'\", 't', 'get', 'back', 'on', 'my', 'parent', \"'\", 's', 'plan', 'because', 'i', \"'\", 'm', '26', '.', 'my', 'med', '##s', 'cost', '$', '80', 'a', 'month', 'and', 'i', 'always', 'get', 'the', 'generic', '.', 'they', 'won', \"'\", 't', 'even', 'give', 'me', 'the', 'old', ',', 'reasonable', 'plan', 'with', 'no', 'ad', '##hd', 'med', 'coverage', '.', 'because', 'i', \"'\", 've', 'been', 'treated', 'in', 'the', 'past', '12', 'months', ',', 'they', \"'\", 're', 'denying', 'coverage', 'of', 'everything', 'due', 'to', 'a', 'me', '##as', '##ly', '$', '80', '.', 'has', 'anyone', 'else', 'had', 'these', 'problems', 'before', '?', 'does', 'anyone', 'know', 'of', 'an', 'insurance', 'company', 'that', 'covers', 'people', 'with', 'ad', '##hd', 'or', 'somewhere', 'to', 'get', 'discount', '##ed', 'med', '##s', '?', 'i', 'knew', 'that', 'insurance', 'reform', 'was', 'important', 'before', ',', 'but', 'now', 'i', 'realize', 'just', 'how', 'important', 'it', 'is', '.', 'i', \"'\", 'm', 'otherwise', 'a', 'very', 'healthy', 'person', 'who', 'doesn', \"'\", 't', 'smoke', ',', 'eats', 'right', ',', 'never', 'gets', 'sick', ',', 'and', 'exercises', 'regularly', '.', 'i', 'couldn', \"'\", 't', 'imagine', 'what', 'it', 'would', 'be', 'like', 'for', 'someone', 'who', 'is', 'un', '##hea', '##lth', '##y', '.']\n",
      "INFO:__main__:Number of tokens: 349\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['insurance', 'company', 'charging', '$', '950', 'a', 'month', 'because', 'of', 'ad', '##hd', '.', 'help', '!', 'hi', 'ad', '##hd', ',', 'i', 'just', 'found', 'this', 'sub', '##red', '##dit', 'and', 'i', \"'\", 'm', 'hoping', 'you', 'can', 'help', 'me', 'out', 'or', 'share', 'your', 'experiences', '.', 'i', 'just', 'graduated', 'in', 'december', 'and', 'am', 'in', 'the', 'process', 'of', 'looking', 'for', 'a', 'job', '.', 'my', 'school', 'insurance', 'expired', 'this', 'month', ',', 'and', 'i', 'applied', 'for', 'an', 'independent', 'plan', 'through', 'anthem', 'blue', 'cross', '.', 'after', 'waiting', 'weeks', 'to', 'hear', 'back', ',', 'i', 'finally', 'called', 'today', 'to', 'find', 'out', 'that', 'my', 'application', 'was', 'declined', ',', 'and', 'they', \"'\", 're', 'offering', 'me', 'two', 'other', 'plan', 'options', '.', 'one', 'costs', '$', '950', 'a', 'month', 'and', 'the', 'other', 'costs', '$', '97', '##0', '.', 'reason', 'for', 'decline', 'and', 'price', '##y', 'plans', ':', 'treatment', 'and', 'medication', 'for', 'my', 'ad', '##hd', '.', 'i', 'am', 'fl', '##ab', '##berg', '##ast', '##ed', '.', 'i', 'just', 'want', 'to', 'cry', ',', 'there', 'is', 'no', 'way', 'that', 'i', 'can', 'afford', '$', '950', 'a', 'month', 'as', 'i', 'am', 'unemployed', 'and', 'spent', 'the', 'last', '3', 'years', 'as', 'a', 'student', '.', 'i', 'can', \"'\", 't', 'get', 'back', 'on', 'my', 'parent', \"'\", 's', 'plan', 'because', 'i', \"'\", 'm', '26', '.', 'my', 'med', '##s', 'cost', '$', '80', 'a', 'month', 'and', 'i', 'always', 'get', 'the', 'generic', '.', 'they', 'won', \"'\", 't', 'even', 'give', 'me', 'the', 'old', ',', 'reasonable', 'plan', 'with', 'no', 'ad', '##hd', 'med', 'coverage', '.', 'because', 'i', \"'\", 've', 'been', 'treated', 'in', 'the', 'past', '12', 'months', ',', 'they', \"'\", 're', 'denying', 'coverage', 'of', 'everything', 'due', 'to', 'a', 'me', '##as', '##ly', '$', '80', '.', 'has', 'anyone', 'else', 'had', 'these', 'problems', 'before', '?', 'does', 'anyone', 'know', 'of', 'an', 'insurance', 'company', 'that', 'covers', 'people', 'with', 'ad', '##hd', 'or', 'somewhere', 'to', 'get', 'discount', '##ed', 'med', '##s', '?', 'i', 'knew', 'that', 'insurance', 'reform', 'was', 'important', 'before', ',', 'but', 'now', 'i', 'realize', 'just', 'how', 'important', 'it', 'is', '.', 'i', \"'\", 'm', 'otherwise', 'a', 'very', 'healthy', 'person', 'who', 'doesn', \"'\", 't', 'smoke', ',', 'eats', 'right', ',', 'never', 'gets', 'sick', ',', 'and', 'exercises', 'regularly', '.', 'i', 'couldn', \"'\", 't', 'imagine', 'what', 'it', 'would', 'be', 'like', 'for', 'someone', 'who', 'is', 'un', '##hea', '##lth', '##y', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'adult', 'ad', '##hd', 'here', '.', 'hey', 'guys', '.', 'i', 'just', 'got', 'back', 'from', 'the', 'doc', ',', 'and', 'after', 'reporting', 'difficulty', 'concentrating', 'at', 'work', '(', 'i', \"'\", 'm', 'a', 'programmer', ')', '+', 'feeling', 'scattered', 'and', 'pro', '##cr', '##ast', '##ina', '##t', '-', 'y', '(', 'that', \"'\", 's', 'a', 'word', 'now', ')', ',', 'i', \"'\", 've', 'been', 'handed', 'a', '10', '##mg', 'add', '##eral', '##l', 'script', '.', 'picked', \"'\", 'em', 'up', 'this', 'afternoon', ',', 'and', 'took', 'the', 'first', 'one', 'a', 'couple', 'hours', 'ago', '.', 'figured', 'i', \"'\", 'd', 'stop', 'by', 'here', ',', 'since', 'i', 'take', 'all', 'my', '_', 'other', '_', 'life', 'advice', 'from', 'red', '##dit', '.', 'what', \"'\", 're', 'some', 'coping', 'mechanisms', 'other', 'than', 'add', '##eral', '##l', 'y', \"'\", 'all', 'take', 'to', 'help', 'cope', '?', '10', '##mg', 'is', 'a', 'low', '-', 'is', '##h', 'dose', 'for', 'a', '170', '##lb', 'male', ';', 'what', \"'\", 're', 'y', \"'\", 'all', 'doing', '?', 'any', '##how', ',', 'hi', '!']\n",
      "INFO:__main__:Number of tokens: 150\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'adult', 'ad', '##hd', 'here', '.', 'hey', 'guys', '.', 'i', 'just', 'got', 'back', 'from', 'the', 'doc', ',', 'and', 'after', 'reporting', 'difficulty', 'concentrating', 'at', 'work', '(', 'i', \"'\", 'm', 'a', 'programmer', ')', '+', 'feeling', 'scattered', 'and', 'pro', '##cr', '##ast', '##ina', '##t', '-', 'y', '(', 'that', \"'\", 's', 'a', 'word', 'now', ')', ',', 'i', \"'\", 've', 'been', 'handed', 'a', '10', '##mg', 'add', '##eral', '##l', 'script', '.', 'picked', \"'\", 'em', 'up', 'this', 'afternoon', ',', 'and', 'took', 'the', 'first', 'one', 'a', 'couple', 'hours', 'ago', '.', 'figured', 'i', \"'\", 'd', 'stop', 'by', 'here', ',', 'since', 'i', 'take', 'all', 'my', '_', 'other', '_', 'life', 'advice', 'from', 'red', '##dit', '.', 'what', \"'\", 're', 'some', 'coping', 'mechanisms', 'other', 'than', 'add', '##eral', '##l', 'y', \"'\", 'all', 'take', 'to', 'help', 'cope', '?', '10', '##mg', 'is', 'a', 'low', '-', 'is', '##h', 'dose', 'for', 'a', '170', '##lb', 'male', ';', 'what', \"'\", 're', 'y', \"'\", 'all', 'doing', '?', 'any', '##how', ',', 'hi', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['body', '##building', '/', 'sports', 'energy', 'supplement', 'greatly', 'improves', 'my', 'focus', 'and', 'mood', '.', 'temporary', 'remedy', 'for', 'my', 'add', '.', 'i', \"'\", 've', 'been', 'using', 'energy', 'drinks', 'and', 'various', 'st', '##im', '##ula', '##nt', 'powder', '##s', 'for', 'years', ',', 'so', 'ot', '##c', 'st', '##im', '##s', 'are', 'nothing', 'new', 'to', 'me', '.', 'i', \"'\", 've', 'taken', 'jack', '##3d', ',', 'coffee', ',', 'red', '##line', ',', 'monster', 'energy', 'etc', '.', 'usually', 'the', 'only', 'energy', 'i', 'get', 'is', 'a', 'wired', 'out', 'fi', '##dget', '##y', 'feeling', 'and', 'a', 'loss', 'of', 'sleep', '##iness', ',', 'nothing', 'concerning', 'focus', 'and', 'motivation', '.', 'well', 'i', 'bought', 'a', 'su', '##pp', '##lam', '##ent', 'called', '\"', 'cr', '##az', '##e', '\"', 'because', 'it', 'has', 'relatively', 'low', 'caf', '##fe', '##ine', '(', 'only', '80', '##mg', 'supposedly', ',', 'i', \"'\", 'm', 'trying', 'to', 'get', 'off', 'st', '##im', '##s', ')', 'and', 'tried', 'it', '.', '.', '.', '.', '.', '.', 'i', 'felt', 'like', 'a', 'completely', 'new', 'person', ',', 'still', 'the', 'same', 'me', ',', 'just', 'the', 'me', 'that', 'i', 'always', 'wanted', 'to', 'feel', 'like', '.', 'i', 'could', 'actually', 'get', 'normal', 'things', 'like', 'laundry', 'and', 'dishes', 'done', ',', 'while', 'even', 'enjoying', 'them', '.', 'my', 'speech', 'greatly', 'improved', ',', 'i', 'could', 'art', '##iculate', 'my', 'words', 'without', 'hesitation', ',', 'over', '-', 'thinking', ',', 'getting', 'tongue', 'tied', 'or', 'mum', '##bling', '.', 'my', 'mind', 'focused', 'sharply', 'and', 'i', 'actually', 'could', 'enjoy', 'simply', 'looking', 'at', 'something', ',', 'instead', 'of', 'starring', 'off', 'blankly', 'day', '##dre', '##ami', '##ng', '.', 'my', 'eyes', '##ight', 'felt', 'better', ',', 'my', 'mood', 'significantly', 'improved', '.', 'everything', 'was', 'just', 'better', '.', 'i', 'felt', 'normal', '.', 'this', 'best', 'describes', 'what', 'i', 'felt', ':', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'flow', '_', '%', '28', '##psy', '##cho', '##logy', '%', '29', 'can', 'anyone', 'please', 'take', 'a', 'look', 'at', 'the', 'ingredients', 'and', 'tell', 'me', 'what', 'specifically', 'could', 'be', 'responsible', 'for', 'this', '?', 'i', \"'\", 'd', 'like', 'to', 'purchase', 'the', 'bulk', 'ingredients', 'without', 'the', 'st', '##im', '##ula', '##nts', 'contained', 'in', 'the', 'cr', '##az', '##e', 'supplement', 'itself', '.', 'link', ':', 'http', ':', '/', '/', 'www', '.', 'body', '##building', '.', 'com', '/', 'store', '/', 'driven', '##sport', '##s', '/', 'cr', '##az', '##e', '.', 'html', 'cr', '##ea', '##tine', 'mono', '##hy', '##dra', '##te', ',', 'trim', '##eth', '##yl', '##gly', '##cine', '(', 'beta', '##ine', 'an', '##hy', '##dro', '##us', ')', ',', 'l', '-', 'ci', '##tr', '##ull', '##ine', ',', 'den', '##dro', '##be', '##x', '##™', '(', 'den', '##dro', '##bi', '##um', 'extract', ')', '(', 'stem', ')', '(', 'concentrated', 'for', 'al', '##kal', '##oid', 'content', 'including', 'den', '##dro', '##bine', ',', 'den', '##dro', '##xin', '##e', ',', 'den', '##dra', '##mine', ',', 'b', '-', 'ph', '##en', '##yle', '##thy', '##lam', '##ine', ',', 'n', ',', 'n', '-', 'dime', '##thy', '##l', '-', 'b', '-', 'ph', '##en', '##yle', '##thy', '##lam', '##ine', ',', 'and', 'n', ',', 'n', '-', 'diet', '##hyl', '-', 'b', '-', 'ph', '##en', '##yle', '##thy', '##lam', '##ine', ')', ',', 'b', '-', 'ph', '##en', '##yle', '##thy', '##lam', '##ine', 'hc', '##l', ',', 'ci', '##tra', '##mine', '##™', '(', 'citrus', 're', '##tic', '##ulata', 'extract', ')', '(', 'fruit', ')', '(', 'concentrated', 'for', 'n', '-', 'methyl', '##ty', '##ram', '##ine', 'content', ')', ',', 'caf', '##fe', '##ine', 'an', '##hy', '##dro', '##us']\n",
      "INFO:__main__:Number of tokens: 500\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['body', '##building', '/', 'sports', 'energy', 'supplement', 'greatly', 'improves', 'my', 'focus', 'and', 'mood', '.', 'temporary', 'remedy', 'for', 'my', 'add', '.', 'i', \"'\", 've', 'been', 'using', 'energy', 'drinks', 'and', 'various', 'st', '##im', '##ula', '##nt', 'powder', '##s', 'for', 'years', ',', 'so', 'ot', '##c', 'st', '##im', '##s', 'are', 'nothing', 'new', 'to', 'me', '.', 'i', \"'\", 've', 'taken', 'jack', '##3d', ',', 'coffee', ',', 'red', '##line', ',', 'monster', 'energy', 'etc', '.', 'usually', 'the', 'only', 'energy', 'i', 'get', 'is', 'a', 'wired', 'out', 'fi', '##dget', '##y', 'feeling', 'and', 'a', 'loss', 'of', 'sleep', '##iness', ',', 'nothing', 'concerning', 'focus', 'and', 'motivation', '.', 'well', 'i', 'bought', 'a', 'su', '##pp', '##lam', '##ent', 'called', '\"', 'cr', '##az', '##e', '\"', 'because', 'it', 'has', 'relatively', 'low', 'caf', '##fe', '##ine', '(', 'only', '80', '##mg', 'supposedly', ',', 'i', \"'\", 'm', 'trying', 'to', 'get', 'off', 'st', '##im', '##s', ')', 'and', 'tried', 'it', '.', '.', '.', '.', '.', '.', 'i', 'felt', 'like', 'a', 'completely', 'new', 'person', ',', 'still', 'the', 'same', 'me', ',', 'just', 'the', 'me', 'that', 'i', 'always', 'wanted', 'to', 'feel', 'like', '.', 'i', 'could', 'actually', 'get', 'normal', 'things', 'like', 'laundry', 'and', 'dishes', 'done', ',', 'while', 'even', 'enjoying', 'them', '.', 'my', 'speech', 'greatly', 'improved', ',', 'i', 'could', 'art', '##iculate', 'my', 'words', 'without', 'hesitation', ',', 'over', '-', 'thinking', ',', 'getting', 'tongue', 'tied', 'or', 'mum', '##bling', '.', 'my', 'mind', 'focused', 'sharply', 'and', 'i', 'actually', 'could', 'enjoy', 'simply', 'looking', 'at', 'something', ',', 'instead', 'of', 'starring', 'off', 'blankly', 'day', '##dre', '##ami', '##ng', '.', 'my', 'eyes', '##ight', 'felt', 'better', ',', 'my', 'mood', 'significantly', 'improved', '.', 'everything', 'was', 'just', 'better', '.', 'i', 'felt', 'normal', '.', 'this', 'best', 'describes', 'what', 'i', 'felt', ':', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'flow', '_', '%', '28', '##psy', '##cho', '##logy', '%', '29', 'can', 'anyone', 'please', 'take', 'a', 'look', 'at', 'the', 'ingredients', 'and', 'tell', 'me', 'what', 'specifically', 'could', 'be', 'responsible', 'for', 'this', '?', 'i', \"'\", 'd', 'like', 'to', 'purchase', 'the', 'bulk', 'ingredients', 'without', 'the', 'st', '##im', '##ula', '##nts', 'contained', 'in', 'the', 'cr', '##az', '##e', 'supplement', 'itself', '.', 'link', ':', 'http', ':', '/', '/', 'www', '.', 'body', '##building', '.', 'com', '/', 'store', '/', 'driven', '##sport', '##s', '/', 'cr', '##az', '##e', '.', 'html', 'cr', '##ea', '##tine', 'mono', '##hy', '##dra', '##te', ',', 'trim', '##eth', '##yl', '##gly', '##cine', '(', 'beta', '##ine', 'an', '##hy', '##dro', '##us', ')', ',', 'l', '-', 'ci', '##tr', '##ull', '##ine', ',', 'den', '##dro', '##be', '##x', '##™', '(', 'den', '##dro', '##bi', '##um', 'extract', ')', '(', 'stem', ')', '(', 'concentrated', 'for', 'al', '##kal', '##oid', 'content', 'including', 'den', '##dro', '##bine', ',', 'den', '##dro', '##xin', '##e', ',', 'den', '##dra', '##mine', ',', 'b', '-', 'ph', '##en', '##yle', '##thy', '##lam', '##ine', ',', 'n', ',', 'n', '-', 'dime', '##thy', '##l', '-', 'b', '-', 'ph', '##en', '##yle', '##thy', '##lam', '##ine', ',', 'and', 'n', ',', 'n', '-', 'diet', '##hyl', '-', 'b', '-', 'ph', '##en', '##yle', '##thy', '##lam', '##ine', ')', ',', 'b', '-', 'ph', '##en', '##yle', '##thy', '##lam', '##ine', 'hc', '##l', ',', 'ci', '##tra', '##mine', '##™', '(', 'citrus', 're', '##tic', '##ulata', 'extract', ')', '(', 'fruit', ')', '(', 'concentrated', 'for', 'n', '-', 'methyl', '##ty', '##ram', '##ine', 'content', ')', ',', 'caf', '##fe', '##ine', 'an', '##hy', '##dro', '##us']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'ten', 'command', '##ments', 'of', 'amp', '##het', '##amine', 'use']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'ten', 'command', '##ments', 'of', 'amp', '##het', '##amine', 'use']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'all', '##er', '##gies', 'i', 'read', 'about', 'ad', '##hd', \"'\", 'er', '##s', 'often', 'having', 'all', '##er', '##gies', '.', 'anyone', 'got', 'some', 'good', 'info', 'explaining', 'why', 'there', \"'\", 's', 'a', 'connection', '?', 'also', ':', 'tell', 'me', 'which', 'all', '##er', '##gies', 'you', 'have', '!', 'myself', ':', 'i', 'have', 'a', 'pretty', 'bad', 'case', 'of', 'hay', '##fe', '##ver', 'from', 'april', 'till', 'well', 'in', 'october', '.', 'also', 'a', 'mild', 'cat', 'all', '##ergy', ',', 'but', 'only', 'if', 'they', \"'\", 're', 'really', 'shed', '##ding', 'hair', '.']\n",
      "INFO:__main__:Number of tokens: 81\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'all', '##er', '##gies', 'i', 'read', 'about', 'ad', '##hd', \"'\", 'er', '##s', 'often', 'having', 'all', '##er', '##gies', '.', 'anyone', 'got', 'some', 'good', 'info', 'explaining', 'why', 'there', \"'\", 's', 'a', 'connection', '?', 'also', ':', 'tell', 'me', 'which', 'all', '##er', '##gies', 'you', 'have', '!', 'myself', ':', 'i', 'have', 'a', 'pretty', 'bad', 'case', 'of', 'hay', '##fe', '##ver', 'from', 'april', 'till', 'well', 'in', 'october', '.', 'also', 'a', 'mild', 'cat', 'all', '##ergy', ',', 'but', 'only', 'if', 'they', \"'\", 're', 'really', 'shed', '##ding', 'hair', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['rita', '##lin', 'gone', 'wrong', 'response', '[', 'tv', 'interview', ']']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['rita', '##lin', 'gone', 'wrong', 'response', '[', 'tv', 'interview', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['rita', '##lin', 'gone', 'wrong', 'response', 'by', 'dr', 'kenny', 'handel', '##man', '(', 'local', 'tv', ')']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['rita', '##lin', 'gone', 'wrong', 'response', 'by', 'dr', 'kenny', 'handel', '##man', '(', 'local', 'tv', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'was', 'it', 'like', 'for', 'you', 'growing', 'up', '?', 'how', 'have', 'you', 'learned', 'to', 'adapt', 'and', 'cope', ',', 'particularly', 'those', 'who', 'are', 'treatment', '-', 'resistant', 'or', 'going', 'without', 'medication', '?', 'aka', 'is', 'there', 'hope', 'for', 'me', 'and', 'my', 'family', 'with', 'a', 'life', 'coach', '?']\n",
      "INFO:__main__:Number of tokens: 44\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'was', 'it', 'like', 'for', 'you', 'growing', 'up', '?', 'how', 'have', 'you', 'learned', 'to', 'adapt', 'and', 'cope', ',', 'particularly', 'those', 'who', 'are', 'treatment', '-', 'resistant', 'or', 'going', 'without', 'medication', '?', 'aka', 'is', 'there', 'hope', 'for', 'me', 'and', 'my', 'family', 'with', 'a', 'life', 'coach', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['wondering', 'if', 'any', 'link', 'to', 'ad', '##hd', 'and', 'breathing', 'difficulties']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['wondering', 'if', 'any', 'link', 'to', 'ad', '##hd', 'and', 'breathing', 'difficulties']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'seven', 'deadly', 'sins', ':', 'pride', '&', 'the', 'ad', '##hd', 'personality']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'seven', 'deadly', 'sins', ':', 'pride', '&', 'the', 'ad', '##hd', 'personality']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['maybe', 'our', 'ability', 'to', 'gauge', 'how', 'long', 'tasks', 'will', 'take', 'is', 'so', 'off', 'because', 'tv', 'tells', 'us', 'that', 'we', 'can', 'conquer', 'rome', 'in', 'an', 'hour']\n",
      "INFO:__main__:Number of tokens: 25\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['maybe', 'our', 'ability', 'to', 'gauge', 'how', 'long', 'tasks', 'will', 'take', 'is', 'so', 'off', 'because', 'tv', 'tells', 'us', 'that', 'we', 'can', 'conquer', 'rome', 'in', 'an', 'hour']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'long', 'term', 'health', 'issues']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'long', 'term', 'health', 'issues']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'does', 'it', 'feel', 'like', 'for', 'you', 'when', 'your', 'ad', '##hd', 'is', 'under', 'control', '?', 'when', 'do', 'you', 'know', 'an', 'issue', 'you', \"'\", 're', 'having', 'is', 'something', 'else', '?', 'be', 'it', 'from', 'medicine', ',', 'supplements', ',', 'wa', '##cky', 'moon', 'magic', ',', 'hard', 'work', ',', 'etc', '.', '(', 'i', 'realize', 'this', 'is', 'probably', 'the', 'most', 'commonly', 'asked', 'question', 'around', 'here', ',', 'but', 'i', \"'\", 'm', 'having', 'my', 'bi', '-', 'weekly', 'ind', '##ec', '##isi', '##ven', '##ess', 'crisis', 'over', 'my', 'mental', 'health', ',', 'and', 'simply', 'must', 'ask', 'it', 'this', 'specific', 'way', '.', 'or', 'something', '.', ')', 'i', 'consider', 'my', 'ad', '##hd', '/', 'depression', '/', 'anxiety', 'to', 'be', 'one', 'big', ',', 'inter', '##connected', 'mass', '-', 'i', 'mean', ',', 'obviously', 'they', 'are', '.', 'but', 'sometimes', ',', 'i', 'think', 'i', 'use', 'one', 'as', 'an', 'excuse', 'to', 'avoid', 'confronting', 'the', 'other', '.', 'i', 'know', 'when', 'i', 'am', 'ad', '##hd', '-', 'l', '##y', 'pro', '##cr', '##ast', '##inating', ',', 'i', 'do', 'very', 'different', 'things', 'than', 'when', 'i', 'am', 'unable', 'to', 'get', 'started', 'on', 'a', 'task', 'because', 'i', \"'\", 'm', 'sad', '.', 'but', 'instead', 'of', 'confronting', 'my', 'depression', ',', 'i', \"'\", 'll', 'stick', 'myself', 'in', 'a', 'feedback', 'loop', 'of', 'self', '-', 'hate', 'for', 'not', 'working', 'hard', 'enough', 'on', 'teaching', 'myself', 'life', 'skills', 'to', 'counter', '##act', 'my', 'ad', '##hd', '.', 'so', 'i', \"'\", 'll', 'ob', '##ses', '##sive', '##ly', 'buy', 'self', '-', 'help', 'books', 'and', 'never', 'open', 'them', ',', 'or', 'just', 'stare', 'at', 'the', 'wall', 'and', 'cry', 'while', 'my', 'mind', 'races', '.', 'when', 'i', \"'\", 'm', 'avoiding', 'doing', 'something', 'because', 'i', 'can', \"'\", 't', 'wrap', 'my', 'mind', 'around', 'say', ',', 'the', 'organization', ',', 'i', \"'\", 'll', 'blame', 'it', 'on', 'my', 'anxiety', 'and', 'pop', 'some', 'k', '##lon', '##op', '##in', '&', \"'\", 'work', 'on', 'it', 'tomorrow', \"'\", 'instead', 'of', 'taking', 'it', 'step', 'by', 'step', 'and', 'using', 'it', 'as', 'an', 'ad', '##hd', 'learning', 'experience', '.', 'on', 'the', 'other', 'hand', ',', 'i', \"'\", 'm', 'horribly', 'anxious', 'about', 'starting', 'a', 'task', 'that', 'requires', 'organization', 'to', 'being', 'with', ',', 'much', 'less', 'if', 'it', \"'\", 's', 'one', 'i', 'don', \"'\", 't', 'instantly', 'understand', '.', 'my', 'lack', 'of', 'life', 'skills', 'to', 'deal', 'with', 'my', 'ad', '##hd', 'often', 'gets', 'me', 'in', 'situations', 'that', 'ex', '##ace', '##rba', '##te', 'my', 'depression', '.', 'i', 'don', \"'\", 't', 'have', 'a', 'therapist', 'and', 'my', 'psychiatrist', 'isn', \"'\", 't', '.', '.', '.', 'the', 'best', 'about', 'listening', '.', 'i', \"'\", 'm', 'moving', 'in', 'six', 'months', 'anyway', ';', 'therapist', '##s', 'i', \"'\", 've', 'had', 'in', 'the', 'past', 'have', 'either', '1', ')', 'not', 'recognized', 'my', 'ad', '##hd', '&', 'resulting', 'thought', 'patterns', ',', 'thus', 'been', 'un', '##hel', '##pf', '##ul', 'or', '2', ')', 'helpful', ',', 'then', 'emotionally', 'abusive', ',', 'then', 'forgetting', 'an', 'appointment', 'and', 'buying', 'me', 'a', 'movie', 'theatre', 'gift', 'card', '(', 'yeah', ',', 'i', 'don', \"'\", 't', 'know', ')', '.', 'the', 'point', 'there', 'was', 'that', 'i', 'don', \"'\", 't', 'feel', 'motivated', 'to', 'switch', '&', 'get', 'involved', 'with', 'new', 'doctors', 'until', 'i', 'move', '.', 'uh', ',', 'so', ',', 'anyway', ',', 'what', \"'\", 's', 'it', 'like', 'for', 'you', 'when', 'you', \"'\", 're', 'feeling', 'good', ',', 'and', 'also', 'when', 'you', \"'\", 're', 'feeling', 'bad', 'and', 'think', 'it', 'might', 'be', 'something', 'else', '?', 'yeah', ',', 'i', 'probably', 'shouldn', \"'\", 't', 'post', 'this', '.']\n",
      "INFO:__main__:Number of tokens: 518\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['what', 'does', 'it', 'feel', 'like', 'for', 'you', 'when', 'your', 'ad', '##hd', 'is', 'under', 'control', '?', 'when', 'do', 'you', 'know', 'an', 'issue', 'you', \"'\", 're', 'having', 'is', 'something', 'else', '?', 'be', 'it', 'from', 'medicine', ',', 'supplements', ',', 'wa', '##cky', 'moon', 'magic', ',', 'hard', 'work', ',', 'etc', '.', '(', 'i', 'realize', 'this', 'is', 'probably', 'the', 'most', 'commonly', 'asked', 'question', 'around', 'here', ',', 'but', 'i', \"'\", 'm', 'having', 'my', 'bi', '-', 'weekly', 'ind', '##ec', '##isi', '##ven', '##ess', 'crisis', 'over', 'my', 'mental', 'health', ',', 'and', 'simply', 'must', 'ask', 'it', 'this', 'specific', 'way', '.', 'or', 'something', '.', ')', 'i', 'consider', 'my', 'ad', '##hd', '/', 'depression', '/', 'anxiety', 'to', 'be', 'one', 'big', ',', 'inter', '##connected', 'mass', '-', 'i', 'mean', ',', 'obviously', 'they', 'are', '.', 'but', 'sometimes', ',', 'i', 'think', 'i', 'use', 'one', 'as', 'an', 'excuse', 'to', 'avoid', 'confronting', 'the', 'other', '.', 'i', 'know', 'when', 'i', 'am', 'ad', '##hd', '-', 'l', '##y', 'pro', '##cr', '##ast', '##inating', ',', 'i', 'do', 'very', 'different', 'things', 'than', 'when', 'i', 'am', 'unable', 'to', 'get', 'started', 'on', 'a', 'task', 'because', 'i', \"'\", 'm', 'sad', '.', 'but', 'instead', 'of', 'confronting', 'my', 'depression', ',', 'i', \"'\", 'll', 'stick', 'myself', 'in', 'a', 'feedback', 'loop', 'of', 'self', '-', 'hate', 'for', 'not', 'working', 'hard', 'enough', 'on', 'teaching', 'myself', 'life', 'skills', 'to', 'counter', '##act', 'my', 'ad', '##hd', '.', 'so', 'i', \"'\", 'll', 'ob', '##ses', '##sive', '##ly', 'buy', 'self', '-', 'help', 'books', 'and', 'never', 'open', 'them', ',', 'or', 'just', 'stare', 'at', 'the', 'wall', 'and', 'cry', 'while', 'my', 'mind', 'races', '.', 'when', 'i', \"'\", 'm', 'avoiding', 'doing', 'something', 'because', 'i', 'can', \"'\", 't', 'wrap', 'my', 'mind', 'around', 'say', ',', 'the', 'organization', ',', 'i', \"'\", 'll', 'blame', 'it', 'on', 'my', 'anxiety', 'and', 'pop', 'some', 'k', '##lon', '##op', '##in', '&', \"'\", 'work', 'on', 'it', 'tomorrow', \"'\", 'instead', 'of', 'taking', 'it', 'step', 'by', 'step', 'and', 'using', 'it', 'as', 'an', 'ad', '##hd', 'learning', 'experience', '.', 'on', 'the', 'other', 'hand', ',', 'i', \"'\", 'm', 'horribly', 'anxious', 'about', 'starting', 'a', 'task', 'that', 'requires', 'organization', 'to', 'being', 'with', ',', 'much', 'less', 'if', 'it', \"'\", 's', 'one', 'i', 'don', \"'\", 't', 'instantly', 'understand', '.', 'my', 'lack', 'of', 'life', 'skills', 'to', 'deal', 'with', 'my', 'ad', '##hd', 'often', 'gets', 'me', 'in', 'situations', 'that', 'ex', '##ace', '##rba', '##te', 'my', 'depression', '.', 'i', 'don', \"'\", 't', 'have', 'a', 'therapist', 'and', 'my', 'psychiatrist', 'isn', \"'\", 't', '.', '.', '.', 'the', 'best', 'about', 'listening', '.', 'i', \"'\", 'm', 'moving', 'in', 'six', 'months', 'anyway', ';', 'therapist', '##s', 'i', \"'\", 've', 'had', 'in', 'the', 'past', 'have', 'either', '1', ')', 'not', 'recognized', 'my', 'ad', '##hd', '&', 'resulting', 'thought', 'patterns', ',', 'thus', 'been', 'un', '##hel', '##pf', '##ul', 'or', '2', ')', 'helpful', ',', 'then', 'emotionally', 'abusive', ',', 'then', 'forgetting', 'an', 'appointment', 'and', 'buying', 'me', 'a', 'movie', 'theatre', 'gift', 'card', '(', 'yeah', ',', 'i', 'don', \"'\", 't', 'know', ')', '.', 'the', 'point', 'there', 'was', 'that', 'i', 'don', \"'\", 't', 'feel', 'motivated', 'to', 'switch', '&', 'get', 'involved', 'with', 'new', 'doctors', 'until', 'i', 'move', '.', 'uh', ',', 'so', ',', 'anyway', ',', 'what', \"'\", 's', 'it', 'like', 'for', 'you', 'when', 'you', \"'\", 're', 'feeling', 'good', ',', 'and', 'also', 'when', 'you', \"'\", 're', 'feeling', 'bad', 'and', 'think', 'it', 'might', 'be', 'something', 'else', '?', 'yeah', ',', 'i', 'probably'], ['shouldn', \"'\", 't', 'post', 'this', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['link', 'between', 'chronic', 'fatigue', 'and', 'ad', '##hd', '?', 'i', 'don', \"'\", 't', 'know', 'how', 'it', 'is', 'for', 'everyone', 'else', ',', 'but', 'for', 'a', 'large', 'number', 'of', 'people', 'i', \"'\", 've', 'met', 'who', 'have', 'add', '/', 'ad', '##hd', ',', 'we', 'all', 'seem', 'to', 'come', 'to', 'an', 'agreement', 'that', 'it', 'must', 'have', 'some', 'relation', 'to', 'energy', 'levels', 'overall', '.', 'is', 'it', 'possible', 'that', 'ad', '##hd', 'as', 'a', 'whole', 'can', 'be', 'either', 'caused', 'by', ',', 'or', 'directly', 'related', 'to', ',', 'the', 'inability', 'to', 'maintain', 'wake', '##fulness', ',', 'rather', 'than', 'simply', 'the', 'inability', 'to', 'control', 'ones', \"'\", 'impulse', '##s', '?', 'i', 'know', 'that', 'i', 'for', 'one', ',', 'have', 'a', 'very', 'very', 'hard', 'time', 'waking', 'up', 'in', 'the', 'morning', '.', 'it', 'doesn', \"'\", 't', 'matter', 'how', 'long', 'i', \"'\", 've', 'slept', ',', 'or', 'how', 'deeply', 'i', 'sleep', '.', 'every', 'single', 'day', ',', 'when', 'i', 'wake', 'up', ',', 'i', \"'\", 'm', 'on', 'another', 'planet', 'for', 'a', 'good', '3', 'hours', '.', 'i', 'can', 'also', 'fall', 'asleep', 'at', 'will', ',', 'pretty', 'much', 'at', 'any', 'time', '-', 'even', 'while', 'standing', '-', 'the', 'moment', 'i', 'get', 'bored', '.', 'i', 'don', \"'\", 't', 'think', 'this', 'is', 'a', 'coincidence', '.', 'also', ',', 'apparently', 'na', '##rco', '##le', '##psy', 'is', 'treated', 'with', 'the', 'same', 'st', '##im', '##ula', '##nt', 'medications', 'as', 'ad', '##hd', '.', 'for', 'those', 'who', 'don', \"'\", 't', 'know', ',', 'na', '##rco', '##le', '##psy', 'is', 'basically', 'the', 'brain', \"'\", 's', 'inability', 'to', 'manage', 'a', 'proper', 'sleep', 'cycle', '.', 'this', 'results', 'in', 'chronic', 'fatigue', ',', 'and', '(', 'sometimes', ')', 'also', 'results', 'in', 'the', 'suffer', '##er', 'randomly', 'falling', 'asleep', ',', 'even', 'while', 'completely', 'alert', 'and', 'active', ',', 'simply', 'because', 'the', 'brain', \"'\", 's', 'sleep', 'mach', '##ani', '##sm', '\"', 'mis', '##fires', '\"', '.', 'however', ',', 'not', 'everyone', 'loses', 'consciousness', 'at', 'random', '.', 'more', 'often', 'than', 'not', ',', 'suffer', '##ers', 'of', 'na', '##rco', '##le', '##psy', 'just', 'basically', 'feel', 'tired', 'all', 'the', 'time', ',', 'with', 'crashes', 'throughout', 'the', 'day', '.', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 314\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['link', 'between', 'chronic', 'fatigue', 'and', 'ad', '##hd', '?', 'i', 'don', \"'\", 't', 'know', 'how', 'it', 'is', 'for', 'everyone', 'else', ',', 'but', 'for', 'a', 'large', 'number', 'of', 'people', 'i', \"'\", 've', 'met', 'who', 'have', 'add', '/', 'ad', '##hd', ',', 'we', 'all', 'seem', 'to', 'come', 'to', 'an', 'agreement', 'that', 'it', 'must', 'have', 'some', 'relation', 'to', 'energy', 'levels', 'overall', '.', 'is', 'it', 'possible', 'that', 'ad', '##hd', 'as', 'a', 'whole', 'can', 'be', 'either', 'caused', 'by', ',', 'or', 'directly', 'related', 'to', ',', 'the', 'inability', 'to', 'maintain', 'wake', '##fulness', ',', 'rather', 'than', 'simply', 'the', 'inability', 'to', 'control', 'ones', \"'\", 'impulse', '##s', '?', 'i', 'know', 'that', 'i', 'for', 'one', ',', 'have', 'a', 'very', 'very', 'hard', 'time', 'waking', 'up', 'in', 'the', 'morning', '.', 'it', 'doesn', \"'\", 't', 'matter', 'how', 'long', 'i', \"'\", 've', 'slept', ',', 'or', 'how', 'deeply', 'i', 'sleep', '.', 'every', 'single', 'day', ',', 'when', 'i', 'wake', 'up', ',', 'i', \"'\", 'm', 'on', 'another', 'planet', 'for', 'a', 'good', '3', 'hours', '.', 'i', 'can', 'also', 'fall', 'asleep', 'at', 'will', ',', 'pretty', 'much', 'at', 'any', 'time', '-', 'even', 'while', 'standing', '-', 'the', 'moment', 'i', 'get', 'bored', '.', 'i', 'don', \"'\", 't', 'think', 'this', 'is', 'a', 'coincidence', '.', 'also', ',', 'apparently', 'na', '##rco', '##le', '##psy', 'is', 'treated', 'with', 'the', 'same', 'st', '##im', '##ula', '##nt', 'medications', 'as', 'ad', '##hd', '.', 'for', 'those', 'who', 'don', \"'\", 't', 'know', ',', 'na', '##rco', '##le', '##psy', 'is', 'basically', 'the', 'brain', \"'\", 's', 'inability', 'to', 'manage', 'a', 'proper', 'sleep', 'cycle', '.', 'this', 'results', 'in', 'chronic', 'fatigue', ',', 'and', '(', 'sometimes', ')', 'also', 'results', 'in', 'the', 'suffer', '##er', 'randomly', 'falling', 'asleep', ',', 'even', 'while', 'completely', 'alert', 'and', 'active', ',', 'simply', 'because', 'the', 'brain', \"'\", 's', 'sleep', 'mach', '##ani', '##sm', '\"', 'mis', '##fires', '\"', '.', 'however', ',', 'not', 'everyone', 'loses', 'consciousness', 'at', 'random', '.', 'more', 'often', 'than', 'not', ',', 'suffer', '##ers', 'of', 'na', '##rco', '##le', '##psy', 'just', 'basically', 'feel', 'tired', 'all', 'the', 'time', ',', 'with', 'crashes', 'throughout', 'the', 'day', '.', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tell', 'me', 'about', 'your', 'ad', '##hd', 'journey']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tell', 'me', 'about', 'your', 'ad', '##hd', 'journey']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['another', 'first', 'time', 'adult', 'ad', '##hd', 'here', 'hey', ',', 'i', 'believe', 'i', \"'\", 'm', 'in', 'a', 'very', 'similar', 'situation', 'to', 'another', 'gentleman', 'who', 'just', 'made', 'the', 'topic', '\"', 'new', 'adult', 'ad', '##hd', 'here', '\"', '.', 'i', 'was', 'prescribed', '10', '##mg', 'add', '##eral', '##l', '(', '15', '##mg', 'is', 'on', 'back', '##ord', '##er', 'i', 'guess', ')', 'just', 'today', 'after', 'seeing', 'my', 'pc', '##p', 'who', 'was', 'pretty', 'confident', 'that', 'this', 'was', 'what', \"'\", 's', 'up', '.', 'i', \"'\", 'm', 'a', 'university', 'student', 'who', \"'\", 's', 'been', 'noticing', 'an', 'increasing', 'number', 'of', 'ad', '##hd', '*', '*', '(', 'not', 'hyper', '##active', ')', '*', '*', 'symptoms', 'over', 'the', 'last', '.', '.', '.', '.', '6', 'years', '?', 'i', 'was', 'one', 'of', 'those', 'people', 'who', 'didn', \"'\", 't', 'think', 'ad', '##hd', 'was', 'a', '\"', 'real', '\"', 'disorder', ',', 'so', 'i', 'tried', 'to', 'force', 'myself', 'to', 'concentrate', ',', 'all', 'the', 'stuff', 'i', \"'\", 'm', 'sure', 'everyone', 'here', 'has', 'tried', '.', 'but', 'a', 'friend', 'who', \"'\", 's', 'brother', 'has', 'ad', '##hd', 'told', 'me', 'that', 'i', 'should', 'seriously', 'be', 'considering', 'this', 'as', 'a', 'possibility', '.', 'since', 'then', 'it', \"'\", 's', 'been', 'pretty', 'apparent', '.', '10', '##mg', 'is', 'a', 'pretty', 'low', 'dose', ',', 'right', '?', 'i', \"'\", 'm', 'a', '5', \"'\", '7', '\"', ',', '130', '##lb', ',', '19', 'year', 'old', 'male', '.', 'is', 'there', 'anything', 'i', 'should', 'be', 'looking', 'for', 'specifically', '?', 'i', \"'\", 'm', 'a', 'little', 'worried', ',', 'though', 'i', \"'\", 'm', 'not', 'sure', 'about', 'what', '.', 'it', \"'\", 's', 'a', 'bit', 'of', 'a', 'relief', '(', 'probably', 'the', 'psychological', 'conditioning', 'bestowed', 'upon', 'me', 'by', 'living', 'in', 'the', '21st', 'century', ')', 'to', 'have', 'a', 'pill', 'bottle', 'in', 'my', 'hand', ',', 'but', 'i', \"'\", 'm', 'afraid', 'that', 'this', 'won', \"'\", 't', 'help', '.', 'it', \"'\", 's', 'been', 'a', 'couple', 'hours', 'since', 'i', 'took', 'my', 'first', 'dose', 'and', 'i', 'don', \"'\", 't', 'feel', 'a', 'lot', 'different', '.', 'just', '.', '.', '.', 'calm', '##er', '?', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'doing', 'the', 'wrong', 'thing', 'by', 'taking', 'med', '##s', '.', 'not', 'sure', 'what', 'i', \"'\", 'm', 'looking', 'for', 'out', 'of', 'this', 'topic', '.', 'it', 'may', 'or', 'may', 'not', 'be', 'apparent', 'through', 'my', 'writing', 'here', 'that', 'i', \"'\", 'm', 'still', 'feeling', 'pretty', 'scattered', '(', 'this', 'has', 'been', 'getting', 'worse', 'over', 'the', 'last', 'year', 'or', 'so', ')', '.', 'i', 'guess', 'any', 'advice', 'or', 'caution', '##s', 'would', 'help', '.', '*', '*', 't', '##l', ';', 'dr', ':', '19', 'year', 'old', 'on', '10', '##mg', 'of', 'add', '##eral', '##l', 'for', 'the', 'first', 'time', '.', 'a', 'little', 'worried', '.', 'advice', '?', '*', '*']\n",
      "INFO:__main__:Number of tokens: 412\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['another', 'first', 'time', 'adult', 'ad', '##hd', 'here', 'hey', ',', 'i', 'believe', 'i', \"'\", 'm', 'in', 'a', 'very', 'similar', 'situation', 'to', 'another', 'gentleman', 'who', 'just', 'made', 'the', 'topic', '\"', 'new', 'adult', 'ad', '##hd', 'here', '\"', '.', 'i', 'was', 'prescribed', '10', '##mg', 'add', '##eral', '##l', '(', '15', '##mg', 'is', 'on', 'back', '##ord', '##er', 'i', 'guess', ')', 'just', 'today', 'after', 'seeing', 'my', 'pc', '##p', 'who', 'was', 'pretty', 'confident', 'that', 'this', 'was', 'what', \"'\", 's', 'up', '.', 'i', \"'\", 'm', 'a', 'university', 'student', 'who', \"'\", 's', 'been', 'noticing', 'an', 'increasing', 'number', 'of', 'ad', '##hd', '*', '*', '(', 'not', 'hyper', '##active', ')', '*', '*', 'symptoms', 'over', 'the', 'last', '.', '.', '.', '.', '6', 'years', '?', 'i', 'was', 'one', 'of', 'those', 'people', 'who', 'didn', \"'\", 't', 'think', 'ad', '##hd', 'was', 'a', '\"', 'real', '\"', 'disorder', ',', 'so', 'i', 'tried', 'to', 'force', 'myself', 'to', 'concentrate', ',', 'all', 'the', 'stuff', 'i', \"'\", 'm', 'sure', 'everyone', 'here', 'has', 'tried', '.', 'but', 'a', 'friend', 'who', \"'\", 's', 'brother', 'has', 'ad', '##hd', 'told', 'me', 'that', 'i', 'should', 'seriously', 'be', 'considering', 'this', 'as', 'a', 'possibility', '.', 'since', 'then', 'it', \"'\", 's', 'been', 'pretty', 'apparent', '.', '10', '##mg', 'is', 'a', 'pretty', 'low', 'dose', ',', 'right', '?', 'i', \"'\", 'm', 'a', '5', \"'\", '7', '\"', ',', '130', '##lb', ',', '19', 'year', 'old', 'male', '.', 'is', 'there', 'anything', 'i', 'should', 'be', 'looking', 'for', 'specifically', '?', 'i', \"'\", 'm', 'a', 'little', 'worried', ',', 'though', 'i', \"'\", 'm', 'not', 'sure', 'about', 'what', '.', 'it', \"'\", 's', 'a', 'bit', 'of', 'a', 'relief', '(', 'probably', 'the', 'psychological', 'conditioning', 'bestowed', 'upon', 'me', 'by', 'living', 'in', 'the', '21st', 'century', ')', 'to', 'have', 'a', 'pill', 'bottle', 'in', 'my', 'hand', ',', 'but', 'i', \"'\", 'm', 'afraid', 'that', 'this', 'won', \"'\", 't', 'help', '.', 'it', \"'\", 's', 'been', 'a', 'couple', 'hours', 'since', 'i', 'took', 'my', 'first', 'dose', 'and', 'i', 'don', \"'\", 't', 'feel', 'a', 'lot', 'different', '.', 'just', '.', '.', '.', 'calm', '##er', '?', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'doing', 'the', 'wrong', 'thing', 'by', 'taking', 'med', '##s', '.', 'not', 'sure', 'what', 'i', \"'\", 'm', 'looking', 'for', 'out', 'of', 'this', 'topic', '.', 'it', 'may', 'or', 'may', 'not', 'be', 'apparent', 'through', 'my', 'writing', 'here', 'that', 'i', \"'\", 'm', 'still', 'feeling', 'pretty', 'scattered', '(', 'this', 'has', 'been', 'getting', 'worse', 'over', 'the', 'last', 'year', 'or', 'so', ')', '.', 'i', 'guess', 'any', 'advice', 'or', 'caution', '##s', 'would', 'help', '.', '*', '*', 't', '##l', ';', 'dr', ':', '19', 'year', 'old', 'on', '10', '##mg', 'of', 'add', '##eral', '##l', 'for', 'the', 'first', 'time', '.', 'a', 'little', 'worried', '.', 'advice', '?', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['v', '##y', '##van', '##se', 'vs', '.', 'concert', '##a', '?', 'my', 'daughter', 'has', 'been', 'on', 'v', '##y', '##van', '##se', 'for', 'a', 'while', ',', 'and', 'her', 'psychiatrist', 'thinks', '(', 'and', 'we', 'agree', ')', 'that', 'it', 'hasn', \"'\", 't', 'been', 'beneficial', 'enough', 'for', 'her', '.', 'there', 'has', 'been', 'some', 'improvement', ',', 'but', 'not', 'enough', 'to', 'get', 'her', 'through', 'her', 'school', 'day', '(', 'she', 'primarily', 'has', 'issues', 'with', 'impulse', 'control', ')', '.', 'we', \"'\", 're', 'going', 'to', 'switch', 'her', 'to', 'concert', '##a', 'on', 'his', 'recommendation', '.', 'can', 'someone', 'who', \"'\", 's', 'taken', 'both', 'describe', 'the', 'effects', 'for', 'one', 'versus', 'the', 'other', '?', 'i', 'know', 'everyone', \"'\", 's', 'different', 'but', 'i', \"'\", 'd', 'like', 'to', 'get', 'some', 'perspective', 'on', 'how', 'this', 'change', 'might', 'feel', 'to', 'her', ',', 'and', 'how', 'well', 'it', \"'\", 's', 'worked', 'for', 'others', '.', 'thanks', '!', 'edit', ':', 'thanks', 'everyone', 'for', 'your', 'input', '.', 'this', 'sub', '##red', '##dit', 'is', 'extremely', 'helpful', 'for', 'me', '.']\n",
      "INFO:__main__:Number of tokens: 152\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['v', '##y', '##van', '##se', 'vs', '.', 'concert', '##a', '?', 'my', 'daughter', 'has', 'been', 'on', 'v', '##y', '##van', '##se', 'for', 'a', 'while', ',', 'and', 'her', 'psychiatrist', 'thinks', '(', 'and', 'we', 'agree', ')', 'that', 'it', 'hasn', \"'\", 't', 'been', 'beneficial', 'enough', 'for', 'her', '.', 'there', 'has', 'been', 'some', 'improvement', ',', 'but', 'not', 'enough', 'to', 'get', 'her', 'through', 'her', 'school', 'day', '(', 'she', 'primarily', 'has', 'issues', 'with', 'impulse', 'control', ')', '.', 'we', \"'\", 're', 'going', 'to', 'switch', 'her', 'to', 'concert', '##a', 'on', 'his', 'recommendation', '.', 'can', 'someone', 'who', \"'\", 's', 'taken', 'both', 'describe', 'the', 'effects', 'for', 'one', 'versus', 'the', 'other', '?', 'i', 'know', 'everyone', \"'\", 's', 'different', 'but', 'i', \"'\", 'd', 'like', 'to', 'get', 'some', 'perspective', 'on', 'how', 'this', 'change', 'might', 'feel', 'to', 'her', ',', 'and', 'how', 'well', 'it', \"'\", 's', 'worked', 'for', 'others', '.', 'thanks', '!', 'edit', ':', 'thanks', 'everyone', 'for', 'your', 'input', '.', 'this', 'sub', '##red', '##dit', 'is', 'extremely', 'helpful', 'for', 'me', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'medication', 'it', 'not', 'a', 'cr', '##ut', '##ch', ':', 'it', 'is', 'a', 'whole', 'new', 'leg', '.', 'i', 'came', 'to', 'this', 'realization', 'today', 'in', 'therapy', ',', 'i', 'wanted', 'to', 'share', '.']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'medication', 'it', 'not', 'a', 'cr', '##ut', '##ch', ':', 'it', 'is', 'a', 'whole', 'new', 'leg', '.', 'i', 'came', 'to', 'this', 'realization', 'today', 'in', 'therapy', ',', 'i', 'wanted', 'to', 'share', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'all', 'ting', '##ly', '!', 'i', 'started', 'st', '##rat', '##tera', 'this', 'morning', '.', 'maybe', '2', 'hours', 'ago', 'i', 'took', 'my', 'first', 'one', '.', '.', '.', 'and', 'holy', 'crap', 'i', \"'\", 'm', 'ting', '##ly', '.', '.', 'it', \"'\", 's', 'super', 'weird', '.', '.', '.', 'mostly', 'on', 'just', 'my', 'left', 'side', 'also', '.', '.', '.', '.', 'i', \"'\", 've', 'been', 'really', 'nervous', 'about', 'restart', '##ing', 'med', '##s', 'after', 'a', 'bad', 'experience', 'on', 'dex', '##ed', '##rine', '.', '.', '.', '.', 'did', 'anyone', 'else', 'have', 'tingle', '##s', '?', 'did', 'you', 'power', 'through', 'it', 'and', 'they', 'went', 'away', '?', 'or', 'am', 'i', 'in', 'for', 'more', 'weird', '##ness', '.', '.', '.', '.', '.', '.', 'edit', ':', 'four', 'hours', 'later', 'the', 'ting', '##lyn', '##ess', 'seems', 'to', 'have', 'subsided', '.', '.', '.', 'every', 'now', 'and', 'then', 'i', 'get', 'a', 'little', 'bit', '.', '.', '.', 'but', 'not', 'like', 'it', 'was', 'first', 'thing', '.', '.', '.', 'edit', '2', ':', 'four', 'days', 'later', 'and', 'i', 'have', 'no', 'side', 'effects', '.', 'the', 'ting', '##lyn', '##ess', 'was', 'only', 'on', 'day', '1', '.', 'otherwise', 'so', 'far', 'so', 'good', '!']\n",
      "INFO:__main__:Number of tokens: 177\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'all', 'ting', '##ly', '!', 'i', 'started', 'st', '##rat', '##tera', 'this', 'morning', '.', 'maybe', '2', 'hours', 'ago', 'i', 'took', 'my', 'first', 'one', '.', '.', '.', 'and', 'holy', 'crap', 'i', \"'\", 'm', 'ting', '##ly', '.', '.', 'it', \"'\", 's', 'super', 'weird', '.', '.', '.', 'mostly', 'on', 'just', 'my', 'left', 'side', 'also', '.', '.', '.', '.', 'i', \"'\", 've', 'been', 'really', 'nervous', 'about', 'restart', '##ing', 'med', '##s', 'after', 'a', 'bad', 'experience', 'on', 'dex', '##ed', '##rine', '.', '.', '.', '.', 'did', 'anyone', 'else', 'have', 'tingle', '##s', '?', 'did', 'you', 'power', 'through', 'it', 'and', 'they', 'went', 'away', '?', 'or', 'am', 'i', 'in', 'for', 'more', 'weird', '##ness', '.', '.', '.', '.', '.', '.', 'edit', ':', 'four', 'hours', 'later', 'the', 'ting', '##lyn', '##ess', 'seems', 'to', 'have', 'subsided', '.', '.', '.', 'every', 'now', 'and', 'then', 'i', 'get', 'a', 'little', 'bit', '.', '.', '.', 'but', 'not', 'like', 'it', 'was', 'first', 'thing', '.', '.', '.', 'edit', '2', ':', 'four', 'days', 'later', 'and', 'i', 'have', 'no', 'side', 'effects', '.', 'the', 'ting', '##lyn', '##ess', 'was', 'only', 'on', 'day', '1', '.', 'otherwise', 'so', 'far', 'so', 'good', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['5', '-', 'h', '##tp', 'and', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['5', '-', 'h', '##tp', 'and', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['til', 'we', \"'\", 're', 'all', 'different', '-', '-', '-', '-', 'add', 'tip', 'of', 'the', 'day', '218']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['til', 'we', \"'\", 're', 'all', 'different', '-', '-', '-', '-', 'add', 'tip', 'of', 'the', 'day', '218']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['if', 'you', \"'\", 're', 'newly', 'diagnosed', ',', 'you', 'should', 'really', 'check', 'out', 'our', 'new', 'chat', ',', 'as', 'mentioned', 'in', 'the', 'side', '##bar', '.', 'the', 'chat', 'is', 'pretty', 'new', 'and', 'not', 'very', 'crowded', 'yet', ',', 'and', 'if', 'you', 'don', \"'\", 't', 'get', 'a', 'reply', 'within', 'minutes', ':', 'don', \"'\", 't', 'give', 'up', '!', 'we', 'do', 'see', 'a', 'lot', 'of', 'conversations', 'there', ',', 'but', 'not', 'everyone', 'is', 'behind', 'their', 'keyboards', 'every', 'hour', 'of', 'the', 'day', '.', 'personally', ',', 'i', \"'\", 've', 'found', 'it', 'to', 'be', 'a', 'great', 'place', 'to', 'talk', 'with', 'people', 'during', 'my', 'first', 'days', 'on', 'medication', ',', 'still', 'getting', 'a', 'grip', 'on', 'my', 'diagnosis', '.', 'the', 'sub', '##red', '##dit', 'is', 'great', 'for', 'discussion', 'and', 'sharing', 'of', 'experience', ',', 'but', 'check', 'out', 'the', 'chat', 'to', 'for', 'a', 'little', 'real', 'time', 'conversation', '.', 'an', 'active', 'sub', '##red', '##dit', 'means', 'an', 'active', 'chat', ',', 'and', 'vice', 'versa', '.']\n",
      "INFO:__main__:Number of tokens: 145\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['if', 'you', \"'\", 're', 'newly', 'diagnosed', ',', 'you', 'should', 'really', 'check', 'out', 'our', 'new', 'chat', ',', 'as', 'mentioned', 'in', 'the', 'side', '##bar', '.', 'the', 'chat', 'is', 'pretty', 'new', 'and', 'not', 'very', 'crowded', 'yet', ',', 'and', 'if', 'you', 'don', \"'\", 't', 'get', 'a', 'reply', 'within', 'minutes', ':', 'don', \"'\", 't', 'give', 'up', '!', 'we', 'do', 'see', 'a', 'lot', 'of', 'conversations', 'there', ',', 'but', 'not', 'everyone', 'is', 'behind', 'their', 'keyboards', 'every', 'hour', 'of', 'the', 'day', '.', 'personally', ',', 'i', \"'\", 've', 'found', 'it', 'to', 'be', 'a', 'great', 'place', 'to', 'talk', 'with', 'people', 'during', 'my', 'first', 'days', 'on', 'medication', ',', 'still', 'getting', 'a', 'grip', 'on', 'my', 'diagnosis', '.', 'the', 'sub', '##red', '##dit', 'is', 'great', 'for', 'discussion', 'and', 'sharing', 'of', 'experience', ',', 'but', 'check', 'out', 'the', 'chat', 'to', 'for', 'a', 'little', 'real', 'time', 'conversation', '.', 'an', 'active', 'sub', '##red', '##dit', 'means', 'an', 'active', 'chat', ',', 'and', 'vice', 'versa', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['yes', 'i', '’', 'm', 'a', 'yes', 'man']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['yes', 'i', '’', 'm', 'a', 'yes', 'man']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mb', '##ti', 'test', 'in', 'ad', '##hd', 'individuals', ',', 'lets', 'draw', 'some', 'correlation', 'perhaps', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mb', '##ti', 'test', 'in', 'ad', '##hd', 'individuals', ',', 'lets', 'draw', 'some', 'correlation', 'perhaps', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mb', '##ti', 'test', ';', 'draw', 'some', 'personality', 'correlation', 'between', 'add', '##ers', '?']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mb', '##ti', 'test', ';', 'draw', 'some', 'personality', 'correlation', 'between', 'add', '##ers', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'unable', 'to', 'get', 'their', 'script', 'for', 'add', '##eral', '##l', 'x', '##r', 'filled', '?', 'can', \"'\", 't', 'find', 'it', 'anywhere', '.', '.', '.', 'yet', 'dea', 'says', 'there', \"'\", 's', 'no', 'shortage', '.', 'fuck', 'them', '.', 'i', 'can', \"'\", 't', 'get', 'my', 'fucking', 'medication', '.']\n",
      "INFO:__main__:Number of tokens: 45\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'unable', 'to', 'get', 'their', 'script', 'for', 'add', '##eral', '##l', 'x', '##r', 'filled', '?', 'can', \"'\", 't', 'find', 'it', 'anywhere', '.', '.', '.', 'yet', 'dea', 'says', 'there', \"'\", 's', 'no', 'shortage', '.', 'fuck', 'them', '.', 'i', 'can', \"'\", 't', 'get', 'my', 'fucking', 'medication', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'have', 'experience', 'with', 'aa', 'meetings', 'as', 'ad', '##hd', 'group', 'therapy', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'have', 'experience', 'with', 'aa', 'meetings', 'as', 'ad', '##hd', 'group', 'therapy', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tips', 'on', 'taking', 'add', '##eral', '##l', '?', 'i', 'work', 'an', '11', 'hour', 'shift', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tips', 'on', 'taking', 'add', '##eral', '##l', '?', 'i', 'work', 'an', '11', 'hour', 'shift', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['late', 'work', 'back', '##log', 'due', 'monday', ':', 'tips', '?', 'so', 'i', 'probably', 'have', 'ad', '##hd', '.', 'my', 'psychiatrist', 'prescribed', 'me', 'a', 'dose', 'of', 'add', '##eral', '##l', ',', 'but', 'he', 'thinks', 'it', 'could', 'still', 'be', 'just', 'anxiety', 'or', 'ad', '##hd', '+', 'anxiety', '.', 'but', 'the', 'only', 'thing', 'it', 'does', 'is', 'make', 'it', '*', 'slightly', '*', 'easier', 'to', 'get', 'up', 'in', 'the', 'morning', '.', 'i', 'don', \"'\", 't', 'feel', 'better', '.', 'on', 'monday', ',', 'i', 'have', 'a', 'huge', 'back', '##log', 'of', 'late', 'work', '(', '~', '500', '-', '800', 'vocabulary', 'note', '##cards', 'and', '~', '5', '-', '6', 'lab', 'reports', ')', 'due', 'from', 'last', 'year', 'that', 'i', \"'\", 've', 'been', 'putting', 'off', 'this', 'year', '.', 'it', \"'\", 's', 'come', 'to', 'the', 'point', 'that', 'delaying', 'it', 'further', 'would', 'be', 'significantly', 'complicated', 'because', 'i', 'need', 'to', 'send', 'out', 'my', 'high', 'school', 'transcript', 'to', 'colleges', 'on', 'the', '15th', '.', 'i', 'planned', 'to', 'work', 'today', 'on', 'note', '##cards', 'and', 'do', 'all', 'the', 'lab', 'reports', 'tomorrow', ',', 'but', 'pro', '##cr', '##ast', '##inating', 'and', 'arguing', 'got', 'in', 'the', 'way', 'of', 'that', '.', 'any', 'tips', 'on', 'accomplish', '##ing', 'all', 'this', 'by', 'monday', '?', 'i', \"'\", 'm', 'kind', 'of', 'scared', 'because', 'it', 'might', 'partially', 'decide', 'whether', 'i', 'get', 'into', 'some', 'colleges', ',', 'and', 'it', 'seems', 'like', 'a', 'really', 'big', 'amount', 'of', 'work', '.']\n",
      "INFO:__main__:Number of tokens: 211\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['late', 'work', 'back', '##log', 'due', 'monday', ':', 'tips', '?', 'so', 'i', 'probably', 'have', 'ad', '##hd', '.', 'my', 'psychiatrist', 'prescribed', 'me', 'a', 'dose', 'of', 'add', '##eral', '##l', ',', 'but', 'he', 'thinks', 'it', 'could', 'still', 'be', 'just', 'anxiety', 'or', 'ad', '##hd', '+', 'anxiety', '.', 'but', 'the', 'only', 'thing', 'it', 'does', 'is', 'make', 'it', '*', 'slightly', '*', 'easier', 'to', 'get', 'up', 'in', 'the', 'morning', '.', 'i', 'don', \"'\", 't', 'feel', 'better', '.', 'on', 'monday', ',', 'i', 'have', 'a', 'huge', 'back', '##log', 'of', 'late', 'work', '(', '~', '500', '-', '800', 'vocabulary', 'note', '##cards', 'and', '~', '5', '-', '6', 'lab', 'reports', ')', 'due', 'from', 'last', 'year', 'that', 'i', \"'\", 've', 'been', 'putting', 'off', 'this', 'year', '.', 'it', \"'\", 's', 'come', 'to', 'the', 'point', 'that', 'delaying', 'it', 'further', 'would', 'be', 'significantly', 'complicated', 'because', 'i', 'need', 'to', 'send', 'out', 'my', 'high', 'school', 'transcript', 'to', 'colleges', 'on', 'the', '15th', '.', 'i', 'planned', 'to', 'work', 'today', 'on', 'note', '##cards', 'and', 'do', 'all', 'the', 'lab', 'reports', 'tomorrow', ',', 'but', 'pro', '##cr', '##ast', '##inating', 'and', 'arguing', 'got', 'in', 'the', 'way', 'of', 'that', '.', 'any', 'tips', 'on', 'accomplish', '##ing', 'all', 'this', 'by', 'monday', '?', 'i', \"'\", 'm', 'kind', 'of', 'scared', 'because', 'it', 'might', 'partially', 'decide', 'whether', 'i', 'get', 'into', 'some', 'colleges', ',', 'and', 'it', 'seems', 'like', 'a', 'really', 'big', 'amount', 'of', 'work', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'and', 'sleep', 'issues']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'and', 'sleep', 'issues']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'pretty', 'sure', 'i', \"'\", 've', 'been', 'suffering', 'from', 'ad', '##hd', 'all', 'my', 'life', ',', 'went', 'to', 'a', 'psychiatrist', 'a', 'few', 'weeks', 'ago', 'and', 'got', 'diagnosed', ',', 'got', 'med', '##s', '.', '.', '.', 'advice', 'on', 'how', 'to', 'take', 'them', 'effectively', '?', 'hi', 'everybody', ',', 'i', \"'\", 've', 'been', 'lurking', 'this', 'sub', '##red', '##dit', 'casually', 'ever', 'since', 'i', 'got', 'diagnosed', 'and', 'prescribed', 'med', '##s', '.', 'my', 'psychiatrist', 'wanted', 'me', 'to', 'try', 'out', '5', 'mg', 'rita', '##lin', 'to', 'start', 'off', ',', 'to', 'see', 'how', 'i', 'felt', 'and', 'how', 'it', 'worked', ',', 'and', 'if', 'i', 'didn', \"'\", 't', 'like', 'it', 'maybe', 'we', 'could', 'try', 'something', 'else', '.', 'for', 'reference', ',', 'i', \"'\", 'm', 'a', '25', 'y', '.', 'o', '.', 'male', ',', 'college', 'gr', '##ad', ',', 'major', '##ed', 'in', 'chemical', 'engineering', 'at', 'a', 'top', '-', '100', 'school', '(', 'though', 'didn', \"'\", 't', 'get', 'very', 'good', 'grades', 'while', 'there', ')', ',', 'and', 'currently', 'working', 'a', 'high', '-', 'paying', ',', 'high', '-', 'demand', 'job', 'in', 'bio', '##tech', '.', 'anyway', ',', 'i', \"'\", 've', 'been', 'experimenting', 'with', 'different', 'dos', '##ages', 'of', 'the', '5', 'mg', 'pills', 'to', 'see', 'when', 'i', 'feel', 'it', 'and', 'when', 'i', 'don', \"'\", 't', ',', 'and', 'the', 'mental', 'effects', 'there', '##in', '.', 'i', 'started', 'out', 'with', 'taking', '10', 'mg', 'in', 'the', 'morning', ',', 'didn', \"'\", 't', 'really', 'feel', 'much', 'of', 'a', 'difference', '.', 'then', 'i', 'tried', '15', 'and', 'i', 'could', 'definitely', 'concentrate', 'much', 'better', '.', 'when', 'i', 'took', '20', ',', 'i', 'felt', 'really', '\"', 'drugged', 'up', '\"', 'for', 'lack', 'of', 'a', 'better', 'term', ',', 'so', 'i', \"'\", 've', 'settled', 'on', 'taking', '15', 'for', 'a', 'first', 'dose', '.', 'i', \"'\", 've', 'been', 'trying', 'to', 'see', 'what', 'a', 'good', 're', '-', 'up', 'schedule', 'is', 'throughout', 'the', 'day', 'and', 'so', 'far', 'i', 'think', 'what', 'works', 'best', 'is', '10', 'mg', 'every', '3', 'hours', 'or', 'so', '.', 'the', 'problem', 'i', \"'\", 'm', 'having', 'now', ',', 'however', ',', 'is', 'that', ',', 'while', 'i', \"'\", 'm', 'on', 'the', 'med', '##s', ',', 'and', 'i', 'know', 'i', \"'\", 'm', 'on', 'the', 'med', '##s', ',', 'everything', 'is', 'great', '.', 'i', 'get', 'work', 'done', ',', 'i', 'stay', 'on', 'task', ',', 'don', \"'\", 't', 'get', 'too', 'distracted', 'with', 'things', 'like', 'red', '##dit', 'during', 'work', 'or', 'jumping', 'around', 'from', 'project', 'to', 'project', '.', 'but', ',', 'when', 'i', \"'\", 'm', 'off', 'of', 'med', '##s', ',', 'my', 'imp', '##uls', '##ivity', 'goes', 'through', 'the', 'roof', '.', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'stay', 'on', 'my', 'diet', 'at', 'all', 'these', 'past', 'couple', 'of', 'weeks', 'even', 'though', 'i', 'didn', \"'\", 't', 'really', 'have', 'too', 'much', 'trouble', 'doing', 'so', 'prior', 'to', 'that', '(', 'i', 'lost', 'about', '30', 'pounds', 'over', 'last', 'summer', ')', '.', 'same', 'deal', 'with', 'going', 'to', 'the', 'gym', 'after', 'work', ',', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'keep', 'a', 'consistent', 'schedule', '.', 'this', 'among', 'other', 'things', 'i', \"'\", 've', 'been', 'trying', 'to', 'control', ',', 'it', 'just', 'feels', 'like', 'i', 'have', 'no', 'control', '.', 'my', 'last', 'dose', 'is', 'usually', 'around', '3', '-', '4', 'pm', 'so', 'i', 'can', 'finish', 'out', 'the', 'work', 'day', 'and', 'fall', 'asleep', 'at', 'a', 'decent', 'time', ',', 'but', 'when', 'the', 'med', '##s', 'wear', 'off', ',', 'it', 'seems', 'as', 'if', 'i', 'am', 'useless', 'and', 'even', 'worse', 'with', 'imp', '##uls', '##ivity', 'than', 'i', 'was', 'before', 'i', 'started', 'taking', 'the', 'rita', '##lin', '.', 'help', '?', 'advice', '?', 'thanks', 'in', 'advance', '.']\n",
      "INFO:__main__:Number of tokens: 543\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'pretty', 'sure', 'i', \"'\", 've', 'been', 'suffering', 'from', 'ad', '##hd', 'all', 'my', 'life', ',', 'went', 'to', 'a', 'psychiatrist', 'a', 'few', 'weeks', 'ago', 'and', 'got', 'diagnosed', ',', 'got', 'med', '##s', '.', '.', '.', 'advice', 'on', 'how', 'to', 'take', 'them', 'effectively', '?', 'hi', 'everybody', ',', 'i', \"'\", 've', 'been', 'lurking', 'this', 'sub', '##red', '##dit', 'casually', 'ever', 'since', 'i', 'got', 'diagnosed', 'and', 'prescribed', 'med', '##s', '.', 'my', 'psychiatrist', 'wanted', 'me', 'to', 'try', 'out', '5', 'mg', 'rita', '##lin', 'to', 'start', 'off', ',', 'to', 'see', 'how', 'i', 'felt', 'and', 'how', 'it', 'worked', ',', 'and', 'if', 'i', 'didn', \"'\", 't', 'like', 'it', 'maybe', 'we', 'could', 'try', 'something', 'else', '.', 'for', 'reference', ',', 'i', \"'\", 'm', 'a', '25', 'y', '.', 'o', '.', 'male', ',', 'college', 'gr', '##ad', ',', 'major', '##ed', 'in', 'chemical', 'engineering', 'at', 'a', 'top', '-', '100', 'school', '(', 'though', 'didn', \"'\", 't', 'get', 'very', 'good', 'grades', 'while', 'there', ')', ',', 'and', 'currently', 'working', 'a', 'high', '-', 'paying', ',', 'high', '-', 'demand', 'job', 'in', 'bio', '##tech', '.', 'anyway', ',', 'i', \"'\", 've', 'been', 'experimenting', 'with', 'different', 'dos', '##ages', 'of', 'the', '5', 'mg', 'pills', 'to', 'see', 'when', 'i', 'feel', 'it', 'and', 'when', 'i', 'don', \"'\", 't', ',', 'and', 'the', 'mental', 'effects', 'there', '##in', '.', 'i', 'started', 'out', 'with', 'taking', '10', 'mg', 'in', 'the', 'morning', ',', 'didn', \"'\", 't', 'really', 'feel', 'much', 'of', 'a', 'difference', '.', 'then', 'i', 'tried', '15', 'and', 'i', 'could', 'definitely', 'concentrate', 'much', 'better', '.', 'when', 'i', 'took', '20', ',', 'i', 'felt', 'really', '\"', 'drugged', 'up', '\"', 'for', 'lack', 'of', 'a', 'better', 'term', ',', 'so', 'i', \"'\", 've', 'settled', 'on', 'taking', '15', 'for', 'a', 'first', 'dose', '.', 'i', \"'\", 've', 'been', 'trying', 'to', 'see', 'what', 'a', 'good', 're', '-', 'up', 'schedule', 'is', 'throughout', 'the', 'day', 'and', 'so', 'far', 'i', 'think', 'what', 'works', 'best', 'is', '10', 'mg', 'every', '3', 'hours', 'or', 'so', '.', 'the', 'problem', 'i', \"'\", 'm', 'having', 'now', ',', 'however', ',', 'is', 'that', ',', 'while', 'i', \"'\", 'm', 'on', 'the', 'med', '##s', ',', 'and', 'i', 'know', 'i', \"'\", 'm', 'on', 'the', 'med', '##s', ',', 'everything', 'is', 'great', '.', 'i', 'get', 'work', 'done', ',', 'i', 'stay', 'on', 'task', ',', 'don', \"'\", 't', 'get', 'too', 'distracted', 'with', 'things', 'like', 'red', '##dit', 'during', 'work', 'or', 'jumping', 'around', 'from', 'project', 'to', 'project', '.', 'but', ',', 'when', 'i', \"'\", 'm', 'off', 'of', 'med', '##s', ',', 'my', 'imp', '##uls', '##ivity', 'goes', 'through', 'the', 'roof', '.', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'stay', 'on', 'my', 'diet', 'at', 'all', 'these', 'past', 'couple', 'of', 'weeks', 'even', 'though', 'i', 'didn', \"'\", 't', 'really', 'have', 'too', 'much', 'trouble', 'doing', 'so', 'prior', 'to', 'that', '(', 'i', 'lost', 'about', '30', 'pounds', 'over', 'last', 'summer', ')', '.', 'same', 'deal', 'with', 'going', 'to', 'the', 'gym', 'after', 'work', ',', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'keep', 'a', 'consistent', 'schedule', '.', 'this', 'among', 'other', 'things', 'i', \"'\", 've', 'been', 'trying', 'to', 'control', ',', 'it', 'just', 'feels', 'like', 'i', 'have', 'no', 'control', '.', 'my', 'last', 'dose', 'is', 'usually', 'around', '3', '-', '4', 'pm', 'so', 'i', 'can', 'finish', 'out', 'the', 'work', 'day', 'and', 'fall', 'asleep', 'at', 'a', 'decent', 'time', ',', 'but', 'when', 'the', 'med', '##s', 'wear', 'off', ',', 'it', 'seems'], ['as', 'if', 'i', 'am', 'useless', 'and', 'even', 'worse', 'with', 'imp', '##uls', '##ivity', 'than', 'i', 'was', 'before', 'i', 'started', 'taking', 'the', 'rita', '##lin', '.', 'help', '?', 'advice', '?', 'thanks', 'in', 'advance', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'not', 'sure', 'if', 'i', \"'\", 'm', 'self', '-', 'dia', '##gno', '##sing', 'myself', 'accurately', ',', 'but', 'i', \"'\", 'm', 'terrified', 'of', 'the', 'stigma', 'of', 'being', 'a', '20', '/', 'm', 'going', 'to', 'a', 'psychiatrist', 'for', 'ad', '##hd', '(', 'more', 'details', 'inside', ')', '.']\n",
      "INFO:__main__:Number of tokens: 44\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'not', 'sure', 'if', 'i', \"'\", 'm', 'self', '-', 'dia', '##gno', '##sing', 'myself', 'accurately', ',', 'but', 'i', \"'\", 'm', 'terrified', 'of', 'the', 'stigma', 'of', 'being', 'a', '20', '/', 'm', 'going', 'to', 'a', 'psychiatrist', 'for', 'ad', '##hd', '(', 'more', 'details', 'inside', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'ad', '##hd', 'make', 'everything', 'seem', 'so', 'green', 'on', 'the', 'other', 'side', '?', 'i', 'will', 'reach', 'out', 'for', 'help', 'from', 'my', 'loved', 'ones', '.', '.', '.', 'happy', 'for', 'a', 'few', 'days', 'then', 'i', 'want', 'to', 'move', 'on', '.', '.', '.', '.', 'im', 'so', 'mixed', 'up', '.', '.', 'i', 'just', 'want', 'to', 'settle', 'down', 'in', 'one', 'place', 'but', 'in', 'my', 'mind', 'there', 'is', 'not', 'a', 'place', 'for', 'me', '!', 'i', 'have', 'ad', '##hd', 'but', 'never', 'got', 'on', 'med', '##s', 'for', 'it', 'went', 'thru', 'a', 'divorce', 'and', 'couldn', '##t', 'afford', 'it', 'and', 'i', 'really', 'gave', 'up', 'on', 'my', 'health', '.', 'are', 'there', 'any', 'adults', 'out', 'there', 'that', 'has', 'this', 'problem', '?']\n",
      "INFO:__main__:Number of tokens: 109\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'ad', '##hd', 'make', 'everything', 'seem', 'so', 'green', 'on', 'the', 'other', 'side', '?', 'i', 'will', 'reach', 'out', 'for', 'help', 'from', 'my', 'loved', 'ones', '.', '.', '.', 'happy', 'for', 'a', 'few', 'days', 'then', 'i', 'want', 'to', 'move', 'on', '.', '.', '.', '.', 'im', 'so', 'mixed', 'up', '.', '.', 'i', 'just', 'want', 'to', 'settle', 'down', 'in', 'one', 'place', 'but', 'in', 'my', 'mind', 'there', 'is', 'not', 'a', 'place', 'for', 'me', '!', 'i', 'have', 'ad', '##hd', 'but', 'never', 'got', 'on', 'med', '##s', 'for', 'it', 'went', 'thru', 'a', 'divorce', 'and', 'couldn', '##t', 'afford', 'it', 'and', 'i', 'really', 'gave', 'up', 'on', 'my', 'health', '.', 'are', 'there', 'any', 'adults', 'out', 'there', 'that', 'has', 'this', 'problem', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'in', 'kids', 'my', 'daughter', '(', '4', 'years', 'old', ')', 'shows', 'symptoms', 'of', 'ad', '##hd', '.', 'is', 'medication', 'the', 'answer', 'or', 'is', 'behavioral', 'therapy', 'worth', 'trying', 'first', '?']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'in', 'kids', 'my', 'daughter', '(', '4', 'years', 'old', ')', 'shows', 'symptoms', 'of', 'ad', '##hd', '.', 'is', 'medication', 'the', 'answer', 'or', 'is', 'behavioral', 'therapy', 'worth', 'trying', 'first', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'gr', '##ad', 'school', '?', '(', 'cross', 'posted', 'from', '/', 'r', '/', 'medical', '##sch', '##ool', ')']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'gr', '##ad', 'school', '?', '(', 'cross', 'posted', 'from', '/', 'r', '/', 'medical', '##sch', '##ool', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['medication', 'advice']\n",
      "INFO:__main__:Number of tokens: 2\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['medication', 'advice']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', '18', 'and', 'i', 'think', 'i', 'might', 'have', 'ad', '##hd', ',', 'but', 'how', 'do', 'i', 'tell', 'my', 'doctor', 'about', 'my', 'concerns', '?', '(', 'x', '-', 'post', 'from', 'r', '/', 'health', ')', 'when', 'i', 'was', 'little', ',', 'i', 'could', 'never', 'focus', 'on', 'my', 'homework', '.', 'it', 'took', 'me', 'hours', 'to', 'do', 'simple', 'math', 'problems', '.', 'i', 'have', 'never', 'been', 'able', 'to', 'read', 'chapters', 'in', 'books', 'because', 'i', 'get', 'distracted', 'after', 'just', 'a', 'few', 'minutes', ',', 'and', 'when', 'i', 'do', 'read', ',', 'i', 'have', 'to', 're', '##rea', '##d', 'the', 'chapters', 'a', 'few', 'times', 'because', 'i', 'zone', 'out', 'and', 'start', 'day', 'dreaming', '.', 'my', 'mom', 'also', 'tells', 'me', 'i', 'forget', 'everything', ',', 'and', 'that', \"'\", 's', 'pretty', 'accurate', '.', 'to', 'this', 'day', ',', 'i', 'can', \"'\", 't', 'remember', 'to', 'do', 'the', 'simplest', 'of', 'things', '.', 'also', ',', 'when', 'i', \"'\", 'm', 'talking', 'to', 'a', 'big', 'group', 'of', 'people', ',', 'or', 'even', 'talking', 'to', 'just', 'a', 'few', 'or', 'one', 'person', ',', 'i', 'usually', 'feel', 'out', 'of', 'place', 'because', 'i', 'never', 'can', 'focus', 'on', 'talking', ',', 'i', \"'\", 'm', 'always', 'zoning', 'out', 'and', 'day', 'dreaming', 'to', 'myself', '.', 'i', \"'\", 've', 'also', 'been', 'known', 'to', 'have', 'troubles', 'sitting', 'still', '.', 'my', 'feet', 'are', 'always', 'bouncing', 'or', 'i', \"'\", 'm', 'wig', '##gling', 'my', 'toes', 'or', 'picking', 'at', 'my', 'fingernails', 'to', 'the', 'point', 'of', 'them', 'bleeding', '.', 'this', 'year', 'was', 'my', 'first', 'year', 'in', 'college', '.', 'last', 'semester', 'was', 'rough', '.', 'i', 'started', 'to', 'deal', 'with', 'a', 'lot', 'of', 'depression', ',', 'social', 'anxiety', ',', 'sleeping', 'problems', ',', 'and', 'a', 'lot', 'of', 'other', 'issues', ',', 'but', 'this', 'semester', 'has', 'been', 'much', 'better', '.', 'my', 'social', 'anxiety', 'has', 'seemed', 'to', 'disappear', 'and', 'my', 'depression', 'is', 'at', 'the', 'lowest', 'it', 'has', 'been', 'in', 'years', 'and', 'i', \"'\", 'm', 'sleeping', 'normally', 'again', ',', 'but', 'i', \"'\", 'm', 'still', 'having', 'horrible', 'trouble', 'focusing', 'on', 'my', 'school', 'work', '.', 'in', 'high', 'school', ',', 'i', 'started', 'out', 'in', 'a', 'very', 'good', 'high', 'school', '.', 'lots', 'of', 'kids', ',', 'lots', 'of', 'different', 'classes', 'that', 'challenged', 'me', ',', 'and', 'i', 'had', 'a', 'bit', 'of', 'trouble', '.', 'then', ',', 'sophomore', 'year', 'i', 'transferred', 'to', 'a', 'really', 'small', 'school', 'where', 'the', 'academics', 'weren', \"'\", 't', 'nearly', 'as', 'good', '.', 'this', 'made', 'me', 'do', 'a', 'lot', 'better', 'in', 'school', ',', 'mostly', 'because', 'i', 'could', 'do', 'all', 'my', 'homework', 'in', 'the', 'class', 'beforehand', 'when', 'i', 'forgot', 'to', 'do', 'it', 'the', 'night', 'before', '.', 'now', ',', 'i', \"'\", 'm', 'going', 'to', 'a', 'very', 'challenging', 'university', 'and', 'i', \"'\", 'm', 'noticing', 'that', 'i', \"'\", 'm', 'having', 'more', 'problems', 'like', 'this', 'than', 'before', '.', 'last', 'semester', 'i', 'ended', 'up', 'failing', 'some', 'really', 'easy', 'classes', 'just', 'because', 'i', 'couldn', \"'\", 't', 'focus', 'enough', 'to', 'read', 'a', 'text', 'book', '.', 'so', ',', 'my', 'question', 'is', 'that', 'if', 'this', 'isn', \"'\", 't', 'ad', '##hd', ',', 'then', 'what', 'could', 'it', 'be', '?', 'how', 'do', 'i', 'bring', 'up', 'these', 'concerns', 'with', 'my', 'doctor', ',', 'when', 'i', \"'\", 've', 'said', 'nothing', 'to', 'him', 'about', 'it', 'in', 'the', 'past', '?', 'edit', ':', 'just', 'to', 'clarify', ',', 'i', \"'\", 've', 'only', 'seen', 'my', 'doctor', '2', 'or', '3', 'times', ',', 'and', 'i', \"'\", 've', 'never', 'said', 'anything', 'to', 'him', 'about', 'my', 'mental', 'problems', ',', 'though', 'i', \"'\", 've', 'been', 'contemplating', 'going', 'to', 'counseling', 'to', 'see', 'if', 'they', 'could', 'help', ',', 'but', 'i', 'haven', \"'\", 't', 'brought', 'myself', 'to', 'do', 'it', 'yet', '.', 'update', ':', 'i', 'went', 'to', 'the', 'counseling', 'center', 'today', 'and', 'met', 'with', 'a', 'woman', 'to', 'talk', 'about', 'my', 'concerns', ',', 'my', 'depression', ',', 'social', 'anxiety', ',', 'and', 'where', 'to', 'go', 'from', 'here', '.', 'she', 'gave', 'me', 'ways', 'to', 'help', 'my', 'ability', 'to', 'focus', 'and', 'we', 'scheduled', 'a', 'follow', 'up', 'meeting', '.', 'she', 'did', 'tell', 'me', 'that', 'i', 'definitely', 'show', 'symptoms', 'of', 'ad', '##hd', ',', 'but', 'she', 'isn', \"'\", 't', 'able', 'to', 'tell', 'whether', 'or', 'not', 'it', 'is', 'ad', '##hd', 'or', 'depression', '.', 'now', ',', 'i', 'realize', 'that', 'i', 'wasn', \"'\", 't', 'clear', 'on', 'when', 'i', 'was', 'depressed', '.', 'she', 'thinks', 'that', 'it', \"'\", 's', 'something', 'i', \"'\", 've', 'struggled', 'with', 'my', 'entire', 'life', ',', 'even', 'though', 'it', \"'\", 's', 'mostly', 'been', 'only', 'the', 'past', 'semester', '.', 'if', 'i', 'were', 'to', 'go', 'through', 'my', 'school', ',', 'it', 'would', 'take', 'a', 'really', 'long', 'time', 'for', 'me', 'to', 'even', 'by', 'diagnosed', ',', 'so', 'she', 'suggested', 'that', 'i', 'go', 'see', 'a', 'psychiatrist', 'or', 'doctor', 'sooner', 'rather', 'than', 'later', '.', 'they', 'would', 'be', 'able', 'to', 'evaluate', 'me', 'and', 'help', 'me', 'faster', 'than', 'anyone', 'else', ',', 'so', ',', 'i', \"'\", 'm', 'going', 'to', 'try', 'to', 'find', 'a', 'doctor', 'in', 'the', 'area', 'that', 'takes', 'me', 'insurance', '.', 'if', 'not', ',', 'i', \"'\", 've', 'gotta', 'wait', 'until', 'spring', 'break', 'to', 'see', 'anyone', '.']\n",
      "INFO:__main__:Number of tokens: 767\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', '18', 'and', 'i', 'think', 'i', 'might', 'have', 'ad', '##hd', ',', 'but', 'how', 'do', 'i', 'tell', 'my', 'doctor', 'about', 'my', 'concerns', '?', '(', 'x', '-', 'post', 'from', 'r', '/', 'health', ')', 'when', 'i', 'was', 'little', ',', 'i', 'could', 'never', 'focus', 'on', 'my', 'homework', '.', 'it', 'took', 'me', 'hours', 'to', 'do', 'simple', 'math', 'problems', '.', 'i', 'have', 'never', 'been', 'able', 'to', 'read', 'chapters', 'in', 'books', 'because', 'i', 'get', 'distracted', 'after', 'just', 'a', 'few', 'minutes', ',', 'and', 'when', 'i', 'do', 'read', ',', 'i', 'have', 'to', 're', '##rea', '##d', 'the', 'chapters', 'a', 'few', 'times', 'because', 'i', 'zone', 'out', 'and', 'start', 'day', 'dreaming', '.', 'my', 'mom', 'also', 'tells', 'me', 'i', 'forget', 'everything', ',', 'and', 'that', \"'\", 's', 'pretty', 'accurate', '.', 'to', 'this', 'day', ',', 'i', 'can', \"'\", 't', 'remember', 'to', 'do', 'the', 'simplest', 'of', 'things', '.', 'also', ',', 'when', 'i', \"'\", 'm', 'talking', 'to', 'a', 'big', 'group', 'of', 'people', ',', 'or', 'even', 'talking', 'to', 'just', 'a', 'few', 'or', 'one', 'person', ',', 'i', 'usually', 'feel', 'out', 'of', 'place', 'because', 'i', 'never', 'can', 'focus', 'on', 'talking', ',', 'i', \"'\", 'm', 'always', 'zoning', 'out', 'and', 'day', 'dreaming', 'to', 'myself', '.', 'i', \"'\", 've', 'also', 'been', 'known', 'to', 'have', 'troubles', 'sitting', 'still', '.', 'my', 'feet', 'are', 'always', 'bouncing', 'or', 'i', \"'\", 'm', 'wig', '##gling', 'my', 'toes', 'or', 'picking', 'at', 'my', 'fingernails', 'to', 'the', 'point', 'of', 'them', 'bleeding', '.', 'this', 'year', 'was', 'my', 'first', 'year', 'in', 'college', '.', 'last', 'semester', 'was', 'rough', '.', 'i', 'started', 'to', 'deal', 'with', 'a', 'lot', 'of', 'depression', ',', 'social', 'anxiety', ',', 'sleeping', 'problems', ',', 'and', 'a', 'lot', 'of', 'other', 'issues', ',', 'but', 'this', 'semester', 'has', 'been', 'much', 'better', '.', 'my', 'social', 'anxiety', 'has', 'seemed', 'to', 'disappear', 'and', 'my', 'depression', 'is', 'at', 'the', 'lowest', 'it', 'has', 'been', 'in', 'years', 'and', 'i', \"'\", 'm', 'sleeping', 'normally', 'again', ',', 'but', 'i', \"'\", 'm', 'still', 'having', 'horrible', 'trouble', 'focusing', 'on', 'my', 'school', 'work', '.', 'in', 'high', 'school', ',', 'i', 'started', 'out', 'in', 'a', 'very', 'good', 'high', 'school', '.', 'lots', 'of', 'kids', ',', 'lots', 'of', 'different', 'classes', 'that', 'challenged', 'me', ',', 'and', 'i', 'had', 'a', 'bit', 'of', 'trouble', '.', 'then', ',', 'sophomore', 'year', 'i', 'transferred', 'to', 'a', 'really', 'small', 'school', 'where', 'the', 'academics', 'weren', \"'\", 't', 'nearly', 'as', 'good', '.', 'this', 'made', 'me', 'do', 'a', 'lot', 'better', 'in', 'school', ',', 'mostly', 'because', 'i', 'could', 'do', 'all', 'my', 'homework', 'in', 'the', 'class', 'beforehand', 'when', 'i', 'forgot', 'to', 'do', 'it', 'the', 'night', 'before', '.', 'now', ',', 'i', \"'\", 'm', 'going', 'to', 'a', 'very', 'challenging', 'university', 'and', 'i', \"'\", 'm', 'noticing', 'that', 'i', \"'\", 'm', 'having', 'more', 'problems', 'like', 'this', 'than', 'before', '.', 'last', 'semester', 'i', 'ended', 'up', 'failing', 'some', 'really', 'easy', 'classes', 'just', 'because', 'i', 'couldn', \"'\", 't', 'focus', 'enough', 'to', 'read', 'a', 'text', 'book', '.', 'so', ',', 'my', 'question', 'is', 'that', 'if', 'this', 'isn', \"'\", 't', 'ad', '##hd', ',', 'then', 'what', 'could', 'it', 'be', '?', 'how', 'do', 'i', 'bring', 'up', 'these', 'concerns', 'with', 'my', 'doctor', ',', 'when', 'i', \"'\", 've', 'said', 'nothing', 'to', 'him', 'about', 'it', 'in', 'the', 'past', '?', 'edit', ':', 'just', 'to', 'clarify', ',', 'i', \"'\", 've', 'only', 'seen', 'my', 'doctor', '2', 'or', '3', 'times', ',', 'and', 'i'], [\"'\", 've', 'never', 'said', 'anything', 'to', 'him', 'about', 'my', 'mental', 'problems', ',', 'though', 'i', \"'\", 've', 'been', 'contemplating', 'going', 'to', 'counseling', 'to', 'see', 'if', 'they', 'could', 'help', ',', 'but', 'i', 'haven', \"'\", 't', 'brought', 'myself', 'to', 'do', 'it', 'yet', '.', 'update', ':', 'i', 'went', 'to', 'the', 'counseling', 'center', 'today', 'and', 'met', 'with', 'a', 'woman', 'to', 'talk', 'about', 'my', 'concerns', ',', 'my', 'depression', ',', 'social', 'anxiety', ',', 'and', 'where', 'to', 'go', 'from', 'here', '.', 'she', 'gave', 'me', 'ways', 'to', 'help', 'my', 'ability', 'to', 'focus', 'and', 'we', 'scheduled', 'a', 'follow', 'up', 'meeting', '.', 'she', 'did', 'tell', 'me', 'that', 'i', 'definitely', 'show', 'symptoms', 'of', 'ad', '##hd', ',', 'but', 'she', 'isn', \"'\", 't', 'able', 'to', 'tell', 'whether', 'or', 'not', 'it', 'is', 'ad', '##hd', 'or', 'depression', '.', 'now', ',', 'i', 'realize', 'that', 'i', 'wasn', \"'\", 't', 'clear', 'on', 'when', 'i', 'was', 'depressed', '.', 'she', 'thinks', 'that', 'it', \"'\", 's', 'something', 'i', \"'\", 've', 'struggled', 'with', 'my', 'entire', 'life', ',', 'even', 'though', 'it', \"'\", 's', 'mostly', 'been', 'only', 'the', 'past', 'semester', '.', 'if', 'i', 'were', 'to', 'go', 'through', 'my', 'school', ',', 'it', 'would', 'take', 'a', 'really', 'long', 'time', 'for', 'me', 'to', 'even', 'by', 'diagnosed', ',', 'so', 'she', 'suggested', 'that', 'i', 'go', 'see', 'a', 'psychiatrist', 'or', 'doctor', 'sooner', 'rather', 'than', 'later', '.', 'they', 'would', 'be', 'able', 'to', 'evaluate', 'me', 'and', 'help', 'me', 'faster', 'than', 'anyone', 'else', ',', 'so', ',', 'i', \"'\", 'm', 'going', 'to', 'try', 'to', 'find', 'a', 'doctor', 'in', 'the', 'area', 'that', 'takes', 'me', 'insurance', '.', 'if', 'not', ',', 'i', \"'\", 've', 'gotta', 'wait', 'until', 'spring', 'break', 'to', 'see', 'anyone', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'jaw', 'hurts', '.', 'i', \"'\", 've', 'been', 'taking', '30', '##mg', 'v', '##y', '##van', '##se', 'nearly', 'every', 'day', 'for', 'the', 'last', 'two', 'weeks', '.', 'it', 'works', 'very', 'well', ';', 'iv', '##e', 'gone', 'from', 'nearly', 'failing', 'half', 'my', 'classes', 'to', 'being', 'that', 'hot', '##shot', 'kid', 'in', 'the', 'com', '##ps', '##ci', 'classes', '.', 'its', 'not', 'a', 'huge', 'deal', ',', 'but', 'the', 'only', 'two', 'noticeable', 'side', '##ef', '##fect', '##s', 'are', 'lack', 'of', 'appetite', 'and', 'jaw', 'sore', '##ness', '.', 'iv', '##e', 'been', 'counting', 'my', 'cal', '##ories', 'heavily', ',', 'and', 'i', 'eat', 'lots', 'of', 'protein', 'shakes', 'and', 'rai', '##sin', '##s', 'and', 'shit', 'to', 'make', 'sure', 'i', 'get', 'over', '2', '##k', 'cal', '##ories', 'a', 'day', '.', 'on', 'the', 'other', 'hand', ',', 'i', 'guess', 'im', 'grinding', 'my', 'jaw', ',', 'which', 'happens', 'with', 'amp', '##het', '##amine', '##s', '.', 'any', 'of', 'you', 'guys', 'have', 'this', 'problem', '?', 'are', 'there', 'any', 'good', 'fix', '##es', ',', 'short', 'of', 'wearing', 'a', 'pac', '##ifier', '?']\n",
      "INFO:__main__:Number of tokens: 152\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'jaw', 'hurts', '.', 'i', \"'\", 've', 'been', 'taking', '30', '##mg', 'v', '##y', '##van', '##se', 'nearly', 'every', 'day', 'for', 'the', 'last', 'two', 'weeks', '.', 'it', 'works', 'very', 'well', ';', 'iv', '##e', 'gone', 'from', 'nearly', 'failing', 'half', 'my', 'classes', 'to', 'being', 'that', 'hot', '##shot', 'kid', 'in', 'the', 'com', '##ps', '##ci', 'classes', '.', 'its', 'not', 'a', 'huge', 'deal', ',', 'but', 'the', 'only', 'two', 'noticeable', 'side', '##ef', '##fect', '##s', 'are', 'lack', 'of', 'appetite', 'and', 'jaw', 'sore', '##ness', '.', 'iv', '##e', 'been', 'counting', 'my', 'cal', '##ories', 'heavily', ',', 'and', 'i', 'eat', 'lots', 'of', 'protein', 'shakes', 'and', 'rai', '##sin', '##s', 'and', 'shit', 'to', 'make', 'sure', 'i', 'get', 'over', '2', '##k', 'cal', '##ories', 'a', 'day', '.', 'on', 'the', 'other', 'hand', ',', 'i', 'guess', 'im', 'grinding', 'my', 'jaw', ',', 'which', 'happens', 'with', 'amp', '##het', '##amine', '##s', '.', 'any', 'of', 'you', 'guys', 'have', 'this', 'problem', '?', 'are', 'there', 'any', 'good', 'fix', '##es', ',', 'short', 'of', 'wearing', 'a', 'pac', '##ifier', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'just', 'not', '\"', 'feeling', 'it', '\"', 'with', '30', '##mg', 'of', 'v', '##y', '##van', '##se', '.']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'just', 'not', '\"', 'feeling', 'it', '\"', 'with', '30', '##mg', 'of', 'v', '##y', '##van', '##se', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tips', 'and', 'tricks', 'for', 'turning', 'your', 'brain', 'off', 'at', 'night', '?', 'i', \"'\", 've', 'always', 'been', 'a', 'problematic', 'sleeper', ',', 'and', 'i', 'know', 'my', 'ina', '##tten', '##tive', 'add', 'doesn', \"'\", 't', 'help', '.', 'i', \"'\", 'll', 'stay', 'up', 'for', 'hours', 'telling', 'myself', ',', '\"', 'well', ',', 'i', \"'\", 'll', 'go', 'to', 'bed', 'after', 'i', 'finish', 'x', '.', '\"', 'but', ',', 'x', 'is', 'always', 'changed', 'each', 'time', 'it', \"'\", 's', 'met', '.', 'so', ',', 'what', 'are', 'some', 'ways', 'you', \"'\", 've', 'learned', 'to', 'just', '.', '.', '.', 'turn', 'off', 'your', 'brain', '?']\n",
      "INFO:__main__:Number of tokens: 91\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tips', 'and', 'tricks', 'for', 'turning', 'your', 'brain', 'off', 'at', 'night', '?', 'i', \"'\", 've', 'always', 'been', 'a', 'problematic', 'sleeper', ',', 'and', 'i', 'know', 'my', 'ina', '##tten', '##tive', 'add', 'doesn', \"'\", 't', 'help', '.', 'i', \"'\", 'll', 'stay', 'up', 'for', 'hours', 'telling', 'myself', ',', '\"', 'well', ',', 'i', \"'\", 'll', 'go', 'to', 'bed', 'after', 'i', 'finish', 'x', '.', '\"', 'but', ',', 'x', 'is', 'always', 'changed', 'each', 'time', 'it', \"'\", 's', 'met', '.', 'so', ',', 'what', 'are', 'some', 'ways', 'you', \"'\", 've', 'learned', 'to', 'just', '.', '.', '.', 'turn', 'off', 'your', 'brain', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['low', 'dose', 'of', 'add', '##eral', '##l', 'induce', '##s', 'calm', ',', 'clear', 'headed', '-', 'ness', '.', 'could', 'i', 'have', 'add', '/', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['low', 'dose', 'of', 'add', '##eral', '##l', 'induce', '##s', 'calm', ',', 'clear', 'headed', '-', 'ness', '.', 'could', 'i', 'have', 'add', '/', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['\"', 'coming', 'down', '\"', 'from', 'rita', '##lin', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['\"', 'coming', 'down', '\"', 'from', 'rita', '##lin', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['already', 'diagnosed', ',', 'how', 'do', 'i', 'get', 'treatment', 'without', 'insurance', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['already', 'diagnosed', ',', 'how', 'do', 'i', 'get', 'treatment', 'without', 'insurance', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dating', 'advice', 'for', 'singles', 'with', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dating', 'advice', 'for', 'singles', 'with', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['cannabis', 'and', 'concert', '##a', '/', 'rita', '##lin', '/', 'methyl', '##ph', '##eni', '##date', '?', 'i', \"'\", 've', 'recently', 'been', 'prescribed', 'concert', '##a', 'to', 'treat', 'my', 'ad', '##hd', '.', 'i', \"'\", 'm', 'an', 'adult', 'who', 'was', 'treated', 'with', 'rita', '##lin', 'as', 'a', 'child', ',', 'and', 'i', 'remember', 'the', 'side', 'effects', 'being', 'a', 'bit', 'severe', ',', 'particularly', 'the', 'loss', 'of', 'ape', '##tite', 'and', 'ins', '##om', '##nia', '.', 'incident', '##ly', ',', 'these', 'are', '2', 'problems', 'which', 'cannabis', 'is', 'known', 'to', 'affect', '(', 'relax', '##es', ',', 'causes', 'increased', 'hunger', 'aka', '\"', 'the', 'mu', '##nch', '##ies', '\"', ')', '.', 'as', 'a', 'casual', 'cannabis', 'user', '(', 'i', 'smoke', 'maybe', '.', '.', '.', 'a', 'gram', 'a', 'month', '?', ')', ',', 'and', 'having', 'not', 'yet', 'started', 'taking', 'the', 'medication', ',', 'i', 'wonder', 'if', 'anyone', 'has', 'attempted', 'to', 'offset', 'the', 'ill', 'effects', 'of', 'ad', '##hd', 'medication', 'with', 'cannabis', ',', 'and', 'if', 'so', ',', 'what', 'were', 'the', 'results', '?', 'please', 'keep', 'in', 'mind', ',', 'i', \"'\", 'm', 'not', 'necessarily', 'trying', 'to', 'end', '##ors', '##e', 'illicit', 'drug', 'use', ',', 'this', 'is', 'a', 'question', 'i', 'am', 'asking', 'out', 'of', 'pure', 'curiosity', ',', 'nothing', 'more', '.']\n",
      "INFO:__main__:Number of tokens: 182\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['cannabis', 'and', 'concert', '##a', '/', 'rita', '##lin', '/', 'methyl', '##ph', '##eni', '##date', '?', 'i', \"'\", 've', 'recently', 'been', 'prescribed', 'concert', '##a', 'to', 'treat', 'my', 'ad', '##hd', '.', 'i', \"'\", 'm', 'an', 'adult', 'who', 'was', 'treated', 'with', 'rita', '##lin', 'as', 'a', 'child', ',', 'and', 'i', 'remember', 'the', 'side', 'effects', 'being', 'a', 'bit', 'severe', ',', 'particularly', 'the', 'loss', 'of', 'ape', '##tite', 'and', 'ins', '##om', '##nia', '.', 'incident', '##ly', ',', 'these', 'are', '2', 'problems', 'which', 'cannabis', 'is', 'known', 'to', 'affect', '(', 'relax', '##es', ',', 'causes', 'increased', 'hunger', 'aka', '\"', 'the', 'mu', '##nch', '##ies', '\"', ')', '.', 'as', 'a', 'casual', 'cannabis', 'user', '(', 'i', 'smoke', 'maybe', '.', '.', '.', 'a', 'gram', 'a', 'month', '?', ')', ',', 'and', 'having', 'not', 'yet', 'started', 'taking', 'the', 'medication', ',', 'i', 'wonder', 'if', 'anyone', 'has', 'attempted', 'to', 'offset', 'the', 'ill', 'effects', 'of', 'ad', '##hd', 'medication', 'with', 'cannabis', ',', 'and', 'if', 'so', ',', 'what', 'were', 'the', 'results', '?', 'please', 'keep', 'in', 'mind', ',', 'i', \"'\", 'm', 'not', 'necessarily', 'trying', 'to', 'end', '##ors', '##e', 'illicit', 'drug', 'use', ',', 'this', 'is', 'a', 'question', 'i', 'am', 'asking', 'out', 'of', 'pure', 'curiosity', ',', 'nothing', 'more', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['starting', 'concert', '##a', 'a', 'decade', 'late', '.', 'anything', 'i', 'need', 'to', 'know', 'about', '?', 'been', 'showing', 'symptoms', 'of', 'ad', '##hd', 'since', 'around', '6', '-', '7', 'years', 'old', '.', 'now', '17', '.', 'i', 'got', 'my', 'first', 'bottle', 'of', 'concert', '##a', 'today', 'and', 'will', 'begin', 'taking', 'it', 'tomorrow', '.', 'story', '##time', ':', 'if', 'you', 'took', 'all', 'of', 'the', 'time', 'i', \"'\", 've', 'spend', 'studying', 'my', 'entire', 'life', 'and', 'combined', 'it', ',', 'you', \"'\", 'd', 'have', 'less', 'than', '24', 'hours', '.', 'this', 'seemed', 'okay', ',', 'but', 'now', 'my', 'c', 'average', 'is', 'slipping', 'away', 'and', 'i', 'recently', 'dumped', 'a', 'lot', 'of', 'extra', 'stress', 'on', 'myself', 'by', 'taking', 'an', 'additional', 'school', 'subject', 'and', 'am', 'busy', 'in', 'writing', 'courses', ',', 'stacked', 'on', 'top', 'of', 'mm', '##o', 'gaming', '.', 'so', 'i', \"'\", 'll', 'start', 'the', 'med', '##s', 'tomorrow', 'morning', 'before', 'school', '.', 'doctor', 'tells', 'me', 'that', 'effects', 'take', 'place', 'between', '2', 'days', 'and', 'a', 'week', 'from', 'first', 'use', '.', 'questions', ':', '1', '.', 'what', 'effects', 'can', 'i', 'expect', 'from', 'concert', '##a', 'in', 'the', 'short', 'term', '(', '2', '-', '3', 'weeks', ')', 'and', 'what', 'effects', 'may', 'i', 'expect', 'in', 'the', 'long', 'term', '(', '1', 'month', '+', ')', '?', '2', '.', 'what', 'are', 'the', 'most', 'common', 'effects', 'experienced', 'with', 'concert', '##a', 'and', 'how', 'may', 'they', 'benefit', 'me', '?', '3', '.', 'anything', 'i', 'should', 'know', 'about', 'before', 'hoc', '##king', 'tablets', 'or', 'while', 'i', \"'\", 'm', 'chu', '##gging', 'them', '?', '4', '.', 'i', \"'\", 've', 'heard', 'things', 'about', 'concert', '##a', 'helping', 'studying', 'and', 'school', '##work', '.', 'care', 'to', 'elaborate', '?', 'please', 'and', 'thank', 'you', '!']\n",
      "INFO:__main__:Number of tokens: 254\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['starting', 'concert', '##a', 'a', 'decade', 'late', '.', 'anything', 'i', 'need', 'to', 'know', 'about', '?', 'been', 'showing', 'symptoms', 'of', 'ad', '##hd', 'since', 'around', '6', '-', '7', 'years', 'old', '.', 'now', '17', '.', 'i', 'got', 'my', 'first', 'bottle', 'of', 'concert', '##a', 'today', 'and', 'will', 'begin', 'taking', 'it', 'tomorrow', '.', 'story', '##time', ':', 'if', 'you', 'took', 'all', 'of', 'the', 'time', 'i', \"'\", 've', 'spend', 'studying', 'my', 'entire', 'life', 'and', 'combined', 'it', ',', 'you', \"'\", 'd', 'have', 'less', 'than', '24', 'hours', '.', 'this', 'seemed', 'okay', ',', 'but', 'now', 'my', 'c', 'average', 'is', 'slipping', 'away', 'and', 'i', 'recently', 'dumped', 'a', 'lot', 'of', 'extra', 'stress', 'on', 'myself', 'by', 'taking', 'an', 'additional', 'school', 'subject', 'and', 'am', 'busy', 'in', 'writing', 'courses', ',', 'stacked', 'on', 'top', 'of', 'mm', '##o', 'gaming', '.', 'so', 'i', \"'\", 'll', 'start', 'the', 'med', '##s', 'tomorrow', 'morning', 'before', 'school', '.', 'doctor', 'tells', 'me', 'that', 'effects', 'take', 'place', 'between', '2', 'days', 'and', 'a', 'week', 'from', 'first', 'use', '.', 'questions', ':', '1', '.', 'what', 'effects', 'can', 'i', 'expect', 'from', 'concert', '##a', 'in', 'the', 'short', 'term', '(', '2', '-', '3', 'weeks', ')', 'and', 'what', 'effects', 'may', 'i', 'expect', 'in', 'the', 'long', 'term', '(', '1', 'month', '+', ')', '?', '2', '.', 'what', 'are', 'the', 'most', 'common', 'effects', 'experienced', 'with', 'concert', '##a', 'and', 'how', 'may', 'they', 'benefit', 'me', '?', '3', '.', 'anything', 'i', 'should', 'know', 'about', 'before', 'hoc', '##king', 'tablets', 'or', 'while', 'i', \"'\", 'm', 'chu', '##gging', 'them', '?', '4', '.', 'i', \"'\", 've', 'heard', 'things', 'about', 'concert', '##a', 'helping', 'studying', 'and', 'school', '##work', '.', 'care', 'to', 'elaborate', '?', 'please', 'and', 'thank', 'you', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['would', 'the', 'student', 'health', 'center', 'at', 'my', 'college', 'be', 'a', 'bad', 'place', 'to', 'start', 'the', 'diagnostic', 'process', '?', 'if', 'so', 'then', 'what', 'should', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['would', 'the', 'student', 'health', 'center', 'at', 'my', 'college', 'be', 'a', 'bad', 'place', 'to', 'start', 'the', 'diagnostic', 'process', '?', 'if', 'so', 'then', 'what', 'should', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['issues', 'with', 'tolerance', 'and', 'swap', '##ping', 'medication', 'i', 'have', 'been', 'taking', 'dex', '##tro', '##amp', '##het', '##amine', 'for', 'the', 'past', '3', 'years', 'and', 'to', 'say', 'that', 'it', 'has', 'changed', 'my', 'life', 'would', 'be', 'an', 'under', '##sta', '##tem', '##ent', '.', 'i', 'started', 'off', 'taking', 'it', 'without', 'a', 'prescription', 'at', 'about', '15', '##mg', 'a', 'day', '.', 'fast', 'forward', '3', 'years', 'and', 'i', 'have', 'a', 'prescription', 'and', 'i', \"'\", 'm', 'on', '30', '##mg', 'a', 'day', '.', 'i', 'usually', 'stop', 'taking', 'it', 'bi', '##ann', '##ually', 'during', 'un', '##i', 'breaks', 'however', 'i', 'have', 'needed', 'to', 'study', 'throughout', 'this', 'break', 'for', 'an', 'entry', 'test', '.', 'i', 'have', 'developed', 'almost', 'complete', 'tolerance', 'to', 'it', 'so', 'i', 'have', 'stopped', 'taking', 'it', 'roughly', 'a', 'week', 'ago', '.', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'study', 'without', 'it', 'and', 'i', 'fear', 'going', 'it', 'to', 'semester', 'having', 'such', 'a', 'high', 'tolerance', 'to', 'it', 'is', 'going', 'to', 'to', 'lead', 'to', 'some', 'problems', '.', 'i', 'have', 'an', 'appointment', 'in', 'a', 'few', 'days', 'with', 'my', 'ps', '##ych', 'and', 'i', 'want', 'to', 'perhaps', 'swap', 'to', 'another', 'medication', 'for', 'a', 'duration', 'so', 'that', 'my', 'tolerance', 'will', 'sub', '##side', '.', 'my', 'questions', 'is', 'how', 'do', 'i', 'go', 'about', 'asking', 'him', 'to', 'change', 'medication', ',', 'do', 'i', 'just', 'tell', 'him', 'straight', 'up', 'that', 'i', 'believe', 'that', 'i', 'have', 'developed', 'tolerance', '?', 'also', 'in', 'terms', 'of', 'alternatives', ',', 'i', 'understand', 'everyone', 'is', 'different', 'and', 'has', 'different', 'takes', 'on', 'medications', 'but', 'how', 'close', 'to', 'the', 'effects', 'of', 'dex', '##tro', '##amp', '##het', '##amine', 'can', 'i', 'expect', 'from', 'an', 'alternative', '?', 'i', \"'\", 've', 'heard', 'a', 'bit', 'about', 'rita', '##lin', ',', 'how', 'similar', '/', 'di', '##ssi', '##mi', '##lar', 'are', 'the', 'effects', '?', 't', '##l', ';', 'dr', 'developed', 'tolerance', 'to', 'dex', '##tro', '##amp', '##het', '##amine', 'and', 'want', 'some', 'advice', 'on', 'efficacy', 'of', 'alternatives', '.']\n",
      "INFO:__main__:Number of tokens: 290\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['issues', 'with', 'tolerance', 'and', 'swap', '##ping', 'medication', 'i', 'have', 'been', 'taking', 'dex', '##tro', '##amp', '##het', '##amine', 'for', 'the', 'past', '3', 'years', 'and', 'to', 'say', 'that', 'it', 'has', 'changed', 'my', 'life', 'would', 'be', 'an', 'under', '##sta', '##tem', '##ent', '.', 'i', 'started', 'off', 'taking', 'it', 'without', 'a', 'prescription', 'at', 'about', '15', '##mg', 'a', 'day', '.', 'fast', 'forward', '3', 'years', 'and', 'i', 'have', 'a', 'prescription', 'and', 'i', \"'\", 'm', 'on', '30', '##mg', 'a', 'day', '.', 'i', 'usually', 'stop', 'taking', 'it', 'bi', '##ann', '##ually', 'during', 'un', '##i', 'breaks', 'however', 'i', 'have', 'needed', 'to', 'study', 'throughout', 'this', 'break', 'for', 'an', 'entry', 'test', '.', 'i', 'have', 'developed', 'almost', 'complete', 'tolerance', 'to', 'it', 'so', 'i', 'have', 'stopped', 'taking', 'it', 'roughly', 'a', 'week', 'ago', '.', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'study', 'without', 'it', 'and', 'i', 'fear', 'going', 'it', 'to', 'semester', 'having', 'such', 'a', 'high', 'tolerance', 'to', 'it', 'is', 'going', 'to', 'to', 'lead', 'to', 'some', 'problems', '.', 'i', 'have', 'an', 'appointment', 'in', 'a', 'few', 'days', 'with', 'my', 'ps', '##ych', 'and', 'i', 'want', 'to', 'perhaps', 'swap', 'to', 'another', 'medication', 'for', 'a', 'duration', 'so', 'that', 'my', 'tolerance', 'will', 'sub', '##side', '.', 'my', 'questions', 'is', 'how', 'do', 'i', 'go', 'about', 'asking', 'him', 'to', 'change', 'medication', ',', 'do', 'i', 'just', 'tell', 'him', 'straight', 'up', 'that', 'i', 'believe', 'that', 'i', 'have', 'developed', 'tolerance', '?', 'also', 'in', 'terms', 'of', 'alternatives', ',', 'i', 'understand', 'everyone', 'is', 'different', 'and', 'has', 'different', 'takes', 'on', 'medications', 'but', 'how', 'close', 'to', 'the', 'effects', 'of', 'dex', '##tro', '##amp', '##het', '##amine', 'can', 'i', 'expect', 'from', 'an', 'alternative', '?', 'i', \"'\", 've', 'heard', 'a', 'bit', 'about', 'rita', '##lin', ',', 'how', 'similar', '/', 'di', '##ssi', '##mi', '##lar', 'are', 'the', 'effects', '?', 't', '##l', ';', 'dr', 'developed', 'tolerance', 'to', 'dex', '##tro', '##amp', '##het', '##amine', 'and', 'want', 'some', 'advice', 'on', 'efficacy', 'of', 'alternatives', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['help', '!', 'my', 'doctors', 'office', 'is', 'ignoring', 'my', 'calls', 'requesting', 'a', 'ref', '##ill', 'for', 'my', 'medication', '!']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['help', '!', 'my', 'doctors', 'office', 'is', 'ignoring', 'my', 'calls', 'requesting', 'a', 'ref', '##ill', 'for', 'my', 'medication', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['memo', '##rization', 'and', 'ad', '##hd', '(', 'skip', 'to', 'the', 'bold', 'since', 'this', 'probably', 'doesn', \"'\", 't', 'matter', ')', 'background', ':', 'so', 'my', 'doctor', 'diagnosed', 'me', 'halfway', 'through', 'freshman', 'year', '(', 'college', ')', '.', 'he', 'showed', 'me', 'the', 'test', 'results', ',', 'and', 'told', 'me', 'that', '*', 'i', 'have', 'ad', '##hd', '/', 'some', 'form', 'of', 'learning', 'disability', '*', 'my', 'iq', 'is', 'in', 'a', 'superior', '(', '?', ')', 'percent', '##ile', '.', 'i', 'don', \"'\", 't', 'know', 'the', 'number', '.', '*', 'i', 'scored', 'really', 'high', 'on', 'some', 'tests', ',', 'and', 'really', 'low', 'on', 'other', 'tests', '(', 'as', 'low', 'as', '2', '%', 'on', 'one', ')', '*', 'i', \"'\", 'm', 'good', 'at', 'conceptual', 'knowledge', ',', 'but', 'bad', 'at', 'memo', '##rization', '.', '*', 'flash', '##cards', 'won', '##t', 'work', 'for', 'me', '.', '*', 'taking', 'notes', 'will', 'work', 'for', 'me', '.', '*', '*', 'so', 'the', 'question', 'is', '*', '*', 'how', 'do', 'you', 'guys', 'go', 'about', 'memo', '##riz', '##ing', 'names', ',', 'dates', ',', 'etc', '?', 'brute', 'force', '(', 'flash', '##cards', ')', 'just', 'don', \"'\", 't', 'work', 'for', 'me', '.', 'thanks']\n",
      "INFO:__main__:Number of tokens: 170\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['memo', '##rization', 'and', 'ad', '##hd', '(', 'skip', 'to', 'the', 'bold', 'since', 'this', 'probably', 'doesn', \"'\", 't', 'matter', ')', 'background', ':', 'so', 'my', 'doctor', 'diagnosed', 'me', 'halfway', 'through', 'freshman', 'year', '(', 'college', ')', '.', 'he', 'showed', 'me', 'the', 'test', 'results', ',', 'and', 'told', 'me', 'that', '*', 'i', 'have', 'ad', '##hd', '/', 'some', 'form', 'of', 'learning', 'disability', '*', 'my', 'iq', 'is', 'in', 'a', 'superior', '(', '?', ')', 'percent', '##ile', '.', 'i', 'don', \"'\", 't', 'know', 'the', 'number', '.', '*', 'i', 'scored', 'really', 'high', 'on', 'some', 'tests', ',', 'and', 'really', 'low', 'on', 'other', 'tests', '(', 'as', 'low', 'as', '2', '%', 'on', 'one', ')', '*', 'i', \"'\", 'm', 'good', 'at', 'conceptual', 'knowledge', ',', 'but', 'bad', 'at', 'memo', '##rization', '.', '*', 'flash', '##cards', 'won', '##t', 'work', 'for', 'me', '.', '*', 'taking', 'notes', 'will', 'work', 'for', 'me', '.', '*', '*', 'so', 'the', 'question', 'is', '*', '*', 'how', 'do', 'you', 'guys', 'go', 'about', 'memo', '##riz', '##ing', 'names', ',', 'dates', ',', 'etc', '?', 'brute', 'force', '(', 'flash', '##cards', ')', 'just', 'don', \"'\", 't', 'work', 'for', 'me', '.', 'thanks']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['frustration', 'towards', 'others', 'who', 'careless', '##ly', 'use', 'the', 'term', \"'\", 'ad', '##hd', \"'\", 'in', 'their', 'own', 'self', '-', 'diagnosis', '?', 'to', 'preface', 'a', 'few', 'things', ':', 'i', 'know', 'ad', '##hd', 'is', 'not', 'the', 'worst', 'thing', 'in', 'the', 'world', 'and', 'i', 'truly', 'am', 'not', 'trying', 'to', 'sound', 'conde', '##sc', '##ending', 'or', 'ins', '##ens', '##itive', 'in', 'the', 'title', '.', 'i', 'know', 'how', 'the', 'title', 'almost', 'make', 'me', 'sound', 'like', 'i', \"'\", 'm', 'saying', '\"', 'how', 'dare', 'someone', 'else', 'talk', 'about', 'having', 'ad', '##hd', '\"', 'like', 'i', 'own', 'it', 'or', 'something', '.', 'really', ',', 'it', \"'\", 's', 'not', 'from', 'a', 'sense', 'of', 'jealousy', 'or', 'exclusive', '##ness', 'or', 'ins', '##ec', '##urity', 'or', 'ya', '##dda', 'ya', '##dda', '.', 'i', 'just', 'lately', 'find', 'myself', 'in', 'conversations', 'where', 'someone', 'is', 'talking', 'about', 'themselves', 'and', 'saying', 'how', 'last', 'night', 'they', 'couldn', \"'\", 't', 'choose', 'between', 'playing', 'a', 'video', 'game', 'and', 'doing', 'their', 'work', 'and', 'how', 'they', \"'\", 're', 'always', 'like', 'that', 'and', 'how', 'they', 'can', 'also', 'be', 'hyper', 'and', 'o', '##oo', '##h', 'they', 'are', 'soo', '##oo', 'ad', '##hd', '.', 'hmm', '##mm', '.', 'i', 'don', \"'\", 't', 'know', '.', 'that', 'bugs', 'me', '.', 'can', \"'\", 't', 'quite', 'define', 'why', '.', 'maybe', 'it', \"'\", 's', 'the', 'loosely', 'used', 'term', 'to', 'careless', '##ly', 'describe', 'something', 'that', 'is', 'more', 'of', 'a', 'burden', 'than', 'a', 'punch', 'line', ';', 'a', 'term', 'for', 'something', 'that', 'i', 'have', ',', 'but', 'that', 'someone', 'is', 'using', 'as', 'a', 'description', 'of', 'their', 'la', '##zine', '##ss', '.', 'i', 'mean', ',', 'i', 'think', 'everyone', 'has', 'some', 'degree', 'of', 'ina', '##tten', '##tive', '##ness', ',', 'right', '?', 'obviously', 'not', 'everyone', 'is', 'ad', '##hd', 'though', '.', 'ad', '##hd', 'has', \"'\", 'attention', 'deficit', \"'\", 'in', 'the', 'name', ',', 'so', 'it', \"'\", 's', 'not', 'too', 'hard', 'to', 'understand', 'why', 'everyone', 'gr', '##avi', '##tate', '##s', 'to', 'this', 'self', '-', 'diagnosis', '.', 'everyone', 'in', 'my', 'office', 'apparently', '\"', 'has', '\"', 'it', 'too', '.', 'my', 'frustration', 'could', 'be', 'that', 'people', 'use', 'it', 'to', 'describe', 'situations', 'here', 'and', 'there', 'when', 'in', 'reality', ',', 'as', 'you', 'guys', 'may', 'all', 'know', ',', 'it', \"'\", 's', 'much', 'more', 'life', 'consuming', 'than', 'that', '.', 'so', 'maybe', 'that', \"'\", 's', 'why', 'i', 'get', 'a', 'little', 'bug', '##ged', 'out', '.', 'it', \"'\", 's', 'being', 'used', 'as', 'an', 'excuse', 'for', 'why', 'a', 'project', 'wasn', \"'\", 't', 'done', 'or', 'why', 'the', 'trash', 'wasn', \"'\", 't', 'taken', 'out', ';', 'some', 'minor', 'event', '.', 'otherwise', ',', 'it', 'doesn', \"'\", 't', 'seem', 'to', 'exist', 'in', 'them', 'anymore', '.', 'and', 'as', 'someone', 'with', 'ad', '##hd', ',', 'it', \"'\", 's', 'not', 'just', 'affecting', 'single', 'events', 'here', 'and', 'there', 'but', 'instead', 'is', 'rooted', 'in', 'everything', 'i', 'do', ',', 'think', 'and', 'say', '.', 'i', 'just', 'want', 'to', 'see', 'if', 'anyone', 'else', 'experiences', 'this', '.', 'again', ',', 'this', 'is', 'truly', 'not', 'in', 'a', 'conde', '##sc', '##ending', 'way', 'or', 'to', 'purpose', '##fully', 'judge', 'people', ',', 'but', 'just', 'from', 'a', 'bit', 'of', 'frustration', '.', '.', '.', '.', 'maybe', 'even', 'because', 'of', 'the', 'add', '##eral', '##l', 'shortage', 'since', 'it', 'seems', 'to', 'just', 'be', 'given', 'away', 'at', 'this', 'point', 'with', 'anyone', 'who', '\"', 'can', \"'\", 't', 'focus', '\"', '.', 'so', ',', 'maybe', 'to', 'sum', '##mar', '##ize', 'in', 'a', 'sentence', ':', 'anyone', 'else', 'getting', 'tired', 'of', 'ad', '##hd', 'being', 'the', 'popular', 'thing', 'that', 'everybody', 'suddenly', '\"', 'has', '\"', '?']\n",
      "INFO:__main__:Number of tokens: 528\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['frustration', 'towards', 'others', 'who', 'careless', '##ly', 'use', 'the', 'term', \"'\", 'ad', '##hd', \"'\", 'in', 'their', 'own', 'self', '-', 'diagnosis', '?', 'to', 'preface', 'a', 'few', 'things', ':', 'i', 'know', 'ad', '##hd', 'is', 'not', 'the', 'worst', 'thing', 'in', 'the', 'world', 'and', 'i', 'truly', 'am', 'not', 'trying', 'to', 'sound', 'conde', '##sc', '##ending', 'or', 'ins', '##ens', '##itive', 'in', 'the', 'title', '.', 'i', 'know', 'how', 'the', 'title', 'almost', 'make', 'me', 'sound', 'like', 'i', \"'\", 'm', 'saying', '\"', 'how', 'dare', 'someone', 'else', 'talk', 'about', 'having', 'ad', '##hd', '\"', 'like', 'i', 'own', 'it', 'or', 'something', '.', 'really', ',', 'it', \"'\", 's', 'not', 'from', 'a', 'sense', 'of', 'jealousy', 'or', 'exclusive', '##ness', 'or', 'ins', '##ec', '##urity', 'or', 'ya', '##dda', 'ya', '##dda', '.', 'i', 'just', 'lately', 'find', 'myself', 'in', 'conversations', 'where', 'someone', 'is', 'talking', 'about', 'themselves', 'and', 'saying', 'how', 'last', 'night', 'they', 'couldn', \"'\", 't', 'choose', 'between', 'playing', 'a', 'video', 'game', 'and', 'doing', 'their', 'work', 'and', 'how', 'they', \"'\", 're', 'always', 'like', 'that', 'and', 'how', 'they', 'can', 'also', 'be', 'hyper', 'and', 'o', '##oo', '##h', 'they', 'are', 'soo', '##oo', 'ad', '##hd', '.', 'hmm', '##mm', '.', 'i', 'don', \"'\", 't', 'know', '.', 'that', 'bugs', 'me', '.', 'can', \"'\", 't', 'quite', 'define', 'why', '.', 'maybe', 'it', \"'\", 's', 'the', 'loosely', 'used', 'term', 'to', 'careless', '##ly', 'describe', 'something', 'that', 'is', 'more', 'of', 'a', 'burden', 'than', 'a', 'punch', 'line', ';', 'a', 'term', 'for', 'something', 'that', 'i', 'have', ',', 'but', 'that', 'someone', 'is', 'using', 'as', 'a', 'description', 'of', 'their', 'la', '##zine', '##ss', '.', 'i', 'mean', ',', 'i', 'think', 'everyone', 'has', 'some', 'degree', 'of', 'ina', '##tten', '##tive', '##ness', ',', 'right', '?', 'obviously', 'not', 'everyone', 'is', 'ad', '##hd', 'though', '.', 'ad', '##hd', 'has', \"'\", 'attention', 'deficit', \"'\", 'in', 'the', 'name', ',', 'so', 'it', \"'\", 's', 'not', 'too', 'hard', 'to', 'understand', 'why', 'everyone', 'gr', '##avi', '##tate', '##s', 'to', 'this', 'self', '-', 'diagnosis', '.', 'everyone', 'in', 'my', 'office', 'apparently', '\"', 'has', '\"', 'it', 'too', '.', 'my', 'frustration', 'could', 'be', 'that', 'people', 'use', 'it', 'to', 'describe', 'situations', 'here', 'and', 'there', 'when', 'in', 'reality', ',', 'as', 'you', 'guys', 'may', 'all', 'know', ',', 'it', \"'\", 's', 'much', 'more', 'life', 'consuming', 'than', 'that', '.', 'so', 'maybe', 'that', \"'\", 's', 'why', 'i', 'get', 'a', 'little', 'bug', '##ged', 'out', '.', 'it', \"'\", 's', 'being', 'used', 'as', 'an', 'excuse', 'for', 'why', 'a', 'project', 'wasn', \"'\", 't', 'done', 'or', 'why', 'the', 'trash', 'wasn', \"'\", 't', 'taken', 'out', ';', 'some', 'minor', 'event', '.', 'otherwise', ',', 'it', 'doesn', \"'\", 't', 'seem', 'to', 'exist', 'in', 'them', 'anymore', '.', 'and', 'as', 'someone', 'with', 'ad', '##hd', ',', 'it', \"'\", 's', 'not', 'just', 'affecting', 'single', 'events', 'here', 'and', 'there', 'but', 'instead', 'is', 'rooted', 'in', 'everything', 'i', 'do', ',', 'think', 'and', 'say', '.', 'i', 'just', 'want', 'to', 'see', 'if', 'anyone', 'else', 'experiences', 'this', '.', 'again', ',', 'this', 'is', 'truly', 'not', 'in', 'a', 'conde', '##sc', '##ending', 'way', 'or', 'to', 'purpose', '##fully', 'judge', 'people', ',', 'but', 'just', 'from', 'a', 'bit', 'of', 'frustration', '.', '.', '.', '.', 'maybe', 'even', 'because', 'of', 'the', 'add', '##eral', '##l', 'shortage', 'since', 'it', 'seems', 'to', 'just', 'be', 'given', 'away', 'at', 'this', 'point', 'with', 'anyone', 'who', '\"', 'can', \"'\", 't', 'focus', '\"', '.', 'so', ',', 'maybe', 'to', 'sum', '##mar', '##ize', 'in', 'a', 'sentence', ':', 'anyone', 'else'], ['getting', 'tired', 'of', 'ad', '##hd', 'being', 'the', 'popular', 'thing', 'that', 'everybody', 'suddenly', '\"', 'has', '\"', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'am', 'ad', '##hd', 'affected', ',', 'and', 'i', 'really', 'want', 'to', 'help', ',', 'but', '.', '.', '.', 'i', 'can', \"'\", 't', 'read', '30', 'pages', 'of', 'text', ',', 'not', 'without', '20', '+', 'mg', 'of', 'rita', '##lin', '.', 'i', \"'\", 'm', 'sorry', ',', 'i', 'feel', 'a', 'family', '-', 'hood', 'here', 'and', 'i', 'know', 'i', 'can', 'contribute', ',', 'but', 'put', 'a', 't', '##l', ';', 'dr', 'at', 'the', 'top', '.', 'i', \"'\", 'm', 'almost', 'in', 'tears', 'posting', 'this', ',', 'but', 'please', 'help', 'me', ',', 'maybe', 'even', 'us', ',', 'help', 'you', '.']\n",
      "INFO:__main__:Number of tokens: 87\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'am', 'ad', '##hd', 'affected', ',', 'and', 'i', 'really', 'want', 'to', 'help', ',', 'but', '.', '.', '.', 'i', 'can', \"'\", 't', 'read', '30', 'pages', 'of', 'text', ',', 'not', 'without', '20', '+', 'mg', 'of', 'rita', '##lin', '.', 'i', \"'\", 'm', 'sorry', ',', 'i', 'feel', 'a', 'family', '-', 'hood', 'here', 'and', 'i', 'know', 'i', 'can', 'contribute', ',', 'but', 'put', 'a', 't', '##l', ';', 'dr', 'at', 'the', 'top', '.', 'i', \"'\", 'm', 'almost', 'in', 'tears', 'posting', 'this', ',', 'but', 'please', 'help', 'me', ',', 'maybe', 'even', 'us', ',', 'help', 'you', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'youtube', '##r', 'hi', '!', 'please', 'let', 'me', 'know', 'if', 'this', 'is', 'considered', 'spa', '##mming', 'and', 'i', \"'\", 'll', 'remove', 'it', 'immediately', '.', 'a', 'while', 'back', 'i', 'started', 'a', 'youtube', 'channel', 'that', 'has', 'evolved', 'into', 'a', 'v', '##log', 'of', 'my', 'drive', 'home', 'after', 'work', '(', 'drive', 'at', 'five', ')', 'with', 'my', 'girlfriend', '.', 'seeing', 'how', 'this', 'is', 'a', 'ad', '##hd', 'category', ',', 'i', \"'\", 'm', 'pretty', 'sure', 'most', 'of', 'you', 'would', 'know', 'that', 'add', '##eral', '##l', 'fades', 'off', 'after', '12', 'hours', '(', 'i', 'take', 'them', 'at', '5', 'am', ')', '.', 'that', 'said', ',', 'when', 'our', 'conversation', 'in', 'the', 'car', 'is', 'being', 'recorded', ',', 'my', 'add', 'is', 'blazing', '.', 'if', 'you', 'think', 'you', \"'\", 'd', 'be', 'interested', 'in', 'such', 'a', 'series', ',', 'one', 'that', 'i', \"'\", 'm', 'sure', 'many', 'of', 'you', 'could', 'relate', 'to', ',', 'i', \"'\", 'd', 'love', 'for', 'you', 'to', 'sub', '##scribe', 'if', 'you', 'have', 'a', 'youtube', '/', 'gma', '##il', 'account', '.', 'i', \"'\", 'd', 'really', 'appreciate', 'the', 'support', '.', 'if', 'not', ',', 'cool', '.', 'if', 'this', 'red', '##dit', 'is', 'annoying', ',', 'let', 'me', 'know', '.', 'i', \"'\", 'll', 'get', 'rid', 'of', 'it', '.', 'thanks', '!', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'the', '##fr', '##ien', '##dly', '##gree', '##k']\n",
      "INFO:__main__:Number of tokens: 204\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'youtube', '##r', 'hi', '!', 'please', 'let', 'me', 'know', 'if', 'this', 'is', 'considered', 'spa', '##mming', 'and', 'i', \"'\", 'll', 'remove', 'it', 'immediately', '.', 'a', 'while', 'back', 'i', 'started', 'a', 'youtube', 'channel', 'that', 'has', 'evolved', 'into', 'a', 'v', '##log', 'of', 'my', 'drive', 'home', 'after', 'work', '(', 'drive', 'at', 'five', ')', 'with', 'my', 'girlfriend', '.', 'seeing', 'how', 'this', 'is', 'a', 'ad', '##hd', 'category', ',', 'i', \"'\", 'm', 'pretty', 'sure', 'most', 'of', 'you', 'would', 'know', 'that', 'add', '##eral', '##l', 'fades', 'off', 'after', '12', 'hours', '(', 'i', 'take', 'them', 'at', '5', 'am', ')', '.', 'that', 'said', ',', 'when', 'our', 'conversation', 'in', 'the', 'car', 'is', 'being', 'recorded', ',', 'my', 'add', 'is', 'blazing', '.', 'if', 'you', 'think', 'you', \"'\", 'd', 'be', 'interested', 'in', 'such', 'a', 'series', ',', 'one', 'that', 'i', \"'\", 'm', 'sure', 'many', 'of', 'you', 'could', 'relate', 'to', ',', 'i', \"'\", 'd', 'love', 'for', 'you', 'to', 'sub', '##scribe', 'if', 'you', 'have', 'a', 'youtube', '/', 'gma', '##il', 'account', '.', 'i', \"'\", 'd', 'really', 'appreciate', 'the', 'support', '.', 'if', 'not', ',', 'cool', '.', 'if', 'this', 'red', '##dit', 'is', 'annoying', ',', 'let', 'me', 'know', '.', 'i', \"'\", 'll', 'get', 'rid', 'of', 'it', '.', 'thanks', '!', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'the', '##fr', '##ien', '##dly', '##gree', '##k']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'for', 'kids', ':', 'increase', 'risk', 'of', 'subsequent', 'drug', 'abuse', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'for', 'kids', ':', 'increase', 'risk', 'of', 'subsequent', 'drug', 'abuse', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['public', 'service', 'announcement', ':', 'ad', '##hd', ',', 'alcohol', 'and', 'add', '##eral', '##l']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['public', 'service', 'announcement', ':', 'ad', '##hd', ',', 'alcohol', 'and', 'add', '##eral', '##l']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['university', 'student', 'with', 'ad', '##hd', 'looking', 'to', 'improve', 'work', 'habits', 'without', 'medication', '.', 'any', 'tips', '?']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['university', 'student', 'with', 'ad', '##hd', 'looking', 'to', 'improve', 'work', 'habits', 'without', 'medication', '.', 'any', 'tips', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', 'avoid', 'certain', 'foods', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', 'avoid', 'certain', 'foods', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'feel', 'they', 'get', 'little', 'to', 'no', 'satisfaction', 'upon', 'completion', 'of', 'some', 'mundane', 'task', '-', 'dishes', ',', 'washing', 'car', ',', 'et', 'al', ',', 'whereas', 'those', 'without', 'ad', '##hd', 'are', 'satisfied', 'having', 'completed', 'something', ',', 'however', 'trivial', '?', 'my', 'g', '/', 'f', 'thinks', 'it', \"'\", 's', 'funny', 'how', 'i', 'always', 'leave', 'a', 'few', 'dishes', 'un', '##washed', 'in', 'the', 'sink', '.', 'when', 'she', 'asks', 'me', 'why', ',', 'i', 'explain', 'to', 'her', 'the', 'nature', 'of', 'people', 'not', 'really', 'having', 'access', 'to', 'the', 'reasons', 'why', 'they', 'do', 'things', '.']\n",
      "INFO:__main__:Number of tokens: 86\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'feel', 'they', 'get', 'little', 'to', 'no', 'satisfaction', 'upon', 'completion', 'of', 'some', 'mundane', 'task', '-', 'dishes', ',', 'washing', 'car', ',', 'et', 'al', ',', 'whereas', 'those', 'without', 'ad', '##hd', 'are', 'satisfied', 'having', 'completed', 'something', ',', 'however', 'trivial', '?', 'my', 'g', '/', 'f', 'thinks', 'it', \"'\", 's', 'funny', 'how', 'i', 'always', 'leave', 'a', 'few', 'dishes', 'un', '##washed', 'in', 'the', 'sink', '.', 'when', 'she', 'asks', 'me', 'why', ',', 'i', 'explain', 'to', 'her', 'the', 'nature', 'of', 'people', 'not', 'really', 'having', 'access', 'to', 'the', 'reasons', 'why', 'they', 'do', 'things', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'tips', 'on', 'safe', 'ways', 'to', 'fi', '##dget', 'while', 'doing', 'passive', 'work', '?', 'in', 'my', 'current', 'job', ',', 'a', 'portion', 'of', 'my', 'week', 'has', 'recently', 'been', 'tasked', 'with', 'monitoring', 'call', 'center', 'calls', 'to', 'find', 'areas', 'for', 'improvement', 'in', 'procedure', '.', 'my', 'boss', 'can', \"'\", 't', 'really', 'grasp', 'how', 'passive', '##ly', 'listening', 'to', 'something', 'and', 'staying', 'engaged', 'is', 'pure', 'hell', 'for', 'me', '.', 'i', \"'\", 've', 'been', 'doo', '##dling', 'and', 'sometimes', 'typing', 'out', 'the', 'recording', 'as', 'i', 'listen', '.', 'both', 'help', 'a', 'little', ',', 'but', 'i', 'need', 'some', 'more', 'ideas', '.', 'this', 'is', 'painful', '.']\n",
      "INFO:__main__:Number of tokens: 94\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'tips', 'on', 'safe', 'ways', 'to', 'fi', '##dget', 'while', 'doing', 'passive', 'work', '?', 'in', 'my', 'current', 'job', ',', 'a', 'portion', 'of', 'my', 'week', 'has', 'recently', 'been', 'tasked', 'with', 'monitoring', 'call', 'center', 'calls', 'to', 'find', 'areas', 'for', 'improvement', 'in', 'procedure', '.', 'my', 'boss', 'can', \"'\", 't', 'really', 'grasp', 'how', 'passive', '##ly', 'listening', 'to', 'something', 'and', 'staying', 'engaged', 'is', 'pure', 'hell', 'for', 'me', '.', 'i', \"'\", 've', 'been', 'doo', '##dling', 'and', 'sometimes', 'typing', 'out', 'the', 'recording', 'as', 'i', 'listen', '.', 'both', 'help', 'a', 'little', ',', 'but', 'i', 'need', 'some', 'more', 'ideas', '.', 'this', 'is', 'painful', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['diagnosed', 'as', '\"', 'mild', '\"', 'ad', '##hd', '-', 'pi']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['diagnosed', 'as', '\"', 'mild', '\"', 'ad', '##hd', '-', 'pi']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'ad', '##hd', '?', '(', 'a', 'photo', 'mont', '##age', ')']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'ad', '##hd', '?', '(', 'a', 'photo', 'mont', '##age', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['great', 'advice', 'from', 'a', 'd', '##ys', '##le', '##xia', 'blog']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['great', 'advice', 'from', 'a', 'd', '##ys', '##le', '##xia', 'blog']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['help', 'with', 'add', '##eral', '##l', 'and', 'timed', 'tests', 'hi', 'everyone', ',', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'add', 'for', 'almost', '20', 'years', '.', 'i', 'have', 'my', 'ma', 'exams', 'in', 'political', 'science', 'in', 'two', 'weeks', '(', 'com', '##ps', 'that', 'determine', 'whether', 'i', 'get', 'a', 'degree', 'or', 'not', ')', '.', 'each', 'exam', '(', 'i', \"'\", 'm', 'taking', '3', 'in', '2', 'weeks', ')', 'is', '48', 'hours', 'long', 'and', 'have', 'to', 'write', '20', '-', '30', 'pages', 'per', 'exam', '.', 'i', 'take', 'my', 'med', '##s', 'in', 'the', 'morning', ',', 'but', 'i', 'feel', 'like', 'i', 'can', \"'\", 't', 'focus', 'without', 'the', 'med', '##s', ',', 'so', 'i', \"'\", 've', 'been', 'taking', 'them', 'around', 'noon', 'in', 'the', 'day', 'and', 'supplement', '##ing', 'coffee', 'around', '6', 'or', '7', ',', 'but', 'i', 'feel', 'by', 'the', 'time', '8', ':', '30', 'rolls', 'around', 'i', 'become', 'kind', 'of', 'useless', 'for', 'this', 'type', 'of', 'work', '.', 'i', 'really', 'need', 'some', 'suggestions', 'for', 'keeping', 'focused', 'because', 'i', 'don', \"'\", 't', 'want', 'to', 'fail', 'these', 'exams', '.', 't', '##l', ';', 'dr', 'i', 'need', 'to', 'find', 'some', 'way', 'to', 'keep', 'focused', 'off', 'my', 'med', '##s', 'to', 'get', 'my', 'ma', 'degree', '.', 'do', 'you', 'have', 'any', 'suggestions', '?', 'i', \"'\", 'm', 'about', '24', 'years', 'old', '.']\n",
      "INFO:__main__:Number of tokens: 197\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['help', 'with', 'add', '##eral', '##l', 'and', 'timed', 'tests', 'hi', 'everyone', ',', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'add', 'for', 'almost', '20', 'years', '.', 'i', 'have', 'my', 'ma', 'exams', 'in', 'political', 'science', 'in', 'two', 'weeks', '(', 'com', '##ps', 'that', 'determine', 'whether', 'i', 'get', 'a', 'degree', 'or', 'not', ')', '.', 'each', 'exam', '(', 'i', \"'\", 'm', 'taking', '3', 'in', '2', 'weeks', ')', 'is', '48', 'hours', 'long', 'and', 'have', 'to', 'write', '20', '-', '30', 'pages', 'per', 'exam', '.', 'i', 'take', 'my', 'med', '##s', 'in', 'the', 'morning', ',', 'but', 'i', 'feel', 'like', 'i', 'can', \"'\", 't', 'focus', 'without', 'the', 'med', '##s', ',', 'so', 'i', \"'\", 've', 'been', 'taking', 'them', 'around', 'noon', 'in', 'the', 'day', 'and', 'supplement', '##ing', 'coffee', 'around', '6', 'or', '7', ',', 'but', 'i', 'feel', 'by', 'the', 'time', '8', ':', '30', 'rolls', 'around', 'i', 'become', 'kind', 'of', 'useless', 'for', 'this', 'type', 'of', 'work', '.', 'i', 'really', 'need', 'some', 'suggestions', 'for', 'keeping', 'focused', 'because', 'i', 'don', \"'\", 't', 'want', 'to', 'fail', 'these', 'exams', '.', 't', '##l', ';', 'dr', 'i', 'need', 'to', 'find', 'some', 'way', 'to', 'keep', 'focused', 'off', 'my', 'med', '##s', 'to', 'get', 'my', 'ma', 'degree', '.', 'do', 'you', 'have', 'any', 'suggestions', '?', 'i', \"'\", 'm', 'about', '24', 'years', 'old', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'year', 'engineering', 'student', '.', 'feeling', 'like', 'i', 'am', 'totally', 'lost', '.', 'any', 'advice', '?', 'hello', 'everyone', ',', 'as', 'a', 'preface', ',', 'sorry', 'if', 'i', 'miss', 'any', 'forum', 'et', '##ique', '##tte', 'in', 'this', 'post', '.', 'although', 'i', 'have', 'spent', 'the', 'last', 'few', 'hours', 'reading', 'here', ',', 'i', 'am', 'still', 'new', '.', 'also', ',', 'sorry', 'for', 'the', 'massive', 'post', ',', 'i', 'have', 'a', 'lot', 'on', 'my', 'mind', '.', 'before', 'i', 'jump', 'into', 'my', 'current', 'problem', ',', 'let', 'me', 'fill', 'you', 'in', 'on', 'a', 'bit', 'of', 'my', 'background', '.', 'through', 'grade', 'school', 'and', 'for', 'the', 'first', 'half', 'of', 'high', 'school', ',', 'i', 'always', 'had', 'stellar', 'marks', '.', 'generally', 'very', 'near', ',', 'or', 'at', 'the', 'top', 'of', 'my', 'class', '.', 'i', 'never', 'studied', ',', 'never', 'worked', ',', 'in', 'all', 'honesty', 'i', 'just', 'got', 'by', 'on', 'intuition', '.', 'everyone', 'else', 'in', 'my', 'family', 'has', 'been', 'very', 'successful', ',', 'and', 'as', 'the', 'youngest', 'one', ',', 'my', 'success', 'came', 'as', 'no', 'surprise', '.', 'this', 'worked', 'great', 'up', 'until', 'about', 'grade', '11', '.', 'at', 'this', 'point', ',', 'things', 'started', 'to', 'change', 'in', 'a', 'way', 'that', 'actual', 'work', 'and', 'follow', '-', 'through', 'was', 'required', 'by', 'me', 'to', 'maintain', 'my', 'top', 'marks', '.', 'unfortunately', 'i', 'didn', \"'\", 't', '.', 'my', 'marks', 'were', 'still', 'solid', ',', 'but', 'between', 'grade', '10', 'and', '12', ',', 'they', 'slipped', 'from', 'mid', '90s', 'to', 'low', '80s', '.', 'i', 'wanted', 'to', 'go', 'to', 'university', 'for', 'engineering', ',', 'and', 'applied', 'to', 'a', 'number', 'of', 'top', 'schools', '(', 'i', \"'\", 'm', 'in', 'canada', ')', '.', 'as', 'it', 'turned', 'out', ',', 'my', 'marks', 'weren', \"'\", 't', 'enough', 'for', 'all', 'of', 'the', 'schools', 'i', 'really', 'wanted', 'to', 'go', 'to', ',', 'and', 'i', 'ended', 'up', 'at', 'my', 'safety', 'school', '.', 'my', 'parents', 'were', 'very', 'understanding', 'and', 'supportive', ',', 'but', 'must', 'have', 'been', 'a', 'little', 'surprised', '.', 'no', 'one', 'else', 'in', 'my', 'family', 'ever', 'ended', 'up', 'at', 'anywhere', 'but', 'their', 'top', 'choice', '.', 'my', 'first', 'semester', 'was', 'pretty', 'rough', '.', 'i', 'started', 'in', '6', 'courses', ',', 'finished', 'with', '4', '.', 'of', 'those', '4', ',', 'i', 'passed', '2', 'of', 'them', '.', 'i', 'might', 'have', 'a', 'chance', 'to', 'pass', 'a', 'third', '.', 'the', 'way', 'the', 'exam', 'structure', 'works', 'at', 'my', 'school', 'is', 'if', 'you', 'score', '45', '-', '50', '%', 'in', 'the', 'course', ',', 'you', 'have', 'a', 'chance', 'to', 're', '-', 'write', 'the', 'final', '.', 'this', 're', '-', 'write', 'is', 'happening', 'in', 'about', '2', 'weeks', ',', 'and', 'i', 'am', 'hoping', 'to', 'salvage', 'a', '3', '/', '4', 'semester', '.', 'i', 'came', 'to', 'this', 'semester', 'with', 'a', 'hope', 'to', 'start', 'fresh', ',', 'but', 'it', 'is', 'so', 'far', 'looking', 'even', 'worse', 'then', 'the', 'first', '.', 'i', 'am', 'currently', 'enrolled', 'in', '5', 'courses', ',', 'but', 'will', 'probably', 'drop', 'on', 'in', 'the', 'next', 'few', 'weeks', '.', 'i', 'have', 'had', '3', 'mid', '##ter', '##ms', 'so', 'far', ',', 'all', 'of', 'which', 'i', 'have', 'failed', '.', 'i', 'have', 'another', 'tomorrow', 'evening', ',', 'which', 'i', 'am', 'really', 'lost', 'for', '.', 'half', '-', 'way', 'through', 'my', 'first', 'semester', ',', 'i', 'decided', 'to', 'seek', 'out', 'professional', 'help', ',', 'and', 'after', 'meeting', 'with', 'a', 'series', 'of', 'doctors', ',', 'was', 'prescribed', 'con', '##cre', '##ta', '.', 'although', 'i', 'feel', 'like', 'it', 'helps', 'a', 'little', 'bit', ',', 'it', 'is', 'certainly', 'not', 'making', 'the', 'difference', 'i', 'need', '.', 'i', 'met', 'with', 'my', 'psychiatrist', 'yesterday', ',', 'and', 'he', 'added', 'a', 'drug', 'called', 'well', '##bu', '##rti', '##n', 'on', 'top', 'of', 'my', 'current', 'dose', 'of', 'concert', '##a', '(', 'current', 'at', '54', '##mg', ')', '.', 'i', 'am', 'seeing', 'him', 'again', 'next', 'week', 'to', 'follow', 'up', 'on', 'its', 'effectiveness', '.', 'it', \"'\", 's', 'very', 'hard', 'for', 'me', 'to', 'art', '##iculate', 'the', 'problems', 'i', \"'\", 'm', 'having', ',', 'but', 'it', 'seems', 'mostly', 'to', 'be', 'a', 'total', 'lack', 'of', 'self', 'discipline', 'and', 'motivation', '.', 'most', 'weeks', 'i', 'will', 'only', 'make', 'it', 'to', '1', 'or', '2', 'classes', 'out', 'of', '~', '15', '.', 'until', 'this', 'year', ',', 'i', 'had', 'never', 'studied', 'for', 'a', 'test', 'before', '.', 'for', 'my', 'first', 'round', 'of', 'finals', ',', 'i', 'managed', 'to', 'make', 'it', 'through', 'about', 'an', 'hour', 'of', 'chemistry', 'review', ',', 'but', 'that', 'was', 'only', 'on', 'something', 'like', 'a', 'triple', 'dose', 'of', 'my', 'med', '##s', 'at', 'the', 'time', '.', 'i', 'was', 'uncomfortable', 'with', 'that', 'solution', ',', 'and', 'haven', \"'\", 't', 'repeated', 'it', 'since', '.', 'generally', 'when', 'i', 'try', 'to', 'talk', 'to', 'people', 'about', 'this', ',', 'they', 'seem', 'to', 'think', 'that', 'i', 'am', 'just', 'lazy', 'and', 'don', \"'\", 't', 'care', ',', 'but', 'this', 'is', 'seriously', 'not', 'the', 'case', '!', 'i', 'am', 'honestly', 'starting', 'to', 'become', 'terrified', 'about', 'the', 'prospect', 'of', 'failing', 'out', '.', 'i', 'genuinely', 'want', 'to', 'work', ',', 'i', 'wish', 'i', 'could', '.', 'i', 'can', \"'\", 't', 'imagine', 'what', 'i', 'will', 'do', 'if', 'i', 'fail', 'all', 'my', 'courses', 'this', 'semester', '.', 'more', 'then', 'anything', 'else', ',', 'i', 'just', 'want', 'a', 'week', 'of', 'what', 'seems', 'to', 'be', 'the', 'normal', 'university', 'lifestyle', '.', 'go', 'to', 'all', 'my', 'classes', ',', 'study', 'for', 'an', 'hour', 'or', 'two', 'every', 'night', '.', 'it', 'seems', 'impossible', 'for', 'me', '.', 'i', 'know', 'it', 'is', 'probably', 'a', 'vague', 'question', ',', 'but', 'has', 'anyone', 'experienced', 'problems', 'similar', 'to', 'mine', 'and', 'figured', 'out', 'how', 'to', 'overcome', 'them', '?', 'i', 'am', 'really', 'out', 'of', 'ideas', 'at', 'this', 'point', '.', 'after', 'talking', 'to', 'my', 'psychiatrist', 'yesterday', ',', 'i', 'am', 'going', 'to', 'look', 'into', 'something', 'like', 'life', 'coaching', 'or', 'therapy', ',', 'apparently', 'there', 'is', 'a', 'center', 'for', 'this', 'on', 'campus', '.', 'thanks', 'so', 'much', 'for', 'any', 'help', 'you', 'can', 'provide', '!', 't', '##l', ';', 'dr', '-', 'i', 'feel', 'totally', 'lost', 'and', 'unable', 'to', 'mo', '##tiv', '##ate', 'myself', 'to', 'study', 'or', 'go', 'to', 'class', '.', 'it', \"'\", 's', 'not', 'because', 'i', \"'\", 'm', 'lazy', ',', 'i', \"'\", 'm', 'starting', 'to', 'become', 'pretty', 'terrified', 'of', 'the', 'reality', 'of', 'failing', '.']\n",
      "INFO:__main__:Number of tokens: 929\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['first', 'year', 'engineering', 'student', '.', 'feeling', 'like', 'i', 'am', 'totally', 'lost', '.', 'any', 'advice', '?', 'hello', 'everyone', ',', 'as', 'a', 'preface', ',', 'sorry', 'if', 'i', 'miss', 'any', 'forum', 'et', '##ique', '##tte', 'in', 'this', 'post', '.', 'although', 'i', 'have', 'spent', 'the', 'last', 'few', 'hours', 'reading', 'here', ',', 'i', 'am', 'still', 'new', '.', 'also', ',', 'sorry', 'for', 'the', 'massive', 'post', ',', 'i', 'have', 'a', 'lot', 'on', 'my', 'mind', '.', 'before', 'i', 'jump', 'into', 'my', 'current', 'problem', ',', 'let', 'me', 'fill', 'you', 'in', 'on', 'a', 'bit', 'of', 'my', 'background', '.', 'through', 'grade', 'school', 'and', 'for', 'the', 'first', 'half', 'of', 'high', 'school', ',', 'i', 'always', 'had', 'stellar', 'marks', '.', 'generally', 'very', 'near', ',', 'or', 'at', 'the', 'top', 'of', 'my', 'class', '.', 'i', 'never', 'studied', ',', 'never', 'worked', ',', 'in', 'all', 'honesty', 'i', 'just', 'got', 'by', 'on', 'intuition', '.', 'everyone', 'else', 'in', 'my', 'family', 'has', 'been', 'very', 'successful', ',', 'and', 'as', 'the', 'youngest', 'one', ',', 'my', 'success', 'came', 'as', 'no', 'surprise', '.', 'this', 'worked', 'great', 'up', 'until', 'about', 'grade', '11', '.', 'at', 'this', 'point', ',', 'things', 'started', 'to', 'change', 'in', 'a', 'way', 'that', 'actual', 'work', 'and', 'follow', '-', 'through', 'was', 'required', 'by', 'me', 'to', 'maintain', 'my', 'top', 'marks', '.', 'unfortunately', 'i', 'didn', \"'\", 't', '.', 'my', 'marks', 'were', 'still', 'solid', ',', 'but', 'between', 'grade', '10', 'and', '12', ',', 'they', 'slipped', 'from', 'mid', '90s', 'to', 'low', '80s', '.', 'i', 'wanted', 'to', 'go', 'to', 'university', 'for', 'engineering', ',', 'and', 'applied', 'to', 'a', 'number', 'of', 'top', 'schools', '(', 'i', \"'\", 'm', 'in', 'canada', ')', '.', 'as', 'it', 'turned', 'out', ',', 'my', 'marks', 'weren', \"'\", 't', 'enough', 'for', 'all', 'of', 'the', 'schools', 'i', 'really', 'wanted', 'to', 'go', 'to', ',', 'and', 'i', 'ended', 'up', 'at', 'my', 'safety', 'school', '.', 'my', 'parents', 'were', 'very', 'understanding', 'and', 'supportive', ',', 'but', 'must', 'have', 'been', 'a', 'little', 'surprised', '.', 'no', 'one', 'else', 'in', 'my', 'family', 'ever', 'ended', 'up', 'at', 'anywhere', 'but', 'their', 'top', 'choice', '.', 'my', 'first', 'semester', 'was', 'pretty', 'rough', '.', 'i', 'started', 'in', '6', 'courses', ',', 'finished', 'with', '4', '.', 'of', 'those', '4', ',', 'i', 'passed', '2', 'of', 'them', '.', 'i', 'might', 'have', 'a', 'chance', 'to', 'pass', 'a', 'third', '.', 'the', 'way', 'the', 'exam', 'structure', 'works', 'at', 'my', 'school', 'is', 'if', 'you', 'score', '45', '-', '50', '%', 'in', 'the', 'course', ',', 'you', 'have', 'a', 'chance', 'to', 're', '-', 'write', 'the', 'final', '.', 'this', 're', '-', 'write', 'is', 'happening', 'in', 'about', '2', 'weeks', ',', 'and', 'i', 'am', 'hoping', 'to', 'salvage', 'a', '3', '/', '4', 'semester', '.', 'i', 'came', 'to', 'this', 'semester', 'with', 'a', 'hope', 'to', 'start', 'fresh', ',', 'but', 'it', 'is', 'so', 'far', 'looking', 'even', 'worse', 'then', 'the', 'first', '.', 'i', 'am', 'currently', 'enrolled', 'in', '5', 'courses', ',', 'but', 'will', 'probably', 'drop', 'on', 'in', 'the', 'next', 'few', 'weeks', '.', 'i', 'have', 'had', '3', 'mid', '##ter', '##ms', 'so', 'far', ',', 'all', 'of', 'which', 'i', 'have', 'failed', '.', 'i', 'have', 'another', 'tomorrow', 'evening', ',', 'which', 'i', 'am', 'really', 'lost', 'for', '.', 'half', '-', 'way', 'through', 'my', 'first', 'semester', ',', 'i', 'decided', 'to', 'seek', 'out', 'professional', 'help', ',', 'and', 'after', 'meeting', 'with', 'a', 'series', 'of', 'doctors', ',', 'was', 'prescribed', 'con', '##cre', '##ta', '.', 'although', 'i', 'feel', 'like', 'it'], ['helps', 'a', 'little', 'bit', ',', 'it', 'is', 'certainly', 'not', 'making', 'the', 'difference', 'i', 'need', '.', 'i', 'met', 'with', 'my', 'psychiatrist', 'yesterday', ',', 'and', 'he', 'added', 'a', 'drug', 'called', 'well', '##bu', '##rti', '##n', 'on', 'top', 'of', 'my', 'current', 'dose', 'of', 'concert', '##a', '(', 'current', 'at', '54', '##mg', ')', '.', 'i', 'am', 'seeing', 'him', 'again', 'next', 'week', 'to', 'follow', 'up', 'on', 'its', 'effectiveness', '.', 'it', \"'\", 's', 'very', 'hard', 'for', 'me', 'to', 'art', '##iculate', 'the', 'problems', 'i', \"'\", 'm', 'having', ',', 'but', 'it', 'seems', 'mostly', 'to', 'be', 'a', 'total', 'lack', 'of', 'self', 'discipline', 'and', 'motivation', '.', 'most', 'weeks', 'i', 'will', 'only', 'make', 'it', 'to', '1', 'or', '2', 'classes', 'out', 'of', '~', '15', '.', 'until', 'this', 'year', ',', 'i', 'had', 'never', 'studied', 'for', 'a', 'test', 'before', '.', 'for', 'my', 'first', 'round', 'of', 'finals', ',', 'i', 'managed', 'to', 'make', 'it', 'through', 'about', 'an', 'hour', 'of', 'chemistry', 'review', ',', 'but', 'that', 'was', 'only', 'on', 'something', 'like', 'a', 'triple', 'dose', 'of', 'my', 'med', '##s', 'at', 'the', 'time', '.', 'i', 'was', 'uncomfortable', 'with', 'that', 'solution', ',', 'and', 'haven', \"'\", 't', 'repeated', 'it', 'since', '.', 'generally', 'when', 'i', 'try', 'to', 'talk', 'to', 'people', 'about', 'this', ',', 'they', 'seem', 'to', 'think', 'that', 'i', 'am', 'just', 'lazy', 'and', 'don', \"'\", 't', 'care', ',', 'but', 'this', 'is', 'seriously', 'not', 'the', 'case', '!', 'i', 'am', 'honestly', 'starting', 'to', 'become', 'terrified', 'about', 'the', 'prospect', 'of', 'failing', 'out', '.', 'i', 'genuinely', 'want', 'to', 'work', ',', 'i', 'wish', 'i', 'could', '.', 'i', 'can', \"'\", 't', 'imagine', 'what', 'i', 'will', 'do', 'if', 'i', 'fail', 'all', 'my', 'courses', 'this', 'semester', '.', 'more', 'then', 'anything', 'else', ',', 'i', 'just', 'want', 'a', 'week', 'of', 'what', 'seems', 'to', 'be', 'the', 'normal', 'university', 'lifestyle', '.', 'go', 'to', 'all', 'my', 'classes', ',', 'study', 'for', 'an', 'hour', 'or', 'two', 'every', 'night', '.', 'it', 'seems', 'impossible', 'for', 'me', '.', 'i', 'know', 'it', 'is', 'probably', 'a', 'vague', 'question', ',', 'but', 'has', 'anyone', 'experienced', 'problems', 'similar', 'to', 'mine', 'and', 'figured', 'out', 'how', 'to', 'overcome', 'them', '?', 'i', 'am', 'really', 'out', 'of', 'ideas', 'at', 'this', 'point', '.', 'after', 'talking', 'to', 'my', 'psychiatrist', 'yesterday', ',', 'i', 'am', 'going', 'to', 'look', 'into', 'something', 'like', 'life', 'coaching', 'or', 'therapy', ',', 'apparently', 'there', 'is', 'a', 'center', 'for', 'this', 'on', 'campus', '.', 'thanks', 'so', 'much', 'for', 'any', 'help', 'you', 'can', 'provide', '!', 't', '##l', ';', 'dr', '-', 'i', 'feel', 'totally', 'lost', 'and', 'unable', 'to', 'mo', '##tiv', '##ate', 'myself', 'to', 'study', 'or', 'go', 'to', 'class', '.', 'it', \"'\", 's', 'not', 'because', 'i', \"'\", 'm', 'lazy', ',', 'i', \"'\", 'm', 'starting', 'to', 'become', 'pretty', 'terrified', 'of', 'the', 'reality', 'of', 'failing', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'some', 'positive', 'ways', 'you', \"'\", 've', 'been', 'affected', 'by', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'some', 'positive', 'ways', 'you', \"'\", 've', 'been', 'affected', 'by', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['are', 'my', 'symptoms', 'common', 'of', 'ad', '##hd', '-', 'pi', '?', 'hey', 'guys', ',', 'lately', 'i', \"'\", 've', 'been', 'worried', 'that', 'i', 'may', 'have', 'ad', '##hd', '-', 'pi', ',', 'my', 'symptoms', 'have', 'existed', 'since', 'the', 'beginning', 'of', 'high', 'school', ',', 'when', 'i', 'started', 'to', 'fall', 'behind', 'in', 'school', '.', 'i', 'used', 'to', 'be', 'an', 'extremely', 'accelerated', 'learn', '##er', 'until', 'then', 'and', 'was', 'doing', 'school', 'work', 'of', 'people', '1', '-', '2', 'years', 'older', 'than', 'me', '.', 'i', 'also', 'suffer', 'from', 'depression', 'and', 'general', '##ised', 'anxiety', 'disorder', '.', 'i', \"'\", 'll', 'list', 'some', 'of', 'my', 'symptoms', 'that', 'i', 'think', 'may', 'relate', 'to', 'ad', '##hd', '-', 'pi', '-', 'being', 'extremely', 'focused', 'on', 'one', 'thing', '(', 'this', 'only', 'happens', 'if', 'it', \"'\", 's', 'something', 'i', \"'\", 'm', 'very', 'interested', 'in', ')', 'and', 'not', 'being', 'able', 'to', 'concentrate', 'on', 'anything', 'else', '.', 'if', 'someone', 'talks', 'to', 'me', 'or', 'there', 'is', 'too', 'much', 'noise', ',', 'my', 'head', 'gets', 'very', 'busy', 'and', 'i', 'become', 'frustrated', 'and', 'angry', 'with', 'myself', ',', 'sometimes', 'snapping', 'at', 'someone', 'trying', 'to', 'talk', 'to', 'me', '.', ')', 'i', 'also', 'have', 'trouble', 'stopping', 'the', 'task', 'i', \"'\", 'm', 'interested', 'in', 'to', 'continue', 'with', 'other', 'more', 'important', 'tasks', '.', '-', 'avoidance', 'and', 'ina', '##tten', '##tive', '##ness', 'towards', 'mundane', 'tasks', ',', 'leading', 'to', 'important', 'things', 'not', 'getting', 'done', '.', 'had', 'thought', 'i', 'was', 'just', 'lazy', 'before', 'considering', 'ad', '##hd', '-', 'pi', 'as', 'a', 'possible', 'cause', '.', '-', 'pro', '##cr', '##ast', '##ination', 'and', 'being', 'un', '##mot', '##ivated', '-', 'forget', '##fulness', '(', 'inability', 'to', 'remember', 'names', ',', 'important', 'dates', '(', 'birthday', '##s', ')', ',', 'important', 'things', 'for', 'work', '.', '(', 'although', 'i', 'do', 'have', 'a', 'freak', '##ish', 'memory', 'for', 'certain', 'things', 'like', 'credit', 'card', 'numbers', ',', 'bank', 'account', 'numbers', ',', 'phone', 'numbers', ',', 'addresses', ',', 'but', 'i', 'have', 'short', 'periods', 'where', 'i', 'am', 'unable', 'to', 'remember', 'these', '.', '-', 'emotionally', 'detached', 'at', 'times', ',', 'other', 'times', 'very', 'expressive', 'emotionally', 'and', 'at', 'times', 'i', \"'\", 'll', 'be', 'too', 'honest', 'with', 'people', 'about', 'my', 'emotions', '.', '-', 'careless', '##ness', 'towards', 'keeping', 'in', 'touch', 'with', 'friends', '/', 'family', 'over', 'the', 'phone', '.', '-', 'overwhelming', 'feeling', 'of', 'under', '##achi', '##eve', '##ment', '-', 'perfection', '##ism', '(', 'if', 'things', 'aren', \"'\", 't', 'done', 'perfectly', ',', 'better', 'not', 'to', 'do', 'them', 'at', 'all', ')', '-', 'having', 'difficulty', 'concentrating', 'on', 'plots', 'in', 'movies', ',', 'books', ',', 'video', 'games', '.', '-', 'not', 'finishing', 'tasks', ',', 'distracted', 'easily', '(', 'i', \"'\", 'll', 'start', 'doing', 'something', 'else', 'mid', '-', 'task', ')', '-', 'impatient', '-', 'constantly', 'mis', '##pl', '##acing', 'things', 'or', 'worried', 'i', \"'\", 've', 'forgotten', 'things', 'or', 'not', 'remembering', 'i', 'have', 'done', 'something', '.', 'i', \"'\", 'm', 'going', 'to', 'see', 'a', 'psychiatrist', 'asa', '##p', ',', 'but', 'what', 'do', 'you', 'guys', 'think', '?']\n",
      "INFO:__main__:Number of tokens: 442\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['are', 'my', 'symptoms', 'common', 'of', 'ad', '##hd', '-', 'pi', '?', 'hey', 'guys', ',', 'lately', 'i', \"'\", 've', 'been', 'worried', 'that', 'i', 'may', 'have', 'ad', '##hd', '-', 'pi', ',', 'my', 'symptoms', 'have', 'existed', 'since', 'the', 'beginning', 'of', 'high', 'school', ',', 'when', 'i', 'started', 'to', 'fall', 'behind', 'in', 'school', '.', 'i', 'used', 'to', 'be', 'an', 'extremely', 'accelerated', 'learn', '##er', 'until', 'then', 'and', 'was', 'doing', 'school', 'work', 'of', 'people', '1', '-', '2', 'years', 'older', 'than', 'me', '.', 'i', 'also', 'suffer', 'from', 'depression', 'and', 'general', '##ised', 'anxiety', 'disorder', '.', 'i', \"'\", 'll', 'list', 'some', 'of', 'my', 'symptoms', 'that', 'i', 'think', 'may', 'relate', 'to', 'ad', '##hd', '-', 'pi', '-', 'being', 'extremely', 'focused', 'on', 'one', 'thing', '(', 'this', 'only', 'happens', 'if', 'it', \"'\", 's', 'something', 'i', \"'\", 'm', 'very', 'interested', 'in', ')', 'and', 'not', 'being', 'able', 'to', 'concentrate', 'on', 'anything', 'else', '.', 'if', 'someone', 'talks', 'to', 'me', 'or', 'there', 'is', 'too', 'much', 'noise', ',', 'my', 'head', 'gets', 'very', 'busy', 'and', 'i', 'become', 'frustrated', 'and', 'angry', 'with', 'myself', ',', 'sometimes', 'snapping', 'at', 'someone', 'trying', 'to', 'talk', 'to', 'me', '.', ')', 'i', 'also', 'have', 'trouble', 'stopping', 'the', 'task', 'i', \"'\", 'm', 'interested', 'in', 'to', 'continue', 'with', 'other', 'more', 'important', 'tasks', '.', '-', 'avoidance', 'and', 'ina', '##tten', '##tive', '##ness', 'towards', 'mundane', 'tasks', ',', 'leading', 'to', 'important', 'things', 'not', 'getting', 'done', '.', 'had', 'thought', 'i', 'was', 'just', 'lazy', 'before', 'considering', 'ad', '##hd', '-', 'pi', 'as', 'a', 'possible', 'cause', '.', '-', 'pro', '##cr', '##ast', '##ination', 'and', 'being', 'un', '##mot', '##ivated', '-', 'forget', '##fulness', '(', 'inability', 'to', 'remember', 'names', ',', 'important', 'dates', '(', 'birthday', '##s', ')', ',', 'important', 'things', 'for', 'work', '.', '(', 'although', 'i', 'do', 'have', 'a', 'freak', '##ish', 'memory', 'for', 'certain', 'things', 'like', 'credit', 'card', 'numbers', ',', 'bank', 'account', 'numbers', ',', 'phone', 'numbers', ',', 'addresses', ',', 'but', 'i', 'have', 'short', 'periods', 'where', 'i', 'am', 'unable', 'to', 'remember', 'these', '.', '-', 'emotionally', 'detached', 'at', 'times', ',', 'other', 'times', 'very', 'expressive', 'emotionally', 'and', 'at', 'times', 'i', \"'\", 'll', 'be', 'too', 'honest', 'with', 'people', 'about', 'my', 'emotions', '.', '-', 'careless', '##ness', 'towards', 'keeping', 'in', 'touch', 'with', 'friends', '/', 'family', 'over', 'the', 'phone', '.', '-', 'overwhelming', 'feeling', 'of', 'under', '##achi', '##eve', '##ment', '-', 'perfection', '##ism', '(', 'if', 'things', 'aren', \"'\", 't', 'done', 'perfectly', ',', 'better', 'not', 'to', 'do', 'them', 'at', 'all', ')', '-', 'having', 'difficulty', 'concentrating', 'on', 'plots', 'in', 'movies', ',', 'books', ',', 'video', 'games', '.', '-', 'not', 'finishing', 'tasks', ',', 'distracted', 'easily', '(', 'i', \"'\", 'll', 'start', 'doing', 'something', 'else', 'mid', '-', 'task', ')', '-', 'impatient', '-', 'constantly', 'mis', '##pl', '##acing', 'things', 'or', 'worried', 'i', \"'\", 've', 'forgotten', 'things', 'or', 'not', 'remembering', 'i', 'have', 'done', 'something', '.', 'i', \"'\", 'm', 'going', 'to', 'see', 'a', 'psychiatrist', 'asa', '##p', ',', 'but', 'what', 'do', 'you', 'guys', 'think', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['another', 'ad', '##hd', 'isn', \"'\", 't', 'real', 'ran', '##t', '.', '.', '.', ':', '/', 'ask', '##red', '##dit', 'had', 'a', '[', 'post', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ask', '##red', '##dit', '/', 'comments', '/', 'ps', '##9', '##j', '##7', '/', 'what', '_', 'is', '_', 'one', '_', 'medical', '_', 'condition', '_', 'that', '_', 'someone', '_', 'you', '/', ')', 'that', 'i', 'clicked', 'on', 'solely', 'because', 'i', 'knew', 'it', 'would', 'say', 'something', 'about', 'ad', '##hd', '.', 'well', ',', 'i', 'found', 'several', '.', 'at', 'least', 'it', 'wasn', \"'\", 't', 'the', 'top', 'comment', 'or', 'something', ',', 'but', 'there', 'were', 'a', 'few', 'and', 'conversations', 'about', 'it', '.', '\"', 'ad', '##hd', 'isn', \"'\", 't', 'real', 'and', 'it', \"'\", 's', 'just', 'bad', 'parenting', '.', 'you', 'don', \"'\", 't', 'need', 'medication', ',', 'you', 'don', \"'\", 't', 'need', 'anything', '.', 'you', 'need', 'to', 'get', 'off', 'your', 'ass', 'and', 'do', 'some', 'god', 'damn', 'work', '.', '\"', '-', 'majority', 'only', 'one', 'i', 'saw', 'that', 'was', 'for', 'ad', '##hd', 'being', 'a', 'real', 'life', 'cr', '##ip', '##pling', 'disorder', ',', 'was', 'someone', 'who', 'had', '\"', 'subtle', '\"', 'ad', '##hd', '.', 'he', 'just', 'had', 'a', 'hard', 'time', 'stud', '##ing', 'sometimes', 'or', 'some', 'shit', '.', 'not', 'realizing', 'he', 'was', 'contributing', 'to', 'the', 'problem', '.', 'ad', '##hd', 'isn', '##t', 'when', 'you', '\"', 'can', '##t', 'study', 'sometimes', '\"', '.', 'it', \"'\", 's', 'a', 'condition', 'that', \"'\", 's', 'ruined', 'my', 'life', '.', 'i', 'think', 'it', \"'\", 's', 'ridiculous', 'how', 'some', 'people', 'can', 'just', 'go', 'to', 'a', 'doctor', 'and', 'get', 'a', 'prescription', 'for', 'add', '##eral', '##l', '/', 'rita', '##lin', '/', 'whatever', 'so', 'easily', '.', 'i', 'don', \"'\", 't', 'need', 'medication', 'to', 'help', 'me', 'get', 'through', '*', 'simple', 'fucking', 'tasks', '*', '?', 'well', 'then', 'give', 'me', 'your', 'glasses', ',', 'you', 'don', \"'\", 't', 'need', 'them', 'to', 'see', '*', 'simple', 'fucking', 'words', '*', '.', 'fuck', 'you', 'teenagers', 'who', 'bitch', 'about', 'their', '\"', 'ad', '##hd', '\"', 'a', 'week', 'before', 'exams', '.', 'fuck', 'you', 'parents', 'who', 'say', 'their', 'kids', 'have', 'ad', '##hd', 'because', 'they', \"'\", 're', 'hyper', 'and', 'don', \"'\", 't', 'want', 'to', 'go', 'to', 'bed', 'at', '7', '##pm', '.', 'fuck', 'you', 'doctors', 'who', 'let', 'them', 'get', 'away', 'with', 'it', '!', 'so', 'how', 'do', 'you', 'guys', 'feel', '?', 'wanna', 'get', 'anything', 'off', 'your', 'chests', '?']\n",
      "INFO:__main__:Number of tokens: 367\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['another', 'ad', '##hd', 'isn', \"'\", 't', 'real', 'ran', '##t', '.', '.', '.', ':', '/', 'ask', '##red', '##dit', 'had', 'a', '[', 'post', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ask', '##red', '##dit', '/', 'comments', '/', 'ps', '##9', '##j', '##7', '/', 'what', '_', 'is', '_', 'one', '_', 'medical', '_', 'condition', '_', 'that', '_', 'someone', '_', 'you', '/', ')', 'that', 'i', 'clicked', 'on', 'solely', 'because', 'i', 'knew', 'it', 'would', 'say', 'something', 'about', 'ad', '##hd', '.', 'well', ',', 'i', 'found', 'several', '.', 'at', 'least', 'it', 'wasn', \"'\", 't', 'the', 'top', 'comment', 'or', 'something', ',', 'but', 'there', 'were', 'a', 'few', 'and', 'conversations', 'about', 'it', '.', '\"', 'ad', '##hd', 'isn', \"'\", 't', 'real', 'and', 'it', \"'\", 's', 'just', 'bad', 'parenting', '.', 'you', 'don', \"'\", 't', 'need', 'medication', ',', 'you', 'don', \"'\", 't', 'need', 'anything', '.', 'you', 'need', 'to', 'get', 'off', 'your', 'ass', 'and', 'do', 'some', 'god', 'damn', 'work', '.', '\"', '-', 'majority', 'only', 'one', 'i', 'saw', 'that', 'was', 'for', 'ad', '##hd', 'being', 'a', 'real', 'life', 'cr', '##ip', '##pling', 'disorder', ',', 'was', 'someone', 'who', 'had', '\"', 'subtle', '\"', 'ad', '##hd', '.', 'he', 'just', 'had', 'a', 'hard', 'time', 'stud', '##ing', 'sometimes', 'or', 'some', 'shit', '.', 'not', 'realizing', 'he', 'was', 'contributing', 'to', 'the', 'problem', '.', 'ad', '##hd', 'isn', '##t', 'when', 'you', '\"', 'can', '##t', 'study', 'sometimes', '\"', '.', 'it', \"'\", 's', 'a', 'condition', 'that', \"'\", 's', 'ruined', 'my', 'life', '.', 'i', 'think', 'it', \"'\", 's', 'ridiculous', 'how', 'some', 'people', 'can', 'just', 'go', 'to', 'a', 'doctor', 'and', 'get', 'a', 'prescription', 'for', 'add', '##eral', '##l', '/', 'rita', '##lin', '/', 'whatever', 'so', 'easily', '.', 'i', 'don', \"'\", 't', 'need', 'medication', 'to', 'help', 'me', 'get', 'through', '*', 'simple', 'fucking', 'tasks', '*', '?', 'well', 'then', 'give', 'me', 'your', 'glasses', ',', 'you', 'don', \"'\", 't', 'need', 'them', 'to', 'see', '*', 'simple', 'fucking', 'words', '*', '.', 'fuck', 'you', 'teenagers', 'who', 'bitch', 'about', 'their', '\"', 'ad', '##hd', '\"', 'a', 'week', 'before', 'exams', '.', 'fuck', 'you', 'parents', 'who', 'say', 'their', 'kids', 'have', 'ad', '##hd', 'because', 'they', \"'\", 're', 'hyper', 'and', 'don', \"'\", 't', 'want', 'to', 'go', 'to', 'bed', 'at', '7', '##pm', '.', 'fuck', 'you', 'doctors', 'who', 'let', 'them', 'get', 'away', 'with', 'it', '!', 'so', 'how', 'do', 'you', 'guys', 'feel', '?', 'wanna', 'get', 'anything', 'off', 'your', 'chests', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'have', 'an', 'ie', '##p', '?', 'what', 'does', 'it', 'allow', 'you', 'to', 'do', '?', 'did', 'it', 'even', 'help', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'have', 'an', 'ie', '##p', '?', 'what', 'does', 'it', 'allow', 'you', 'to', 'do', '?', 'did', 'it', 'even', 'help', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', 'and', 'imp', '##uls', '##ivity', 'hi', 'i', 'went', 'to', 'see', 'a', 'doctor', 'a', 'month', 'ago', 'to', 'get', 'diagnosed', ',', 'because', 'i', 'have', 'almost', 'all', 'the', 'symptoms', 'of', 'the', 'ina', '##tten', '##tive', 'type', '(', 'i', \"'\", 'm', 'not', 'going', 'to', 'go', 'into', 'details', 'here', ',', 'it', \"'\", 's', 'not', 'the', 'point', ')', ';', 'however', 'she', 'told', 'me', 'that', 'she', 'wasn', \"'\", 't', 'sure', 'i', 'had', 'it', '.', 'she', 'agrees', 'i', 'have', 'some', 'similarities', 'with', 'someone', 'with', 'ad', '##hd', ',', 'but', 'the', 'thing', 'that', 'makes', 'her', 'think', 'it', 'is', 'something', 'else', 'is', 'that', 'i', 'told', 'her', 'i', 'was', 'not', 'imp', '##ulsive', 'at', 'all', ';', 'and', 'it', \"'\", 's', 'true', ',', 'for', 'example', 'i', 'sometimes', 'hesitate', 'for', 'ages', 'before', 'making', 'a', 'simple', 'decision', 'like', 'buying', 'a', 'ts', '##hir', '##t', 'or', 'not', '.', 'i', \"'\", 'd', 'like', 'to', 'know', 'if', 'some', 'of', 'you', 'have', 'been', 'diagnosed', 'with', 'ad', '##hd', ',', 'despite', 'being', 'anything', 'but', 'imp', '##ulsive', '.', 'the', 'reason', 'i', \"'\", 'd', 'like', 'to', 'know', 'is', 'because', 'i', 'am', 'about', 'to', 'call', 'a', 'specialist', 'of', 'ad', '##hd', 'from', 'my', 'hometown', 'to', 'know', 'for', 'sure', '(', 'on', 'the', 'advice', 'of', 'my', 'doctor', ',', 'who', 'believes', 'i', 'don', \"'\", 't', 'have', 'it', 'but', 'isn', \"'\", 't', 'sure', ')', ';', 'if', 'everyone', 'here', 'is', 'imp', '##ulsive', ',', 'i', 'might', 'as', 'well', 'save', 'my', 'money', 'and', 'start', 'a', 'psycho', '##therapy', 'with', 'the', 'psychiatrist', 'i', 'already', 'saw', 'once', '.', 'the', 'only', 'sure', 'thing', 'is', 'that', 'i', 'do', 'have', 'problems', 'that', 'need', 'to', 'be', 'fixed', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'has', 'anyone', 'here', 'been', 'diagnosed', 'with', 'add', 'despite', 'not', 'being', 'imp', '##ulsive', 'at', 'all', '?', '*', '*']\n",
      "INFO:__main__:Number of tokens: 270\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', 'and', 'imp', '##uls', '##ivity', 'hi', 'i', 'went', 'to', 'see', 'a', 'doctor', 'a', 'month', 'ago', 'to', 'get', 'diagnosed', ',', 'because', 'i', 'have', 'almost', 'all', 'the', 'symptoms', 'of', 'the', 'ina', '##tten', '##tive', 'type', '(', 'i', \"'\", 'm', 'not', 'going', 'to', 'go', 'into', 'details', 'here', ',', 'it', \"'\", 's', 'not', 'the', 'point', ')', ';', 'however', 'she', 'told', 'me', 'that', 'she', 'wasn', \"'\", 't', 'sure', 'i', 'had', 'it', '.', 'she', 'agrees', 'i', 'have', 'some', 'similarities', 'with', 'someone', 'with', 'ad', '##hd', ',', 'but', 'the', 'thing', 'that', 'makes', 'her', 'think', 'it', 'is', 'something', 'else', 'is', 'that', 'i', 'told', 'her', 'i', 'was', 'not', 'imp', '##ulsive', 'at', 'all', ';', 'and', 'it', \"'\", 's', 'true', ',', 'for', 'example', 'i', 'sometimes', 'hesitate', 'for', 'ages', 'before', 'making', 'a', 'simple', 'decision', 'like', 'buying', 'a', 'ts', '##hir', '##t', 'or', 'not', '.', 'i', \"'\", 'd', 'like', 'to', 'know', 'if', 'some', 'of', 'you', 'have', 'been', 'diagnosed', 'with', 'ad', '##hd', ',', 'despite', 'being', 'anything', 'but', 'imp', '##ulsive', '.', 'the', 'reason', 'i', \"'\", 'd', 'like', 'to', 'know', 'is', 'because', 'i', 'am', 'about', 'to', 'call', 'a', 'specialist', 'of', 'ad', '##hd', 'from', 'my', 'hometown', 'to', 'know', 'for', 'sure', '(', 'on', 'the', 'advice', 'of', 'my', 'doctor', ',', 'who', 'believes', 'i', 'don', \"'\", 't', 'have', 'it', 'but', 'isn', \"'\", 't', 'sure', ')', ';', 'if', 'everyone', 'here', 'is', 'imp', '##ulsive', ',', 'i', 'might', 'as', 'well', 'save', 'my', 'money', 'and', 'start', 'a', 'psycho', '##therapy', 'with', 'the', 'psychiatrist', 'i', 'already', 'saw', 'once', '.', 'the', 'only', 'sure', 'thing', 'is', 'that', 'i', 'do', 'have', 'problems', 'that', 'need', 'to', 'be', 'fixed', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'has', 'anyone', 'here', 'been', 'diagnosed', 'with', 'add', 'despite', 'not', 'being', 'imp', '##ulsive', 'at', 'all', '?', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'suicidal', 'tend', '##an', '##cies', '.', '[', 'tried', 'naming', 'it', 'something', 'less', 'harsh', ',', 'failed', ']']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'suicidal', 'tend', '##an', '##cies', '.', '[', 'tried', 'naming', 'it', 'something', 'less', 'harsh', ',', 'failed', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['wondering', 'if', 'i', 'might', 'be', 'mis', '##dia', '##gno', '##sed', 'well', 'i', 'was', 'just', 'diagnosed', 'with', 'add', 'a', 'week', 'ago', ',', 'and', 'was', 'just', 'prescribed', '18', '##mg', 'of', 'concert', '##a', 'but', 'i', 'am', 'scared', 'to', 'start', 'taking', 'any', 'medication', 'before', 'i', 'am', 'absolutely', 'sure', 'i', 'have', 'add', '.', 'i', 'am', 'a', '19', 'year', 'old', 'university', 'student', ',', 'and', 'grades', 'really', 'haven', \"'\", 't', 'been', 'probe', '##lm', 'for', 'me', 'until', 'i', 'got', 'to', 'university', '.', 'i', 'generally', 'never', 'really', 'had', 'to', 'study', 'as', 'most', 'things', 'just', 'came', 'naturally', ',', 'or', 'what', 'i', 'partially', 'remember', 'from', 'last', 'minute', 'studying', '.', 'i', 'was', 'also', 'an', 'awesome', 'bullshit', '##ter', 'with', 'word', 'answers', 'and', 'essays', '.', 'but', 'once', 'i', 'came', 'to', 'university', ',', 'i', 'noticed', 'that', 'i', 'couldn', \"'\", 't', 'study', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'if', 'it', 'is', 'because', 'i', 'am', 'lazy', 'or', 'i', 'have', 'add', '.', 'studying', 'for', 'kind', 'of', 'goes', 'like', 'opening', 'the', 'book', ',', 'studying', 'for', 'maybe', '15', '-', '20', '##min', '##ute', '##s', 'and', 'then', 'finding', 'every', 'con', '##cie', '##vable', 'excuse', 'to', 'get', 'up', 'being', 'it', 'going', 'to', 'take', 'a', 'shower', 'or', 'even', 'a', 'dump', '.', 'when', 'i', 'did', 'study', ',', 'i', 'would', 'never', 'actually', 'absorb', 'anything', 'and', 'would', 'have', 'to', 'continually', 're', 'reading', 'a', 'line', 'since', ',', 'well', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'explain', 'it', 'but', 'maybe', 'a', 'white', 'noise', 'or', 'something', 'going', 'on', 'in', 'the', 'back', 'of', 'my', 'head', ',', 'like', 'i', 'am', 'thinking', 'about', 'seo', '##mt', '##hing', 'else', 'and', 'studying', 'at', 'the', 'same', 'time', '.', 'and', 'again', ',', 'after', 'i', 'few', 'minutes', 'i', 'would', 'just', 'naturally', 'go', 'do', 'something', 'else', ',', 'half', 'the', 'time', 'without', 'me', 'realizing', 'it', '.', 'when', 'i', 'do', 'realize', 'it', ',', 'my', 'mind', 'even', 'fights', 'back', 'with', 'me', 'and', 'i', 'continue', 'becoming', 'distracted', '.', 'but', 'then', 'there', 'are', 'times', 'when', 'i', 'write', 'an', 'essay', 'or', 'do', 'an', 'assignment', 'and', 'i', 'become', 'really', 'eng', '##ross', '##ed', 'in', 'it', 'for', 'a', 'while', ',', 'but', 'eventually', 'get', 'distracted', 'or', 'tire', 'out', '.', 'but', 'the', 'fact', 'that', 'i', 'am', 'able', 'to', 'become', 'eng', '##ross', '##ed', 'makes', 'me', 'think', 'that', 'maybe', 'i', 'might', 'not', 'have', 'add', '.', 'my', 'doctor', 'or', 'my', 'psychological', 'aren', \"'\", 't', 'that', 'great', 'and', 'practically', 'pill', 'happy', '.', 'lectures', 'are', 'the', 'worse', ',', 'but', 'then', 'again', 'i', 'see', 'lots', 'of', 'people', 'never', 'paying', 'attention', 'in', 'a', 'lecture', 'or', 'doing', 'other', 'stuff', 'on', 'their', 'laptop', '##s', 'and', 'never', 'seem', 'to', 'pay', 'attention', 'so', 'i', 'don', \"'\", 't', 'really', 'consider', 'that', 'sy', '##mpt', '##om', '.', 'but', 'the', 'worse', 'part', 'is', 'my', 'imagination', ',', 'since', 'i', 'day', '##dre', '##am', 'or', 'visit', 'my', 'own', 'little', 'universe', 'a', 'lot', '.', 'i', 'don', \"'\", 't', 'even', 'realize', 'i', 'am', 'zoning', 'out', 'and', 'don', \"'\", 't', 'even', 'remember', 'how', 'i', 'got', 'to', 'the', 'topic', 'formulated', 'in', 'my', 'head', '.', 'it', \"'\", 's', 'just', 'sometimes', 'i', 'am', 'actually', 'able', 'to', 'concentrate', 'that', 'has', 'me', 'baffled', ',', 'but', 'it', 'happens', 'rarely', 'and', 'when', 'push', 'comes', 'to', 'shove', 'and', 'i', 'have', 'no', 'time', 'left', '.', 'i', 'don', \"'\", 't', 'know', 'whether', 'it', \"'\", 's', 'because', 'i', \"'\", 'm', 'lazy', ',', 'board', 'or', 'have', 'add', '.', 'soo', 'any', 'advice', 'from', 'people', 'who', 'have', 'confirmed', 'add', 'would', 'be', 'very', 'helpful', 'and', 'appreciated', '.', 'edit', ':', 'forgot', 'to', 'mention', 'that', 'i', 'think', 'i', 'have', 'anxiety', 'since', 'whenever', 'or', 'wherever', 'i', 'go', 'i', 'generally', 'feel', 'like', 'people', 'are', 'harshly', 'judging', 'me', ',', 'and', 'hate', 'just', 'going', 'into', 'any', 'public', 'places', '.', 'sometimes', 'even', 'puts', 'suicidal', 'thoughts', 'into', 'my', 'head', 'but', 'only', 'briefly', 'as', 'i', 'remind', 'myself', 'it', \"'\", 's', 'just', 'the', 'anxiety', 'talking', 'and', 'not', 'me', '.', 'don', \"'\", 't', 'know', 'if', 'it', \"'\", 'll', 'help', 'but', 'there', 'it', 'is', '.', 't', '##ld', '##r', ':', 'just', 'wondering', 'if', 'i', 'have', 'add', ',', 'if', 'i', 'am', 'board', 'or', 'if', 'i', 'am', 'lazy', '.']\n",
      "INFO:__main__:Number of tokens: 626\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['wondering', 'if', 'i', 'might', 'be', 'mis', '##dia', '##gno', '##sed', 'well', 'i', 'was', 'just', 'diagnosed', 'with', 'add', 'a', 'week', 'ago', ',', 'and', 'was', 'just', 'prescribed', '18', '##mg', 'of', 'concert', '##a', 'but', 'i', 'am', 'scared', 'to', 'start', 'taking', 'any', 'medication', 'before', 'i', 'am', 'absolutely', 'sure', 'i', 'have', 'add', '.', 'i', 'am', 'a', '19', 'year', 'old', 'university', 'student', ',', 'and', 'grades', 'really', 'haven', \"'\", 't', 'been', 'probe', '##lm', 'for', 'me', 'until', 'i', 'got', 'to', 'university', '.', 'i', 'generally', 'never', 'really', 'had', 'to', 'study', 'as', 'most', 'things', 'just', 'came', 'naturally', ',', 'or', 'what', 'i', 'partially', 'remember', 'from', 'last', 'minute', 'studying', '.', 'i', 'was', 'also', 'an', 'awesome', 'bullshit', '##ter', 'with', 'word', 'answers', 'and', 'essays', '.', 'but', 'once', 'i', 'came', 'to', 'university', ',', 'i', 'noticed', 'that', 'i', 'couldn', \"'\", 't', 'study', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'if', 'it', 'is', 'because', 'i', 'am', 'lazy', 'or', 'i', 'have', 'add', '.', 'studying', 'for', 'kind', 'of', 'goes', 'like', 'opening', 'the', 'book', ',', 'studying', 'for', 'maybe', '15', '-', '20', '##min', '##ute', '##s', 'and', 'then', 'finding', 'every', 'con', '##cie', '##vable', 'excuse', 'to', 'get', 'up', 'being', 'it', 'going', 'to', 'take', 'a', 'shower', 'or', 'even', 'a', 'dump', '.', 'when', 'i', 'did', 'study', ',', 'i', 'would', 'never', 'actually', 'absorb', 'anything', 'and', 'would', 'have', 'to', 'continually', 're', 'reading', 'a', 'line', 'since', ',', 'well', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'explain', 'it', 'but', 'maybe', 'a', 'white', 'noise', 'or', 'something', 'going', 'on', 'in', 'the', 'back', 'of', 'my', 'head', ',', 'like', 'i', 'am', 'thinking', 'about', 'seo', '##mt', '##hing', 'else', 'and', 'studying', 'at', 'the', 'same', 'time', '.', 'and', 'again', ',', 'after', 'i', 'few', 'minutes', 'i', 'would', 'just', 'naturally', 'go', 'do', 'something', 'else', ',', 'half', 'the', 'time', 'without', 'me', 'realizing', 'it', '.', 'when', 'i', 'do', 'realize', 'it', ',', 'my', 'mind', 'even', 'fights', 'back', 'with', 'me', 'and', 'i', 'continue', 'becoming', 'distracted', '.', 'but', 'then', 'there', 'are', 'times', 'when', 'i', 'write', 'an', 'essay', 'or', 'do', 'an', 'assignment', 'and', 'i', 'become', 'really', 'eng', '##ross', '##ed', 'in', 'it', 'for', 'a', 'while', ',', 'but', 'eventually', 'get', 'distracted', 'or', 'tire', 'out', '.', 'but', 'the', 'fact', 'that', 'i', 'am', 'able', 'to', 'become', 'eng', '##ross', '##ed', 'makes', 'me', 'think', 'that', 'maybe', 'i', 'might', 'not', 'have', 'add', '.', 'my', 'doctor', 'or', 'my', 'psychological', 'aren', \"'\", 't', 'that', 'great', 'and', 'practically', 'pill', 'happy', '.', 'lectures', 'are', 'the', 'worse', ',', 'but', 'then', 'again', 'i', 'see', 'lots', 'of', 'people', 'never', 'paying', 'attention', 'in', 'a', 'lecture', 'or', 'doing', 'other', 'stuff', 'on', 'their', 'laptop', '##s', 'and', 'never', 'seem', 'to', 'pay', 'attention', 'so', 'i', 'don', \"'\", 't', 'really', 'consider', 'that', 'sy', '##mpt', '##om', '.', 'but', 'the', 'worse', 'part', 'is', 'my', 'imagination', ',', 'since', 'i', 'day', '##dre', '##am', 'or', 'visit', 'my', 'own', 'little', 'universe', 'a', 'lot', '.', 'i', 'don', \"'\", 't', 'even', 'realize', 'i', 'am', 'zoning', 'out', 'and', 'don', \"'\", 't', 'even', 'remember', 'how', 'i', 'got', 'to', 'the', 'topic', 'formulated', 'in', 'my', 'head', '.', 'it', \"'\", 's', 'just', 'sometimes', 'i', 'am', 'actually', 'able', 'to', 'concentrate', 'that', 'has', 'me', 'baffled', ',', 'but', 'it', 'happens', 'rarely', 'and', 'when', 'push', 'comes', 'to', 'shove', 'and', 'i', 'have', 'no', 'time', 'left', '.', 'i', 'don', \"'\", 't', 'know', 'whether', 'it', \"'\", 's', 'because', 'i', \"'\", 'm', 'lazy', ',', 'board', 'or', 'have', 'add'], ['.', 'soo', 'any', 'advice', 'from', 'people', 'who', 'have', 'confirmed', 'add', 'would', 'be', 'very', 'helpful', 'and', 'appreciated', '.', 'edit', ':', 'forgot', 'to', 'mention', 'that', 'i', 'think', 'i', 'have', 'anxiety', 'since', 'whenever', 'or', 'wherever', 'i', 'go', 'i', 'generally', 'feel', 'like', 'people', 'are', 'harshly', 'judging', 'me', ',', 'and', 'hate', 'just', 'going', 'into', 'any', 'public', 'places', '.', 'sometimes', 'even', 'puts', 'suicidal', 'thoughts', 'into', 'my', 'head', 'but', 'only', 'briefly', 'as', 'i', 'remind', 'myself', 'it', \"'\", 's', 'just', 'the', 'anxiety', 'talking', 'and', 'not', 'me', '.', 'don', \"'\", 't', 'know', 'if', 'it', \"'\", 'll', 'help', 'but', 'there', 'it', 'is', '.', 't', '##ld', '##r', ':', 'just', 'wondering', 'if', 'i', 'have', 'add', ',', 'if', 'i', 'am', 'board', 'or', 'if', 'i', 'am', 'lazy', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'had', 'a', 'reaction', 'to', 'add', '##eral', '##l', 'like', 'this', '?', 'what', 'do', 'you', 'think', 'happened', '?']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'had', 'a', 'reaction', 'to', 'add', '##eral', '##l', 'like', 'this', '?', 'what', 'do', 'you', 'think', 'happened', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'and', 'time', 'while', 'i', \"'\", 'm', 'very', 'much', 'an', 'adult', 'i', 'was', 'just', 'diagnosed', 'with', '(', 'rather', 'severe', ')', 'ad', '##hd', '.', 'having', 'just', 'started', 'taking', 'add', '##eral', '##l', '(', '2', 'days', ',', '10', '##mg', 'generic', ')', 'i', \"'\", 've', 'found', 'that', 'while', 'in', 'the', 'past', 'i', 'have', 'been', 'very', 'con', '##sc', '##ient', '##ous', 'about', 'time', '(', 'can', \"'\", 't', 'stand', 'being', 'late', 'and', 'i', 'schedule', 'everything', ')', 'i', 'am', 'now', 'newly', 'conscious', 'of', 'time', '-', 'i', 'find', 'myself', 'dividing', 'up', 'my', 'day', 'in', 'productivity', 'chunks', 'and', 'feel', 'amazed', 'at', 'how', 'little', 'time', 'has', 'passed', 'when', 'i', \"'\", 've', 'done', 'something', '.', 'is', 'this', 'typical', 'and', 'how', 'can', 'i', 'both', 'work', 'on', 'using', 'and', 'improving', 'the', 'technique', '?', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'an', 'all', 'about', 'productivity', 'asshole', 'but', 'since', 'this', 'is', 'entirely', 'new', 'to', 'me', 'i', \"'\", 'd', 'like', 'to', 'take', 'as', 'much', 'advantage', 'of', 'it', 'as', 'i', 'can', '.']\n",
      "INFO:__main__:Number of tokens: 156\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'and', 'time', 'while', 'i', \"'\", 'm', 'very', 'much', 'an', 'adult', 'i', 'was', 'just', 'diagnosed', 'with', '(', 'rather', 'severe', ')', 'ad', '##hd', '.', 'having', 'just', 'started', 'taking', 'add', '##eral', '##l', '(', '2', 'days', ',', '10', '##mg', 'generic', ')', 'i', \"'\", 've', 'found', 'that', 'while', 'in', 'the', 'past', 'i', 'have', 'been', 'very', 'con', '##sc', '##ient', '##ous', 'about', 'time', '(', 'can', \"'\", 't', 'stand', 'being', 'late', 'and', 'i', 'schedule', 'everything', ')', 'i', 'am', 'now', 'newly', 'conscious', 'of', 'time', '-', 'i', 'find', 'myself', 'dividing', 'up', 'my', 'day', 'in', 'productivity', 'chunks', 'and', 'feel', 'amazed', 'at', 'how', 'little', 'time', 'has', 'passed', 'when', 'i', \"'\", 've', 'done', 'something', '.', 'is', 'this', 'typical', 'and', 'how', 'can', 'i', 'both', 'work', 'on', 'using', 'and', 'improving', 'the', 'technique', '?', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'an', 'all', 'about', 'productivity', 'asshole', 'but', 'since', 'this', 'is', 'entirely', 'new', 'to', 'me', 'i', \"'\", 'd', 'like', 'to', 'take', 'as', 'much', 'advantage', 'of', 'it', 'as', 'i', 'can', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['not', 'sure', 'what', 'to', 'do', 'after', 'months', 'of', 'tests', 'and', 'observations', 'my', 'almost', '7', 'year', 'old', 'has', 'been', 'diagnosed', 'with', 'ad', '##hd', 'we', 'have', 'now', 'been', 'given', 'a', 'low', 'dose', 'of', 'rita', '##lin', 'to', 'try', 'with', 'him', 'and', 'see', 'how', 'he', 'does', '.', 'i', 'am', 'so', 'scared', 'to', 'give', 'it', 'to', 'him', 'but', 'we', 'have', 'tried', 'everything', 'else', 'he', 'is', 'failing', 'every', 'class', 'in', 'school', 'even', 'with', 'a', 'personal', 'aid', 'that', 'sits', 'with', 'him', 'all', 'day', ',', 'he', 'can', 'do', 'the', 'work', '##for', 'when', 'you', 'can', 'get', 'him', 'to', 'focus', 'for', 'more', 'them', '10', 'seconds', '.', 'i', 'know', 'giving', 'him', 'the', 'med', '##s', 'will', 'help', 'him', 'in', 'school', 'and', 'in', 'lifestyle', 'in', 'general', 'but', 'im', 'so', 'scared', 'he', '##s', 'going', 'to', 'loose', 'his', 'amazing', 'out', 'going', 'personality', '.', 'i', 'feel', 'like', 'a', 'failure', 'as', 'a', 'parent', '.', 'i', \"'\", 'm', 'not', 'sure', 'what', 'to', 'do', 'here', 'help']\n",
      "INFO:__main__:Number of tokens: 147\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['not', 'sure', 'what', 'to', 'do', 'after', 'months', 'of', 'tests', 'and', 'observations', 'my', 'almost', '7', 'year', 'old', 'has', 'been', 'diagnosed', 'with', 'ad', '##hd', 'we', 'have', 'now', 'been', 'given', 'a', 'low', 'dose', 'of', 'rita', '##lin', 'to', 'try', 'with', 'him', 'and', 'see', 'how', 'he', 'does', '.', 'i', 'am', 'so', 'scared', 'to', 'give', 'it', 'to', 'him', 'but', 'we', 'have', 'tried', 'everything', 'else', 'he', 'is', 'failing', 'every', 'class', 'in', 'school', 'even', 'with', 'a', 'personal', 'aid', 'that', 'sits', 'with', 'him', 'all', 'day', ',', 'he', 'can', 'do', 'the', 'work', '##for', 'when', 'you', 'can', 'get', 'him', 'to', 'focus', 'for', 'more', 'them', '10', 'seconds', '.', 'i', 'know', 'giving', 'him', 'the', 'med', '##s', 'will', 'help', 'him', 'in', 'school', 'and', 'in', 'lifestyle', 'in', 'general', 'but', 'im', 'so', 'scared', 'he', '##s', 'going', 'to', 'loose', 'his', 'amazing', 'out', 'going', 'personality', '.', 'i', 'feel', 'like', 'a', 'failure', 'as', 'a', 'parent', '.', 'i', \"'\", 'm', 'not', 'sure', 'what', 'to', 'do', 'here', 'help']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['fatigue', 'when', 'off', 'add', '##eral', '##l', 'x', '##r', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['fatigue', 'when', 'off', 'add', '##eral', '##l', 'x', '##r', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'got', 'back', 'on', 'my', 'ad', '##hd', 'drugs', '/', 'medicine', 'after', 'being', 'without', 'for', '3', 'months', '.', 'this', 'is', 'how', 'i', 'feel', '.', '(', 'image', ')']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'got', 'back', 'on', 'my', 'ad', '##hd', 'drugs', '/', 'medicine', 'after', 'being', 'without', 'for', '3', 'months', '.', 'this', 'is', 'how', 'i', 'feel', '.', '(', 'image', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['two', 'questions', 'i', 'am', 'worried', 'that', 'ad', '##hd', 'is', 'affecting', 'my', 'professional', 'life', '.', 'in', 'general', ',', 'i', 'would', 'like', 'to', 'be', 'more', 'productive', 'and', 'not', 'have', 'to', 'constantly', 'play', 'catch', '-', 'up', 'on', 'weekends', 'for', 'all', 'the', 'internet', 'dick', '##ing', 'around', 'i', 'do', 'during', 'the', 'week', '.', 'so', 'my', 'first', 'question', 'is', ':', 'how', 'does', 'one', 'go', 'from', 'zero', 'to', 'properly', 'diagnosed', '?', 'i', 'have', 'average', '-', 'is', '##h', 'insurance', '.', 'my', 'second', 'question', 'is', 'if', 'any', 'medications', 'work', 'like', ':', 'take', 'some', 'in', 'the', 'morning', ',', 'be', 'productive', 'during', 'the', 'day', ',', 'and', 'it', 'wears', 'off', 'at', 'around', '5', '-', '6', 'in', 'the', 'afternoon', 'so', 'i', 'can', 'go', 'back', 'to', 'normal', 'when', 'i', 'get', 'off', 'work', '?', 'really', ',', 'i', 'don', \"'\", 't', 'mind', 'having', 'ad', '##hd', 'except', 'for', 'its', 'impact', 'on', 'my', 'job', 'performance', '.', 'i', 'feel', 'like', 'when', 'i', 'try', 'to', 'get', 'something', 'done', 'i', 'get', 'pulled', 'off', 'a', 'tangent', 'and', 'a', 'bunch', 'of', 'rabbit', 'holes', 'and', 'before', 'i', 'know', 'it', 'it', \"'\", 's', 'been', 'like', '45', 'minutes', 'since', 'the', 'original', 'tangent', '.', 'like', 'my', 'mind', 'is', 'being', 'controlled', 'by', 'a', 'child', 'that', \"'\", 's', 'constantly', 'flipping', 'the', 'channels', 'and', 'has', 'an', 'extremely', 'low', 'tolerance', 'for', 'rep', '##iti', '##tion', 'and', '/', 'or', 'an', 'extremely', 'irresistible', 'impulse', 'to', 'think', 'about', 'something', 'else', 'after', 'a', 'few', 'seconds', '.', 'it', \"'\", 's', 'very', 'rare', 'that', 'i', 'finish', 'anything', 'i', 'set', 'out', 'to', 'do', '.']\n",
      "INFO:__main__:Number of tokens: 235\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['two', 'questions', 'i', 'am', 'worried', 'that', 'ad', '##hd', 'is', 'affecting', 'my', 'professional', 'life', '.', 'in', 'general', ',', 'i', 'would', 'like', 'to', 'be', 'more', 'productive', 'and', 'not', 'have', 'to', 'constantly', 'play', 'catch', '-', 'up', 'on', 'weekends', 'for', 'all', 'the', 'internet', 'dick', '##ing', 'around', 'i', 'do', 'during', 'the', 'week', '.', 'so', 'my', 'first', 'question', 'is', ':', 'how', 'does', 'one', 'go', 'from', 'zero', 'to', 'properly', 'diagnosed', '?', 'i', 'have', 'average', '-', 'is', '##h', 'insurance', '.', 'my', 'second', 'question', 'is', 'if', 'any', 'medications', 'work', 'like', ':', 'take', 'some', 'in', 'the', 'morning', ',', 'be', 'productive', 'during', 'the', 'day', ',', 'and', 'it', 'wears', 'off', 'at', 'around', '5', '-', '6', 'in', 'the', 'afternoon', 'so', 'i', 'can', 'go', 'back', 'to', 'normal', 'when', 'i', 'get', 'off', 'work', '?', 'really', ',', 'i', 'don', \"'\", 't', 'mind', 'having', 'ad', '##hd', 'except', 'for', 'its', 'impact', 'on', 'my', 'job', 'performance', '.', 'i', 'feel', 'like', 'when', 'i', 'try', 'to', 'get', 'something', 'done', 'i', 'get', 'pulled', 'off', 'a', 'tangent', 'and', 'a', 'bunch', 'of', 'rabbit', 'holes', 'and', 'before', 'i', 'know', 'it', 'it', \"'\", 's', 'been', 'like', '45', 'minutes', 'since', 'the', 'original', 'tangent', '.', 'like', 'my', 'mind', 'is', 'being', 'controlled', 'by', 'a', 'child', 'that', \"'\", 's', 'constantly', 'flipping', 'the', 'channels', 'and', 'has', 'an', 'extremely', 'low', 'tolerance', 'for', 'rep', '##iti', '##tion', 'and', '/', 'or', 'an', 'extremely', 'irresistible', 'impulse', 'to', 'think', 'about', 'something', 'else', 'after', 'a', 'few', 'seconds', '.', 'it', \"'\", 's', 'very', 'rare', 'that', 'i', 'finish', 'anything', 'i', 'set', 'out', 'to', 'do', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['have', 'any', 'of', 'you', 'lost', 'friends', 'because', 'they', 'don', \"'\", 't', 'believe', 'in', 'ad', '##hd', '?', 'bit', 'of', 'background', ':', 'i', 'work', 'in', 'a', 'internet', 'support', 'centre', 'and', 'i', 'am', 'currently', 'a', 'te', '##mp', '.', 'i', 'recently', 'found', 'out', 'that', 'my', 'manager', 'was', 'thinking', 'about', 'taking', 'me', 'on', 'as', 'a', 'permanent', 'member', 'of', 'staff', 'but', 'had', 'chosen', 'not', 'to', 'because', 'when', 'i', 'knew', 'my', 'time', 'there', 'was', 'ending', 'there', 'i', 'stopped', 'being', 'interested', 'in', 'the', 'job', '.', 'i', 'found', 'this', 'out', 'from', 'someone', 'i', 'considered', 'a', 'friend', 'and', 'he', 'explained', 'the', 'situation', 'to', 'me', '.', 'he', 'said', '\"', 'it', \"'\", 's', 'too', 'late', 'now', ',', 'you', \"'\", 're', 'too', 'lazy', '.', '\"', 'i', 'got', 'that', 'a', 'a', 'lot', 'in', 'school', 'and', 'it', 'really', 'piss', '##es', 'me', 'off', '.', 'so', 'i', 'told', 'him', 'what', 'it', 'was', 'like', 'to', 'have', 'ad', '##hd', 'and', 'he', 'just', 'pretty', 'much', 'said', 'to', 'me', '\"', 'stop', 'making', 'excuse', \"'\", 's', ',', 'you', \"'\", 're', 'lazy', 'and', 'you', 'should', 'of', 'got', 'your', 'act', 'together', '.', '\"', 'so', 'i', 'explained', 'to', 'him', 'that', 'i', \"'\", 'knew', \"'\", 'i', 'wasn', \"'\", 't', 'getting', 'a', 'job', 'because', 'we', 'were', 'told', 'all', 'of', 'the', 'positions', 'available', 'had', 'been', 'filled', ',', 'if', 'someone', 'had', 'told', 'me', 'there', 'was', 'another', 'position', 'available', 'i', 'would', 'have', 'tried', 'my', 'hardest', '.', '\"', 'he', 'told', 'me', 'i', 'was', 'an', 'idiot', 'and', 'that', 'it', 'was', 'obvious', '.', 'i', 'told', 'him', 'to', 'go', 'fuck', 'himself', 'and', 'jump', 'off', 'a', 'bridge', '.', 't', '##l', ';', 'dr', ':', 'me', 'and', 'a', 'work', 'friend', 'had', 'an', 'argument', 'regarding', 'my', \"'\", 'la', '##zine', '##ss', ',', \"'\", 'now', 'we', 'don', \"'\", 't', 'talk', '.']\n",
      "INFO:__main__:Number of tokens: 271\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['have', 'any', 'of', 'you', 'lost', 'friends', 'because', 'they', 'don', \"'\", 't', 'believe', 'in', 'ad', '##hd', '?', 'bit', 'of', 'background', ':', 'i', 'work', 'in', 'a', 'internet', 'support', 'centre', 'and', 'i', 'am', 'currently', 'a', 'te', '##mp', '.', 'i', 'recently', 'found', 'out', 'that', 'my', 'manager', 'was', 'thinking', 'about', 'taking', 'me', 'on', 'as', 'a', 'permanent', 'member', 'of', 'staff', 'but', 'had', 'chosen', 'not', 'to', 'because', 'when', 'i', 'knew', 'my', 'time', 'there', 'was', 'ending', 'there', 'i', 'stopped', 'being', 'interested', 'in', 'the', 'job', '.', 'i', 'found', 'this', 'out', 'from', 'someone', 'i', 'considered', 'a', 'friend', 'and', 'he', 'explained', 'the', 'situation', 'to', 'me', '.', 'he', 'said', '\"', 'it', \"'\", 's', 'too', 'late', 'now', ',', 'you', \"'\", 're', 'too', 'lazy', '.', '\"', 'i', 'got', 'that', 'a', 'a', 'lot', 'in', 'school', 'and', 'it', 'really', 'piss', '##es', 'me', 'off', '.', 'so', 'i', 'told', 'him', 'what', 'it', 'was', 'like', 'to', 'have', 'ad', '##hd', 'and', 'he', 'just', 'pretty', 'much', 'said', 'to', 'me', '\"', 'stop', 'making', 'excuse', \"'\", 's', ',', 'you', \"'\", 're', 'lazy', 'and', 'you', 'should', 'of', 'got', 'your', 'act', 'together', '.', '\"', 'so', 'i', 'explained', 'to', 'him', 'that', 'i', \"'\", 'knew', \"'\", 'i', 'wasn', \"'\", 't', 'getting', 'a', 'job', 'because', 'we', 'were', 'told', 'all', 'of', 'the', 'positions', 'available', 'had', 'been', 'filled', ',', 'if', 'someone', 'had', 'told', 'me', 'there', 'was', 'another', 'position', 'available', 'i', 'would', 'have', 'tried', 'my', 'hardest', '.', '\"', 'he', 'told', 'me', 'i', 'was', 'an', 'idiot', 'and', 'that', 'it', 'was', 'obvious', '.', 'i', 'told', 'him', 'to', 'go', 'fuck', 'himself', 'and', 'jump', 'off', 'a', 'bridge', '.', 't', '##l', ';', 'dr', ':', 'me', 'and', 'a', 'work', 'friend', 'had', 'an', 'argument', 'regarding', 'my', \"'\", 'la', '##zine', '##ss', ',', \"'\", 'now', 'we', 'don', \"'\", 't', 'talk', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'you', 'do', 'before', 'bed', 'to', 'try', 'to', 'set', 'up', 'the', 'next', 'morning', 'to', 'be', 'more', 'productive', '?', 'or', 'are', 'you', 'like', 'me', ',', 'and', 'wait', 'until', 'you', 'are', 'too', 'exhausted', 'to', 'do', 'anything', 'except', 'brush', 'your', 'teeth', 'and', 'go', 'to', 'bed', '?']\n",
      "INFO:__main__:Number of tokens: 44\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'you', 'do', 'before', 'bed', 'to', 'try', 'to', 'set', 'up', 'the', 'next', 'morning', 'to', 'be', 'more', 'productive', '?', 'or', 'are', 'you', 'like', 'me', ',', 'and', 'wait', 'until', 'you', 'are', 'too', 'exhausted', 'to', 'do', 'anything', 'except', 'brush', 'your', 'teeth', 'and', 'go', 'to', 'bed', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['completing', 'a', 'project', '.', '.', '.', 'stop', 'me', 'if', 'you', \"'\", 've', 'heard', 'this', 'one', 'before', '.', 'you', 'have', 'a', 'project', '.', 'you', 'complete', '80', '%', 'of', 'the', 'project', '.', 'the', 'last', '20', '%', 'suddenly', 'becomes', 'a', 'mountain', 'three', 'times', 'larger', 'than', 'the', 'first', '80', '%', '.', 'the', 'mountain', 'is', 'ted', '##ious', 'and', 'gr', '##uel', '##ing', 'with', 'hundreds', 'of', 'obstacles', 'and', 'pit', '##falls', 'along', 'the', 'way', '.', 'and', 'ever', 'since', 'you', \"'\", 've', 'started', 'the', 'journey', 'of', 'this', 'project', 'you', \"'\", 've', 'only', 'had', 'a', 'crap', '##py', 'gps', 'that', 'never', 'takes', 'you', 'directly', 'where', 'you', 'need', 'to', 'go', ',', 'and', 'one', 'large', 'sack', 'to', 'store', 'all', 'of', 'the', 'things', 'you', \"'\", 've', 'learned', 'along', 'the', 'way', ',', 'but', 'the', 'sack', 'has', 'a', 'huge', 'hole', 'in', 'it', '.', 'in', 'the', 'end', 'you', 'wind', 'up', 'completing', '85', '%', 'of', 'the', 'project', 'and', 'only', 'two', 'concepts', 'remain', 'in', 'your', 'sack', 'because', 'they', 'were', 'too', 'big', 'and', 'general', 'to', 'fall', 'through', 'the', 'hole', '.']\n",
      "INFO:__main__:Number of tokens: 160\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['completing', 'a', 'project', '.', '.', '.', 'stop', 'me', 'if', 'you', \"'\", 've', 'heard', 'this', 'one', 'before', '.', 'you', 'have', 'a', 'project', '.', 'you', 'complete', '80', '%', 'of', 'the', 'project', '.', 'the', 'last', '20', '%', 'suddenly', 'becomes', 'a', 'mountain', 'three', 'times', 'larger', 'than', 'the', 'first', '80', '%', '.', 'the', 'mountain', 'is', 'ted', '##ious', 'and', 'gr', '##uel', '##ing', 'with', 'hundreds', 'of', 'obstacles', 'and', 'pit', '##falls', 'along', 'the', 'way', '.', 'and', 'ever', 'since', 'you', \"'\", 've', 'started', 'the', 'journey', 'of', 'this', 'project', 'you', \"'\", 've', 'only', 'had', 'a', 'crap', '##py', 'gps', 'that', 'never', 'takes', 'you', 'directly', 'where', 'you', 'need', 'to', 'go', ',', 'and', 'one', 'large', 'sack', 'to', 'store', 'all', 'of', 'the', 'things', 'you', \"'\", 've', 'learned', 'along', 'the', 'way', ',', 'but', 'the', 'sack', 'has', 'a', 'huge', 'hole', 'in', 'it', '.', 'in', 'the', 'end', 'you', 'wind', 'up', 'completing', '85', '%', 'of', 'the', 'project', 'and', 'only', 'two', 'concepts', 'remain', 'in', 'your', 'sack', 'because', 'they', 'were', 'too', 'big', 'and', 'general', 'to', 'fall', 'through', 'the', 'hole', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', ',', 'i', 'think', 'i', 'may', 'be', 'ad', '##hd', '.', '.', '.', 'so', 'a', 'little', 'background', 'first', ':', 'i', \"'\", 'm', 'a', 'freshman', 'in', 'college', '.', 'i', \"'\", 've', 'never', 'been', 'good', 'in', 'school', ',', 'during', 'elementary', 'school', 'it', 'was', 'harder', 'to', 'tell', ',', 'i', \"'\", 'd', 'get', 'decent', 'grades', ',', 'but', 'once', 'i', 'got', 'into', 'middle', 'school', 'it', 'was', 'quite', 'obvious', 'that', 'i', 'wasn', \"'\", 't', 'doing', 'well', ',', 'i', 'flu', '##nk', '##ed', 'out', 'of', 'my', 'advanced', 'math', 'class', ',', 'nearly', 'failed', 'english', 'and', 'humanities', ',', 'the', 'only', 'class', 'i', 'did', 'decent', 'in', 'was', 'science', '.', 'so', 'this', 'trend', 'continued', 'all', 'the', 'way', 'through', 'high', 'school', ',', 'with', 'me', 'doing', 'just', 'good', 'enough', 'to', 'not', 'warrant', 'any', 'special', 'attention', 'or', 'help', '.', 'anyway', '##s', ',', 'that', 'changed', 'once', 'i', 'got', 'to', 'college', ',', 'last', 'quarter', 'i', 'was', 'failing', 'all', 'my', 'courses', ',', 'and', 'it', 'wasn', '’', 't', 'because', 'i', 'didn', '’', 't', 'understand', 'the', 'course', 'material', 'or', 'that', 'i', 'couldn', '’', 't', 'write', 'a', 'good', 'essay', ',', 'i', 'understood', 'everything', 'perfectly', 'and', 'i', 'could', 'write', 'brilliant', 'essays', 'that', 'received', 'praise', 'from', 'my', 'professors', ',', 'but', 'that', 'was', 'very', 'rare', '.', '‘', 'the', 'problem', '’', 'essentially', 'was', 'that', 'i', 'had', 'no', 'idea', 'how', 'to', 'mo', '##tiv', '##ate', 'myself', ',', 'i', 'tried', 'just', 'telling', 'myself', 'to', 'simply', '‘', 'be', 'motivated', '’', ',', 'but', 'i', 'found', 'that', 'it', 'was', 'much', 'like', 'telling', 'a', 'smoke', '##r', 'to', 'simply', '‘', 'stop', 'smoking', '’', 'or', 'like', 'telling', 'an', 'obe', '##se', 'person', 'to', 'simply', '‘', 'start', 'eating', 'healthy', '’', ':', 'it', 'simply', 'didn', '’', 't', 'work', '.', 'whenever', 'i', 'know', 'i', 'need', 'to', 'work', 'on', 'something', 'it', 'almost', 'always', 'took', 'an', 'enormous', 'amount', 'of', 'effort', 'to', 'get', 'myself', 'to', 'even', 'sit', 'down', 'with', 'the', 'intention', 'to', 'do', 'that', 'thing', ',', 'and', 'even', 'if', 'i', 'managed', 'to', 'si', '##ft', 'past', 'the', 'enormous', 'amount', 'of', 'things', 'that', 'i', 'would', 'have', 'much', 'rather', 'been', 'doing', '(', 'playing', 'video', 'games', ',', 'reading', 'a', 'book', ',', 'brows', '##ing', 'the', 'internet', ',', 'etc', '.', ')', 'and', 'force', 'myself', 'to', 'start', 'working', ',', 'it', 'was', 'like', 'there', 'was', 'this', 'huge', 'road', 'block', 'in', 'my', 'mental', 'capacity', ',', 'i', 'couldn', '’', 't', 'think', '.', 'i', 'could', 'not', 'access', 'the', 'mental', 'capacity', 'that', 'i', 'knew', 'was', 'there', '.', 'it', 'was', 'immensely', 'frustrating', 'when', 'i', 'had', 'a', 'brilliant', 'idea', 'earlier', 'in', 'the', 'day', ',', 'but', 'when', 'i', 'finally', 'was', 'able', 'to', 'sit', 'down', 'and', 'work', 'at', 'it', ',', 'i', 'couldn', '’', 't', 'follow', 'it', 'further', 'because', 'of', 'some', 'mental', 'block', '##age', 'that', 'i', 'had', 'no', 'idea', 'how', 'to', 'clear', '.', 'so', 'i', 'went', 'and', 'talked', 'with', 'one', 'of', 'the', 'school', 'counselor', '##s', ',', 'and', 'after', 'i', 'explained', 'what', 'the', 'situation', 'was', 'and', 'how', 'i', 'felt', 'about', 'it', ',', 'he', 'suggested', 'that', 'i', 'might', 'be', 'ad', '##hd', ',', 'and', 'suggested', 'a', 'book', 'i', 'could', 'read', 'on', 'the', 'subject', ',', 'as', 'well', 'as', 'go', 'talk', 'to', 'my', 'doctor', 'about', 'it', '.', 'so', 'i', 'did', ',', 'i', 'read', 'the', 'book', 'and', 'talked', 'to', 'my', 'doctor', 'and', 'both', 'seem', 'to', 'suggest', 'that', 'i', 'am', 'ad', '##hd', ',', 'but', 'my', 'parents', ',', 'who', 'i', \"'\", 'm', 'still', 'dependent', 'on', 'for', 'tuition', ',', 'refuse', 'to', 'pay', 'for', 'any', 'sort', 'of', 'treatment', 'at', 'all', '.', 'what', 'should', 'i', 'do', '?', 'edit', ':', 'sorry', ',', 'i', 'was', 'going', 'to', 'add', 'a', 't', '##l', ';', 'dr', ',', 'but', 'by', 'the', 'end', 'of', 'it', 'i', 'had', 'completely', 'forgotten', ',', 'so', '.', '.', '.', 't', '##l', ';', 'dr', ':', 'i', 'think', 'i', 'have', 'ad', '##hd', ',', 'my', 'parents', 'completely', 'refuse', 'to', 'pay', 'for', 'any', 'treatment', ',', 'what', 'should', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 594\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['so', ',', 'i', 'think', 'i', 'may', 'be', 'ad', '##hd', '.', '.', '.', 'so', 'a', 'little', 'background', 'first', ':', 'i', \"'\", 'm', 'a', 'freshman', 'in', 'college', '.', 'i', \"'\", 've', 'never', 'been', 'good', 'in', 'school', ',', 'during', 'elementary', 'school', 'it', 'was', 'harder', 'to', 'tell', ',', 'i', \"'\", 'd', 'get', 'decent', 'grades', ',', 'but', 'once', 'i', 'got', 'into', 'middle', 'school', 'it', 'was', 'quite', 'obvious', 'that', 'i', 'wasn', \"'\", 't', 'doing', 'well', ',', 'i', 'flu', '##nk', '##ed', 'out', 'of', 'my', 'advanced', 'math', 'class', ',', 'nearly', 'failed', 'english', 'and', 'humanities', ',', 'the', 'only', 'class', 'i', 'did', 'decent', 'in', 'was', 'science', '.', 'so', 'this', 'trend', 'continued', 'all', 'the', 'way', 'through', 'high', 'school', ',', 'with', 'me', 'doing', 'just', 'good', 'enough', 'to', 'not', 'warrant', 'any', 'special', 'attention', 'or', 'help', '.', 'anyway', '##s', ',', 'that', 'changed', 'once', 'i', 'got', 'to', 'college', ',', 'last', 'quarter', 'i', 'was', 'failing', 'all', 'my', 'courses', ',', 'and', 'it', 'wasn', '’', 't', 'because', 'i', 'didn', '’', 't', 'understand', 'the', 'course', 'material', 'or', 'that', 'i', 'couldn', '’', 't', 'write', 'a', 'good', 'essay', ',', 'i', 'understood', 'everything', 'perfectly', 'and', 'i', 'could', 'write', 'brilliant', 'essays', 'that', 'received', 'praise', 'from', 'my', 'professors', ',', 'but', 'that', 'was', 'very', 'rare', '.', '‘', 'the', 'problem', '’', 'essentially', 'was', 'that', 'i', 'had', 'no', 'idea', 'how', 'to', 'mo', '##tiv', '##ate', 'myself', ',', 'i', 'tried', 'just', 'telling', 'myself', 'to', 'simply', '‘', 'be', 'motivated', '’', ',', 'but', 'i', 'found', 'that', 'it', 'was', 'much', 'like', 'telling', 'a', 'smoke', '##r', 'to', 'simply', '‘', 'stop', 'smoking', '’', 'or', 'like', 'telling', 'an', 'obe', '##se', 'person', 'to', 'simply', '‘', 'start', 'eating', 'healthy', '’', ':', 'it', 'simply', 'didn', '’', 't', 'work', '.', 'whenever', 'i', 'know', 'i', 'need', 'to', 'work', 'on', 'something', 'it', 'almost', 'always', 'took', 'an', 'enormous', 'amount', 'of', 'effort', 'to', 'get', 'myself', 'to', 'even', 'sit', 'down', 'with', 'the', 'intention', 'to', 'do', 'that', 'thing', ',', 'and', 'even', 'if', 'i', 'managed', 'to', 'si', '##ft', 'past', 'the', 'enormous', 'amount', 'of', 'things', 'that', 'i', 'would', 'have', 'much', 'rather', 'been', 'doing', '(', 'playing', 'video', 'games', ',', 'reading', 'a', 'book', ',', 'brows', '##ing', 'the', 'internet', ',', 'etc', '.', ')', 'and', 'force', 'myself', 'to', 'start', 'working', ',', 'it', 'was', 'like', 'there', 'was', 'this', 'huge', 'road', 'block', 'in', 'my', 'mental', 'capacity', ',', 'i', 'couldn', '’', 't', 'think', '.', 'i', 'could', 'not', 'access', 'the', 'mental', 'capacity', 'that', 'i', 'knew', 'was', 'there', '.', 'it', 'was', 'immensely', 'frustrating', 'when', 'i', 'had', 'a', 'brilliant', 'idea', 'earlier', 'in', 'the', 'day', ',', 'but', 'when', 'i', 'finally', 'was', 'able', 'to', 'sit', 'down', 'and', 'work', 'at', 'it', ',', 'i', 'couldn', '’', 't', 'follow', 'it', 'further', 'because', 'of', 'some', 'mental', 'block', '##age', 'that', 'i', 'had', 'no', 'idea', 'how', 'to', 'clear', '.', 'so', 'i', 'went', 'and', 'talked', 'with', 'one', 'of', 'the', 'school', 'counselor', '##s', ',', 'and', 'after', 'i', 'explained', 'what', 'the', 'situation', 'was', 'and', 'how', 'i', 'felt', 'about', 'it', ',', 'he', 'suggested', 'that', 'i', 'might', 'be', 'ad', '##hd', ',', 'and', 'suggested', 'a', 'book', 'i', 'could', 'read', 'on', 'the', 'subject', ',', 'as', 'well', 'as', 'go', 'talk', 'to', 'my', 'doctor', 'about', 'it', '.', 'so', 'i', 'did', ',', 'i', 'read', 'the', 'book', 'and', 'talked', 'to', 'my', 'doctor', 'and', 'both', 'seem', 'to', 'suggest', 'that', 'i', 'am', 'ad', '##hd', ',', 'but', 'my', 'parents', ',', 'who'], ['i', \"'\", 'm', 'still', 'dependent', 'on', 'for', 'tuition', ',', 'refuse', 'to', 'pay', 'for', 'any', 'sort', 'of', 'treatment', 'at', 'all', '.', 'what', 'should', 'i', 'do', '?', 'edit', ':', 'sorry', ',', 'i', 'was', 'going', 'to', 'add', 'a', 't', '##l', ';', 'dr', ',', 'but', 'by', 'the', 'end', 'of', 'it', 'i', 'had', 'completely', 'forgotten', ',', 'so', '.', '.', '.', 't', '##l', ';', 'dr', ':', 'i', 'think', 'i', 'have', 'ad', '##hd', ',', 'my', 'parents', 'completely', 'refuse', 'to', 'pay', 'for', 'any', 'treatment', ',', 'what', 'should', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['little', 'advice', 'so', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', 'about', '8', 'and', 'stopped', 'taking', 'add', '##eral', 'when', 'i', 'was', '19', ',', '24', 'now', '.', 'i', 'have', 'been', 'promoted', 'at', 'work', 'and', 'have', 'a', 'larger', 'work', 'load', 'and', 'am', 'starting', 'to', 'struggle', 'with', 'all', 'the', 'new', 'tasks', '.', 'spend', 'my', 'day', 'watching', 'videos', 'or', 'on', 'red', '##dit', '.', 'then', 'freaking', 'out', 'because', 'i', 'ran', 'out', 'of', 'time', 'and', 'getting', 'pissed', 'and', 'wanting', 'to', 'give', 'up', '.', 'what', 'do', 'you', 'guys', 'and', 'gal', '##s', 'do', 'to', 'focus', 'on', 'work', '.', 'any', 'rec', '##com', '##ended', 'drugs', 'i', 'should', 'ask', 'my', 'doctor', 'about', 'to', 'help', 'me', 'neutral', 'out', '.']\n",
      "INFO:__main__:Number of tokens: 108\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['little', 'advice', 'so', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', 'about', '8', 'and', 'stopped', 'taking', 'add', '##eral', 'when', 'i', 'was', '19', ',', '24', 'now', '.', 'i', 'have', 'been', 'promoted', 'at', 'work', 'and', 'have', 'a', 'larger', 'work', 'load', 'and', 'am', 'starting', 'to', 'struggle', 'with', 'all', 'the', 'new', 'tasks', '.', 'spend', 'my', 'day', 'watching', 'videos', 'or', 'on', 'red', '##dit', '.', 'then', 'freaking', 'out', 'because', 'i', 'ran', 'out', 'of', 'time', 'and', 'getting', 'pissed', 'and', 'wanting', 'to', 'give', 'up', '.', 'what', 'do', 'you', 'guys', 'and', 'gal', '##s', 'do', 'to', 'focus', 'on', 'work', '.', 'any', 'rec', '##com', '##ended', 'drugs', 'i', 'should', 'ask', 'my', 'doctor', 'about', 'to', 'help', 'me', 'neutral', 'out', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'it', 'normal', 'to', 'feel', 'depressed', 'while', 'taking', 'a', 'break', 'from', 'add', '##eral', '##l', '?', 'i', 'was', 'just', 'diagnosed', 'with', 'ad', '##hd', '-', 'pi', 'a', 'little', 'while', 'ago', ',', 'and', 'started', 'on', 'add', '##eral', '##l', '.', 'my', 'doctor', 'told', 'me', 'i', 'should', 'take', 'breaks', 'every', 'so', 'often', ',', 'like', 'on', 'weekends', ',', 'so', 'i', 'don', \"'\", 't', 'build', 'up', 'tolerance', '.', 'well', ',', 'i', 'had', 'a', 'test', 'yesterday', ',', 'and', 'was', 'taking', 'the', 'maximum', 'amount', 'my', 'doctor', 'told', 'me', 'i', 'could', 'for', 'a', 'few', 'days', 'beforehand', '(', '15', '##mg', 'x', '##2', '/', 'day', ')', '.', '.', '.', 'and', 'now', 'today', ',', 'i', 'didn', \"'\", 't', 'take', 'any', ',', 'and', 'i', \"'\", 'm', 'feeling', 'ridiculous', '##ly', 'down', 'in', 'the', 'dump', '##s', '.', 'now', ',', 'i', 'can', \"'\", 't', 'tell', 'if', 'this', 'is', 'withdrawal', 'effects', ',', 'or', 'if', 'it', \"'\", 's', 'just', 'my', 'own', 'personal', 'problems', 'in', 'that', 'i', 'had', 'a', 'disappointing', 'night', 'last', 'night', '(', 'tried', 'to', 'hook', 'up', 'with', 'my', 'crush', ',', 'it', 'didn', \"'\", 't', 'happen', ',', 'sadness', '.', ')', 'or', 'maybe', 'it', \"'\", 's', 'a', 'combination', 'of', 'both', '.', 'but', 'now', 'i', \"'\", 'm', 'feeling', 'really', 'anti', '-', 'social', ',', 'and', 'i', 'took', 'a', 'nap', 'for', 'an', 'hour', 'and', 'i', 'don', \"'\", 't', 'feel', 'like', 'doing', 'anything', '.', 'and', 'sometimes', 'i', 'randomly', 'feel', 'like', 'crying', '.', 'is', 'this', 'a', 'normal', 'reaction', 'to', 'taking', 'a', 'break', 'from', 'add', '##eral', '##l', '?', 'i', 'feel', 'like', 'it', 'would', 'make', 'sense', '(', 'brain', 'gets', 'used', 'to', 'more', 'do', '##pa', '##mine', ',', 'then', 'suddenly', 'stops', ',', 'etc', '.', ')', 'has', 'anyone', 'else', 'had', 'any', 'experiences', 'like', 'this', '?', 'any', 'strategies', 'for', 'avoiding', 'it', 'or', 'coping', 'with', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 275\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'it', 'normal', 'to', 'feel', 'depressed', 'while', 'taking', 'a', 'break', 'from', 'add', '##eral', '##l', '?', 'i', 'was', 'just', 'diagnosed', 'with', 'ad', '##hd', '-', 'pi', 'a', 'little', 'while', 'ago', ',', 'and', 'started', 'on', 'add', '##eral', '##l', '.', 'my', 'doctor', 'told', 'me', 'i', 'should', 'take', 'breaks', 'every', 'so', 'often', ',', 'like', 'on', 'weekends', ',', 'so', 'i', 'don', \"'\", 't', 'build', 'up', 'tolerance', '.', 'well', ',', 'i', 'had', 'a', 'test', 'yesterday', ',', 'and', 'was', 'taking', 'the', 'maximum', 'amount', 'my', 'doctor', 'told', 'me', 'i', 'could', 'for', 'a', 'few', 'days', 'beforehand', '(', '15', '##mg', 'x', '##2', '/', 'day', ')', '.', '.', '.', 'and', 'now', 'today', ',', 'i', 'didn', \"'\", 't', 'take', 'any', ',', 'and', 'i', \"'\", 'm', 'feeling', 'ridiculous', '##ly', 'down', 'in', 'the', 'dump', '##s', '.', 'now', ',', 'i', 'can', \"'\", 't', 'tell', 'if', 'this', 'is', 'withdrawal', 'effects', ',', 'or', 'if', 'it', \"'\", 's', 'just', 'my', 'own', 'personal', 'problems', 'in', 'that', 'i', 'had', 'a', 'disappointing', 'night', 'last', 'night', '(', 'tried', 'to', 'hook', 'up', 'with', 'my', 'crush', ',', 'it', 'didn', \"'\", 't', 'happen', ',', 'sadness', '.', ')', 'or', 'maybe', 'it', \"'\", 's', 'a', 'combination', 'of', 'both', '.', 'but', 'now', 'i', \"'\", 'm', 'feeling', 'really', 'anti', '-', 'social', ',', 'and', 'i', 'took', 'a', 'nap', 'for', 'an', 'hour', 'and', 'i', 'don', \"'\", 't', 'feel', 'like', 'doing', 'anything', '.', 'and', 'sometimes', 'i', 'randomly', 'feel', 'like', 'crying', '.', 'is', 'this', 'a', 'normal', 'reaction', 'to', 'taking', 'a', 'break', 'from', 'add', '##eral', '##l', '?', 'i', 'feel', 'like', 'it', 'would', 'make', 'sense', '(', 'brain', 'gets', 'used', 'to', 'more', 'do', '##pa', '##mine', ',', 'then', 'suddenly', 'stops', ',', 'etc', '.', ')', 'has', 'anyone', 'else', 'had', 'any', 'experiences', 'like', 'this', '?', 'any', 'strategies', 'for', 'avoiding', 'it', 'or', 'coping', 'with', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'have', 'you', 'adjusted', 'your', 'diet', 'to', 'help', 'your', 'add', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'have', 'you', 'adjusted', 'your', 'diet', 'to', 'help', 'your', 'add', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'here', 'have', 'ad', '##hd', 'and', 'depression', '?', 'i', \"'\", 'm', 'living', 'with', 'both', 'and', 'have', 'some', 'questions', '.']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'here', 'have', 'ad', '##hd', 'and', 'depression', '?', 'i', \"'\", 'm', 'living', 'with', 'both', 'and', 'have', 'some', 'questions', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['quick', 'question', 'regarding', 'video', 'games', '.', 'okay', 'is', 'ad', '##hd', 'in', 'anyway', 'related', 'to', 'unable', 'to', 'finish', 'video', 'game', 'campaigns', '.', 'like', 'you', 'start', 'playing', 'sky', '##rim', ',', 'bf', '##3', ',', 'ark', '##ham', 'city', 'and', 'you', 'just', 'get', 'the', 'feeling', 'of', 'not', 'finishing', 'them', ',', 'losing', 'interest', ',', 'etc', '.', '.', '.', 'like', 'you', 'just', 'get', 'severely', 'demo', '##tiv', '##ated', 'to', 'finish', 'them', 'after', 'playing', 'for', 'like', '15', 'min', '.']\n",
      "INFO:__main__:Number of tokens: 70\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['quick', 'question', 'regarding', 'video', 'games', '.', 'okay', 'is', 'ad', '##hd', 'in', 'anyway', 'related', 'to', 'unable', 'to', 'finish', 'video', 'game', 'campaigns', '.', 'like', 'you', 'start', 'playing', 'sky', '##rim', ',', 'bf', '##3', ',', 'ark', '##ham', 'city', 'and', 'you', 'just', 'get', 'the', 'feeling', 'of', 'not', 'finishing', 'them', ',', 'losing', 'interest', ',', 'etc', '.', '.', '.', 'like', 'you', 'just', 'get', 'severely', 'demo', '##tiv', '##ated', 'to', 'finish', 'them', 'after', 'playing', 'for', 'like', '15', 'min', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'high', 'iq', '-', 'advantage', 'or', 'disadvantage', '?', 'do', 'any', 'ad', '##hd', \"'\", 'er', '##s', 'think', 'that', 'they', 'would', 'be', 'better', 'off', 'in', 'life', ',', 'or', 'happier', ',', 'if', 'a', 'dozen', 'or', 'two', 'iq', 'points', 'could', 'get', 'za', '##pped', 'off', '?', 'i', \"'\", 'm', 'around', '130', '.', 'i', 'think', 'the', 'sweet', 'spot', 'might', 'be', 'around', '110', '-', '115', '.']\n",
      "INFO:__main__:Number of tokens: 61\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'high', 'iq', '-', 'advantage', 'or', 'disadvantage', '?', 'do', 'any', 'ad', '##hd', \"'\", 'er', '##s', 'think', 'that', 'they', 'would', 'be', 'better', 'off', 'in', 'life', ',', 'or', 'happier', ',', 'if', 'a', 'dozen', 'or', 'two', 'iq', 'points', 'could', 'get', 'za', '##pped', 'off', '?', 'i', \"'\", 'm', 'around', '130', '.', 'i', 'think', 'the', 'sweet', 'spot', 'might', 'be', 'around', '110', '-', '115', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'mysteries', 'of', 'add', 'and', 'high', 'iq', '|', 'psychology', 'today']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'mysteries', 'of', 'add', 'and', 'high', 'iq', '|', 'psychology', 'today']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['wondering', 'about', 'st', '##rat', '##tera', 'and', 'int', '##uni', '##v', '.', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'add', 'for', 'as', 'long', 'as', 'i', 'can', 'remember', ',', 'but', 'i', \"'\", 've', 'never', 'been', 'put', 'on', 'medication', 'since', 'most', 'of', 'them', 'are', 'st', '##im', '##ula', '##nts', 'and', 'i', 'have', 'some', 'heart', 'problems', '.', 'the', 'two', 'medications', 'i', 'listed', 'in', 'the', 'title', ',', 'st', '##rat', '##tera', 'and', 'int', '##uni', '##v', ',', 'are', 'not', 'st', '##im', '##ula', '##nts', ',', 'so', 'i', 'may', 'be', 'able', 'to', 'take', 'them', '.', 'of', 'course', 'i', \"'\", 'll', 'only', 'be', 'taking', 'one', ',', 'but', 'does', 'anyone', 'have', 'experience', 'with', 'either', 'of', 'them', '?', 'like', ',', 'how', 'well', 'do', 'they', 'work', ',', 'side', 'effects', ',', 'etc', '.', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 118\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['wondering', 'about', 'st', '##rat', '##tera', 'and', 'int', '##uni', '##v', '.', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'add', 'for', 'as', 'long', 'as', 'i', 'can', 'remember', ',', 'but', 'i', \"'\", 've', 'never', 'been', 'put', 'on', 'medication', 'since', 'most', 'of', 'them', 'are', 'st', '##im', '##ula', '##nts', 'and', 'i', 'have', 'some', 'heart', 'problems', '.', 'the', 'two', 'medications', 'i', 'listed', 'in', 'the', 'title', ',', 'st', '##rat', '##tera', 'and', 'int', '##uni', '##v', ',', 'are', 'not', 'st', '##im', '##ula', '##nts', ',', 'so', 'i', 'may', 'be', 'able', 'to', 'take', 'them', '.', 'of', 'course', 'i', \"'\", 'll', 'only', 'be', 'taking', 'one', ',', 'but', 'does', 'anyone', 'have', 'experience', 'with', 'either', 'of', 'them', '?', 'like', ',', 'how', 'well', 'do', 'they', 'work', ',', 'side', 'effects', ',', 'etc', '.', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'experience', 'a', 'change', 'in', 'their', 'voice', 'when', 'taking', 'concert', '##a', 'or', 'rita', '##lin', '?', 'i', 'haven', \"'\", 't', 'been', 'taking', 'rita', '##lin', 'very', 'long', ',', 'but', 'i', \"'\", 'm', 'noticing', 'a', 'strange', ',', 'though', 'welcome', 'side', 'effect', '.', 'one', 'of', 'the', 'physical', 'sensations', 'that', 'i', 'first', 'started', 'using', 'to', 'recognize', 'that', 'the', 'drug', 'was', 'working', 'was', 'a', 'change', 'in', 'my', 'voice', '.', 'it', 'gains', 'a', 'tim', '##bre', 'that', 'at', 'least', 'i', 'can', 'feel', '.', 'kinda', 'like', 'how', 'your', 'throat', 'feels', 'after', 'doing', 'a', 'bunch', 'of', 'vocal', 'exercise', '.', 'to', 'me', ',', 'it', 'makes', 'sense', 'that', 'the', 'physical', 'vocal', 'chords', 'could', 'be', 'affected', 'by', 'a', 'st', '##im', '##ula', '##nt', ',', 'just', 'wondering', 'why', 'people', 'don', \"'\", 't', 'mention', 'this', 'if', 'it', 'is', 'widespread', '.', 'regardless', ',', 'when', 'singing', 'in', 'my', 'car', 'or', 'shower', 'i', \"'\", 'm', 'now', 'always', 'on', 'pitch', 'and', 'feel', 'like', 'i', 'have', 'the', 'voice', 'of', 'an', 'angel', ':', 'p', '.', 'anyone', 'else', 'experience', 'something', 'similar', '?']\n",
      "INFO:__main__:Number of tokens: 160\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'experience', 'a', 'change', 'in', 'their', 'voice', 'when', 'taking', 'concert', '##a', 'or', 'rita', '##lin', '?', 'i', 'haven', \"'\", 't', 'been', 'taking', 'rita', '##lin', 'very', 'long', ',', 'but', 'i', \"'\", 'm', 'noticing', 'a', 'strange', ',', 'though', 'welcome', 'side', 'effect', '.', 'one', 'of', 'the', 'physical', 'sensations', 'that', 'i', 'first', 'started', 'using', 'to', 'recognize', 'that', 'the', 'drug', 'was', 'working', 'was', 'a', 'change', 'in', 'my', 'voice', '.', 'it', 'gains', 'a', 'tim', '##bre', 'that', 'at', 'least', 'i', 'can', 'feel', '.', 'kinda', 'like', 'how', 'your', 'throat', 'feels', 'after', 'doing', 'a', 'bunch', 'of', 'vocal', 'exercise', '.', 'to', 'me', ',', 'it', 'makes', 'sense', 'that', 'the', 'physical', 'vocal', 'chords', 'could', 'be', 'affected', 'by', 'a', 'st', '##im', '##ula', '##nt', ',', 'just', 'wondering', 'why', 'people', 'don', \"'\", 't', 'mention', 'this', 'if', 'it', 'is', 'widespread', '.', 'regardless', ',', 'when', 'singing', 'in', 'my', 'car', 'or', 'shower', 'i', \"'\", 'm', 'now', 'always', 'on', 'pitch', 'and', 'feel', 'like', 'i', 'have', 'the', 'voice', 'of', 'an', 'angel', ':', 'p', '.', 'anyone', 'else', 'experience', 'something', 'similar', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '(', 'h', ')', 'd', 'and', 'social', 'development']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '(', 'h', ')', 'd', 'and', 'social', 'development']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'your', 'motivation', 'like', 'off', 'medication', 'versus', 'on', 'medication', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'your', 'motivation', 'like', 'off', 'medication', 'versus', 'on', 'medication', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['li', '##bid', '##o', 'is', 'out', '-', 'of', '-', 'control', 'on', 'add', '##eral', '##l', '.', '.', '.', 'i', \"'\", 'm', 'a', '28', 'year', 'old', 'female', 'on', 'add', '##eral', '##l', 'for', 'ad', '##hd', '(', 'ina', '##tten', '##tive', 'type', ')', '.', 'just', 'started', 'it', 'about', '6', 'months', 'ago', '.', 'prior', 'to', 'this', ',', 'i', 'didn', \"'\", 't', 'have', 'much', 'of', 'a', 'sex', 'drive', 'and', 'it', 'was', 'very', 'difficult', 'for', 'me', 'to', 'reach', 'orgasm', 'either', 'with', 'a', 'partner', 'or', 'by', 'myself', '.', 'since', 'starting', 'add', '##eral', '##l', ',', 'i', 'can', 'feel', 'my', 'clit', 'throbbing', 'and', 'can', 'orgasm', 'very', 'easily', '.', 'this', 'is', 'good', ',', 'to', 'a', 'degree', '.', 'i', 'find', 'myself', 'mast', '##ur', '##bat', '##ing', 'a', 'lot', 'now', ',', 'to', 'the', 'point', 'where', 'it', 'is', 'preventing', 'me', 'from', 'getting', 'things', 'done', '.', 'i', \"'\", 'm', 'on', 'a', 'fairly', 'low', 'dos', '##age', ',', 'too', '.', 'anyone', 'else', 'experience', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 145\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['li', '##bid', '##o', 'is', 'out', '-', 'of', '-', 'control', 'on', 'add', '##eral', '##l', '.', '.', '.', 'i', \"'\", 'm', 'a', '28', 'year', 'old', 'female', 'on', 'add', '##eral', '##l', 'for', 'ad', '##hd', '(', 'ina', '##tten', '##tive', 'type', ')', '.', 'just', 'started', 'it', 'about', '6', 'months', 'ago', '.', 'prior', 'to', 'this', ',', 'i', 'didn', \"'\", 't', 'have', 'much', 'of', 'a', 'sex', 'drive', 'and', 'it', 'was', 'very', 'difficult', 'for', 'me', 'to', 'reach', 'orgasm', 'either', 'with', 'a', 'partner', 'or', 'by', 'myself', '.', 'since', 'starting', 'add', '##eral', '##l', ',', 'i', 'can', 'feel', 'my', 'clit', 'throbbing', 'and', 'can', 'orgasm', 'very', 'easily', '.', 'this', 'is', 'good', ',', 'to', 'a', 'degree', '.', 'i', 'find', 'myself', 'mast', '##ur', '##bat', '##ing', 'a', 'lot', 'now', ',', 'to', 'the', 'point', 'where', 'it', 'is', 'preventing', 'me', 'from', 'getting', 'things', 'done', '.', 'i', \"'\", 'm', 'on', 'a', 'fairly', 'low', 'dos', '##age', ',', 'too', '.', 'anyone', 'else', 'experience', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['had', 'my', 'first', 'psychiatrist', 'visit', 'today', ';', 'prescribed', 'me', 'lex', '##ap', '##ro', '(', 'an', 'ssr', '##i', ')', '.', 'hey', 'guys', ',', 'had', 'my', 'first', 'session', 'today', '.', 'talked', 'with', 'the', 'ps', '##ych', 'a', 'fair', 'bit', 'about', 'my', 'symptoms', 'and', 'he', 'prescribed', 'me', 'lex', '##ap', '##ro', '(', 'probably', 'due', 'to', 'my', 'history', 'of', 'depression', ',', 'yet', 'this', 'is', 'the', 'first', 'time', 'i', \"'\", 've', 'ever', 'been', 'prescribed', 'an', 'ssr', '##i', '.', 'i', 'ha', '##vn', \"'\", 't', 'had', 'any', 'symptoms', 'of', 'depression', 'in', 'several', 'months', ')', '.', 'does', 'anyone', 'have', 'any', 'experience', 'with', 'it', '?', 'from', 'what', 'i', \"'\", 've', 'seen', 'it', 'doesn', \"'\", 't', 'seem', 'to', 'be', 'promising', 'treatment', '.', 'in', 'my', 'stupidity', ',', 'i', 'didn', \"'\", 't', 'completely', 'outline', 'all', 'my', 'symptoms', ',', 'such', 'as', 'the', 'perpetual', 'stream', '-', 'of', '-', 'consciousness', 'thinking', 'or', \"'\", 'white', 'noise', \"'\", '.', 'i', 'feel', 'this', 'may', 'have', 'caused', 'a', 'bit', 'of', 'confusion', ',', 'maybe', '.', 'i', \"'\", 'm', 'going', 'back', 'in', 'a', 'month', 'to', 'see', 'how', 'lex', '##ap', '##ro', 'fares', 'for', 'me', ',', 'and', 'if', 'i', 'should', 'pursue', 'a', 'specialist', '.', 'he', 'also', 'said', 'that', 'if', 'i', 'were', 'to', 'pursue', 'a', 'prescription', 'from', 'an', 'ad', '##hd', 'specialist', ',', 'the', 'chances', 'of', 'me', 'getting', 'something', 'along', 'the', 'lines', 'of', 'add', '##eral', '##l', 'is', 'highly', 'unlikely', 'due', 'to', 'the', 'tight', 'restrictions', 'the', 'australian', 'govt', 'places', 'on', 'amp', '##het', '##amine', '##s', '.', 'doctors', 'would', 'rather', 'not', 'pre', '##scribe', 'it', 'even', 'if', 'i', 'have', 'all', 'the', 'symptoms', ':', '\\\\', '.']\n",
      "INFO:__main__:Number of tokens: 243\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['had', 'my', 'first', 'psychiatrist', 'visit', 'today', ';', 'prescribed', 'me', 'lex', '##ap', '##ro', '(', 'an', 'ssr', '##i', ')', '.', 'hey', 'guys', ',', 'had', 'my', 'first', 'session', 'today', '.', 'talked', 'with', 'the', 'ps', '##ych', 'a', 'fair', 'bit', 'about', 'my', 'symptoms', 'and', 'he', 'prescribed', 'me', 'lex', '##ap', '##ro', '(', 'probably', 'due', 'to', 'my', 'history', 'of', 'depression', ',', 'yet', 'this', 'is', 'the', 'first', 'time', 'i', \"'\", 've', 'ever', 'been', 'prescribed', 'an', 'ssr', '##i', '.', 'i', 'ha', '##vn', \"'\", 't', 'had', 'any', 'symptoms', 'of', 'depression', 'in', 'several', 'months', ')', '.', 'does', 'anyone', 'have', 'any', 'experience', 'with', 'it', '?', 'from', 'what', 'i', \"'\", 've', 'seen', 'it', 'doesn', \"'\", 't', 'seem', 'to', 'be', 'promising', 'treatment', '.', 'in', 'my', 'stupidity', ',', 'i', 'didn', \"'\", 't', 'completely', 'outline', 'all', 'my', 'symptoms', ',', 'such', 'as', 'the', 'perpetual', 'stream', '-', 'of', '-', 'consciousness', 'thinking', 'or', \"'\", 'white', 'noise', \"'\", '.', 'i', 'feel', 'this', 'may', 'have', 'caused', 'a', 'bit', 'of', 'confusion', ',', 'maybe', '.', 'i', \"'\", 'm', 'going', 'back', 'in', 'a', 'month', 'to', 'see', 'how', 'lex', '##ap', '##ro', 'fares', 'for', 'me', ',', 'and', 'if', 'i', 'should', 'pursue', 'a', 'specialist', '.', 'he', 'also', 'said', 'that', 'if', 'i', 'were', 'to', 'pursue', 'a', 'prescription', 'from', 'an', 'ad', '##hd', 'specialist', ',', 'the', 'chances', 'of', 'me', 'getting', 'something', 'along', 'the', 'lines', 'of', 'add', '##eral', '##l', 'is', 'highly', 'unlikely', 'due', 'to', 'the', 'tight', 'restrictions', 'the', 'australian', 'govt', 'places', 'on', 'amp', '##het', '##amine', '##s', '.', 'doctors', 'would', 'rather', 'not', 'pre', '##scribe', 'it', 'even', 'if', 'i', 'have', 'all', 'the', 'symptoms', ':', '\\\\', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['help', '!', '-', 'alternatives', 'to', 'add', '##eral', '##l', 'hi', 'guys', ',', 'i', 'was', 'diagnosed', 'with', 'add', 'several', 'years', 'ago', ',', 'and', 'was', 'put', 'on', 'add', '##eral', '##l', 'to', 'help', 'deal', 'with', 'the', 'problem', '.', 'it', 'worked', 'great', ',', 'but', 'i', 'suffered', 'several', 'side', 'effects', 'like', 'an', 'increase', 'in', 'heart', 'rate', '(', 'which', 'often', 'made', 'me', 'sweat', 'prof', '##use', '##ly', ')', 'and', 'a', 'lack', 'of', 'appetite', '.', 'i', 'stopped', 'taking', 'add', '##eral', '##l', 'around', 'my', 'senior', 'year', 'of', 'high', 'school', ',', 'and', 'then', 'went', 'off', '-', 'and', '-', 'on', 'during', 'my', 'freshman', 'year', 'of', 'college', '.', 'i', \"'\", 'm', 'currently', 'a', 'sophomore', 'and', 'am', 'off', 'of', 'the', 'drug', 'completely', ',', 'but', 'my', 'grades', 'are', 'falling', '.', 'i', 'would', 'like', 'to', 'get', 'back', 'on', 'some', 'add', 'drugs', ',', 'but', 'i', 'want', 'to', 'avoid', 'add', '##eral', '##l', 'because', 'of', 'the', 'aforementioned', 'side', 'effects', '.', 'what', 'other', 'options', 'to', 'i', 'have', '?']\n",
      "INFO:__main__:Number of tokens: 148\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['help', '!', '-', 'alternatives', 'to', 'add', '##eral', '##l', 'hi', 'guys', ',', 'i', 'was', 'diagnosed', 'with', 'add', 'several', 'years', 'ago', ',', 'and', 'was', 'put', 'on', 'add', '##eral', '##l', 'to', 'help', 'deal', 'with', 'the', 'problem', '.', 'it', 'worked', 'great', ',', 'but', 'i', 'suffered', 'several', 'side', 'effects', 'like', 'an', 'increase', 'in', 'heart', 'rate', '(', 'which', 'often', 'made', 'me', 'sweat', 'prof', '##use', '##ly', ')', 'and', 'a', 'lack', 'of', 'appetite', '.', 'i', 'stopped', 'taking', 'add', '##eral', '##l', 'around', 'my', 'senior', 'year', 'of', 'high', 'school', ',', 'and', 'then', 'went', 'off', '-', 'and', '-', 'on', 'during', 'my', 'freshman', 'year', 'of', 'college', '.', 'i', \"'\", 'm', 'currently', 'a', 'sophomore', 'and', 'am', 'off', 'of', 'the', 'drug', 'completely', ',', 'but', 'my', 'grades', 'are', 'falling', '.', 'i', 'would', 'like', 'to', 'get', 'back', 'on', 'some', 'add', 'drugs', ',', 'but', 'i', 'want', 'to', 'avoid', 'add', '##eral', '##l', 'because', 'of', 'the', 'aforementioned', 'side', 'effects', '.', 'what', 'other', 'options', 'to', 'i', 'have', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['where', 'to', 'get', 'tested', 'in', 'vancouver', '?', 'i', \"'\", 'm', '20', 'and', 'looking', 'to', 'get', 'tested', '.', 'i', 'have', 'no', 'idea', 'where', 'to', 'start', '.', 'also', 'i', 'believe', 'my', 'insurance', 'is', 'called', 'something', 'like', 'blue', 'cross', '?', 'will', 'this', 'cover', 'any', 'of', 'the', 'tests', 'or', 'anything', '?', 'thanks', 'a', 'lot', '.']\n",
      "INFO:__main__:Number of tokens: 51\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['where', 'to', 'get', 'tested', 'in', 'vancouver', '?', 'i', \"'\", 'm', '20', 'and', 'looking', 'to', 'get', 'tested', '.', 'i', 'have', 'no', 'idea', 'where', 'to', 'start', '.', 'also', 'i', 'believe', 'my', 'insurance', 'is', 'called', 'something', 'like', 'blue', 'cross', '?', 'will', 'this', 'cover', 'any', 'of', 'the', 'tests', 'or', 'anything', '?', 'thanks', 'a', 'lot', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'helped', 'by', 'halo', 'gaming']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'helped', 'by', 'halo', 'gaming']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['focusing', 'on', 'your', 'own', 'triggers', 'when', 'dealing', 'with', 'difficult', 'people']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['focusing', 'on', 'your', 'own', 'triggers', 'when', 'dealing', 'with', 'difficult', 'people']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tip', ':', 'use', 'a', 'text', '-', 'to', '-', 'speech', 'add', '##on', 'for', 'your', 'web', 'browser', 'to', 'help', 'you', 'manage', 'getting', 'through', 'long', 'articles', '.', 'i', 'have', 'a', 'very', 'hard', 'time', 'reading', 'long', 'texts', 'like', 'articles', 'because', 'of', 'my', 'short', 'attention', '##sp', '##an', '.', 'i', 'just', 'realised', 'that', 'if', 'i', 'use', 'a', 'text', '-', 'to', '-', 'speech', 'tool', ',', '[', 'like', 'this', 'one', 'for', 'chrome', ']', '(', 'https', ':', '/', '/', 'chrome', '.', 'google', '.', 'com', '/', 'web', '##stor', '##e', '/', 'detail', '/', 'pg', '##eo', '##lal', '##ili', '##fp', '##od', '##hee', '##oc', '##dm', '##bh', '##eh', '##gn', '##kk', '##ba', '##k', ')', ',', 'i', 'just', 'set', 'it', 'to', 'read', 'the', 'text', 'at', 'the', 'same', 'time', 'i', 'am', 'reading', 'it', '.', 'having', 'the', 'text', 'being', 'red', 'to', 'me', 'at', 'the', 'same', 'time', 'as', 'i', 'am', 'reading', 'it', 'the', 'text', '-', 'to', '-', 'speech', 'program', 'is', 'holding', 'my', 'hand', 'through', 'the', 'entire', 'article', '.', 'it', \"'\", 's', 'helped', 'me', 'al', '##ot', ',', 'hope', 'it', 'works', 'for', 'you', 'to', ':', ')']\n",
      "INFO:__main__:Number of tokens: 164\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tip', ':', 'use', 'a', 'text', '-', 'to', '-', 'speech', 'add', '##on', 'for', 'your', 'web', 'browser', 'to', 'help', 'you', 'manage', 'getting', 'through', 'long', 'articles', '.', 'i', 'have', 'a', 'very', 'hard', 'time', 'reading', 'long', 'texts', 'like', 'articles', 'because', 'of', 'my', 'short', 'attention', '##sp', '##an', '.', 'i', 'just', 'realised', 'that', 'if', 'i', 'use', 'a', 'text', '-', 'to', '-', 'speech', 'tool', ',', '[', 'like', 'this', 'one', 'for', 'chrome', ']', '(', 'https', ':', '/', '/', 'chrome', '.', 'google', '.', 'com', '/', 'web', '##stor', '##e', '/', 'detail', '/', 'pg', '##eo', '##lal', '##ili', '##fp', '##od', '##hee', '##oc', '##dm', '##bh', '##eh', '##gn', '##kk', '##ba', '##k', ')', ',', 'i', 'just', 'set', 'it', 'to', 'read', 'the', 'text', 'at', 'the', 'same', 'time', 'i', 'am', 'reading', 'it', '.', 'having', 'the', 'text', 'being', 'red', 'to', 'me', 'at', 'the', 'same', 'time', 'as', 'i', 'am', 'reading', 'it', 'the', 'text', '-', 'to', '-', 'speech', 'program', 'is', 'holding', 'my', 'hand', 'through', 'the', 'entire', 'article', '.', 'it', \"'\", 's', 'helped', 'me', 'al', '##ot', ',', 'hope', 'it', 'works', 'for', 'you', 'to', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['loki', '##ng', 'for', 'a', 'book', 'for', 'parent', 'of', 'adults', 'with', 'ad', '##hd', 'both', 'my', 'sister', '(', 'age', '19', ')', 'and', 'i', '(', 'age', '31', ')', 'have', 'ad', '##hd', '.', 'i', 'am', 'looking', 'for', 'a', 'book', 'to', 'give', 'to', 'my', 'parents', 'to', 'better', 'understand', 'their', '(', 'adult', ')', 'children', 'with', 'ad', '##hd', '.', 'there', 'are', 'plenty', 'for', 'parents', 'with', 'children', 'that', 'have', 'ad', '##hd', 'but', 'i', 'can', \"'\", 't', 'seem', 'to', 'find', 'any', 'for', 'adults', '.', 'my', 'sister', 'has', 'more', 'hyper', '##active', ',', 'imp', '##ulsive', 'behavior', '.', 'i', \"'\", 'm', 'the', 'intro', '##vert', '.', 'does', 'anyone', 'have', 'any', 'suggestions', '?', 'thanks']\n",
      "INFO:__main__:Number of tokens: 100\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['loki', '##ng', 'for', 'a', 'book', 'for', 'parent', 'of', 'adults', 'with', 'ad', '##hd', 'both', 'my', 'sister', '(', 'age', '19', ')', 'and', 'i', '(', 'age', '31', ')', 'have', 'ad', '##hd', '.', 'i', 'am', 'looking', 'for', 'a', 'book', 'to', 'give', 'to', 'my', 'parents', 'to', 'better', 'understand', 'their', '(', 'adult', ')', 'children', 'with', 'ad', '##hd', '.', 'there', 'are', 'plenty', 'for', 'parents', 'with', 'children', 'that', 'have', 'ad', '##hd', 'but', 'i', 'can', \"'\", 't', 'seem', 'to', 'find', 'any', 'for', 'adults', '.', 'my', 'sister', 'has', 'more', 'hyper', '##active', ',', 'imp', '##ulsive', 'behavior', '.', 'i', \"'\", 'm', 'the', 'intro', '##vert', '.', 'does', 'anyone', 'have', 'any', 'suggestions', '?', 'thanks']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sometimes', 'i', 'just', 'feel', 'abnormal', 'it', '’', 's', 'always', 'been', 'this', 'way', ',', 'really', '.', 'but', 'being', 'in', 'college', 'and', 'under', 'extreme', 'academic', 'scrutiny', 'has', 'made', 'it', 'impossible', 'to', 'ignore', '.', 'the', 'things', '‘', 'normal', '’', 'people', 'find', 'second', 'nature', 'when', 'doing', '‘', 'normal', '’', 'tasks', ',', 'even', 'from', 'how', 'we', '’', 're', 'supposed', 'to', 'approach', 'this', 'essay', 'to', 'the', 'proper', 'way', 'to', 'blow', 'your', 'nose', ',', '(', 'not', 'literally', 'that', 'but', 'this', 'is', 'the', 'level', 'of', \"'\", 'basic', \"'\", 'at', 'which', 'i', 'do', 'things', 'differently', ')', 'the', 'things', 'that', 'they', 'just', 'come', 'up', 'with', 'automatically', 'every', 'day', 'and', 'don', '’', 't', 'realize', 'they', '’', 're', 'doing', '…', 'i', 'just', 'don', '’', 't', 'have', '.', 'i', 'always', 'have', 'to', 'ask', 'for', 'cl', '##ari', '##fication', 'on', 'assignments', 'that', 'everyone', 'else', 'takes', 'to', 'automatically', 'to', 'the', 'point', 'where', 'i', 'feel', 'like', 'i', '’', 'm', 'missing', 'something', '.', 'nobody', 'else', 'ever', 'has', 'to', 'ask', 'the', 'teacher', 'these', 'things', '.', 'ever', '.', 'i', 'even', 'lead', 'my', 'groups', 'as', '##tray', 'when', 'classes', 'are', 'broken', 'up', 'into', 'groups', 'to', 'work', 'on', 'things', '.', 'though', 'i', 'am', 'a', 'leader', ',', 'i', 'mis', '##under', '##stand', 'the', 'assignment', ',', 'over', '##thi', '##nk', 'it', ',', 'waste', 'time', 'figuring', 'it', 'out', ',', 'and', 'come', 'up', 'with', 'a', 'far', 'more', 'complicated', 'and', 'difficult', 'and', 'completely', 'far', 'and', 'away', 'thing', 'to', 'do', 'and', 'goal', 'to', 'reach', 'rather', 'than', 'what', 'we', 'were', 'supposed', 'to', 'be', 'doing', 'and', 'take', 'my', 'classmates', 'down', 'with', 'me', '.', 'we', 'get', 'an', 'assignment', 'and', 'i', 'get', 'extremely', 'confused', 'and', 'spend', 'the', 'next', 'few', 'days', 'sweating', 'and', 'then', 'come', 'in', 'on', 'due', 'day', 'and', 'everyone', 'else', 'has', 'a', 'perfect', 'little', 'on', '-', 'point', 'similar', 'projects', 'and', 'i', 'did', 'totally', 'the', 'wrong', 'thing', '.', 'it', '’', 's', 'like', 'we', '’', 're', 'all', 'playing', 'a', 'video', 'game', ',', 'and', 'there', '’', 's', 'a', 'very', 'detailed', 'and', 'complicated', 'options', 'menu', ',', 'and', 'everybody', 'else', 'has', 'a', 'list', 'of', 'default', 'settings', ',', 'but', 'i', 'don', '’', 't', '.', 'on', 'the', 'flip', '##side', 'i', 'tend', 'to', 'have', 'automatic', 'and', 'easy', 'answers', 'to', 'things', 'that', 'everyone', 'else', 'doesn', '’', 't', 'understand', ',', 'things', 'that', 'are', 'viewed', 'as', 'difficult', 'and', 'intellectual', 'or', 'philosophical', '.', 'in', 'seeing', 'these', 'things', 'myself', 'i', 'can', 'em', '##path', '##ize', 'with', 'the', 'sheer', 'level', 'of', 'inc', '##omp', '##re', '##hen', '##sion', 'they', 'have', 'towards', 'me', 'with', 'the', 'difficulty', 'i', 'have', 'on', 'something', 'that', 'comes', 'automatically', 'to', 'them', ';', 'often', 'with', 'certain', 'things', 'i', 'just', 'get', 'it', 'so', 'quickly', 'i', 'just', 'don', '’', 't', 'understand', 'why', 'anyone', 'would', 'have', 'a', 'problem', 'with', 'it', '.', 'it', '’', 's', 'not', 'ass', '##hat', '##tery', 'or', 's', '##nob', '##bis', '##hn', '##ess', ',', 'it', 'just', '…', 'is', 'that', 'way', '.', 'perhaps', 'this', 'would', 'all', 'be', 'okay', 'if', 'this', 'were', 'all', 'a', 'wo', '##e', '-', 'is', '-', 'me', 'illusion', 'that', 'was', 'all', 'in', 'my', 'head', ',', 'a', 'mind', 'game', 'set', 'upon', 'a', 'teenager', 'by', 'her', 'own', 'adolescent', 'and', 'ang', '##st', '-', 'ridden', 'mind', ',', 'but', 'it', '’', 's', 'not', '.', 'i', 'know', 'that', 'it', '’', 's', 'my', 'add', '-', 'pi', '.', 'i', 'can', '’', 't', 'get', 'rid', 'of', 'it', ',', 'i', 'can', '’', 't', 'change', 'my', 'mind', ',', 'i', 'can', '’', 't', 'grow', 'out', 'of', 'it', ',', 'i', 'can', '’', 't', '‘', 'change', '’', 'myself', 'in', 'any', 'way', 'and', 'make', 'everything', 'better', '.', 'there', 'is', 'something', '*', 'wrong', '*', 'with', 'me', 'and', 'it', 'is', 'the', 'most', 'difficult', 'thing', 'in', 'the', 'world', 'to', 'come', 'to', 'terms', 'with', 'that', '.', ':', '(']\n",
      "INFO:__main__:Number of tokens: 569\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['sometimes', 'i', 'just', 'feel', 'abnormal', 'it', '’', 's', 'always', 'been', 'this', 'way', ',', 'really', '.', 'but', 'being', 'in', 'college', 'and', 'under', 'extreme', 'academic', 'scrutiny', 'has', 'made', 'it', 'impossible', 'to', 'ignore', '.', 'the', 'things', '‘', 'normal', '’', 'people', 'find', 'second', 'nature', 'when', 'doing', '‘', 'normal', '’', 'tasks', ',', 'even', 'from', 'how', 'we', '’', 're', 'supposed', 'to', 'approach', 'this', 'essay', 'to', 'the', 'proper', 'way', 'to', 'blow', 'your', 'nose', ',', '(', 'not', 'literally', 'that', 'but', 'this', 'is', 'the', 'level', 'of', \"'\", 'basic', \"'\", 'at', 'which', 'i', 'do', 'things', 'differently', ')', 'the', 'things', 'that', 'they', 'just', 'come', 'up', 'with', 'automatically', 'every', 'day', 'and', 'don', '’', 't', 'realize', 'they', '’', 're', 'doing', '…', 'i', 'just', 'don', '’', 't', 'have', '.', 'i', 'always', 'have', 'to', 'ask', 'for', 'cl', '##ari', '##fication', 'on', 'assignments', 'that', 'everyone', 'else', 'takes', 'to', 'automatically', 'to', 'the', 'point', 'where', 'i', 'feel', 'like', 'i', '’', 'm', 'missing', 'something', '.', 'nobody', 'else', 'ever', 'has', 'to', 'ask', 'the', 'teacher', 'these', 'things', '.', 'ever', '.', 'i', 'even', 'lead', 'my', 'groups', 'as', '##tray', 'when', 'classes', 'are', 'broken', 'up', 'into', 'groups', 'to', 'work', 'on', 'things', '.', 'though', 'i', 'am', 'a', 'leader', ',', 'i', 'mis', '##under', '##stand', 'the', 'assignment', ',', 'over', '##thi', '##nk', 'it', ',', 'waste', 'time', 'figuring', 'it', 'out', ',', 'and', 'come', 'up', 'with', 'a', 'far', 'more', 'complicated', 'and', 'difficult', 'and', 'completely', 'far', 'and', 'away', 'thing', 'to', 'do', 'and', 'goal', 'to', 'reach', 'rather', 'than', 'what', 'we', 'were', 'supposed', 'to', 'be', 'doing', 'and', 'take', 'my', 'classmates', 'down', 'with', 'me', '.', 'we', 'get', 'an', 'assignment', 'and', 'i', 'get', 'extremely', 'confused', 'and', 'spend', 'the', 'next', 'few', 'days', 'sweating', 'and', 'then', 'come', 'in', 'on', 'due', 'day', 'and', 'everyone', 'else', 'has', 'a', 'perfect', 'little', 'on', '-', 'point', 'similar', 'projects', 'and', 'i', 'did', 'totally', 'the', 'wrong', 'thing', '.', 'it', '’', 's', 'like', 'we', '’', 're', 'all', 'playing', 'a', 'video', 'game', ',', 'and', 'there', '’', 's', 'a', 'very', 'detailed', 'and', 'complicated', 'options', 'menu', ',', 'and', 'everybody', 'else', 'has', 'a', 'list', 'of', 'default', 'settings', ',', 'but', 'i', 'don', '’', 't', '.', 'on', 'the', 'flip', '##side', 'i', 'tend', 'to', 'have', 'automatic', 'and', 'easy', 'answers', 'to', 'things', 'that', 'everyone', 'else', 'doesn', '’', 't', 'understand', ',', 'things', 'that', 'are', 'viewed', 'as', 'difficult', 'and', 'intellectual', 'or', 'philosophical', '.', 'in', 'seeing', 'these', 'things', 'myself', 'i', 'can', 'em', '##path', '##ize', 'with', 'the', 'sheer', 'level', 'of', 'inc', '##omp', '##re', '##hen', '##sion', 'they', 'have', 'towards', 'me', 'with', 'the', 'difficulty', 'i', 'have', 'on', 'something', 'that', 'comes', 'automatically', 'to', 'them', ';', 'often', 'with', 'certain', 'things', 'i', 'just', 'get', 'it', 'so', 'quickly', 'i', 'just', 'don', '’', 't', 'understand', 'why', 'anyone', 'would', 'have', 'a', 'problem', 'with', 'it', '.', 'it', '’', 's', 'not', 'ass', '##hat', '##tery', 'or', 's', '##nob', '##bis', '##hn', '##ess', ',', 'it', 'just', '…', 'is', 'that', 'way', '.', 'perhaps', 'this', 'would', 'all', 'be', 'okay', 'if', 'this', 'were', 'all', 'a', 'wo', '##e', '-', 'is', '-', 'me', 'illusion', 'that', 'was', 'all', 'in', 'my', 'head', ',', 'a', 'mind', 'game', 'set', 'upon', 'a', 'teenager', 'by', 'her', 'own', 'adolescent', 'and', 'ang', '##st', '-', 'ridden', 'mind', ',', 'but', 'it', '’', 's', 'not', '.', 'i', 'know', 'that', 'it', '’', 's', 'my', 'add', '-', 'pi', '.', 'i', 'can', '’', 't', 'get', 'rid', 'of', 'it', ',', 'i', 'can', '’'], ['t', 'change', 'my', 'mind', ',', 'i', 'can', '’', 't', 'grow', 'out', 'of', 'it', ',', 'i', 'can', '’', 't', '‘', 'change', '’', 'myself', 'in', 'any', 'way', 'and', 'make', 'everything', 'better', '.', 'there', 'is', 'something', '*', 'wrong', '*', 'with', 'me', 'and', 'it', 'is', 'the', 'most', 'difficult', 'thing', 'in', 'the', 'world', 'to', 'come', 'to', 'terms', 'with', 'that', '.', ':', '(']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['on', 'avoiding', 'describing', 'ad', '##hd', 'symptoms', 'to', 'people', 'i', 'lo', '##ath', '##e', 'describing', 'ad', '##hd', 'symptoms', 'to', 'people', 'because', 'it', 'seems', 'they', 'will', 'often', 'relate', 'way', 'too', 'fast', 'to', 'it', '.', 'e', '##g', '-', '\"', 'oh', ',', 'you', 'get', 'bored', 'and', 'distracted', 'by', 'things', 'that', 'don', \"'\", 't', 'interest', 'you', '?', 'you', 'pro', '##cr', '##ast', '##inate', '?', 'me', 'too', '!', '!', '\"', 'they', 'can', 'understand', 'these', 'symptoms', ',', 'but', 'they', 'don', \"'\", 't', 'understand', 'that', 'you', 'experience', 'them', 'to', 'a', 'point', 'where', 'they', 'are', 'ruining', 'your', 'fucking', 'life', 'and', 'spiral', '##ing', 'out', 'of', 'control', 'and', 'find', 'it', 'difficult', 'or', 'impossible', 'to', 'muster', 'up', 'the', 'slightest', 'shit', 'for', 'what', 'could', 'otherwise', 'be', 'a', 'to', '##ler', '##able', 'life', '.', 'anyone', 'else', 'hate', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 122\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['on', 'avoiding', 'describing', 'ad', '##hd', 'symptoms', 'to', 'people', 'i', 'lo', '##ath', '##e', 'describing', 'ad', '##hd', 'symptoms', 'to', 'people', 'because', 'it', 'seems', 'they', 'will', 'often', 'relate', 'way', 'too', 'fast', 'to', 'it', '.', 'e', '##g', '-', '\"', 'oh', ',', 'you', 'get', 'bored', 'and', 'distracted', 'by', 'things', 'that', 'don', \"'\", 't', 'interest', 'you', '?', 'you', 'pro', '##cr', '##ast', '##inate', '?', 'me', 'too', '!', '!', '\"', 'they', 'can', 'understand', 'these', 'symptoms', ',', 'but', 'they', 'don', \"'\", 't', 'understand', 'that', 'you', 'experience', 'them', 'to', 'a', 'point', 'where', 'they', 'are', 'ruining', 'your', 'fucking', 'life', 'and', 'spiral', '##ing', 'out', 'of', 'control', 'and', 'find', 'it', 'difficult', 'or', 'impossible', 'to', 'muster', 'up', 'the', 'slightest', 'shit', 'for', 'what', 'could', 'otherwise', 'be', 'a', 'to', '##ler', '##able', 'life', '.', 'anyone', 'else', 'hate', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'i', 'have', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'i', 'have', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['gaming', '=', 'life', 'improvement', '!', '(', 'x', '-', 'post', 'from', '/', 'r', '/', 'gets', '##tu', '##dy', '##ing', ')']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['gaming', '=', 'life', 'improvement', '!', '(', 'x', '-', 'post', 'from', '/', 'r', '/', 'gets', '##tu', '##dy', '##ing', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['final', 'update', ':', 'got', 'a', 'second', 'opinion', ',', 'and', 'now', 'on', 'a', 'trial', 'run', 'of', 'medication', '!', 'first', 'off', ':', 'ye', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ah', '.', '[', 'op', ',', 'received', 'a', 'diagnosis', 'of', 'de', '##pressive', 'disorder', '-', 'not', 'otherwise', 'specified', '.', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'l', '##ns', '##j', '##y', '/', 'went', '_', 'in', '_', 'to', '_', 'be', '_', 'tested', '_', 'for', '_', 'ad', '##hd', '_', 'diagnosed', '_', 'with', '/', ')', '[', 'second', 'post', '.', 'raging', 'pretty', 'hard', 'at', 'life', 'and', 'failing', 'classes', '.', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'm', '##q', '##x', '##z', '##q', '/', 'at', '_', 'some', '_', 'point', '_', 'you', '_', 'just', '_', 'start', '_', 'giving', '_', 'up', '/', ')', 'a', 'month', 'into', 'the', 'semester', ',', 'i', 'receive', 'news', 'that', 'i', \"'\", 'm', 'getting', 'a', 'grant', 'for', 'over', '$', '1', ',', '000', '.', 'and', 'i', 'knew', 'exactly', 'what', 'i', 'was', 'going', 'to', 'use', 'that', 'money', 'on', '-', '-', 'getting', 'the', 'second', 'opinion', '.', 'which', 'i', 'did', '.', 'a', 'long', 'visit', ',', 'going', 'over', 'some', 'test', 'results', ',', 'and', 'talking', 'in', 'depth', 'about', 'my', 'childhood', 'and', 'the', 'problems', 'i', \"'\", 've', 'been', 'going', 'through', ',', 'and', 'the', 'doctor', 'came', 'to', 'the', 'conclusion', 'that', 'the', 'de', '##pressive', 'disorder', 'was', 'being', 'caused', 'by', 'the', 'ad', '##hd', '-', '-', 'not', 'the', 'other', 'way', 'around', '.', 'i', \"'\", 'm', 'doing', 'a', 'trial', 'run', 'of', 'medication', 'now', ',', 'and', 'quite', 'honestly', '-', '-', 'what', 'the', 'fuck', '?', 'is', 'this', 'what', 'life', 'is', 'supposed', 'to', 'be', 'like', '?', 'doing', 'things', 'isn', \"'\", 't', 'a', 'struggle', ',', 'and', 'i', 'can', 'carry', 'on', 'conversations', '-', '-', 'i', 'don', \"'\", 't', 'even', 'understand', '.', 'i', \"'\", 'm', 'still', 'kind', 'of', 'shocked', 'at', 'the', 'whole', 'thing', '.', 'now', 'the', 'problem', 'isn', \"'\", 't', '\"', 'what', 'can', 'i', 'mo', '##tiv', '##ate', 'myself', 'to', 'do', '?', '\"', 'now', ',', 'it', \"'\", 's', '\"', 'what', 'dream', 'do', 'i', 'accomplish', 'first', '?', '\"', 'now', 'i', 'just', 'have', 'to', 'start', 'remembering', 'to', 'eat', '.', 'x', '##d']\n",
      "INFO:__main__:Number of tokens: 365\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['final', 'update', ':', 'got', 'a', 'second', 'opinion', ',', 'and', 'now', 'on', 'a', 'trial', 'run', 'of', 'medication', '!', 'first', 'off', ':', 'ye', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ee', '##ah', '.', '[', 'op', ',', 'received', 'a', 'diagnosis', 'of', 'de', '##pressive', 'disorder', '-', 'not', 'otherwise', 'specified', '.', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'l', '##ns', '##j', '##y', '/', 'went', '_', 'in', '_', 'to', '_', 'be', '_', 'tested', '_', 'for', '_', 'ad', '##hd', '_', 'diagnosed', '_', 'with', '/', ')', '[', 'second', 'post', '.', 'raging', 'pretty', 'hard', 'at', 'life', 'and', 'failing', 'classes', '.', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'm', '##q', '##x', '##z', '##q', '/', 'at', '_', 'some', '_', 'point', '_', 'you', '_', 'just', '_', 'start', '_', 'giving', '_', 'up', '/', ')', 'a', 'month', 'into', 'the', 'semester', ',', 'i', 'receive', 'news', 'that', 'i', \"'\", 'm', 'getting', 'a', 'grant', 'for', 'over', '$', '1', ',', '000', '.', 'and', 'i', 'knew', 'exactly', 'what', 'i', 'was', 'going', 'to', 'use', 'that', 'money', 'on', '-', '-', 'getting', 'the', 'second', 'opinion', '.', 'which', 'i', 'did', '.', 'a', 'long', 'visit', ',', 'going', 'over', 'some', 'test', 'results', ',', 'and', 'talking', 'in', 'depth', 'about', 'my', 'childhood', 'and', 'the', 'problems', 'i', \"'\", 've', 'been', 'going', 'through', ',', 'and', 'the', 'doctor', 'came', 'to', 'the', 'conclusion', 'that', 'the', 'de', '##pressive', 'disorder', 'was', 'being', 'caused', 'by', 'the', 'ad', '##hd', '-', '-', 'not', 'the', 'other', 'way', 'around', '.', 'i', \"'\", 'm', 'doing', 'a', 'trial', 'run', 'of', 'medication', 'now', ',', 'and', 'quite', 'honestly', '-', '-', 'what', 'the', 'fuck', '?', 'is', 'this', 'what', 'life', 'is', 'supposed', 'to', 'be', 'like', '?', 'doing', 'things', 'isn', \"'\", 't', 'a', 'struggle', ',', 'and', 'i', 'can', 'carry', 'on', 'conversations', '-', '-', 'i', 'don', \"'\", 't', 'even', 'understand', '.', 'i', \"'\", 'm', 'still', 'kind', 'of', 'shocked', 'at', 'the', 'whole', 'thing', '.', 'now', 'the', 'problem', 'isn', \"'\", 't', '\"', 'what', 'can', 'i', 'mo', '##tiv', '##ate', 'myself', 'to', 'do', '?', '\"', 'now', ',', 'it', \"'\", 's', '\"', 'what', 'dream', 'do', 'i', 'accomplish', 'first', '?', '\"', 'now', 'i', 'just', 'have', 'to', 'start', 'remembering', 'to', 'eat', '.', 'x', '##d']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'the', 'difference', 'between', 'being', 'lazy', 'and', 'having', 'add', '?', 'sometimes', 'i', 'don', \"'\", 't', 'know', 'which', 'i', 'have', 'when', 'i', 'don', \"'\", 't', 'do', 'something', 'ha', '##ha', '.']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'the', 'difference', 'between', 'being', 'lazy', 'and', 'having', 'add', '?', 'sometimes', 'i', 'don', \"'\", 't', 'know', 'which', 'i', 'have', 'when', 'i', 'don', \"'\", 't', 'do', 'something', 'ha', '##ha', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['question', 'from', 'a', 'future', 'student', 'hey', ',', 'i', \"'\", 'm', 'thinking', 'about', 'going', 'back', 'to', 'university', '.', 'was', 'diagnosed', 'with', 'ad', '##hd', 'and', 'been', 'taking', 'rita', '##lin', '.', 'i', 'occasional', 'take', 'a', 'vacation', 'from', 'it', 'to', 'make', 'sure', 'that', 'i', 'really', 'need', 'it', '(', 'weird', 'but', 'just', 'to', 'prove', 'to', 'my', 'self', 'i', \"'\", 'm', 'not', 'a', 'junk', '##ie', ')', '.', 'i', \"'\", 've', 'been', 'out', 'of', 'university', 'for', '2', 'years', '(', 'spent', '6', 'years', 'going', 'from', 'faculty', 'to', 'faculty', 'struggling', 'trying', 'to', 'find', 'my', 'niche', ')', 'and', 'have', 'been', 'working', ',', 'and', 'saving', 'money', '.', 'i', \"'\", 've', 'been', 'doing', 'some', 'more', 'research', 'on', 'learning', 'strategies', 'for', 'people', 'with', 'ad', '##hd', ',', 'but', 'i', 'thought', 'that', 'it', 'might', 'be', 'more', 'useful', 'to', 'ask', 'this', 'community', 'as', 'well', '.', 'if', 'i', 'go', 'back', 'i', 'want', 'to', 'try', 'for', 'medicine', '(', 'long', 'shot', 'at', 'this', 'point', ')', 'but', 'i', 'need', 'to', 're', '-', 'evaluate', 'my', 'learning', 'techniques', 'first', 'before', 'i', 'begin', '.', 'even', 'if', 'its', 'just', 'a', 'link', 'to', 'something', 'that', 'you', 'might', 'have', 'found', 'useful', ',', 'it', 'would', 'be', 'greatly', 'appreciated', 'p', '.', 's', '.', 'apologies', 'for', 'if', 'this', 'already', 'been', 'covered', 'in', 'a', 'previous', 'thread', ',', 'i', 'tried', 'to', 'search', 'but', 'it', 'couldn', \"'\", 't', 'find', 'anything', 'relevant', '.']\n",
      "INFO:__main__:Number of tokens: 210\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['question', 'from', 'a', 'future', 'student', 'hey', ',', 'i', \"'\", 'm', 'thinking', 'about', 'going', 'back', 'to', 'university', '.', 'was', 'diagnosed', 'with', 'ad', '##hd', 'and', 'been', 'taking', 'rita', '##lin', '.', 'i', 'occasional', 'take', 'a', 'vacation', 'from', 'it', 'to', 'make', 'sure', 'that', 'i', 'really', 'need', 'it', '(', 'weird', 'but', 'just', 'to', 'prove', 'to', 'my', 'self', 'i', \"'\", 'm', 'not', 'a', 'junk', '##ie', ')', '.', 'i', \"'\", 've', 'been', 'out', 'of', 'university', 'for', '2', 'years', '(', 'spent', '6', 'years', 'going', 'from', 'faculty', 'to', 'faculty', 'struggling', 'trying', 'to', 'find', 'my', 'niche', ')', 'and', 'have', 'been', 'working', ',', 'and', 'saving', 'money', '.', 'i', \"'\", 've', 'been', 'doing', 'some', 'more', 'research', 'on', 'learning', 'strategies', 'for', 'people', 'with', 'ad', '##hd', ',', 'but', 'i', 'thought', 'that', 'it', 'might', 'be', 'more', 'useful', 'to', 'ask', 'this', 'community', 'as', 'well', '.', 'if', 'i', 'go', 'back', 'i', 'want', 'to', 'try', 'for', 'medicine', '(', 'long', 'shot', 'at', 'this', 'point', ')', 'but', 'i', 'need', 'to', 're', '-', 'evaluate', 'my', 'learning', 'techniques', 'first', 'before', 'i', 'begin', '.', 'even', 'if', 'its', 'just', 'a', 'link', 'to', 'something', 'that', 'you', 'might', 'have', 'found', 'useful', ',', 'it', 'would', 'be', 'greatly', 'appreciated', 'p', '.', 's', '.', 'apologies', 'for', 'if', 'this', 'already', 'been', 'covered', 'in', 'a', 'previous', 'thread', ',', 'i', 'tried', 'to', 'search', 'but', 'it', 'couldn', \"'\", 't', 'find', 'anything', 'relevant', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['possibly', 'mis', '##dia', '##gno', '##sed', 'with', 'add', '.', 'so', 'i', 'went', 'to', 'some', 'lectures', 'and', 'information', 'meetings', 'about', 'add', ',', 'and', 'i', 'seriously', 'think', 'i', 'have', 'been', 'mis', '##dia', '##gno', '##sed', '.', 'sure', 'there', 'are', 'some', 'things', 'i', 'have', ',', 'but', 'i', 'can', 'concentrate', ',', 'sleep', 'well', ',', 'am', 'a', 'fairly', 'calm', 'person', '.', 'but', 'they', 'have', 'officially', 'diagnosed', 'me', 'with', 'add', 'and', 'i', 'think', 'this', 'just', 'isn', \"'\", 't', 'fair', '.', 'how', 'can', 'i', 'set', 'this', 'straight', '?', 'i', 'could', 'really', 'use', 'your', 'help', 'red', '##dit', '!', 'thanks', '.']\n",
      "INFO:__main__:Number of tokens: 90\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['possibly', 'mis', '##dia', '##gno', '##sed', 'with', 'add', '.', 'so', 'i', 'went', 'to', 'some', 'lectures', 'and', 'information', 'meetings', 'about', 'add', ',', 'and', 'i', 'seriously', 'think', 'i', 'have', 'been', 'mis', '##dia', '##gno', '##sed', '.', 'sure', 'there', 'are', 'some', 'things', 'i', 'have', ',', 'but', 'i', 'can', 'concentrate', ',', 'sleep', 'well', ',', 'am', 'a', 'fairly', 'calm', 'person', '.', 'but', 'they', 'have', 'officially', 'diagnosed', 'me', 'with', 'add', 'and', 'i', 'think', 'this', 'just', 'isn', \"'\", 't', 'fair', '.', 'how', 'can', 'i', 'set', 'this', 'straight', '?', 'i', 'could', 'really', 'use', 'your', 'help', 'red', '##dit', '!', 'thanks', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'want', 'to', 'enlist', 'in', 'the', 'army', ',', 'how', 'does', 'add', 'affect', 'this', '?', 'also', ',', 'i', 'think', 'i', \"'\", 'm', 'mis', '##dia', '##gno', '##sed', '.', 'i', \"'\", 'm', 'trying', 'to', 'set', 'it', 'straight', ',', 'but', 'what', 'if', 'that', 'doesn', \"'\", 't', 'work', '.', 'can', 'i', 'still', 'enlist', 'if', 'i', 'have', 'add', '?']\n",
      "INFO:__main__:Number of tokens: 53\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'want', 'to', 'enlist', 'in', 'the', 'army', ',', 'how', 'does', 'add', 'affect', 'this', '?', 'also', ',', 'i', 'think', 'i', \"'\", 'm', 'mis', '##dia', '##gno', '##sed', '.', 'i', \"'\", 'm', 'trying', 'to', 'set', 'it', 'straight', ',', 'but', 'what', 'if', 'that', 'doesn', \"'\", 't', 'work', '.', 'can', 'i', 'still', 'enlist', 'if', 'i', 'have', 'add', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'caused', 'by', 'digest', '##ive', 'disorder', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'caused', 'by', 'digest', '##ive', 'disorder', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'kids', ':', 'nur', '##ture', 'their', 'strengths', ',', 'value', 'their', 'weaknesses']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'kids', ':', 'nur', '##ture', 'their', 'strengths', ',', 'value', 'their', 'weaknesses']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'you', 'describe', 'add', '/', 'ad', '##hd', 'to', 'non', '-', 'ad', '(', 'h', ')', 'd', \"'\", 'er', '##s', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'you', 'describe', 'add', '/', 'ad', '##hd', 'to', 'non', '-', 'ad', '(', 'h', ')', 'd', \"'\", 'er', '##s', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'super', '##power']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'super', '##power']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', 'have', 'ad', '##hd', '?', 'ds', '##m', '-', 'iv', '-', 'tr', 'criteria', 'screening', 'test', '.', '[', 'more', 'about', 'it', 'in', 'comments', ']']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', 'have', 'ad', '##hd', '?', 'ds', '##m', '-', 'iv', '-', 'tr', 'criteria', 'screening', 'test', '.', '[', 'more', 'about', 'it', 'in', 'comments', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'deal', 'with', 'sweating', 'from', 'your', 'medication', '?', 'nothing', 'popped', 'up', 'in', 'the', 'search', 'so', 'i', 'figured', 'some', 'extra', 'input', 'would', 'be', 'great', '.', 'for', 'those', 'of', 'you', 'who', 'take', 'st', '##im', '##ula', '##nt', 'med', '##s', 'for', 'your', 'add', '/', 'ad', '##hd', ',', 'how', 'do', 'you', 'deal', 'with', 'the', 'sweating', 'side', 'effects', '?', 'is', 'there', 'any', 'way', 'to', 'reduce', 'this', '?', 'i', \"'\", 've', 'been', 'taking', 'rita', '##lin', 'for', 'some', 'time', 'now', 'and', 'most', 'of', 'the', 'side', '-', 'effects', 'have', 'less', '##ened', 'by', 'i', 'still', 'sweat', 'a', 'fair', 'amount', 'from', 'my', 'arm', 'pits', '.', 'i', 'don', \"'\", 't', 'end', 'up', 'smelling', 'bad', 'but', 'it', \"'\", 's', 'still', 'pretty', 'ag', '##gra', '##vating', '.', 'i', \"'\", 've', 'tried', 'a', 'few', 'different', 'de', '##od', '##oran', '##ts', '/', 'anti', '##pers', '##pi', '##rant', '##s', 'but', 'none', 'really', 'seem', 'to', 'reduce', 'it', 'any', '.', 'has', 'anything', 'in', 'particular', 'helped', 'you', '?']\n",
      "INFO:__main__:Number of tokens: 147\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'deal', 'with', 'sweating', 'from', 'your', 'medication', '?', 'nothing', 'popped', 'up', 'in', 'the', 'search', 'so', 'i', 'figured', 'some', 'extra', 'input', 'would', 'be', 'great', '.', 'for', 'those', 'of', 'you', 'who', 'take', 'st', '##im', '##ula', '##nt', 'med', '##s', 'for', 'your', 'add', '/', 'ad', '##hd', ',', 'how', 'do', 'you', 'deal', 'with', 'the', 'sweating', 'side', 'effects', '?', 'is', 'there', 'any', 'way', 'to', 'reduce', 'this', '?', 'i', \"'\", 've', 'been', 'taking', 'rita', '##lin', 'for', 'some', 'time', 'now', 'and', 'most', 'of', 'the', 'side', '-', 'effects', 'have', 'less', '##ened', 'by', 'i', 'still', 'sweat', 'a', 'fair', 'amount', 'from', 'my', 'arm', 'pits', '.', 'i', 'don', \"'\", 't', 'end', 'up', 'smelling', 'bad', 'but', 'it', \"'\", 's', 'still', 'pretty', 'ag', '##gra', '##vating', '.', 'i', \"'\", 've', 'tried', 'a', 'few', 'different', 'de', '##od', '##oran', '##ts', '/', 'anti', '##pers', '##pi', '##rant', '##s', 'but', 'none', 'really', 'seem', 'to', 'reduce', 'it', 'any', '.', 'has', 'anything', 'in', 'particular', 'helped', 'you', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'cooking', 'during', 'my', 'first', 'appointment', 'with', 'the', 'psychiatrist', ',', 'we', 'tried', 'to', 'determine', 'if', 'i', 'was', 'impatient', ';', 'because', 'i', 'didn', \"'\", 't', 'have', 'clear', 'examples', 'in', 'my', 'mind', ',', 'we', 'agreed', 'that', 'i', 'was', 'no', 'more', 'impatient', 'than', 'the', 'average', 'person', '.', 'however', ',', 'i', 'thought', 'later', 'about', 'a', 'behavior', ',', 'and', 'i', 'wouldn', \"'\", 't', 'be', 'surprised', 'if', 'it', 'was', 'shared', 'by', 'many', 'people', 'with', 'ad', '##hd', '.', 'when', 'it', 'comes', 'to', 'cooking', ',', 'the', 'faster', 'and', '/', 'or', 'easier', 'to', 'prepare', ',', 'the', 'better', '.', 'i', 'mean', ',', 'when', 'i', 'decide', 'i', 'want', 'to', 'eat', ',', 'i', 'have', 'to', 'eat', 'now', ',', 'even', 'if', 'i', \"'\", 'm', 'not', 'really', 'hungry', ',', 'it', \"'\", 's', 'just', 'that', 'i', 'don', \"'\", 't', 'want', 'to', 'wait', '.', 'also', ',', 'when', 'i', 'am', 'not', 'hungry', 'but', 'i', 'know', 'i', 'will', 'be', 'in', 'an', 'hour', ',', 'there', 'is', 'no', 'way', 'i', 'will', 'start', 'preparing', 'a', 'meal', 'that', 'takes', 'an', 'hour', 'to', 'cook', 'in', 'the', 'oven', ';', 'the', '\"', 'reward', '\"', 'is', 'just', 'too', 'far', 'away', 'to', 'be', 'even', 'considered', '.', 'consequently', ',', 'if', 'i', 'know', 'i', 'will', 'be', 'hungry', 'in', 'an', 'hour', ',', 'i', 'will', 'start', '\"', 'preparing', '\"', 'something', 'in', 'an', 'hour', '(', 'but', 'most', 'likely', 'two', 'hours', 'or', 'more', ')', 'and', 'i', 'will', 'want', 'it', 'to', 'be', 'ready', 'as', 'fast', 'as', 'possible', '.', 'also', ',', 'i', 'almost', 'never', 'keep', 'left', '##overs', '(', 'unless', 'there', 'are', 'a', 'lot', ')', '.', 'yeah', ',', 'left', '##overs', 'can', 'be', 'cooked', 'quickly', 'in', 'the', 'microwave', 'tomorrow', ',', 'but', 'tomorrow', 'is', 'too', 'far', 'away', '.', 'i', \"'\", 'm', 'posting', 'cause', 'i', 'believe', 'this', 'fits', 'with', 'the', 'deficiency', 'ad', '##hd', 'brains', 'have', 'when', 'it', 'comes', 'to', '\"', 'long', '-', 'term', '\"', 'rewards', '.', 'of', 'course', ',', 'those', 'of', 'you', 'who', 'love', 'cooking', 'will', 'disagree', 'with', 'me', ',', 'but', 'i', \"'\", 'd', 'like', 'to', 'know', 'if', 'some', 'behave', 'the', 'same', 'way', ',', 'and', 'i', \"'\", 'm', 'even', 'more', 'interested', 'in', 'the', 'opinion', 'of', 'people', 'with', 'ad', '##hd', 'who', 'don', \"'\", 't', 'think', 'or', 'behave', 'that', 'way', ';', 'after', 'all', ',', 'this', 'might', 'have', 'nothing', 'to', 'do', 'with', 'ad', '##hd', '(', 'not', 'everything', 'is', 'determined', 'by', 'this', 'disorder', 'we', 'have', ')', 'and', 'could', 'simply', 'be', 'explained', 'by', 'the', 'fact', 'that', 'i', \"'\", 'm', 'a', 'single', 'guy', 'who', 'hasn', \"'\", 't', 'yet', 'acquired', 'the', 'maturity', 'to', 'appreciate', 'complex', 'meals', '.']\n",
      "INFO:__main__:Number of tokens: 393\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'cooking', 'during', 'my', 'first', 'appointment', 'with', 'the', 'psychiatrist', ',', 'we', 'tried', 'to', 'determine', 'if', 'i', 'was', 'impatient', ';', 'because', 'i', 'didn', \"'\", 't', 'have', 'clear', 'examples', 'in', 'my', 'mind', ',', 'we', 'agreed', 'that', 'i', 'was', 'no', 'more', 'impatient', 'than', 'the', 'average', 'person', '.', 'however', ',', 'i', 'thought', 'later', 'about', 'a', 'behavior', ',', 'and', 'i', 'wouldn', \"'\", 't', 'be', 'surprised', 'if', 'it', 'was', 'shared', 'by', 'many', 'people', 'with', 'ad', '##hd', '.', 'when', 'it', 'comes', 'to', 'cooking', ',', 'the', 'faster', 'and', '/', 'or', 'easier', 'to', 'prepare', ',', 'the', 'better', '.', 'i', 'mean', ',', 'when', 'i', 'decide', 'i', 'want', 'to', 'eat', ',', 'i', 'have', 'to', 'eat', 'now', ',', 'even', 'if', 'i', \"'\", 'm', 'not', 'really', 'hungry', ',', 'it', \"'\", 's', 'just', 'that', 'i', 'don', \"'\", 't', 'want', 'to', 'wait', '.', 'also', ',', 'when', 'i', 'am', 'not', 'hungry', 'but', 'i', 'know', 'i', 'will', 'be', 'in', 'an', 'hour', ',', 'there', 'is', 'no', 'way', 'i', 'will', 'start', 'preparing', 'a', 'meal', 'that', 'takes', 'an', 'hour', 'to', 'cook', 'in', 'the', 'oven', ';', 'the', '\"', 'reward', '\"', 'is', 'just', 'too', 'far', 'away', 'to', 'be', 'even', 'considered', '.', 'consequently', ',', 'if', 'i', 'know', 'i', 'will', 'be', 'hungry', 'in', 'an', 'hour', ',', 'i', 'will', 'start', '\"', 'preparing', '\"', 'something', 'in', 'an', 'hour', '(', 'but', 'most', 'likely', 'two', 'hours', 'or', 'more', ')', 'and', 'i', 'will', 'want', 'it', 'to', 'be', 'ready', 'as', 'fast', 'as', 'possible', '.', 'also', ',', 'i', 'almost', 'never', 'keep', 'left', '##overs', '(', 'unless', 'there', 'are', 'a', 'lot', ')', '.', 'yeah', ',', 'left', '##overs', 'can', 'be', 'cooked', 'quickly', 'in', 'the', 'microwave', 'tomorrow', ',', 'but', 'tomorrow', 'is', 'too', 'far', 'away', '.', 'i', \"'\", 'm', 'posting', 'cause', 'i', 'believe', 'this', 'fits', 'with', 'the', 'deficiency', 'ad', '##hd', 'brains', 'have', 'when', 'it', 'comes', 'to', '\"', 'long', '-', 'term', '\"', 'rewards', '.', 'of', 'course', ',', 'those', 'of', 'you', 'who', 'love', 'cooking', 'will', 'disagree', 'with', 'me', ',', 'but', 'i', \"'\", 'd', 'like', 'to', 'know', 'if', 'some', 'behave', 'the', 'same', 'way', ',', 'and', 'i', \"'\", 'm', 'even', 'more', 'interested', 'in', 'the', 'opinion', 'of', 'people', 'with', 'ad', '##hd', 'who', 'don', \"'\", 't', 'think', 'or', 'behave', 'that', 'way', ';', 'after', 'all', ',', 'this', 'might', 'have', 'nothing', 'to', 'do', 'with', 'ad', '##hd', '(', 'not', 'everything', 'is', 'determined', 'by', 'this', 'disorder', 'we', 'have', ')', 'and', 'could', 'simply', 'be', 'explained', 'by', 'the', 'fact', 'that', 'i', \"'\", 'm', 'a', 'single', 'guy', 'who', 'hasn', \"'\", 't', 'yet', 'acquired', 'the', 'maturity', 'to', 'appreciate', 'complex', 'meals', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['without', 'my', 'med', '##s', '.']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['without', 'my', 'med', '##s', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'makes', 'sleeping', 'a', 'nightmare', '.', 'advice', '?', 'i', 'was', 'diagnosed', 'with', 'add', 'when', 'i', 'was', 'nine', ',', 'i', \"'\", 'm', 'now', '19', ',', 'and', 'during', 'the', 'week', 'mon', '-', 'fr', '##i', 'i', 'find', 'it', 'hard', 'to', 'get', 'to', 'sleep', '.', 'on', 'average', '1', '-', '3', 'hours', 'of', 'laying', 'in', 'bed', 'before', 'i', 'fall', 'asleep', '.', 'my', 'thoughts', 'just', 'keep', 'racing', ',', 'and', 'i', 'can', \"'\", 't', 'shut', 'it', 'out', '.', 'even', 'if', 'my', 'med', '##s', 'wore', 'off', 'over', '8', 'hours', 'ago', 'i', 'still', 'have', 'trouble', 'falling', 'to', 'sleep', '.', 'on', 'the', 'weekends', 'when', 'i', 'don', \"'\", 't', 'take', 'my', 'med', '##s', 'i', 'get', 'to', 'sleep', 'just', 'fine', ',', 'so', 'i', 'believe', 'the', 'med', '##s', 'to', 'be', 'the', 'problem', '.', 'i', \"'\", 'm', 'in', 'college', 'currently', 'so', 'i', 'can', \"'\", 't', 'stop', 'taking', 'add', '##eral', '##l', ',', 'but', 'some', 'advice', 'would', 'be', 'welcomed', '.']\n",
      "INFO:__main__:Number of tokens: 146\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'makes', 'sleeping', 'a', 'nightmare', '.', 'advice', '?', 'i', 'was', 'diagnosed', 'with', 'add', 'when', 'i', 'was', 'nine', ',', 'i', \"'\", 'm', 'now', '19', ',', 'and', 'during', 'the', 'week', 'mon', '-', 'fr', '##i', 'i', 'find', 'it', 'hard', 'to', 'get', 'to', 'sleep', '.', 'on', 'average', '1', '-', '3', 'hours', 'of', 'laying', 'in', 'bed', 'before', 'i', 'fall', 'asleep', '.', 'my', 'thoughts', 'just', 'keep', 'racing', ',', 'and', 'i', 'can', \"'\", 't', 'shut', 'it', 'out', '.', 'even', 'if', 'my', 'med', '##s', 'wore', 'off', 'over', '8', 'hours', 'ago', 'i', 'still', 'have', 'trouble', 'falling', 'to', 'sleep', '.', 'on', 'the', 'weekends', 'when', 'i', 'don', \"'\", 't', 'take', 'my', 'med', '##s', 'i', 'get', 'to', 'sleep', 'just', 'fine', ',', 'so', 'i', 'believe', 'the', 'med', '##s', 'to', 'be', 'the', 'problem', '.', 'i', \"'\", 'm', 'in', 'college', 'currently', 'so', 'i', 'can', \"'\", 't', 'stop', 'taking', 'add', '##eral', '##l', ',', 'but', 'some', 'advice', 'would', 'be', 'welcomed', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'was', 'wondering', 'why', 'we', 'didn', \"'\", 't', 'have', 'an', 'ad', '##hd', 'advice', 'animal', '.', 'then', 'i', 'remembered', '.']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'was', 'wondering', 'why', 'we', 'didn', \"'\", 't', 'have', 'an', 'ad', '##hd', 'advice', 'animal', '.', 'then', 'i', 'remembered', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'getting', 'worse', 'into', 'my', 'mid', '-', '20', \"'\", 's', '.', 'i', \"'\", 'm', '26', 'years', 'old', 'and', 'it', 'seems', 'like', 'my', 'hyper', '##act', '##ivity', ',', 'imp', '##ulsive', '##ness', ',', 'inability', 'to', 'tear', 'myself', 'away', 'from', 'things', 'and', 'other', 'ad', '##hd', 'symptoms', 'are', 'actually', 'getting', 'worse', '.', 'i', 'was', 'diagnosed', 'when', 'i', 'was', '20', 'and', 'my', 'symptoms', 'were', 'pretty', 'stable', '(', 'in', 'that', 'they', 'were', 'the', 'same', 'as', 'they', 'were', 'when', 'i', 'was', 'diagnosed', ')', 'until', 'the', 'past', '2', 'years', 'when', 'they', 'started', 'getting', 'worse', '.', 'is', 'this', 'common', '?', 'have', 'other', 'people', 'experienced', 'periods', 'like', 'this', '?', 'do', 'i', 'just', 'need', 'to', 'take', 'my', 'med', '##s', 'more', 'often', '(', 'i', 'take', 'them', 'as', 'needed', '.', ')']\n",
      "INFO:__main__:Number of tokens: 118\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'getting', 'worse', 'into', 'my', 'mid', '-', '20', \"'\", 's', '.', 'i', \"'\", 'm', '26', 'years', 'old', 'and', 'it', 'seems', 'like', 'my', 'hyper', '##act', '##ivity', ',', 'imp', '##ulsive', '##ness', ',', 'inability', 'to', 'tear', 'myself', 'away', 'from', 'things', 'and', 'other', 'ad', '##hd', 'symptoms', 'are', 'actually', 'getting', 'worse', '.', 'i', 'was', 'diagnosed', 'when', 'i', 'was', '20', 'and', 'my', 'symptoms', 'were', 'pretty', 'stable', '(', 'in', 'that', 'they', 'were', 'the', 'same', 'as', 'they', 'were', 'when', 'i', 'was', 'diagnosed', ')', 'until', 'the', 'past', '2', 'years', 'when', 'they', 'started', 'getting', 'worse', '.', 'is', 'this', 'common', '?', 'have', 'other', 'people', 'experienced', 'periods', 'like', 'this', '?', 'do', 'i', 'just', 'need', 'to', 'take', 'my', 'med', '##s', 'more', 'often', '(', 'i', 'take', 'them', 'as', 'needed', '.', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['annoying', 'me', '##me', 'ad', '##hd', 'version', '.']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['annoying', 'me', '##me', 'ad', '##hd', 'version', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['5', 'attention', 'deficit', 'drugs', 'reviewed', '(', 'by', 'taking', 'them', ')']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['5', 'attention', 'deficit', 'drugs', 'reviewed', '(', 'by', 'taking', 'them', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'not', 'sure', 'if', 'i', 'have', 'add', '.', '.', '.', 'looking', 'for', 'some', 'advice', 'first', 'off', ',', 'i', 'read', 'through', 'some', 'comments', 'about', 'how', 'add', '/', 'ad', '##hd', 'feels', 'to', 'other', 'people', ';', 'i', 'don', \"'\", 't', 'feel', 'like', 'i', 'have', 'thoughts', 'bouncing', 'around', 'in', 'my', 'brain', 'like', 'ping', 'po', '##ng', 'balls', ',', 'nor', 'do', 'i', 'feel', 'hyper', '##active', '.', 'what', 'i', 'do', 'feel', 'is', 'a', 'lack', 'of', 'concentration', 'and', 'focus', '.', 'i', 'find', 'it', 'very', 'difficult', 'to', 'keep', 'my', 'mind', 'on', 'one', 'task', ',', 'even', 'at', 'work', '.', 'i', 'know', 'i', 'have', 'work', 'to', 'do', ',', 'i', 'have', 'a', 'specific', 'deadline', 'for', 'it', ',', 'but', 'whenever', 'i', 'get', 'down', 'to', 'it', ',', 'i', 'feel', 'an', 'overwhelming', 'urge', 'to', 'do', 'something', 'else', '.', 'so', 'i', \"'\", 'll', 'brows', '##e', 'red', '##dit', ',', 'or', 'din', '##k', 'around', 'with', 'twitter', ',', 'or', 'research', 'some', 'world', 'of', 'war', '##craft', 'thing', '.', 'it', 'isn', \"'\", 't', 'that', 'i', 'don', \"'\", 't', 'like', 'the', 'work', 'i', 'do', '.', 'i', 'enjoy', 'it', ',', 'but', 'i', 'constantly', 'feel', 'this', 'ave', '##rs', '##ion', 'to', 'doing', 'what', 'i', \"'\", 'm', 'supposed', 'to', 'do', ',', 'whether', 'that', \"'\", 's', 'work', 'or', 'a', 'personal', 'hobby', '.', 'at', 'home', ',', 'this', 'affects', 'things', 'like', 'paying', 'bills', ',', 'cleaning', 'up', ',', 'etc', '.', 'but', 'also', 'my', 'ho', '##bbies', '.', 'once', 'in', 'a', 'while', ',', 'i', \"'\", 'll', 'be', 'struck', 'with', 'a', 'bolt', 'of', 'focus', 'and', 'get', 'really', 'excited', 'about', 'something', '-', 'learning', 'php', ',', 'building', 'a', 'blog', ',', 'writing', 'stories', ',', 'etc', '.', '-', 'and', 'i', \"'\", 'll', 'do', 'that', 'thing', 'for', 'the', 'evening', '.', 'then', 'after', 'that', ',', 'nothing', '.', 'i', 'can', \"'\", 't', 'tell', 'you', 'how', 'many', 'times', 'i', \"'\", 've', 'gone', 'gun', '##g', '-', 'ho', 'about', 'starting', 'a', 'blog', ',', 'getting', 'excited', 'about', 'designing', 'it', ',', 'knowing', 'what', 'to', 'blog', 'about', '.', '.', '.', 'and', 'then', 'losing', 'interest', 'after', 'a', 'few', 'days', '.', 'the', 'same', 'thing', 'with', 'writing', '.', 'i', \"'\", 've', 'been', 'told', 'i', 'have', 'talent', ',', 'and', 'i', \"'\", 've', 'sold', 'a', 'few', 'of', 'my', 'short', 'stories', '.', 'i', 'would', 'love', 'to', 'be', 'able', 'to', 'make', 'a', 'career', 'out', 'of', 'writing', '.', 'but', 'any', 'time', 'i', 'sit', 'down', 'to', 'write', ',', 'i', 'immediately', 'lose', 'interest', '.', 'i', \"'\", 've', 'set', 'up', 'a', 'website', 'for', 'my', 'writing', ',', 'put', 'a', 'few', 'stories', 'on', 'smashwords', ',', 'but', 'i', 'lost', 'interest', 'after', 'a', 'couple', 'weeks', '.', 'i', \"'\", 'm', 'imp', '##ulsive', 'too', ',', 'which', 'i', 'guess', 'has', 'already', 'been', 'indicated', '.', 'i', 'read', 'about', 'a', 'new', 'ga', '##dget', 'or', 'what', '-', 'not', ',', 'and', 'am', 'nearly', 'obsessed', 'with', 'having', 'it', '.', 'i', 'bought', 'a', 'nintendo', '3d', '##s', 'because', 'i', 'wanted', 'to', 'play', 'the', '3d', 'o', '##car', '##ina', 'of', 'time', ';', 'i', 'played', 'it', 'for', 'a', 'week', 'or', 'so', ',', 'then', 'haven', \"'\", 't', 'touched', 'it', '.', 'same', 'thing', 'with', 'my', 'ipad', '.', 'i', 'got', 'in', '##fat', '##uated', 'with', 'bmw', '##s', 'last', 'summer', ';', 'out', 'of', 'curiosity', 'i', 'looked', 'for', 'bmw', '##s', 'for', 'sale', 'and', 'found', 'one', 'in', 'my', 'price', 'range', 'so', 'i', 'traded', 'in', 'my', 'car', 'and', 'got', 'the', 'bmw', '.', 'granted', ',', 'it', 'was', 'a', 'good', 'purchase', 'and', 'i', 'love', 'the', 'car', 'to', 'death', ',', 'but', 'the', 'idea', 'came', 'out', 'of', 'nowhere', 'and', 'the', 'entire', 'process', 'of', 'looking', ',', 'test', 'driving', ',', 'and', 'buying', 'took', 'less', 'than', 'a', 'week', '.', 'the', 'car', 'that', 'i', \"'\", 'd', 'traded', 'for', 'the', 'bmw', 'was', 'a', 'similar', 'purchase', '.', 'so', ',', 'i', 'don', \"'\", 't', 'really', 'feel', 'like', 'my', 'thoughts', 'are', 'bouncing', 'around', 'chaotic', '##ally', '.', '.', '.', 'it', 'feels', 'more', 'like', 'my', 'concentration', 'is', 'a', 'g', '##lo', '##b', 'of', 'slippery', 'goo', 'that', 'whenever', 'i', 'try', 'to', 'hold', 'it', 'in', 'one', 'place', ',', 'it', 'slips', 'through', 'my', 'fingers', '.', 'sometimes', 'i', 'feel', 'like', 'i', 'might', 'have', 'a', 'good', 'grip', 'on', 'it', ',', 'but', 'that', 'only', 'lasts', 'a', 'short', 'time', 'before', 'it', 'slips', 'through', 'my', 'fingers', 'again', '.']\n",
      "INFO:__main__:Number of tokens: 645\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'not', 'sure', 'if', 'i', 'have', 'add', '.', '.', '.', 'looking', 'for', 'some', 'advice', 'first', 'off', ',', 'i', 'read', 'through', 'some', 'comments', 'about', 'how', 'add', '/', 'ad', '##hd', 'feels', 'to', 'other', 'people', ';', 'i', 'don', \"'\", 't', 'feel', 'like', 'i', 'have', 'thoughts', 'bouncing', 'around', 'in', 'my', 'brain', 'like', 'ping', 'po', '##ng', 'balls', ',', 'nor', 'do', 'i', 'feel', 'hyper', '##active', '.', 'what', 'i', 'do', 'feel', 'is', 'a', 'lack', 'of', 'concentration', 'and', 'focus', '.', 'i', 'find', 'it', 'very', 'difficult', 'to', 'keep', 'my', 'mind', 'on', 'one', 'task', ',', 'even', 'at', 'work', '.', 'i', 'know', 'i', 'have', 'work', 'to', 'do', ',', 'i', 'have', 'a', 'specific', 'deadline', 'for', 'it', ',', 'but', 'whenever', 'i', 'get', 'down', 'to', 'it', ',', 'i', 'feel', 'an', 'overwhelming', 'urge', 'to', 'do', 'something', 'else', '.', 'so', 'i', \"'\", 'll', 'brows', '##e', 'red', '##dit', ',', 'or', 'din', '##k', 'around', 'with', 'twitter', ',', 'or', 'research', 'some', 'world', 'of', 'war', '##craft', 'thing', '.', 'it', 'isn', \"'\", 't', 'that', 'i', 'don', \"'\", 't', 'like', 'the', 'work', 'i', 'do', '.', 'i', 'enjoy', 'it', ',', 'but', 'i', 'constantly', 'feel', 'this', 'ave', '##rs', '##ion', 'to', 'doing', 'what', 'i', \"'\", 'm', 'supposed', 'to', 'do', ',', 'whether', 'that', \"'\", 's', 'work', 'or', 'a', 'personal', 'hobby', '.', 'at', 'home', ',', 'this', 'affects', 'things', 'like', 'paying', 'bills', ',', 'cleaning', 'up', ',', 'etc', '.', 'but', 'also', 'my', 'ho', '##bbies', '.', 'once', 'in', 'a', 'while', ',', 'i', \"'\", 'll', 'be', 'struck', 'with', 'a', 'bolt', 'of', 'focus', 'and', 'get', 'really', 'excited', 'about', 'something', '-', 'learning', 'php', ',', 'building', 'a', 'blog', ',', 'writing', 'stories', ',', 'etc', '.', '-', 'and', 'i', \"'\", 'll', 'do', 'that', 'thing', 'for', 'the', 'evening', '.', 'then', 'after', 'that', ',', 'nothing', '.', 'i', 'can', \"'\", 't', 'tell', 'you', 'how', 'many', 'times', 'i', \"'\", 've', 'gone', 'gun', '##g', '-', 'ho', 'about', 'starting', 'a', 'blog', ',', 'getting', 'excited', 'about', 'designing', 'it', ',', 'knowing', 'what', 'to', 'blog', 'about', '.', '.', '.', 'and', 'then', 'losing', 'interest', 'after', 'a', 'few', 'days', '.', 'the', 'same', 'thing', 'with', 'writing', '.', 'i', \"'\", 've', 'been', 'told', 'i', 'have', 'talent', ',', 'and', 'i', \"'\", 've', 'sold', 'a', 'few', 'of', 'my', 'short', 'stories', '.', 'i', 'would', 'love', 'to', 'be', 'able', 'to', 'make', 'a', 'career', 'out', 'of', 'writing', '.', 'but', 'any', 'time', 'i', 'sit', 'down', 'to', 'write', ',', 'i', 'immediately', 'lose', 'interest', '.', 'i', \"'\", 've', 'set', 'up', 'a', 'website', 'for', 'my', 'writing', ',', 'put', 'a', 'few', 'stories', 'on', 'smashwords', ',', 'but', 'i', 'lost', 'interest', 'after', 'a', 'couple', 'weeks', '.', 'i', \"'\", 'm', 'imp', '##ulsive', 'too', ',', 'which', 'i', 'guess', 'has', 'already', 'been', 'indicated', '.', 'i', 'read', 'about', 'a', 'new', 'ga', '##dget', 'or', 'what', '-', 'not', ',', 'and', 'am', 'nearly', 'obsessed', 'with', 'having', 'it', '.', 'i', 'bought', 'a', 'nintendo', '3d', '##s', 'because', 'i', 'wanted', 'to', 'play', 'the', '3d', 'o', '##car', '##ina', 'of', 'time', ';', 'i', 'played', 'it', 'for', 'a', 'week', 'or', 'so', ',', 'then', 'haven', \"'\", 't', 'touched', 'it', '.', 'same', 'thing', 'with', 'my', 'ipad', '.', 'i', 'got', 'in', '##fat', '##uated', 'with', 'bmw', '##s', 'last', 'summer', ';', 'out', 'of', 'curiosity', 'i', 'looked', 'for', 'bmw', '##s', 'for', 'sale', 'and', 'found', 'one', 'in', 'my', 'price', 'range', 'so', 'i', 'traded', 'in', 'my', 'car', 'and', 'got', 'the', 'bmw', '.'], ['granted', ',', 'it', 'was', 'a', 'good', 'purchase', 'and', 'i', 'love', 'the', 'car', 'to', 'death', ',', 'but', 'the', 'idea', 'came', 'out', 'of', 'nowhere', 'and', 'the', 'entire', 'process', 'of', 'looking', ',', 'test', 'driving', ',', 'and', 'buying', 'took', 'less', 'than', 'a', 'week', '.', 'the', 'car', 'that', 'i', \"'\", 'd', 'traded', 'for', 'the', 'bmw', 'was', 'a', 'similar', 'purchase', '.', 'so', ',', 'i', 'don', \"'\", 't', 'really', 'feel', 'like', 'my', 'thoughts', 'are', 'bouncing', 'around', 'chaotic', '##ally', '.', '.', '.', 'it', 'feels', 'more', 'like', 'my', 'concentration', 'is', 'a', 'g', '##lo', '##b', 'of', 'slippery', 'goo', 'that', 'whenever', 'i', 'try', 'to', 'hold', 'it', 'in', 'one', 'place', ',', 'it', 'slips', 'through', 'my', 'fingers', '.', 'sometimes', 'i', 'feel', 'like', 'i', 'might', 'have', 'a', 'good', 'grip', 'on', 'it', ',', 'but', 'that', 'only', 'lasts', 'a', 'short', 'time', 'before', 'it', 'slips', 'through', 'my', 'fingers', 'again', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['re', ':', 'posts', 'like', '\"', 'i', 'think', 'i', 'have', 'ad', '##hd', '.', '.', '.', '.', '\"']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['re', ':', 'posts', 'like', '\"', 'i', 'think', 'i', 'have', 'ad', '##hd', '.', '.', '.', '.', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['in', 'spite', 'of', '-', 'good', 'new', 'resource']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['in', 'spite', 'of', '-', 'good', 'new', 'resource']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['close', 'friendships', 'vs', 'acquaintances', 'in', 'ad', '##hd', '##ers', 'it', \"'\", 's', 'like', 'at', 'any', 'given', 'time', 'i', 'can', 'regard', 'an', 'acquaintance', 'as', 'a', 'good', 'friend', 'and', 'vice', '-', 'versa', '.', 'i', 'think', 'it', \"'\", 's', 'due', 'to', 'being', 'so', '\"', 'in', 'the', 'moment', '\"', 'all', 'the', 'time', ',', 'which', 'happens', 'without', 'effort', '.', 'really', ',', 'it', \"'\", 's', 'hard', 'for', 'me', 'to', 'know', 'what', 'others', \"'\", 'opinion', 'of', 'me', 'is', ',', 'as', 'i', 'usually', 'present', 'a', 'strong', 'dil', '##ution', 'of', 'my', 'personality', 'that', 'i', 'have', 'learned', 'leaves', 'a', 'better', 'impression', ',', 'but', 'perhaps', 'makes', 'me', 'feel', 'less', 'connected', '.', 'what', 'are', 'your', 'thoughts', 'on', 'friends', 'vs', 'acquaintances', '?', 'do', 'you', 'make', 'both', 'easily', '?', 'ever', 'have', 'a', 'good', 'friend', 'who', 'seemingly', 'likes', 'you', 'just', 'completely', 'fall', 'out', 'of', 'your', 'life', 'and', 'avoid', 'you', '?', 'are', 'you', 'good', 'at', 'assessing', 'what', 'people', 'think', 'of', 'you', '?', 'do', 'you', 'dil', '##ute', 'your', 'ad', '##hd', 'personality', 'most', 'of', 'the', 'time', '?']\n",
      "INFO:__main__:Number of tokens: 158\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['close', 'friendships', 'vs', 'acquaintances', 'in', 'ad', '##hd', '##ers', 'it', \"'\", 's', 'like', 'at', 'any', 'given', 'time', 'i', 'can', 'regard', 'an', 'acquaintance', 'as', 'a', 'good', 'friend', 'and', 'vice', '-', 'versa', '.', 'i', 'think', 'it', \"'\", 's', 'due', 'to', 'being', 'so', '\"', 'in', 'the', 'moment', '\"', 'all', 'the', 'time', ',', 'which', 'happens', 'without', 'effort', '.', 'really', ',', 'it', \"'\", 's', 'hard', 'for', 'me', 'to', 'know', 'what', 'others', \"'\", 'opinion', 'of', 'me', 'is', ',', 'as', 'i', 'usually', 'present', 'a', 'strong', 'dil', '##ution', 'of', 'my', 'personality', 'that', 'i', 'have', 'learned', 'leaves', 'a', 'better', 'impression', ',', 'but', 'perhaps', 'makes', 'me', 'feel', 'less', 'connected', '.', 'what', 'are', 'your', 'thoughts', 'on', 'friends', 'vs', 'acquaintances', '?', 'do', 'you', 'make', 'both', 'easily', '?', 'ever', 'have', 'a', 'good', 'friend', 'who', 'seemingly', 'likes', 'you', 'just', 'completely', 'fall', 'out', 'of', 'your', 'life', 'and', 'avoid', 'you', '?', 'are', 'you', 'good', 'at', 'assessing', 'what', 'people', 'think', 'of', 'you', '?', 'do', 'you', 'dil', '##ute', 'your', 'ad', '##hd', 'personality', 'most', 'of', 'the', 'time', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['know', 'your', 'medication', '.', 'there', 'are', 'a', 'lot', 'of', 'questions', 'posted', 'about', 'medication', 'and', 'reactions', 'to', 'medication', '.', 'this', 'is', 'a', 'good', 'thing', 'and', 'provides', 'activity', 'to', 'the', 'community', '.', 'however', 'there', 'is', 'some', 'level', 'of', 'information', 'we', 'all', 'need', 'to', 'understand', 'about', 'the', 'different', 'medications', '.', 'i', 'have', 'seen', 'many', 'posts', 'where', 'people', 'are', 'asking', 'about', 'their', 'medication', 'and', 'the', 'under', '##line', 'issue', 'is', 'they', 'don', \"'\", 't', 'understand', 'the', 'medication', '.', 'this', 'seems', 'to', 'be', 'especially', 'common', 'with', 'the', 'amp', '##het', '##amine', 'medications', '.', 'it', 'is', 'very', 'important', 'everyone', 'understands', 'the', 'medication', 'they', 'are', 'taking', '.', 'here', 'are', 'some', 'wikipedia', 'pages', 'everyone', 'should', 'read', 'and', 'understand', 'to', 'the', 'best', 'of', 'their', 'ability', '.', '[', 'ad', '##hd', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'attention', '_', 'deficit', '_', 'hyper', '##act', '##ivity', '_', 'disorder', ')', 'these', 'are', 'some', 'of', 'the', 'most', 'common', 'and', 'a', 'few', 'less', 'common', 'chemicals', 'used', 'for', 'primary', 'or', 'supportive', 'aid', 'in', 'the', 'treatment', 'of', 'ad', '##hd', '.', '[', 'amp', '##het', '##amine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'amp', '##het', '##amine', ')', '>', 'example', '(', 's', ')', ':', 'add', '##eral', '##l', ',', 'dex', '##ed', '##rine', 'and', 'v', '##y', '##van', '##se', '.', '[', 'methyl', '##ph', '##eni', '##date', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'methyl', '##ph', '##eni', '##date', ')', '>', 'example', '(', 's', ')', ':', 'rita', '##lin', 'and', 'concert', '##a', '.', '[', 'atom', '##ox', '##eti', '##ne', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'atom', '##ox', '##eti', '##ne', ')', '>', 'example', '(', 's', ')', ':', 'st', '##rat', '##tera', '.', '[', 'nor', '##trip', '##ty', '##line', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'nor', '##trip', '##ty', '##line', ')', '>', 'example', '(', 's', ')', ':', 'pam', '##elo', '##r', ',', 'sen', '##sov', '##al', ',', 'and', 'ave', '##nty', '##l', '.', '[', 'cl', '##oni', '##dine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'cl', '##oni', '##dine', ')', '>', 'example', '(', 's', ')', ':', 'cat', '##ap', '##res', '.', '[', 'gu', '##an', '##fa', '##cine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'gu', '##an', '##fa', '##cine', ')', '>', 'example', '(', 's', ')', ':', 'ten', '##ex', '.', '[', 'mod', '##af', '##ini', '##l', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'mod', '##af', '##ini', '##l', ')', '>', 'example', '(', 's', ')', ':', 'pro', '##vi', '##gil', '[', 'arm', '##oda', '##fin', '##il', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'arm', '##oda', '##fin', '##il', ')', '>', 'example', '(', 's', ')', ':', 'nu', '##vi', '##gil', '>', 'note', '(', 's', ')', ':', 'this', 'contains', 'only', 'the', 'active', '(', 'r', ')', '-', 'en', '##ant', '##iom', '##er', 'of', 'mod', '##af', '##ini', '##l', '.', 'gu', '##an', '##fa', '##cine', 'is', 'being', 'specifically', 'targeted', 'for', 'ad', '##hd', 'as', 'the', 'brand', 'name', 'int', '##uni', '##v', '.', 'big', 'note', 'for', 'the', 'blood', 'pressure', 'ad', '##hd', 'med', '##s', '-', 'do', 'not', 'take', 'cl', '##oni', '##dine', 'or', 'gu', '##an', '##fa', '##cine', 'if', 'you', 'have', 'low', 'blood', 'pressure', 'or', 'are', 'prone', 'to', 'h', '##yp', '##ote', '##ns', '##ion', '.', 'if', 'you', 'experience', 'symptoms', 'of', 'h', '##yp', '##ote', '##ns', '##ion', 'when', 'taking', 'it', '-', 'immediately', 'call', 'your', 'pre', '##sc', '##ri', '##bing', 'doctor', '!', 'get', 'yourself', 'checked', 'out', 'for', 'any', 'potential', 'co', '-', 'mor', '##bid', '##ities', 'before', 'starting', 'any', 'st', '##im', '##ula', '##nt', 'medication', '.', 'they', 'do', 'run', 'the', 'risk', 'of', 'increasing', 'the', 'chance', 'of', 'a', 'h', '##yp', '##oman', '##ic', 'or', 'mani', '##c', 'episode', '.', '*', '*', 'sum', '##med', 'up', 'information', ':', '*', '*', 'st', '##im', '##ula', '##nts', ':', '>', 'the', 'st', '##im', '##ula', '##nts', ',', 'aka', 'psycho', '##sti', '##mu', '##lan', '##ts', ',', 'used', 'in', 'ad', '##hd', 'treatment', 'are', 'amp', '##het', '##amine', 'and', 'methyl', '##ph', '##eni', '##date', '.', 'the', 'main', 'difference', 'between', 'the', 'two', 'st', '##im', '##ula', '##nts', 'is', 'the', 'way', 'they', 'effect', '[', 'do', '##pa', '##mine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'do', '##pa', '##mine', ')', ',', '[', 'nor', '##ep', '##ine', '##ph', '##rine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'nor', '##ep', '##ine', '##ph', '##rine', ')', ',', 'and', '[', 'ser', '##oton', '##in', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'ser', '##oton', '##in', ')', 'levels', '.', 'amp', '##het', '##amine', 'primarily', 'causes', 'the', 'release', 'of', 'these', 'chemicals', 'while', 'methyl', '##ph', '##eni', '##date', 'is', 'a', 're', '##upt', '##ake', 'inhibitor', 'meaning', 'it', 'keeps', 'the', 'chemicals', 'from', 'being', 're', '##ab', '##sor', '##bed', 'and', 'thus', 'leaving', 'them', 'to', 'stay', 'and', 'build', 'up', 'in', 'the', 'neural', 'syn', '##ap', '##ses', ',', 'it', 'also', 'has', 'some', 'releasing', 'effect', 'but', 'much', 'less', 'than', 'that', 'of', 'amp', '##het', '##amine', '.', 'st', '##im', '##ula', '##nts', 'ex', '##cite', 'the', 'brain', 'so', 'it', 'is', 'common', 'that', 'they', 'can', 'cause', 'sleep', 'problems', '.', 'if', 'you', 'are', 'experiencing', 'sleep', 'problems', 'you', 'may', 'need', 'a', 'lower', 'does', ',', 'a', 'sleep', 'aid', ',', 'or', 'a', 'different', 'medication', 'all', 'together', '.', '>', 'amp', '##het', '##amine', 'medication', ':', 'amp', '##eta', '##mine', 'is', 'a', 'race', '##mic', 'mixture', 'of', 'the', 'iso', '##mers', 'dex', '##tro', '##amp', '##het', '##amine', 'and', 'lev', '##oa', '##mp', '##het', '##amine', ',', 'it', 'is', 'largely', 'thought', 'that', 'the', 'd', '-', 'amp', '##het', '##amine', 'is', 'primary', 'in', 'do', '##pa', '##mine', 'release', 'while', 'the', 'l', '-', 'amp', '##het', '##amine', 'is', 'primary', 'in', 'nor', '##ep', '##ine', '##ph', '##rine', 'release', '.', 'two', 'commonly', 'used', 'amp', '##het', '##amine', 'medications', 'are', 'add', '##eral', '##l', 'and', 'v', '##y', '##van', '##se', '.', 'add', '##eral', '##l', 'is', 'a', 'mixture', 'of', 'the', 'separated', 'd', '-', 'amp', '##het', '##amine', 'salts', 'and', 'two', 'amp', '##het', '##amine', 'race', '##mic', 'mixture', '.', 'giving', 'it', 'a', '1', ':', '3', 'ratio', 'of', 'l', '-', 'amp', '##het', '##amine', 'to', 'd', '-', 'amp', '##het', '##amine', '.', 'half', 'being', 'pure', 'd', '-', 'amp', '##het', '##amine', 'and', '2', '/', '8', 'from', 'the', 'two', 'race', '##mic', 'mixture', '##s', '.', 'v', '##y', '##van', '##se', 'is', 'a', 'molecular', 'structure', 'called', 'li', '##sd', '##ex', '##tro', '##amp', '##het', '##amine', '.', 'it', 'is', 'the', 'd', '-', 'amp', '##het', '##amine', 'bound', 'with', 'l', '-', 'l', '##ys', '##ine', 'protein', '.', 'the', 'purpose', 'of', 'bonding', 'the', 'amp', '##het', '##amine', 'to', 'l', '##ys', '##ine', 'is', 'to', 'reduce', 'abuse', 'risks', ',', 'making', 'it', 'a', '\"', 'pro', '-', 'drug', '\"', '.', 'v', '##y', '##van', '##se', 'can', 'not', 'be', 'absorbed', 'directly', 'through', 'the', 'blood', 'stream', ',', 'so', 'it', 'can', 'not', 'be', 'snorted', 'or', 'injected', '.', 'it', 'must', 'be', 'delivered', 'via', 'the', 'small', 'int', '##est', '##ine', 'where', 'your', 'body', 'can', 'break', 'the', 'l', '##ys', '##ine', 'from', 'the', 'amp', '##het', '##amine', '.', '*', '*', 'remember', 'it', 'is', 'very', 'important', 'that', 'you', 'understand', 'both', 'ad', '##hd', 'as', 'a', 'disorder', 'as', 'well', 'as', 'the', 'medication', 'you', 'are', 'taking', '.', '*', '*', 'edit', ':', 'thank', 'you', 'to', 'everyone', 'for', 'posting', 'more', 'information', '.', 'keep', 'posting', 'information', 'about', 'medication', 'in', 'this', 'thread', '!', 'edit', ':', 'added', 'more', 'suggested', 'information', '.']\n",
      "INFO:__main__:Number of tokens: 1160\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['know', 'your', 'medication', '.', 'there', 'are', 'a', 'lot', 'of', 'questions', 'posted', 'about', 'medication', 'and', 'reactions', 'to', 'medication', '.', 'this', 'is', 'a', 'good', 'thing', 'and', 'provides', 'activity', 'to', 'the', 'community', '.', 'however', 'there', 'is', 'some', 'level', 'of', 'information', 'we', 'all', 'need', 'to', 'understand', 'about', 'the', 'different', 'medications', '.', 'i', 'have', 'seen', 'many', 'posts', 'where', 'people', 'are', 'asking', 'about', 'their', 'medication', 'and', 'the', 'under', '##line', 'issue', 'is', 'they', 'don', \"'\", 't', 'understand', 'the', 'medication', '.', 'this', 'seems', 'to', 'be', 'especially', 'common', 'with', 'the', 'amp', '##het', '##amine', 'medications', '.', 'it', 'is', 'very', 'important', 'everyone', 'understands', 'the', 'medication', 'they', 'are', 'taking', '.', 'here', 'are', 'some', 'wikipedia', 'pages', 'everyone', 'should', 'read', 'and', 'understand', 'to', 'the', 'best', 'of', 'their', 'ability', '.', '[', 'ad', '##hd', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'attention', '_', 'deficit', '_', 'hyper', '##act', '##ivity', '_', 'disorder', ')', 'these', 'are', 'some', 'of', 'the', 'most', 'common', 'and', 'a', 'few', 'less', 'common', 'chemicals', 'used', 'for', 'primary', 'or', 'supportive', 'aid', 'in', 'the', 'treatment', 'of', 'ad', '##hd', '.', '[', 'amp', '##het', '##amine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'amp', '##het', '##amine', ')', '>', 'example', '(', 's', ')', ':', 'add', '##eral', '##l', ',', 'dex', '##ed', '##rine', 'and', 'v', '##y', '##van', '##se', '.', '[', 'methyl', '##ph', '##eni', '##date', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'methyl', '##ph', '##eni', '##date', ')', '>', 'example', '(', 's', ')', ':', 'rita', '##lin', 'and', 'concert', '##a', '.', '[', 'atom', '##ox', '##eti', '##ne', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'atom', '##ox', '##eti', '##ne', ')', '>', 'example', '(', 's', ')', ':', 'st', '##rat', '##tera', '.', '[', 'nor', '##trip', '##ty', '##line', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'nor', '##trip', '##ty', '##line', ')', '>', 'example', '(', 's', ')', ':', 'pam', '##elo', '##r', ',', 'sen', '##sov', '##al', ',', 'and', 'ave', '##nty', '##l', '.', '[', 'cl', '##oni', '##dine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'cl', '##oni', '##dine', ')', '>', 'example', '(', 's', ')', ':', 'cat', '##ap', '##res', '.', '[', 'gu', '##an', '##fa', '##cine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'gu', '##an', '##fa', '##cine', ')', '>', 'example', '(', 's', ')', ':', 'ten', '##ex', '.', '[', 'mod', '##af', '##ini', '##l', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'mod', '##af', '##ini', '##l', ')', '>', 'example', '(', 's', ')', ':', 'pro', '##vi', '##gil', '[', 'arm', '##oda', '##fin', '##il', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'arm', '##oda', '##fin', '##il', ')', '>', 'example', '(', 's', ')', ':', 'nu', '##vi', '##gil', '>', 'note', '(', 's', ')', ':', 'this', 'contains', 'only', 'the', 'active', '(', 'r', ')', '-', 'en', '##ant', '##iom', '##er', 'of', 'mod', '##af', '##ini', '##l', '.', 'gu', '##an', '##fa', '##cine', 'is', 'being', 'specifically', 'targeted', 'for', 'ad', '##hd', 'as', 'the', 'brand', 'name', 'int', '##uni', '##v', '.', 'big', 'note', 'for', 'the', 'blood', 'pressure'], ['ad', '##hd', 'med', '##s', '-', 'do', 'not', 'take', 'cl', '##oni', '##dine', 'or', 'gu', '##an', '##fa', '##cine', 'if', 'you', 'have', 'low', 'blood', 'pressure', 'or', 'are', 'prone', 'to', 'h', '##yp', '##ote', '##ns', '##ion', '.', 'if', 'you', 'experience', 'symptoms', 'of', 'h', '##yp', '##ote', '##ns', '##ion', 'when', 'taking', 'it', '-', 'immediately', 'call', 'your', 'pre', '##sc', '##ri', '##bing', 'doctor', '!', 'get', 'yourself', 'checked', 'out', 'for', 'any', 'potential', 'co', '-', 'mor', '##bid', '##ities', 'before', 'starting', 'any', 'st', '##im', '##ula', '##nt', 'medication', '.', 'they', 'do', 'run', 'the', 'risk', 'of', 'increasing', 'the', 'chance', 'of', 'a', 'h', '##yp', '##oman', '##ic', 'or', 'mani', '##c', 'episode', '.', '*', '*', 'sum', '##med', 'up', 'information', ':', '*', '*', 'st', '##im', '##ula', '##nts', ':', '>', 'the', 'st', '##im', '##ula', '##nts', ',', 'aka', 'psycho', '##sti', '##mu', '##lan', '##ts', ',', 'used', 'in', 'ad', '##hd', 'treatment', 'are', 'amp', '##het', '##amine', 'and', 'methyl', '##ph', '##eni', '##date', '.', 'the', 'main', 'difference', 'between', 'the', 'two', 'st', '##im', '##ula', '##nts', 'is', 'the', 'way', 'they', 'effect', '[', 'do', '##pa', '##mine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'do', '##pa', '##mine', ')', ',', '[', 'nor', '##ep', '##ine', '##ph', '##rine', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'nor', '##ep', '##ine', '##ph', '##rine', ')', ',', 'and', '[', 'ser', '##oton', '##in', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'ser', '##oton', '##in', ')', 'levels', '.', 'amp', '##het', '##amine', 'primarily', 'causes', 'the', 'release', 'of', 'these', 'chemicals', 'while', 'methyl', '##ph', '##eni', '##date', 'is', 'a', 're', '##upt', '##ake', 'inhibitor', 'meaning', 'it', 'keeps', 'the', 'chemicals', 'from', 'being', 're', '##ab', '##sor', '##bed', 'and', 'thus', 'leaving', 'them', 'to', 'stay', 'and', 'build', 'up', 'in', 'the', 'neural', 'syn', '##ap', '##ses', ',', 'it', 'also', 'has', 'some', 'releasing', 'effect', 'but', 'much', 'less', 'than', 'that', 'of', 'amp', '##het', '##amine', '.', 'st', '##im', '##ula', '##nts', 'ex', '##cite', 'the', 'brain', 'so', 'it', 'is', 'common', 'that', 'they', 'can', 'cause', 'sleep', 'problems', '.', 'if', 'you', 'are', 'experiencing', 'sleep', 'problems', 'you', 'may', 'need', 'a', 'lower', 'does', ',', 'a', 'sleep', 'aid', ',', 'or', 'a', 'different', 'medication', 'all', 'together', '.', '>', 'amp', '##het', '##amine', 'medication', ':', 'amp', '##eta', '##mine', 'is', 'a', 'race', '##mic', 'mixture', 'of', 'the', 'iso', '##mers', 'dex', '##tro', '##amp', '##het', '##amine', 'and', 'lev', '##oa', '##mp', '##het', '##amine', ',', 'it', 'is', 'largely', 'thought', 'that', 'the', 'd', '-', 'amp', '##het', '##amine', 'is', 'primary', 'in', 'do', '##pa', '##mine', 'release', 'while', 'the', 'l', '-', 'amp', '##het', '##amine', 'is', 'primary', 'in', 'nor', '##ep', '##ine', '##ph', '##rine', 'release', '.', 'two', 'commonly', 'used', 'amp', '##het', '##amine', 'medications', 'are', 'add', '##eral', '##l', 'and', 'v', '##y', '##van', '##se', '.', 'add', '##eral', '##l', 'is', 'a', 'mixture', 'of', 'the', 'separated', 'd', '-', 'amp', '##het', '##amine', 'salts', 'and', 'two', 'amp', '##het', '##amine', 'race', '##mic', 'mixture', '.', 'giving', 'it', 'a', '1', ':', '3', 'ratio', 'of', 'l', '-', 'amp', '##het', '##amine', 'to', 'd', '-', 'amp', '##het', '##amine', '.', 'half', 'being', 'pure', 'd', '-', 'amp', '##het', '##amine', 'and', '2', '/', '8', 'from', 'the', 'two', 'race', '##mic', 'mixture', '##s', '.', 'v', '##y', '##van', '##se', 'is', 'a', 'molecular', 'structure', 'called', 'li', '##sd', '##ex', '##tro', '##amp', '##het', '##amine', '.', 'it', 'is', 'the', 'd', '-', 'amp', '##het', '##amine', 'bound', 'with'], ['l', '-', 'l', '##ys', '##ine', 'protein', '.', 'the', 'purpose', 'of', 'bonding', 'the', 'amp', '##het', '##amine', 'to', 'l', '##ys', '##ine', 'is', 'to', 'reduce', 'abuse', 'risks', ',', 'making', 'it', 'a', '\"', 'pro', '-', 'drug', '\"', '.', 'v', '##y', '##van', '##se', 'can', 'not', 'be', 'absorbed', 'directly', 'through', 'the', 'blood', 'stream', ',', 'so', 'it', 'can', 'not', 'be', 'snorted', 'or', 'injected', '.', 'it', 'must', 'be', 'delivered', 'via', 'the', 'small', 'int', '##est', '##ine', 'where', 'your', 'body', 'can', 'break', 'the', 'l', '##ys', '##ine', 'from', 'the', 'amp', '##het', '##amine', '.', '*', '*', 'remember', 'it', 'is', 'very', 'important', 'that', 'you', 'understand', 'both', 'ad', '##hd', 'as', 'a', 'disorder', 'as', 'well', 'as', 'the', 'medication', 'you', 'are', 'taking', '.', '*', '*', 'edit', ':', 'thank', 'you', 'to', 'everyone', 'for', 'posting', 'more', 'information', '.', 'keep', 'posting', 'information', 'about', 'medication', 'in', 'this', 'thread', '!', 'edit', ':', 'added', 'more', 'suggested', 'information', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'should', 'i', 'expect', 'from', 'concert', '##a', '/', 'methyl', '##ph', '##eni', '##date', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'should', 'i', 'expect', 'from', 'concert', '##a', '/', 'methyl', '##ph', '##eni', '##date', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['27', 'and', 'a', 'prisoner', 'in', 'my', 'own', 'mind', '.']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['27', 'and', 'a', 'prisoner', 'in', 'my', 'own', 'mind', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'diagnosed', 'but', 'not', 'sure']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'diagnosed', 'but', 'not', 'sure']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['coming', 'out', 'in', 'college', 'hello', ',', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'about', 'a', 'year', 'and', 'half', 'ago', '.', 'this', 'last', 'fall', 'i', 'started', 'university', '.', '.', '.', 'my', 'problem', 'is', 'that', 'i', 'recently', 'came', 'to', 'accept', 'my', 'mental', 'state', '.', 'i', 'would', 'like', 'to', 'get', 'more', 'help', 'from', 'the', 'school', 'and', 'i', 'was', 'wondering', 'if', 'anyone', 'had', 'any', 'advice', 'looking', 'for', 'local', 'support', 'groups', ',', 'counselor', '##s', ',', 'help', 'in', 'general', '.', 'i', 'am', 'an', 'active', 'user', 'on', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'already', ',', 'but', 'i', \"'\", 'm', 'using', 'a', 'throw', '##away', 'account', 'because', 'honestly', 'i', \"'\", 'm', 'a', 'bit', 'ashamed', '.', 'the', 'online', 'community', 'is', 'great', ',', 'but', 'i', 'think', 'its', 'time', 'to', 'meet', 'some', 'people', 'in', 'the', 'real', 'world', '.', 'any', 'advice', 'or', 'words', 'of', 'encouragement', 'would', 'be', 'so', 'helpful', 'to', 'me', '.', 'thank', 'you', '.']\n",
      "INFO:__main__:Number of tokens: 149\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['coming', 'out', 'in', 'college', 'hello', ',', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'about', 'a', 'year', 'and', 'half', 'ago', '.', 'this', 'last', 'fall', 'i', 'started', 'university', '.', '.', '.', 'my', 'problem', 'is', 'that', 'i', 'recently', 'came', 'to', 'accept', 'my', 'mental', 'state', '.', 'i', 'would', 'like', 'to', 'get', 'more', 'help', 'from', 'the', 'school', 'and', 'i', 'was', 'wondering', 'if', 'anyone', 'had', 'any', 'advice', 'looking', 'for', 'local', 'support', 'groups', ',', 'counselor', '##s', ',', 'help', 'in', 'general', '.', 'i', 'am', 'an', 'active', 'user', 'on', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'already', ',', 'but', 'i', \"'\", 'm', 'using', 'a', 'throw', '##away', 'account', 'because', 'honestly', 'i', \"'\", 'm', 'a', 'bit', 'ashamed', '.', 'the', 'online', 'community', 'is', 'great', ',', 'but', 'i', 'think', 'its', 'time', 'to', 'meet', 'some', 'people', 'in', 'the', 'real', 'world', '.', 'any', 'advice', 'or', 'words', 'of', 'encouragement', 'would', 'be', 'so', 'helpful', 'to', 'me', '.', 'thank', 'you', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'waiting', 'game']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'waiting', 'game']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['concerned', 'about', 'building', 'a', 'tolerance', 'to', 'concert', '##a', 'so', 'i', 'started', 'taking', 'concert', '##a', 'about', 'age', '9', '.', '(', 'before', 'that', ',', 'i', 'took', 'add', '##eral', '##l', ',', 'but', 'i', 'began', 'to', 'get', 'some', 'wi', '##er', '##d', 'side', 'effects', ')', '.', 'i', 'don', \"'\", 't', 'remember', 'the', 'dos', '##age', 'but', 'i', 'believe', 'it', 'was', 'maybe', '54', '##mg', 'daily', '.', 'a', 'year', 'or', 'so', 'later', 'later', ',', 'my', 'pediatric', '##ian', 'up', '##ped', 'the', 'dos', '##age', 'to', '72', '##mg', ',', 'which', 'is', ',', 'i', 'believe', ',', 'the', 'maximum', 'amount', 'that', 'can', 'be', 'prescribed', '.', 'i', 'am', '18', 'now', 'and', 'i', 'feel', 'that', 'i', 'am', 'beginning', 'to', 'build', 'up', 'a', 'tolerance', 'to', 'it', '.', 'can', 'this', 'even', 'happen', '?', 'i', \"'\", 've', 'done', 'a', 'bit', 'of', 'research', 'and', 'i', \"'\", 've', 'noticed', 'a', 'few', 'things', '.', 'people', 'say', 'that', 'concert', '##a', 'will', 'last', 'around', '9', '-', '10', 'hours', '.', 'i', 'am', 'probably', 'getting', '6', 'hours', 'of', 'benefit', 'from', 'it', ',', 'and', 'usually', 'nothing', 'more', '.', 'i', 'find', 'this', 'odd', ',', 'because', 'i', 'am', 'taking', 'higher', 'than', 'the', 'average', 'dose', 'and', 'not', 'receiving', 'the', 'full', 'benefit', 'for', 'a', 'proper', 'amount', 'of', 'time', '.', 'occasionally', 'i', 'will', 'need', 'to', 'take', 'a', '10', '##mg', 'rita', '##lin', 'as', 'a', 'booster', '.', 'so', 'am', 'i', 'actually', 'building', 'a', 'tolerance', 'or', 'is', 'it', 'all', 'in', 'my', 'head', '(', 'pun', 'intended', ')', '.', 'thanks', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'edit', ':', 'i', 'have', 'add', 'not', 'ad', '##hd', 'if', 'that', 'changes', 'anything', '.']\n",
      "INFO:__main__:Number of tokens: 251\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['concerned', 'about', 'building', 'a', 'tolerance', 'to', 'concert', '##a', 'so', 'i', 'started', 'taking', 'concert', '##a', 'about', 'age', '9', '.', '(', 'before', 'that', ',', 'i', 'took', 'add', '##eral', '##l', ',', 'but', 'i', 'began', 'to', 'get', 'some', 'wi', '##er', '##d', 'side', 'effects', ')', '.', 'i', 'don', \"'\", 't', 'remember', 'the', 'dos', '##age', 'but', 'i', 'believe', 'it', 'was', 'maybe', '54', '##mg', 'daily', '.', 'a', 'year', 'or', 'so', 'later', 'later', ',', 'my', 'pediatric', '##ian', 'up', '##ped', 'the', 'dos', '##age', 'to', '72', '##mg', ',', 'which', 'is', ',', 'i', 'believe', ',', 'the', 'maximum', 'amount', 'that', 'can', 'be', 'prescribed', '.', 'i', 'am', '18', 'now', 'and', 'i', 'feel', 'that', 'i', 'am', 'beginning', 'to', 'build', 'up', 'a', 'tolerance', 'to', 'it', '.', 'can', 'this', 'even', 'happen', '?', 'i', \"'\", 've', 'done', 'a', 'bit', 'of', 'research', 'and', 'i', \"'\", 've', 'noticed', 'a', 'few', 'things', '.', 'people', 'say', 'that', 'concert', '##a', 'will', 'last', 'around', '9', '-', '10', 'hours', '.', 'i', 'am', 'probably', 'getting', '6', 'hours', 'of', 'benefit', 'from', 'it', ',', 'and', 'usually', 'nothing', 'more', '.', 'i', 'find', 'this', 'odd', ',', 'because', 'i', 'am', 'taking', 'higher', 'than', 'the', 'average', 'dose', 'and', 'not', 'receiving', 'the', 'full', 'benefit', 'for', 'a', 'proper', 'amount', 'of', 'time', '.', 'occasionally', 'i', 'will', 'need', 'to', 'take', 'a', '10', '##mg', 'rita', '##lin', 'as', 'a', 'booster', '.', 'so', 'am', 'i', 'actually', 'building', 'a', 'tolerance', 'or', 'is', 'it', 'all', 'in', 'my', 'head', '(', 'pun', 'intended', ')', '.', 'thanks', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'edit', ':', 'i', 'have', 'add', 'not', 'ad', '##hd', 'if', 'that', 'changes', 'anything', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'feel', 'great', 'guys', '!', '(', 'happy', 'story', '[', 'also', 'semi', 'long', ']', ')', 'so', 'i', 'was', 'recently', 'diagnosed', 'with', 'ad', '##hd', 'and', 'am', 'no', 'med', '##icated', 'and', 'have', 'energy', 'for', 'the', 'first', 'time', 'and', 'can', 'think', 'clearly', '.', 'my', 'life', 'before', 'wasn', \"'\", 't', 'nearly', 'as', 'good', '.', 'i', 'played', 'high', 'pace', 'video', 'games', 'all', 'day', 'because', 'it', 'was', 'the', 'one', 'thing', 'that', 'could', 'keep', 'my', 'attention', '(', 'not', 'even', 'breaking', 'bad', 'could', 'keep', 'it', ')', '.', 'i', 'ate', 'junk', 'food', 'because', 'it', 'was', 'the', 'best', 'tasting', '.', 'i', 'would', 'spend', 'hours', 'on', 'my', 'butt', 'doing', 'nothing', '.', 'now', 'that', 'i', \"'\", 'm', 'med', '##icated', 'i', 'feel', 'amazing', '!', 'my', 'mind', 'has', 'never', 'been', 'as', 'clear', 'as', 'it', 'is', 'now', '.', 'i', 'still', 'have', 'a', 'long', 'way', 'to', 'go', 'in', 'my', 'medication', ',', 'finding', 'a', 'dos', '##age', 'that', 'fits', 'me', ',', 'and', 'changing', 'some', 'habits', '.', 'but', 'it', \"'\", 's', 'ind', '##es', '##cr', '##iba', '##ble', 'for', 'me', '.', 'i', 'went', 'for', 'a', 'run', 'the', 'other', 'day', ',', 'i', 'had', 'been', 'telling', 'myself', 'i', 'would', 'start', 'running', 'for', 'a', 'long', 'time', 'and', 'i', 'finally', 'did', 'it', '!', 'not', 'only', 'that', 'but', 'i', 'ended', 'up', 'running', '4', 'miles', '!', 'in', 'all', 'honesty', 'how', 'great', 'i', 'feel', 'is', 'ind', '##es', '##cr', '##iba', '##ble', 'for', 'anyone', 'who', 'hasn', \"'\", 't', 'experienced', 'it', '.', 'if', 'you', 'were', 'to', 'come', 'up', 'to', 'me', 'on', 'the', 'street', '2', 'months', 'ago', 'and', 'put', 'a', 'gun', 'to', 'my', 'head', 'and', 'tell', 'me', 'to', 'read', '2', '(', 'medium', ')', 'paragraph', '##s', 'in', 'under', '2', 'minutes', 'and', 'retain', 'the', 'knowledge', 'i', 'may', 'have', 'been', 'able', 'finished', '1', 'and', 'wouldn', \"'\", 't', 'have', 'retained', 'any', 'knowledge', '.', 'sorry', 'that', 'this', 'is', 'long', ',', 'it', \"'\", 's', 'just', '.', '.', '.', 'i', \"'\", 'm', 'really', 'happy', 'and', 'feel', 'great', '.', 't', '##l', ';', 'dr', 'i', \"'\", 'm', 'super', 'happy', 'and', 'feel', 'amazing', '.']\n",
      "INFO:__main__:Number of tokens: 311\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'feel', 'great', 'guys', '!', '(', 'happy', 'story', '[', 'also', 'semi', 'long', ']', ')', 'so', 'i', 'was', 'recently', 'diagnosed', 'with', 'ad', '##hd', 'and', 'am', 'no', 'med', '##icated', 'and', 'have', 'energy', 'for', 'the', 'first', 'time', 'and', 'can', 'think', 'clearly', '.', 'my', 'life', 'before', 'wasn', \"'\", 't', 'nearly', 'as', 'good', '.', 'i', 'played', 'high', 'pace', 'video', 'games', 'all', 'day', 'because', 'it', 'was', 'the', 'one', 'thing', 'that', 'could', 'keep', 'my', 'attention', '(', 'not', 'even', 'breaking', 'bad', 'could', 'keep', 'it', ')', '.', 'i', 'ate', 'junk', 'food', 'because', 'it', 'was', 'the', 'best', 'tasting', '.', 'i', 'would', 'spend', 'hours', 'on', 'my', 'butt', 'doing', 'nothing', '.', 'now', 'that', 'i', \"'\", 'm', 'med', '##icated', 'i', 'feel', 'amazing', '!', 'my', 'mind', 'has', 'never', 'been', 'as', 'clear', 'as', 'it', 'is', 'now', '.', 'i', 'still', 'have', 'a', 'long', 'way', 'to', 'go', 'in', 'my', 'medication', ',', 'finding', 'a', 'dos', '##age', 'that', 'fits', 'me', ',', 'and', 'changing', 'some', 'habits', '.', 'but', 'it', \"'\", 's', 'ind', '##es', '##cr', '##iba', '##ble', 'for', 'me', '.', 'i', 'went', 'for', 'a', 'run', 'the', 'other', 'day', ',', 'i', 'had', 'been', 'telling', 'myself', 'i', 'would', 'start', 'running', 'for', 'a', 'long', 'time', 'and', 'i', 'finally', 'did', 'it', '!', 'not', 'only', 'that', 'but', 'i', 'ended', 'up', 'running', '4', 'miles', '!', 'in', 'all', 'honesty', 'how', 'great', 'i', 'feel', 'is', 'ind', '##es', '##cr', '##iba', '##ble', 'for', 'anyone', 'who', 'hasn', \"'\", 't', 'experienced', 'it', '.', 'if', 'you', 'were', 'to', 'come', 'up', 'to', 'me', 'on', 'the', 'street', '2', 'months', 'ago', 'and', 'put', 'a', 'gun', 'to', 'my', 'head', 'and', 'tell', 'me', 'to', 'read', '2', '(', 'medium', ')', 'paragraph', '##s', 'in', 'under', '2', 'minutes', 'and', 'retain', 'the', 'knowledge', 'i', 'may', 'have', 'been', 'able', 'finished', '1', 'and', 'wouldn', \"'\", 't', 'have', 'retained', 'any', 'knowledge', '.', 'sorry', 'that', 'this', 'is', 'long', ',', 'it', \"'\", 's', 'just', '.', '.', '.', 'i', \"'\", 'm', 'really', 'happy', 'and', 'feel', 'great', '.', 't', '##l', ';', 'dr', 'i', \"'\", 'm', 'super', 'happy', 'and', 'feel', 'amazing', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hi', '.', '.', '.', '.', 'has', 'anyone', 'experienced', 'this', '?', 'al', '##ot', 'of', 'experiences', 'i', 'read', 'on', 'this', 'sub', '##red', '##dit', 'sound', 'just', 'like', 'me', '.', 'but', 'i', 'also', 'wonder', '.', '.', '.', '.', '-', 'do', 'any', 'of', 'you', 'have', 'lots', 'of', 'trouble', 'being', 'motivated', 'to', 'be', 'social', ',', 'or', 'talk', 'to', 'your', 'friends', ',', 'even', 'when', 'you', 'like', 'them', ',', 'so', 'you', 'might', 'come', 'off', 'as', 'rude', '?', 'look', 'them', 'straight', 'in', 'the', 'eyes', 'while', 'they', 'talk', 'to', 'you', 'and', 'not', 'hear', 'a', 'damn', 'thing', 'they', 'say', 'because', 'your', 'mind', 'is', 'on', 'mars', '?', 'i', 'get', 'so', 'exhausted', 'around', 'people', 'and', 'i', 'wonder', 'if', 'it', 'is', 'some', 'aspect', 'of', 'this', 'or', 'just', 'my', 'intro', '##version', 'and', 'an', 'unrelated', 'thing', '.']\n",
      "INFO:__main__:Number of tokens: 121\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hi', '.', '.', '.', '.', 'has', 'anyone', 'experienced', 'this', '?', 'al', '##ot', 'of', 'experiences', 'i', 'read', 'on', 'this', 'sub', '##red', '##dit', 'sound', 'just', 'like', 'me', '.', 'but', 'i', 'also', 'wonder', '.', '.', '.', '.', '-', 'do', 'any', 'of', 'you', 'have', 'lots', 'of', 'trouble', 'being', 'motivated', 'to', 'be', 'social', ',', 'or', 'talk', 'to', 'your', 'friends', ',', 'even', 'when', 'you', 'like', 'them', ',', 'so', 'you', 'might', 'come', 'off', 'as', 'rude', '?', 'look', 'them', 'straight', 'in', 'the', 'eyes', 'while', 'they', 'talk', 'to', 'you', 'and', 'not', 'hear', 'a', 'damn', 'thing', 'they', 'say', 'because', 'your', 'mind', 'is', 'on', 'mars', '?', 'i', 'get', 'so', 'exhausted', 'around', 'people', 'and', 'i', 'wonder', 'if', 'it', 'is', 'some', 'aspect', 'of', 'this', 'or', 'just', 'my', 'intro', '##version', 'and', 'an', 'unrelated', 'thing', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['your', 'thoughts', 'on', 'romantic', 'relationships', 'and', 'add', '/', 'ad', '##hd', '.', 'well', ',', 'the', 'girlfriend', 'and', 'i', 'just', 'broke', 'up', '.', 'i', \"'\", 'm', '22', 'and', 'was', 'diagnosed', 'with', 'add', 'a', 'few', 'years', 'back', '(', 'ina', '##tten', '##tive', ')', '.', 'i', \"'\", 've', 'been', 'reading', 'a', 'lot', 'about', 'the', 'effect', 'of', 'add', '/', 'ad', '##hd', 'on', 'romantic', 'relationships', 'and', 'how', 'people', 'with', 'add', '/', 'ad', '##hd', 'have', 'a', 'harder', 'time', 'in', 'relationships', 'than', 'others', '.', 'so', ',', 'my', 'question', 'to', 'you', 'red', '##dit', ',', 'is', 'what', 'are', 'your', 'experiences', '(', 'if', 'you', 'don', \"'\", 't', 'mind', 'sharing', ')', 'and', 'what', 'have', 'you', 'been', 'able', 'to', 'do', 'that', 'helps', '?']\n",
      "INFO:__main__:Number of tokens: 109\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['your', 'thoughts', 'on', 'romantic', 'relationships', 'and', 'add', '/', 'ad', '##hd', '.', 'well', ',', 'the', 'girlfriend', 'and', 'i', 'just', 'broke', 'up', '.', 'i', \"'\", 'm', '22', 'and', 'was', 'diagnosed', 'with', 'add', 'a', 'few', 'years', 'back', '(', 'ina', '##tten', '##tive', ')', '.', 'i', \"'\", 've', 'been', 'reading', 'a', 'lot', 'about', 'the', 'effect', 'of', 'add', '/', 'ad', '##hd', 'on', 'romantic', 'relationships', 'and', 'how', 'people', 'with', 'add', '/', 'ad', '##hd', 'have', 'a', 'harder', 'time', 'in', 'relationships', 'than', 'others', '.', 'so', ',', 'my', 'question', 'to', 'you', 'red', '##dit', ',', 'is', 'what', 'are', 'your', 'experiences', '(', 'if', 'you', 'don', \"'\", 't', 'mind', 'sharing', ')', 'and', 'what', 'have', 'you', 'been', 'able', 'to', 'do', 'that', 'helps', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'mis', '##dia', '##gno', '##sed', ':', 'beyond', 'behaviors', '(', 'the', 'best', 'explanation', 'i', \"'\", 've', 'seen', 'of', 'why', 'ad', '##hd', 'is', 'mis', '##dia', '##gno', '##sed', ',', 'and', 'how', 'diagnosis', 'should', 'be', 'improved', ')']\n",
      "INFO:__main__:Number of tokens: 34\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'mis', '##dia', '##gno', '##sed', ':', 'beyond', 'behaviors', '(', 'the', 'best', 'explanation', 'i', \"'\", 've', 'seen', 'of', 'why', 'ad', '##hd', 'is', 'mis', '##dia', '##gno', '##sed', ',', 'and', 'how', 'diagnosis', 'should', 'be', 'improved', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['life', 'changed', 'a', 'lot', ',', 'should', 'my', 'med', '##s', 'change', 'as', 'well', '?', 'i', 'have', 'been', 'struggling', 'with', 'add', 'for', 'as', 'long', 'as', 'i', 'can', 'really', 'remember', '.', 'i', 'was', 'diagnosed', 'and', 'started', 'taking', 'medication', 'this', 'past', 'year', '.', 'last', 'semester', 'i', 'was', 'only', 'taking', 'one', 'class', 'so', 'i', 'never', 'had', 'the', 'slightest', 'problem', 'with', 'my', 'medication', 'wearing', 'out', '(', 'add', '##eral', '##l', '20', '##mg', 'x', '##r', 'once', 'a', 'day', ')', ',', 'this', 'semester', 'is', 'quite', 'different', 'because', 'i', 'am', 'taking', 'four', 'classes', '.', 'it', 'doesn', \"'\", 't', 'seem', 'to', 'me', 'that', 'my', 'dos', '##age', 'is', 'too', 'low', ',', 'although', 'i', 'think', 'i', 'am', 'building', 'somewhat', 'of', 'a', 'tolerance', '.', 'i', 'am', 'quite', 'un', '##in', '##formed', 'about', 'most', 'of', 'the', 'medication', 'options', 'for', 'add', 'so', 'i', 'was', 'hoping', 'for', 'some', 'advice', 'if', 'anyone', 'has', 'any', '.', 'i', 'think', 'that', 'switching', 'to', 'ir', 'might', 'be', 'better', 'for', 'my', 'needs', 'provided', 'it', 'works', 'in', 'the', 'way', 'i', 'think', 'it', 'does', '.', 'if', 'i', 'switched', 'would', 'i', 'take', 'one', 'dose', 'every', 'so', 'many', 'hours', 'as', 'needed', '?', 'this', 'would', 'be', 'a', 'big', 'improvement', 'for', 'me', 'because', 'the', 'length', 'of', 'x', '##r', 'is', 'not', 'long', 'enough', 'to', 'last', 'my', 'day', 'of', 'classes', '(', 'leave', 'my', 'house', 'at', '7', ':', '00', '##am', 'and', 'get', 'back', 'at', 'about', '5', ':', '00pm', ')', 'but', 'i', 'think', 'it', 'is', 'too', 'long', 'to', 'make', 'taking', 'multiple', 'doses', 'of', 'x', '##r', 'an', 'option', '.', 'one', 'of', 'my', 'good', 'friends', 'told', 'me', 'about', 'his', 'regime', '##n', ',', 'he', 'takes', 'both', 'st', '##rat', '##tera', 'and', 'add', '##eral', '##l', ',', 'the', 'way', 'he', 'put', 'it', 'was', 'that', 'st', '##rat', '##tera', 'helps', 'him', 'with', 'a', 'general', 'increase', 'across', 'the', 'board', 'but', 'its', 'doesn', \"'\", 't', 'help', 'him', 'enough', 'to', 'do', 'school', 'work', 'so', 'hat', 'is', 'what', 'the', 'add', '##eral', '##l', 'is', 'for', '.', 'does', 'anyone', 'have', 'any', 'experience', 'with', 'this', 'type', 'of', 'setup', '?', 'i', 'have', 'also', 'been', 'trying', 'to', 'work', 'up', 'the', 'nerve', 'to', 'talk', 'to', 'my', 'doctor', 'about', 'social', 'anxiety', 'disorder', ',', 'i', 'have', 'read', 'that', 'ssr', '##i', \"'\", 's', 'can', 'be', 'used', 'to', 'help', 'without', 'interfering', 'with', 'add', 'medication', '.']\n",
      "INFO:__main__:Number of tokens: 349\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['life', 'changed', 'a', 'lot', ',', 'should', 'my', 'med', '##s', 'change', 'as', 'well', '?', 'i', 'have', 'been', 'struggling', 'with', 'add', 'for', 'as', 'long', 'as', 'i', 'can', 'really', 'remember', '.', 'i', 'was', 'diagnosed', 'and', 'started', 'taking', 'medication', 'this', 'past', 'year', '.', 'last', 'semester', 'i', 'was', 'only', 'taking', 'one', 'class', 'so', 'i', 'never', 'had', 'the', 'slightest', 'problem', 'with', 'my', 'medication', 'wearing', 'out', '(', 'add', '##eral', '##l', '20', '##mg', 'x', '##r', 'once', 'a', 'day', ')', ',', 'this', 'semester', 'is', 'quite', 'different', 'because', 'i', 'am', 'taking', 'four', 'classes', '.', 'it', 'doesn', \"'\", 't', 'seem', 'to', 'me', 'that', 'my', 'dos', '##age', 'is', 'too', 'low', ',', 'although', 'i', 'think', 'i', 'am', 'building', 'somewhat', 'of', 'a', 'tolerance', '.', 'i', 'am', 'quite', 'un', '##in', '##formed', 'about', 'most', 'of', 'the', 'medication', 'options', 'for', 'add', 'so', 'i', 'was', 'hoping', 'for', 'some', 'advice', 'if', 'anyone', 'has', 'any', '.', 'i', 'think', 'that', 'switching', 'to', 'ir', 'might', 'be', 'better', 'for', 'my', 'needs', 'provided', 'it', 'works', 'in', 'the', 'way', 'i', 'think', 'it', 'does', '.', 'if', 'i', 'switched', 'would', 'i', 'take', 'one', 'dose', 'every', 'so', 'many', 'hours', 'as', 'needed', '?', 'this', 'would', 'be', 'a', 'big', 'improvement', 'for', 'me', 'because', 'the', 'length', 'of', 'x', '##r', 'is', 'not', 'long', 'enough', 'to', 'last', 'my', 'day', 'of', 'classes', '(', 'leave', 'my', 'house', 'at', '7', ':', '00', '##am', 'and', 'get', 'back', 'at', 'about', '5', ':', '00pm', ')', 'but', 'i', 'think', 'it', 'is', 'too', 'long', 'to', 'make', 'taking', 'multiple', 'doses', 'of', 'x', '##r', 'an', 'option', '.', 'one', 'of', 'my', 'good', 'friends', 'told', 'me', 'about', 'his', 'regime', '##n', ',', 'he', 'takes', 'both', 'st', '##rat', '##tera', 'and', 'add', '##eral', '##l', ',', 'the', 'way', 'he', 'put', 'it', 'was', 'that', 'st', '##rat', '##tera', 'helps', 'him', 'with', 'a', 'general', 'increase', 'across', 'the', 'board', 'but', 'its', 'doesn', \"'\", 't', 'help', 'him', 'enough', 'to', 'do', 'school', 'work', 'so', 'hat', 'is', 'what', 'the', 'add', '##eral', '##l', 'is', 'for', '.', 'does', 'anyone', 'have', 'any', 'experience', 'with', 'this', 'type', 'of', 'setup', '?', 'i', 'have', 'also', 'been', 'trying', 'to', 'work', 'up', 'the', 'nerve', 'to', 'talk', 'to', 'my', 'doctor', 'about', 'social', 'anxiety', 'disorder', ',', 'i', 'have', 'read', 'that', 'ssr', '##i', \"'\", 's', 'can', 'be', 'used', 'to', 'help', 'without', 'interfering', 'with', 'add', 'medication', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['son', 'was', 'diagnosed', 'and', 'started', 'med', '##s', 'this', 'week']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['son', 'was', 'diagnosed', 'and', 'started', 'med', '##s', 'this', 'week']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['are', 'you', 'a', 'failure', '?']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['are', 'you', 'a', 'failure', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', 'few', 'mis', '##c', 'questions', 'from', 'a', 'block', '##head', 'who', 'has', 'ad', '##hd', 'predominantly', 'ina', '##tten', '##tive', '.', 'to', 'start', 'off', ',', 'i', 'don', \"'\", 't', 'believe', 'in', 'many', 'mental', 'illnesses', '/', 'disabilities', '.', 'bear', 'in', 'mind', 'i', \"'\", 'm', 'kind', 'of', 'a', 'jerk', 'and', 'a', 'block', '##head', '.', 'while', 'i', \"'\", 'm', 'skeptical', 'about', 'ad', '##hd', ',', 'i', 'have', 'classic', 'add', '/', 'ad', '##hd', 'predominantly', 'ina', '##tten', '##tive', 'symptoms', 'since', 'i', 'was', 'in', '3rd', 'grade', '.', 'i', 'did', 'well', 'anyway', '##s', 'thanks', 'to', 'sheer', 'smart', '##s', ',', 'but', 'towards', 'the', 'end', 'of', '11th', 'and', 'in', '12th', 'i', 'struggled', 'to', 'pay', 'attention', 'and', 'it', 'got', 'much', 'harder', 'to', 'maintain', 'my', 'grades', '.', 'i', 'won', \"'\", 't', 'bore', 'you', 'with', 'the', 'details', ',', 'but', 'it', \"'\", 's', 'basically', 'certain', 'i', 'have', 'it', 'and', 'it', \"'\", 's', 'getting', 'worse', '.', 'what', 'it', 'comes', 'down', 'to', 'is', ',', 'i', 'want', 'st', '##im', '##ula', '##nts', 'or', 'similar', 'proven', 'med', '##s', '.', 'partly', 'to', 'fix', 'my', 'memory', ',', 'partly', 'because', 'there', \"'\", 's', 'like', 'fifty', 'studies', 'out', 'there', 'that', 'say', 'it', \"'\", 'll', 'make', 'me', 'smarter', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'one', 'of', 'the', 'big', 'drugs', ',', 'add', '##eral', '##l', 'or', 'rita', '##lin', 'i', 'think', ',', 'recently', 'got', 'screwed', 'over', 'by', 'us', 'law', 'and', 'now', 'there', \"'\", 's', 'quite', 'a', 'limited', 'supply', '.', 'any', 'news', 'on', 'when', 'or', 'if', 'they', \"'\", 'll', 'lift', 'the', 'cap', '?', '-', 'how', 'long', 'does', 'it', 'take', 'to', 'get', 'diagnosed', 'by', 'a', 'professional', '?', 'like', ',', 'the', 'actual', 'appointment', '?', '-', 'is', 'there', 'a', 'such', 'thing', 'as', 'side', '-', 'effect', 'free', '?', 'how', 'common', 'is', 'it', 'to', 'cope', 'with', 'minor', 'side', 'effects', 'while', 'on', 'ad', '##hd', 'drugs', '?', '-', 'i', \"'\", 'm', 'genuinely', 'worried', 'about', 'my', 'college', 'roommate', '##s', 'stealing', 'my', 'ad', '##hd', 'drugs', '.', 'any', 'advice', 'for', 'stopping', 'them', 'from', 'stealing', 'my', 'med', '##s', '?', '-', 'the', 'way', 'i', 'understand', 'it', ',', 'there', \"'\", 's', 'two', 'basic', 'styles', 'for', 'taking', 'pills', ',', 'depending', 'on', 'your', 'needs', '.', 'high', '-', 'is', '##h', 'dose', 'once', 'a', 'day', 'in', 'the', 'morning', ',', 'or', 'two', 'medium', '-', 'is', '##h', 'doses', ',', 'morning', 'and', 'mid', 'afternoon', '.', 'is', 'this', 'accurate', '?', '-', 'are', 'there', 'any', 'actual', 'effective', 'drugs', 'besides', 'add', '##eral', '##l', 'and', 'rita', '##lin', 'that', 'are', 'regularly', 'prescribed', '?', '-', 'is', 'it', 'possible', 'to', 'become', 'resistant', 'to', 'the', 'med', '##s', '?', 'do', 'longtime', 'ad', '##hd', 'suffer', '##ers', 'have', 'to', 'up', 'their', 'doses', '?', 'thanks', 'for', 'your', 'time', '.']\n",
      "INFO:__main__:Number of tokens: 427\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', 'few', 'mis', '##c', 'questions', 'from', 'a', 'block', '##head', 'who', 'has', 'ad', '##hd', 'predominantly', 'ina', '##tten', '##tive', '.', 'to', 'start', 'off', ',', 'i', 'don', \"'\", 't', 'believe', 'in', 'many', 'mental', 'illnesses', '/', 'disabilities', '.', 'bear', 'in', 'mind', 'i', \"'\", 'm', 'kind', 'of', 'a', 'jerk', 'and', 'a', 'block', '##head', '.', 'while', 'i', \"'\", 'm', 'skeptical', 'about', 'ad', '##hd', ',', 'i', 'have', 'classic', 'add', '/', 'ad', '##hd', 'predominantly', 'ina', '##tten', '##tive', 'symptoms', 'since', 'i', 'was', 'in', '3rd', 'grade', '.', 'i', 'did', 'well', 'anyway', '##s', 'thanks', 'to', 'sheer', 'smart', '##s', ',', 'but', 'towards', 'the', 'end', 'of', '11th', 'and', 'in', '12th', 'i', 'struggled', 'to', 'pay', 'attention', 'and', 'it', 'got', 'much', 'harder', 'to', 'maintain', 'my', 'grades', '.', 'i', 'won', \"'\", 't', 'bore', 'you', 'with', 'the', 'details', ',', 'but', 'it', \"'\", 's', 'basically', 'certain', 'i', 'have', 'it', 'and', 'it', \"'\", 's', 'getting', 'worse', '.', 'what', 'it', 'comes', 'down', 'to', 'is', ',', 'i', 'want', 'st', '##im', '##ula', '##nts', 'or', 'similar', 'proven', 'med', '##s', '.', 'partly', 'to', 'fix', 'my', 'memory', ',', 'partly', 'because', 'there', \"'\", 's', 'like', 'fifty', 'studies', 'out', 'there', 'that', 'say', 'it', \"'\", 'll', 'make', 'me', 'smarter', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'one', 'of', 'the', 'big', 'drugs', ',', 'add', '##eral', '##l', 'or', 'rita', '##lin', 'i', 'think', ',', 'recently', 'got', 'screwed', 'over', 'by', 'us', 'law', 'and', 'now', 'there', \"'\", 's', 'quite', 'a', 'limited', 'supply', '.', 'any', 'news', 'on', 'when', 'or', 'if', 'they', \"'\", 'll', 'lift', 'the', 'cap', '?', '-', 'how', 'long', 'does', 'it', 'take', 'to', 'get', 'diagnosed', 'by', 'a', 'professional', '?', 'like', ',', 'the', 'actual', 'appointment', '?', '-', 'is', 'there', 'a', 'such', 'thing', 'as', 'side', '-', 'effect', 'free', '?', 'how', 'common', 'is', 'it', 'to', 'cope', 'with', 'minor', 'side', 'effects', 'while', 'on', 'ad', '##hd', 'drugs', '?', '-', 'i', \"'\", 'm', 'genuinely', 'worried', 'about', 'my', 'college', 'roommate', '##s', 'stealing', 'my', 'ad', '##hd', 'drugs', '.', 'any', 'advice', 'for', 'stopping', 'them', 'from', 'stealing', 'my', 'med', '##s', '?', '-', 'the', 'way', 'i', 'understand', 'it', ',', 'there', \"'\", 's', 'two', 'basic', 'styles', 'for', 'taking', 'pills', ',', 'depending', 'on', 'your', 'needs', '.', 'high', '-', 'is', '##h', 'dose', 'once', 'a', 'day', 'in', 'the', 'morning', ',', 'or', 'two', 'medium', '-', 'is', '##h', 'doses', ',', 'morning', 'and', 'mid', 'afternoon', '.', 'is', 'this', 'accurate', '?', '-', 'are', 'there', 'any', 'actual', 'effective', 'drugs', 'besides', 'add', '##eral', '##l', 'and', 'rita', '##lin', 'that', 'are', 'regularly', 'prescribed', '?', '-', 'is', 'it', 'possible', 'to', 'become', 'resistant', 'to', 'the', 'med', '##s', '?', 'do', 'longtime', 'ad', '##hd', 'suffer', '##ers', 'have', 'to', 'up', 'their', 'doses', '?', 'thanks', 'for', 'your', 'time', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['long', 'term', 'effects', 'of', 'v', '##y', '##van', '##se', '/', 'add', '##eral', '##l', '?', 'i', \"'\", 've', 'been', 'prescribed', 'v', '##vy', '##ans', '##e', 'for', 'approximately', 'the', 'past', 'two', 'years', 'and', 'i', \"'\", 've', 'noticed', ',', 'among', 'other', 'things', ',', 'that', 'my', 'tolerance', 'has', 'drastically', 'gone', 'up', 'and', 'my', 'ability', 'to', 'function', 'without', 'focus', 'drugs', 'is', 'all', 'but', 'gone', '.', 'has', 'anyone', 'else', 'noticed', 'any', 'more', 'severe', 'long', '-', 'term', 'consequences', 'of', 'amp', '##het', '##amine', '-', 'based', 'ad', '##hd', 'drugs', '?', 'i', \"'\", 'm', 'prescribed', '40', 'mg', 'v', '##y', '##van', '##se', 'per', 'day', 'but', 'i', \"'\", 've', 'taken', 'as', 'much', 'as', '120', 'mg', 'to', 'help', 'with', 'extended', 'study', 'sessions', '.']\n",
      "INFO:__main__:Number of tokens: 108\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['long', 'term', 'effects', 'of', 'v', '##y', '##van', '##se', '/', 'add', '##eral', '##l', '?', 'i', \"'\", 've', 'been', 'prescribed', 'v', '##vy', '##ans', '##e', 'for', 'approximately', 'the', 'past', 'two', 'years', 'and', 'i', \"'\", 've', 'noticed', ',', 'among', 'other', 'things', ',', 'that', 'my', 'tolerance', 'has', 'drastically', 'gone', 'up', 'and', 'my', 'ability', 'to', 'function', 'without', 'focus', 'drugs', 'is', 'all', 'but', 'gone', '.', 'has', 'anyone', 'else', 'noticed', 'any', 'more', 'severe', 'long', '-', 'term', 'consequences', 'of', 'amp', '##het', '##amine', '-', 'based', 'ad', '##hd', 'drugs', '?', 'i', \"'\", 'm', 'prescribed', '40', 'mg', 'v', '##y', '##van', '##se', 'per', 'day', 'but', 'i', \"'\", 've', 'taken', 'as', 'much', 'as', '120', 'mg', 'to', 'help', 'with', 'extended', 'study', 'sessions', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'feel', 'the', 'same', 'way', '?']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'feel', 'the', 'same', 'way', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['finally', 'got', 'the', 'courage', 'after', 'months', 'to', 'tell', 'my', 'own', 'mother', 'at', '21', 'that', 'i', 'was', 'going', 'to', 'get', 'tested', 'for', 'add', '.', 'got', 'shot', 'down', '.', 'i', 'finally', 'got', 'the', 'courage', 'to', 'tell', 'my', 'mother', 'that', 'i', 'was', 'going', 'tomorrow', 'at', 'the', 'age', 'of', '21', 'and', 'she', 'straight', 'up', 'told', 'me', 'that', 'it', 'was', 'all', 'in', 'my', 'head', '.', '.', '.', '.', 'umm', 'yeah', ',', 'that', '##s', 'the', 'point', '.', 'apparently', 'i', 'was', 'tested', 'in', 'grade', '7', 'for', 'a', 'learning', 'disability', 'and', 'they', 'cam', 'to', 'some', 'conclusion', 'that', 'i', 'saw', 'slower', 'with', 'languages', 'and', 'spelling', '.', 'she', 'goes', 'on', 'to', 'tell', 'me', 'that', 'she', \"'\", 's', 'not', 'convinced', 'that', 'there', \"'\", 's', 'anything', 'wrong', 'with', 'me', 'or', 'my', 'marks', 'in', 'high', 'school', '(', '50', '##s', '-', 'mid', '60s', ')', 'due', 'to', 'me', 'not', 'working', 'as', 'hard', 'as', 'i', 'could', 'have', '.', 'i', 'have', 'taken', 'many', 'online', 'tests', 'and', 'been', 'around', 'this', 'sub', '##red', '##dit', 'for', 'some', 'time', 'now', 'seeing', 'if', 'it', 'was', 'even', 'worth', 'going', 'to', 'the', 'doctors', 'for', 'help', ',', 'from', 'what', 'the', 'tests', 'say', 'and', 'what', 'i', \"'\", 've', 'gathered', 'here', 'i', 'definitely', 'need', 'to', 'at', 'least', 'talk', 'to', 'someone', 'about', 'it', '.', 'i', 'feel', 'blown', 'off', 'and', 'completely', 'put', 'down', 'about', 'going', 'tomorrow', ',', 'by', 'my', 'own', 'mother', '.', 't', '##l', ';', 'dr', 'told', 'my', 'mother', 'i', 'was', 'finally', 'going', 'to', 'seek', 'help', 'at', '21', 'for', 'suspected', 'add', 'and', 'she', 'shot', 'me', 'down', '.', 'edit', ':', 'my', 'doctor', 'is', 'one', 'of', 'the', '\"', 'no', 'pill', 'is', 'a', 'good', 'pill', '\"', 'kind', 'of', 'guy', ',', 'but', 'after', 'talking', 'we', 'are', 'going', 'with', 'a', 'trial', 'for', 'the', 'next', 'couple', 'of', 'months', '.', 'he', 'started', 'me', 'off', 'on', '10', '##mg', 'dex', '##ed', '##rine', 'and', 'we', 'are', 'going', 'to', 'take', 'it', 'from', 'there', '.', 'thank', 'all', 'of', 'you', 'for', 'your', 'kind', 'words', 'and', 'support', '.', 'sometimes', 'that', '##s', 'all', 'we', 'need', '.']\n",
      "INFO:__main__:Number of tokens: 312\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['finally', 'got', 'the', 'courage', 'after', 'months', 'to', 'tell', 'my', 'own', 'mother', 'at', '21', 'that', 'i', 'was', 'going', 'to', 'get', 'tested', 'for', 'add', '.', 'got', 'shot', 'down', '.', 'i', 'finally', 'got', 'the', 'courage', 'to', 'tell', 'my', 'mother', 'that', 'i', 'was', 'going', 'tomorrow', 'at', 'the', 'age', 'of', '21', 'and', 'she', 'straight', 'up', 'told', 'me', 'that', 'it', 'was', 'all', 'in', 'my', 'head', '.', '.', '.', '.', 'umm', 'yeah', ',', 'that', '##s', 'the', 'point', '.', 'apparently', 'i', 'was', 'tested', 'in', 'grade', '7', 'for', 'a', 'learning', 'disability', 'and', 'they', 'cam', 'to', 'some', 'conclusion', 'that', 'i', 'saw', 'slower', 'with', 'languages', 'and', 'spelling', '.', 'she', 'goes', 'on', 'to', 'tell', 'me', 'that', 'she', \"'\", 's', 'not', 'convinced', 'that', 'there', \"'\", 's', 'anything', 'wrong', 'with', 'me', 'or', 'my', 'marks', 'in', 'high', 'school', '(', '50', '##s', '-', 'mid', '60s', ')', 'due', 'to', 'me', 'not', 'working', 'as', 'hard', 'as', 'i', 'could', 'have', '.', 'i', 'have', 'taken', 'many', 'online', 'tests', 'and', 'been', 'around', 'this', 'sub', '##red', '##dit', 'for', 'some', 'time', 'now', 'seeing', 'if', 'it', 'was', 'even', 'worth', 'going', 'to', 'the', 'doctors', 'for', 'help', ',', 'from', 'what', 'the', 'tests', 'say', 'and', 'what', 'i', \"'\", 've', 'gathered', 'here', 'i', 'definitely', 'need', 'to', 'at', 'least', 'talk', 'to', 'someone', 'about', 'it', '.', 'i', 'feel', 'blown', 'off', 'and', 'completely', 'put', 'down', 'about', 'going', 'tomorrow', ',', 'by', 'my', 'own', 'mother', '.', 't', '##l', ';', 'dr', 'told', 'my', 'mother', 'i', 'was', 'finally', 'going', 'to', 'seek', 'help', 'at', '21', 'for', 'suspected', 'add', 'and', 'she', 'shot', 'me', 'down', '.', 'edit', ':', 'my', 'doctor', 'is', 'one', 'of', 'the', '\"', 'no', 'pill', 'is', 'a', 'good', 'pill', '\"', 'kind', 'of', 'guy', ',', 'but', 'after', 'talking', 'we', 'are', 'going', 'with', 'a', 'trial', 'for', 'the', 'next', 'couple', 'of', 'months', '.', 'he', 'started', 'me', 'off', 'on', '10', '##mg', 'dex', '##ed', '##rine', 'and', 'we', 'are', 'going', 'to', 'take', 'it', 'from', 'there', '.', 'thank', 'all', 'of', 'you', 'for', 'your', 'kind', 'words', 'and', 'support', '.', 'sometimes', 'that', '##s', 'all', 'we', 'need', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hello', 'all', '.', 'i', \"'\", 'm', 'very', 'happy', 'to', 'have', 'found', 'you', 'all', '.', 'i', 'am', 'a', '28', 'year', 'old', 'diagnosed', 'as', 'ad', '##hd', 'as', 'a', 'child', ';', 'quit', 'taking', 'med', '##s', 'on', 'my', 'own', 'at', '12', '.', 'it', \"'\", 's', 'been', 'a', 'rough', 'ride', 'but', ',', 'hey', ',', 'life', \"'\", 's', 'a', 'trip', '!']\n",
      "INFO:__main__:Number of tokens: 55\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hello', 'all', '.', 'i', \"'\", 'm', 'very', 'happy', 'to', 'have', 'found', 'you', 'all', '.', 'i', 'am', 'a', '28', 'year', 'old', 'diagnosed', 'as', 'ad', '##hd', 'as', 'a', 'child', ';', 'quit', 'taking', 'med', '##s', 'on', 'my', 'own', 'at', '12', '.', 'it', \"'\", 's', 'been', 'a', 'rough', 'ride', 'but', ',', 'hey', ',', 'life', \"'\", 's', 'a', 'trip', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'smoking', 'tell', 'me', 'about', 'your', 'experiences', 'with', 'smoking', 'ci', '##gs', '/', 'weed', '.', 'i', 'find', 'off', 'my', 'med', '##s', 'i', \"'\", 'm', 'more', 'prone', 'to', 'smoking', 'weed', ',', 'while', 'if', 'i', 'smoke', 'on', 'it', 'its', 'usually', 'a', 'good', 'time', '.', 'cigarettes', 'on', 'the', 'other', 'hand', '.', 'off', 'my', 'med', '##s', '-', '2', 'smoke', '##s', 'a', 'day', 'maybe', '.', '.', '.', '.', 'on', 'them', '?', 'half', 'a', 'pack', '.']\n",
      "INFO:__main__:Number of tokens: 71\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'smoking', 'tell', 'me', 'about', 'your', 'experiences', 'with', 'smoking', 'ci', '##gs', '/', 'weed', '.', 'i', 'find', 'off', 'my', 'med', '##s', 'i', \"'\", 'm', 'more', 'prone', 'to', 'smoking', 'weed', ',', 'while', 'if', 'i', 'smoke', 'on', 'it', 'its', 'usually', 'a', 'good', 'time', '.', 'cigarettes', 'on', 'the', 'other', 'hand', '.', 'off', 'my', 'med', '##s', '-', '2', 'smoke', '##s', 'a', 'day', 'maybe', '.', '.', '.', '.', 'on', 'them', '?', 'half', 'a', 'pack', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['feeling', 'completely', 'overwhelmed', 'in', 'college', 'courses', ',', 'despite', 'being', 'on', 'medication', '.', '.', '.', 'this', 'is', 'mostly', 'just', 'to', 'let', 'off', 'some', 'steam', ',', 'as', 'i', \"'\", 'm', 'not', 'sure', 'much', 'can', 'be', 'done', 'about', 'my', 'situation', '.', 'i', \"'\", 'm', 'a', '22', '-', 'year', 'old', 'female', ',', 'diagnosed', 'with', 'ina', '##tten', '##tive', '-', 'type', 'ad', '##hd', 'my', 'senior', 'year', 'of', 'high', 'school', '.', 'school', 'has', 'always', 'been', 'a', 'big', 'struggle', 'for', 'me', '-', 'i', \"'\", 'm', 'sure', 'many', 'of', 'you', 'can', 'sy', '##mp', '##athi', '##ze', '.', 'its', 'difficult', 'for', 'me', 'to', 'pay', 'attention', 'to', 'a', 'subject', 'unless', 'i', \"'\", 'm', 'very', 'interested', 'in', 'it', ',', 'my', 'organizational', 'skills', 'are', 'ab', '##ys', '##mal', ',', 'as', 'are', 'my', 'sense', 'of', 'time', 'and', 'ability', 'to', 'prior', '##iti', '##ze', 'tasks', '.', 'i', 'used', 'to', 'mask', 'my', 'struggles', 'out', 'of', 'shame', 'and', 'a', 'desire', 'to', 'do', 'well', 'in', 'school', 'and', 'make', 'my', 'parents', 'proud', ',', 'but', 'even', 'my', 'greatest', 'efforts', 'did', 'little', 'more', 'than', 'keep', 'my', 'head', 'above', 'water', '.', 'when', 'i', 'was', 'finally', 'diagnosed', 'and', 'put', 'on', 'medication', ',', 'everything', 'was', 'so', 'much', 'easier', '-', 'for', 'awhile', '.', 'by', 'my', 'so', '##ph', '##more', 'year', 'of', 'college', ',', 'things', 'started', 'to', 'un', '##rave', '##l', 'quickly', '.', 'now', ',', 'in', 'my', 'senior', 'year', 'of', 'college', ',', 'i', 'feel', 'like', 'i', \"'\", 'm', 'drowning', 'in', 'the', 'demands', 'of', 'homework', ',', 'classes', 'and', 'work', '.', 'i', 'am', 'completely', 'exhausted', 'and', 'i', \"'\", 'm', 'falling', 'further', 'behind', 'all', 'the', 'time', '.', 'my', 'grades', 'were', 'only', 'ok', 'to', 'begin', 'with', '.', 'i', \"'\", 've', 'already', 'tried', '3', 'times', 'to', 'gain', 'acceptance', 'into', 'my', 'major', ',', 'but', 'my', 'gp', '##a', 'has', 'just', 'barely', 'missed', 'the', 'minimum', 'requirement', '.', 'i', 'have', 'been', 'trying', 'so', 'hard', ',', 'and', 'i', 'just', 'can', \"'\", 't', 'seem', 'to', 'do', 'it', '.', 'i', 'am', 'so', 'worried', 'that', 'this', 'semester', ',', 'i', 'won', \"'\", 't', 'make', 'it', 'into', 'my', 'major', ',', 'either', '.', 'i', \"'\", 'm', 'so', 'overwhelmed', 'and', 'frustrated', ',', 'i', 'just', 'want', 'to', 'pl', '##op', 'down', 'on', 'the', 'floor', 'and', 'cry', '.', 'i', 'hate', 'that', 'i', \"'\", 'm', 'ad', '##hd', '.', 'accomplish', '##ing', 'anything', 'seems', 'to', 'be', '10', 'times', 'harder', 'for', 'me', 'than', 'it', 'is', 'for', 'anyone', 'around', 'me', '.', 'i', 'watch', 'all', 'these', 'people', 'my', 'age', 'accomplish', '##ing', 'all', 'the', 'goals', 'i', 'have', 'for', 'myself', ',', 'and', 'i', \"'\", 'm', 'so', 'angry', 'at', 'myself', 'that', 'i', 'can', \"'\", 't', 'do', 'it', 'too', '.', 'i', 'just', 'don', \"'\", 't', 'know', 'how', 'to', 'go', 'about', 'fixing', 'this', '.', 'and', 'i', \"'\", 'm', 'tired', 'and', 'overwhelmed', 'and', 'stressed', 'beyond', 'belief', '.', 'thanks', 'for', 'listening', 'to', 'my', 'ran', '##t', ',', 'everyone', '.']\n",
      "INFO:__main__:Number of tokens: 434\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['feeling', 'completely', 'overwhelmed', 'in', 'college', 'courses', ',', 'despite', 'being', 'on', 'medication', '.', '.', '.', 'this', 'is', 'mostly', 'just', 'to', 'let', 'off', 'some', 'steam', ',', 'as', 'i', \"'\", 'm', 'not', 'sure', 'much', 'can', 'be', 'done', 'about', 'my', 'situation', '.', 'i', \"'\", 'm', 'a', '22', '-', 'year', 'old', 'female', ',', 'diagnosed', 'with', 'ina', '##tten', '##tive', '-', 'type', 'ad', '##hd', 'my', 'senior', 'year', 'of', 'high', 'school', '.', 'school', 'has', 'always', 'been', 'a', 'big', 'struggle', 'for', 'me', '-', 'i', \"'\", 'm', 'sure', 'many', 'of', 'you', 'can', 'sy', '##mp', '##athi', '##ze', '.', 'its', 'difficult', 'for', 'me', 'to', 'pay', 'attention', 'to', 'a', 'subject', 'unless', 'i', \"'\", 'm', 'very', 'interested', 'in', 'it', ',', 'my', 'organizational', 'skills', 'are', 'ab', '##ys', '##mal', ',', 'as', 'are', 'my', 'sense', 'of', 'time', 'and', 'ability', 'to', 'prior', '##iti', '##ze', 'tasks', '.', 'i', 'used', 'to', 'mask', 'my', 'struggles', 'out', 'of', 'shame', 'and', 'a', 'desire', 'to', 'do', 'well', 'in', 'school', 'and', 'make', 'my', 'parents', 'proud', ',', 'but', 'even', 'my', 'greatest', 'efforts', 'did', 'little', 'more', 'than', 'keep', 'my', 'head', 'above', 'water', '.', 'when', 'i', 'was', 'finally', 'diagnosed', 'and', 'put', 'on', 'medication', ',', 'everything', 'was', 'so', 'much', 'easier', '-', 'for', 'awhile', '.', 'by', 'my', 'so', '##ph', '##more', 'year', 'of', 'college', ',', 'things', 'started', 'to', 'un', '##rave', '##l', 'quickly', '.', 'now', ',', 'in', 'my', 'senior', 'year', 'of', 'college', ',', 'i', 'feel', 'like', 'i', \"'\", 'm', 'drowning', 'in', 'the', 'demands', 'of', 'homework', ',', 'classes', 'and', 'work', '.', 'i', 'am', 'completely', 'exhausted', 'and', 'i', \"'\", 'm', 'falling', 'further', 'behind', 'all', 'the', 'time', '.', 'my', 'grades', 'were', 'only', 'ok', 'to', 'begin', 'with', '.', 'i', \"'\", 've', 'already', 'tried', '3', 'times', 'to', 'gain', 'acceptance', 'into', 'my', 'major', ',', 'but', 'my', 'gp', '##a', 'has', 'just', 'barely', 'missed', 'the', 'minimum', 'requirement', '.', 'i', 'have', 'been', 'trying', 'so', 'hard', ',', 'and', 'i', 'just', 'can', \"'\", 't', 'seem', 'to', 'do', 'it', '.', 'i', 'am', 'so', 'worried', 'that', 'this', 'semester', ',', 'i', 'won', \"'\", 't', 'make', 'it', 'into', 'my', 'major', ',', 'either', '.', 'i', \"'\", 'm', 'so', 'overwhelmed', 'and', 'frustrated', ',', 'i', 'just', 'want', 'to', 'pl', '##op', 'down', 'on', 'the', 'floor', 'and', 'cry', '.', 'i', 'hate', 'that', 'i', \"'\", 'm', 'ad', '##hd', '.', 'accomplish', '##ing', 'anything', 'seems', 'to', 'be', '10', 'times', 'harder', 'for', 'me', 'than', 'it', 'is', 'for', 'anyone', 'around', 'me', '.', 'i', 'watch', 'all', 'these', 'people', 'my', 'age', 'accomplish', '##ing', 'all', 'the', 'goals', 'i', 'have', 'for', 'myself', ',', 'and', 'i', \"'\", 'm', 'so', 'angry', 'at', 'myself', 'that', 'i', 'can', \"'\", 't', 'do', 'it', 'too', '.', 'i', 'just', 'don', \"'\", 't', 'know', 'how', 'to', 'go', 'about', 'fixing', 'this', '.', 'and', 'i', \"'\", 'm', 'tired', 'and', 'overwhelmed', 'and', 'stressed', 'beyond', 'belief', '.', 'thanks', 'for', 'listening', 'to', 'my', 'ran', '##t', ',', 'everyone', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'premature', 'e', '##ja', '##cula', '##tion', '?', 'hey', 'guys', ',', 'long', 'time', 'red', '##dit', '##or', 'on', 'a', 'throw', '##away', 'here', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'since', 'my', 'teens', 'but', 'only', 'took', 'medication', 'when', 'i', 'was', 'at', 'university', '.', 'now', 'that', 'i', 'am', 'out', 'of', 'un', '##i', 'i', 'have', 'not', 'been', 'taking', 'any', 'med', '##s', 'for', 'close', 'to', 'a', 'year', '.', 'i', 'have', 'always', 'struggled', 'with', 'premature', 'e', '##ja', '##cula', '##tion', 'and', 'i', 'have', 'wondered', 'if', 'this', 'has', 'something', 'to', 'do', 'with', 'my', 'ad', '##hd', '.', 'in', 'the', 'middle', 'of', 'having', 'sex', 'sometimes', 'i', 'sort', 'of', 'get', 'too', 'focused', 'on', 'the', 'orgasm', 'that', 'i', 'cannot', 'pace', 'myself', 'for', 'the', 'benefit', 'of', 'my', 'partner', 'and', 'i', 'orgasm', 'in', 'less', 'than', '1', '-', '2', 'minutes', '.', 'other', 'times', ',', 'when', 'i', 'try', 'to', 'calm', 'down', 'during', 'sex', 'and', 'try', 'to', 'think', 'of', 'other', 'things', 'my', 'mind', 'wander', '##s', 'a', 'bit', 'too', 'much', 'and', 'after', 'a', 'while', 'i', 'just', 'lose', 'my', 'erection', '.', 'i', 'am', 'having', 'trouble', 'trying', 'to', 'find', 'a', 'middle', 'ground', 'of', 'what', 'techniques', 'to', 'use', 'and', 'i', 'am', 'wondering', 'if', 'this', 'happens', 'to', 'you', 'guys', 'too', ',', 'and', 'if', 'this', 'is', 'something', 'more', 'common', 'in', 'men', 'with', 'ad', '##hd', '.', 'thanks']\n",
      "INFO:__main__:Number of tokens: 204\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'premature', 'e', '##ja', '##cula', '##tion', '?', 'hey', 'guys', ',', 'long', 'time', 'red', '##dit', '##or', 'on', 'a', 'throw', '##away', 'here', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'since', 'my', 'teens', 'but', 'only', 'took', 'medication', 'when', 'i', 'was', 'at', 'university', '.', 'now', 'that', 'i', 'am', 'out', 'of', 'un', '##i', 'i', 'have', 'not', 'been', 'taking', 'any', 'med', '##s', 'for', 'close', 'to', 'a', 'year', '.', 'i', 'have', 'always', 'struggled', 'with', 'premature', 'e', '##ja', '##cula', '##tion', 'and', 'i', 'have', 'wondered', 'if', 'this', 'has', 'something', 'to', 'do', 'with', 'my', 'ad', '##hd', '.', 'in', 'the', 'middle', 'of', 'having', 'sex', 'sometimes', 'i', 'sort', 'of', 'get', 'too', 'focused', 'on', 'the', 'orgasm', 'that', 'i', 'cannot', 'pace', 'myself', 'for', 'the', 'benefit', 'of', 'my', 'partner', 'and', 'i', 'orgasm', 'in', 'less', 'than', '1', '-', '2', 'minutes', '.', 'other', 'times', ',', 'when', 'i', 'try', 'to', 'calm', 'down', 'during', 'sex', 'and', 'try', 'to', 'think', 'of', 'other', 'things', 'my', 'mind', 'wander', '##s', 'a', 'bit', 'too', 'much', 'and', 'after', 'a', 'while', 'i', 'just', 'lose', 'my', 'erection', '.', 'i', 'am', 'having', 'trouble', 'trying', 'to', 'find', 'a', 'middle', 'ground', 'of', 'what', 'techniques', 'to', 'use', 'and', 'i', 'am', 'wondering', 'if', 'this', 'happens', 'to', 'you', 'guys', 'too', ',', 'and', 'if', 'this', 'is', 'something', 'more', 'common', 'in', 'men', 'with', 'ad', '##hd', '.', 'thanks']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['boring', 'college', 'work', 'and', 'other', 'attention', 'issues', ',', 'do', 'i', 'have', 'add', '?', 'hi', 'all', ',', 'wondering', 'if', 'you', 'could', 'help', 'me', 'out', '.', 'not', 'sure', 'if', 'i', 'really', 'have', 'add', ',', 'but', 'definitely', 'have', 'some', 'attention', 'problems', '.', 'a', 'few', 'characteristics', 'of', 'mine', ':', 'i', \"'\", 'm', 'quite', 'impatient', 'and', 'a', 'great', 'pro', '##cr', '##ast', '##inator', ',', 'i', 'leave', 'everything', 'non', '-', 'interesting', 'to', 'the', 'last', 'minute', ',', 'i', \"'\", 'm', 'in', 'college', '.', 'i', 'have', 'huge', 'problems', 'actually', 'sticking', 'to', 'any', 'goals', 'i', 'set', '.', 'i', 'often', 'have', 'to', 'read', 'sentences', 'over', 'and', 'over', 'before', 'i', 'actually', 'process', 'what', 'they', 'mean', '.', 'during', 'school', 'i', 'was', 'constantly', 'bored', 'and', 'told', 'by', 'teachers', 'to', 'focus', 'and', 'stop', 'talking', 'to', 'other', 'kids', '.', 'that', 'said', ',', 'i', 'wasn', \"'\", 't', 'really', 'a', 'hyper', 'bounce', '-', 'off', '-', 'the', '-', 'walls', 'kid', '.', 'i', 'forgot', 'my', 'dentist', 'appointment', '2', 'weeks', 'ago', 'when', 'i', 'had', 'a', 'reminder', 'note', 'literally', 'right', 'in', 'front', 'of', 'me', 'at', 'the', 'time', 'of', 'the', 'appointment', '.', 'i', 'have', 'a', 'paying', 'side', 'project', 'on', 'now', 'that', '##s', '99', '%', 'done', 'but', 'i', 'can', '##t', 'bring', 'myself', 'to', 'finish', 'it', 'off', ',', 'im', 'really', 'perfection', '##ist', 'about', 'things', '.', 'i', 'get', 'restless', 'and', 'fi', '##dget', '##y', 'when', 'doing', 'boring', 'work', ',', 'ill', 'shake', 'my', 'legs', 'or', 'look', 'out', 'the', 'window', 'or', 'brows', '##e', 'a', 'website', 'or', 'if', 'im', 'bored', 'socially', 'ill', 'fi', '##dget', 'with', 'a', 'table', 'object', 'or', 'clock', 'watch', '.', 'i', 'have', 'several', 'interesting', 'web', '##page', 'tab', '##s', 'open', 'from', 'weeks', 'ago', 'that', 'i', 'put', 'aside', 'to', 'read', 'another', 'time', 'but', 'still', 'haven', '##t', 'done', 'so', '.', 'very', 'rare', 'are', 'the', 'days', 'where', 'i', \"'\", 'll', 'wake', 'up', 'and', 'just', 'work', 'for', 'a', 'solid', 'amount', 'of', 'hours', ',', 'i', 'wish', 'i', 'could', 'go', 'into', 'that', 'working', 'drone', 'mode', 'more', 'often', 'where', 'i', 'just', 'sit', 'down', 'and', 'work', '.', 'very', 'often', 'i', \"'\", 'll', 'be', 'having', 'a', 'conversation', 'with', 'someone', 'while', 'entirely', 'not', 'processing', 'what', 'they', \"'\", 're', 'saying', 'at', 'all', '.', 'like', 'i', 'can', \"'\", 't', 'process', 'it', 'fast', 'enough', 'and', 'it', 'snow', '##balls', '.', 'i', 'find', 'some', 'movies', 'and', 'shows', 'difficult', 'to', 'process', ',', 'they', \"'\", 're', 'too', 'fast', ',', 'too', 'much', 'info', 'and', 'conversation', 'too', 'quickly', 'jumping', 'from', 'one', 'thing', 'to', 'the', 'next', '.', 'the', 'boring', 'work', 'is', 'the', 'most', 'trouble', 'at', 'the', 'moment', '.', 'i', 'have', 'a', 'large', 'body', 'of', 'repetitive', 'sum', '##mar', '##ising', 'of', 'ted', '##ious', 'content', 'to', 'do', 'with', 'a', 'deadline', 'before', 'exams', ',', 'i', 'can', '##t', 'bring', 'myself', 'to', 'do', 'it', '.', 'i', 'keep', 'reading', 'for', '5', 'minutes', 'then', 'get', 'bored', ',', 'annoyed', 'and', 'hastily', 'read', 'and', 'end', 'up', 'not', 'processing', 'it', '.', 'then', 'i', 'do', 'something', 'else', 'like', 'brows', '##e', 'a', 'website', ',', 'only', 'to', 'tell', 'myself', 'to', 'get', 'back', 'to', 'work', ',', 'and', 'the', 'cycle', 'continues', '.', 'most', 'days', 'i', 'wake', 'up', 'and', 'quickly', 'realize', 'in', 'a', 'few', 'minutes', 'that', '\"', 'today', 'is', 'not', 'going', 'to', 'be', 'a', 'working', 'day', '\"', 'so', 'i', 'don', \"'\", 't', 'even', 'bother', 'trying', 'to', 'work', '.', 'i', \"'\", 'm', 'not', 'sure', 'if', 'i', 'have', 'full', 'add', ',', 'i', 'mean', 'i', 'can', 'work', ',', 'its', 'just', 'super', 'hard', 'to', 'focus', 'when', 'i', 'know', 'its', 'so', 'boring', '.', 'i', 'don', \"'\", 't', 'think', 'that', 'just', '##ifies', 'an', 'add', 'diagnosis', ',', 'i', 'mean', 'everyone', 'finds', 'boring', 'work', 'boring', ',', 'i', 'just', 'find', 'it', 'extra', 'hard', '.', 'what', 'do', 'you', 'think', 'about', 'going', 'on', 'a', 'drug', 'of', 'sorts', 'for', 'a', 'month', 'or', 'two', 'to', 'finish', 'this', 'work', 'off', '?', '(', 'these', 'are', 'my', 'final', 'exams', ')', '.', 'and', 'what', 'would', 'you', 'recommend', '?', 'i', 'will', 'see', 'a', 'doctor', 'of', 'course', '.', 'side', 'note', ':', 'i', \"'\", 'm', 'also', 'taking', 'an', 'ssr', '##i', 'for', 'minor', 'anxiety', 'and', 'mood', 'issues', '.', 'i', 'thought', 'my', 'anxiety', 'was', 'the', 'root', 'of', 'these', 'concentration', 'problems', 'but', 'now', 'that', 'i', \"'\", 've', 'sorted', 'my', 'anxiety', 'out', 'and', 'i', \"'\", 'm', 'still', 'having', 'trouble', ',', 'i', \"'\", 'm', 'looking', 'to', 'other', 'reasons', '.', 'thanks', 'a', 'lot', '.']\n",
      "INFO:__main__:Number of tokens: 662\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['boring', 'college', 'work', 'and', 'other', 'attention', 'issues', ',', 'do', 'i', 'have', 'add', '?', 'hi', 'all', ',', 'wondering', 'if', 'you', 'could', 'help', 'me', 'out', '.', 'not', 'sure', 'if', 'i', 'really', 'have', 'add', ',', 'but', 'definitely', 'have', 'some', 'attention', 'problems', '.', 'a', 'few', 'characteristics', 'of', 'mine', ':', 'i', \"'\", 'm', 'quite', 'impatient', 'and', 'a', 'great', 'pro', '##cr', '##ast', '##inator', ',', 'i', 'leave', 'everything', 'non', '-', 'interesting', 'to', 'the', 'last', 'minute', ',', 'i', \"'\", 'm', 'in', 'college', '.', 'i', 'have', 'huge', 'problems', 'actually', 'sticking', 'to', 'any', 'goals', 'i', 'set', '.', 'i', 'often', 'have', 'to', 'read', 'sentences', 'over', 'and', 'over', 'before', 'i', 'actually', 'process', 'what', 'they', 'mean', '.', 'during', 'school', 'i', 'was', 'constantly', 'bored', 'and', 'told', 'by', 'teachers', 'to', 'focus', 'and', 'stop', 'talking', 'to', 'other', 'kids', '.', 'that', 'said', ',', 'i', 'wasn', \"'\", 't', 'really', 'a', 'hyper', 'bounce', '-', 'off', '-', 'the', '-', 'walls', 'kid', '.', 'i', 'forgot', 'my', 'dentist', 'appointment', '2', 'weeks', 'ago', 'when', 'i', 'had', 'a', 'reminder', 'note', 'literally', 'right', 'in', 'front', 'of', 'me', 'at', 'the', 'time', 'of', 'the', 'appointment', '.', 'i', 'have', 'a', 'paying', 'side', 'project', 'on', 'now', 'that', '##s', '99', '%', 'done', 'but', 'i', 'can', '##t', 'bring', 'myself', 'to', 'finish', 'it', 'off', ',', 'im', 'really', 'perfection', '##ist', 'about', 'things', '.', 'i', 'get', 'restless', 'and', 'fi', '##dget', '##y', 'when', 'doing', 'boring', 'work', ',', 'ill', 'shake', 'my', 'legs', 'or', 'look', 'out', 'the', 'window', 'or', 'brows', '##e', 'a', 'website', 'or', 'if', 'im', 'bored', 'socially', 'ill', 'fi', '##dget', 'with', 'a', 'table', 'object', 'or', 'clock', 'watch', '.', 'i', 'have', 'several', 'interesting', 'web', '##page', 'tab', '##s', 'open', 'from', 'weeks', 'ago', 'that', 'i', 'put', 'aside', 'to', 'read', 'another', 'time', 'but', 'still', 'haven', '##t', 'done', 'so', '.', 'very', 'rare', 'are', 'the', 'days', 'where', 'i', \"'\", 'll', 'wake', 'up', 'and', 'just', 'work', 'for', 'a', 'solid', 'amount', 'of', 'hours', ',', 'i', 'wish', 'i', 'could', 'go', 'into', 'that', 'working', 'drone', 'mode', 'more', 'often', 'where', 'i', 'just', 'sit', 'down', 'and', 'work', '.', 'very', 'often', 'i', \"'\", 'll', 'be', 'having', 'a', 'conversation', 'with', 'someone', 'while', 'entirely', 'not', 'processing', 'what', 'they', \"'\", 're', 'saying', 'at', 'all', '.', 'like', 'i', 'can', \"'\", 't', 'process', 'it', 'fast', 'enough', 'and', 'it', 'snow', '##balls', '.', 'i', 'find', 'some', 'movies', 'and', 'shows', 'difficult', 'to', 'process', ',', 'they', \"'\", 're', 'too', 'fast', ',', 'too', 'much', 'info', 'and', 'conversation', 'too', 'quickly', 'jumping', 'from', 'one', 'thing', 'to', 'the', 'next', '.', 'the', 'boring', 'work', 'is', 'the', 'most', 'trouble', 'at', 'the', 'moment', '.', 'i', 'have', 'a', 'large', 'body', 'of', 'repetitive', 'sum', '##mar', '##ising', 'of', 'ted', '##ious', 'content', 'to', 'do', 'with', 'a', 'deadline', 'before', 'exams', ',', 'i', 'can', '##t', 'bring', 'myself', 'to', 'do', 'it', '.', 'i', 'keep', 'reading', 'for', '5', 'minutes', 'then', 'get', 'bored', ',', 'annoyed', 'and', 'hastily', 'read', 'and', 'end', 'up', 'not', 'processing', 'it', '.', 'then', 'i', 'do', 'something', 'else', 'like', 'brows', '##e', 'a', 'website', ',', 'only', 'to', 'tell', 'myself', 'to', 'get', 'back', 'to', 'work', ',', 'and', 'the', 'cycle', 'continues', '.', 'most', 'days', 'i', 'wake', 'up', 'and', 'quickly', 'realize', 'in', 'a', 'few', 'minutes', 'that', '\"', 'today', 'is', 'not', 'going', 'to', 'be', 'a', 'working', 'day', '\"', 'so', 'i', 'don', \"'\", 't', 'even', 'bother', 'trying', 'to', 'work', '.', 'i', \"'\", 'm', 'not', 'sure', 'if', 'i', 'have', 'full', 'add'], [',', 'i', 'mean', 'i', 'can', 'work', ',', 'its', 'just', 'super', 'hard', 'to', 'focus', 'when', 'i', 'know', 'its', 'so', 'boring', '.', 'i', 'don', \"'\", 't', 'think', 'that', 'just', '##ifies', 'an', 'add', 'diagnosis', ',', 'i', 'mean', 'everyone', 'finds', 'boring', 'work', 'boring', ',', 'i', 'just', 'find', 'it', 'extra', 'hard', '.', 'what', 'do', 'you', 'think', 'about', 'going', 'on', 'a', 'drug', 'of', 'sorts', 'for', 'a', 'month', 'or', 'two', 'to', 'finish', 'this', 'work', 'off', '?', '(', 'these', 'are', 'my', 'final', 'exams', ')', '.', 'and', 'what', 'would', 'you', 'recommend', '?', 'i', 'will', 'see', 'a', 'doctor', 'of', 'course', '.', 'side', 'note', ':', 'i', \"'\", 'm', 'also', 'taking', 'an', 'ssr', '##i', 'for', 'minor', 'anxiety', 'and', 'mood', 'issues', '.', 'i', 'thought', 'my', 'anxiety', 'was', 'the', 'root', 'of', 'these', 'concentration', 'problems', 'but', 'now', 'that', 'i', \"'\", 've', 'sorted', 'my', 'anxiety', 'out', 'and', 'i', \"'\", 'm', 'still', 'having', 'trouble', ',', 'i', \"'\", 'm', 'looking', 'to', 'other', 'reasons', '.', 'thanks', 'a', 'lot', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['therapist', '##s', 'in', 'eastern', 'ma', 'i', \"'\", 've', 'been', 'seeing', 'a', 'guy', 'named', 'bros', '##to', '##ff', 'in', 'lexington', 'but', 'the', 'guy', 'is', 'ridiculous', '##ly', 'expensive', 'and', 'also', 'a', 'bit', 'odd', '.', 'i', \"'\", 've', 'got', 'insurance', 'but', 'the', 'costs', 'still', 'ir', '##k', 'me', '.', 'any', 'recommendations', 'on', 'good', 'therapist', '##s', 'in', 'the', 'mer', '##rim', '##ack', 'valley', 'area', '?', 'i', \"'\", 've', 'just', 'started', 'add', '##eral', '##l', 'and', 'want', 'to', 'learn', 'how', 'to', 'maximize', 'the', 'benefits', 'and', 'learn', 'non', '-', 'drug', 'related', 'techniques', 'for', 'improving', 'my', 'social', 'interaction', ',', 'happiness', 'and', 'productivity', '.']\n",
      "INFO:__main__:Number of tokens: 92\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['therapist', '##s', 'in', 'eastern', 'ma', 'i', \"'\", 've', 'been', 'seeing', 'a', 'guy', 'named', 'bros', '##to', '##ff', 'in', 'lexington', 'but', 'the', 'guy', 'is', 'ridiculous', '##ly', 'expensive', 'and', 'also', 'a', 'bit', 'odd', '.', 'i', \"'\", 've', 'got', 'insurance', 'but', 'the', 'costs', 'still', 'ir', '##k', 'me', '.', 'any', 'recommendations', 'on', 'good', 'therapist', '##s', 'in', 'the', 'mer', '##rim', '##ack', 'valley', 'area', '?', 'i', \"'\", 've', 'just', 'started', 'add', '##eral', '##l', 'and', 'want', 'to', 'learn', 'how', 'to', 'maximize', 'the', 'benefits', 'and', 'learn', 'non', '-', 'drug', 'related', 'techniques', 'for', 'improving', 'my', 'social', 'interaction', ',', 'happiness', 'and', 'productivity', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'expected', 'that', 'sudden', 'drop', 'in', 'productivity', 'during', 'this', 'period', 'where', 'i', \"'\", 'm', 'off', 'add', '##eral', '##l', 'during', 'the', 'shortage', ',', 'but', 'it', 'wasn', \"'\", 't', 'as', 'bad', 'as', 'i', 'thought', '.', 'i', \"'\", 'm', 'currently', 'off', 'med', '##s', 'during', 'the', 'shortage', '.', 'i', \"'\", 've', 'noticed', 'that', 'so', 'far', ',', 'in', 'the', 'first', 'week', 'at', 'least', ',', 'that', 'my', 'symptoms', 'aren', \"'\", 't', 'as', 'severe', 'as', 'they', 'were', 'before', 'ever', 'taking', 'med', '##s', '.', 'still', 'there', ',', 'of', 'course', '.', '.', '.', 'just', 'a', 'little', 'better', '.', 'almost', 'like', 'when', 'i', 'was', 'on', 'the', 'medication', 'i', 'was', 'able', 'to', 'learn', ',', 'train', 'myself', 'and', 'better', 'adapt', 'on', 'how', 'to', 'do', 'things', 'easier', '.', 'not', 'that', 'things', 'are', 'way', 'easier', ',', 'but', 'that', 'during', 'the', 'time', 'i', 'was', 'on', 'the', 'medication', ',', 'my', 'mind', 'was', 'allowed', 'to', 'be', 'productive', 'and', 'develop', 'some', 'coping', 'techniques', 'that', 'i', 'wouldn', \"'\", 't', 'have', 'had', 'the', 'attention', 'for', 'before', '.', 'don', \"'\", 't', 'get', 'me', 'wrong', ',', 'medication', 'is', 'loads', 'better', 'and', 'the', 'shortage', 'sucks', ',', 'but', 'i', 'feel', 'better', 'than', 'before', 'i', 'was', 'ever', 'on', 'the', 'med', '##s', 'and', 'i', 'think', 'it', \"'\", 's', 'because', 'the', 'med', '##s', 'allowed', 'for', 'me', 'to', 'develop', 'new', 'methods', 'of', 'coping', '.', 'does', 'that', 'make', 'sense', '?', 'wondering', 'if', 'anyone', 'else', 'felt', 'that', 'way', '.']\n",
      "INFO:__main__:Number of tokens: 219\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'expected', 'that', 'sudden', 'drop', 'in', 'productivity', 'during', 'this', 'period', 'where', 'i', \"'\", 'm', 'off', 'add', '##eral', '##l', 'during', 'the', 'shortage', ',', 'but', 'it', 'wasn', \"'\", 't', 'as', 'bad', 'as', 'i', 'thought', '.', 'i', \"'\", 'm', 'currently', 'off', 'med', '##s', 'during', 'the', 'shortage', '.', 'i', \"'\", 've', 'noticed', 'that', 'so', 'far', ',', 'in', 'the', 'first', 'week', 'at', 'least', ',', 'that', 'my', 'symptoms', 'aren', \"'\", 't', 'as', 'severe', 'as', 'they', 'were', 'before', 'ever', 'taking', 'med', '##s', '.', 'still', 'there', ',', 'of', 'course', '.', '.', '.', 'just', 'a', 'little', 'better', '.', 'almost', 'like', 'when', 'i', 'was', 'on', 'the', 'medication', 'i', 'was', 'able', 'to', 'learn', ',', 'train', 'myself', 'and', 'better', 'adapt', 'on', 'how', 'to', 'do', 'things', 'easier', '.', 'not', 'that', 'things', 'are', 'way', 'easier', ',', 'but', 'that', 'during', 'the', 'time', 'i', 'was', 'on', 'the', 'medication', ',', 'my', 'mind', 'was', 'allowed', 'to', 'be', 'productive', 'and', 'develop', 'some', 'coping', 'techniques', 'that', 'i', 'wouldn', \"'\", 't', 'have', 'had', 'the', 'attention', 'for', 'before', '.', 'don', \"'\", 't', 'get', 'me', 'wrong', ',', 'medication', 'is', 'loads', 'better', 'and', 'the', 'shortage', 'sucks', ',', 'but', 'i', 'feel', 'better', 'than', 'before', 'i', 'was', 'ever', 'on', 'the', 'med', '##s', 'and', 'i', 'think', 'it', \"'\", 's', 'because', 'the', 'med', '##s', 'allowed', 'for', 'me', 'to', 'develop', 'new', 'methods', 'of', 'coping', '.', 'does', 'that', 'make', 'sense', '?', 'wondering', 'if', 'anyone', 'else', 'felt', 'that', 'way', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['depression', 'and', 'ad', '##hd', 'med', '##s', '?', 'hi', 'all', ',', 'i', 'just', 'got', 'diagnosed', 'two', 'weeks', 'ago', 'with', 'ad', '##hd', 'and', 'was', 'started', 'on', 'add', '##eral', '##l', ',', 'and', 'yes', '##ted', '##ay', 'i', 'was', 'diagnosed', 'with', 'a', 'general', 'mood', 'disorder', 'and', 'was', 'put', 'on', 'z', '##olo', '##ft', '.', 'has', 'anyone', 'experienced', 'the', 'two', 'med', '##s', 'at', 'the', 'same', 'time', '?', 'i', 'want', 'some', 'perspective', 'on', 'what', 'to', 'expect', '.', 'during', 'the', 'two', 'weeks', 'of', 'ad', '##hd', 'med', '##s', 'alone', 'i', 'was', 'very', 'focused', 'but', 'had', 'no', 'motivation', 'at', 'all', '(', 'an', '##hed', '##onia', ')', ',', 'and', 'now', 'that', 'i', \"'\", 'm', 'on', 'z', '##olo', '##ft', 'hopefully', 'i', \"'\", 'll', 'be', 'motivated', 'and', 'focused', 'at', 'the', 'same', 'time', '.']\n",
      "INFO:__main__:Number of tokens: 118\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['depression', 'and', 'ad', '##hd', 'med', '##s', '?', 'hi', 'all', ',', 'i', 'just', 'got', 'diagnosed', 'two', 'weeks', 'ago', 'with', 'ad', '##hd', 'and', 'was', 'started', 'on', 'add', '##eral', '##l', ',', 'and', 'yes', '##ted', '##ay', 'i', 'was', 'diagnosed', 'with', 'a', 'general', 'mood', 'disorder', 'and', 'was', 'put', 'on', 'z', '##olo', '##ft', '.', 'has', 'anyone', 'experienced', 'the', 'two', 'med', '##s', 'at', 'the', 'same', 'time', '?', 'i', 'want', 'some', 'perspective', 'on', 'what', 'to', 'expect', '.', 'during', 'the', 'two', 'weeks', 'of', 'ad', '##hd', 'med', '##s', 'alone', 'i', 'was', 'very', 'focused', 'but', 'had', 'no', 'motivation', 'at', 'all', '(', 'an', '##hed', '##onia', ')', ',', 'and', 'now', 'that', 'i', \"'\", 'm', 'on', 'z', '##olo', '##ft', 'hopefully', 'i', \"'\", 'll', 'be', 'motivated', 'and', 'focused', 'at', 'the', 'same', 'time', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'your', 'med', '##s', 'do', '(', 'to', 'you', ')', '?', 'i', 'have', 'add', '##eral', '##l', 'and', 'i', 'know', 'what', 'effects', 'it', 'has', 'on', 'me', '.', 'it', 'makes', 'me', 'not', 'tired', ',', 'more', 'active', 'in', 'o', '##cd', 'activities', 'and', 'after', 'taking', 'it', 'for', '3', 'days', 'or', 'more', ',', 'all', 'my', 'emotions', 'are', 'dead', ',', 'i', 'can', 'only', 'have', 'two', 'feelings', '(', 'add', '##eral', '##l', 'and', 'no', 'add', '##eral', '##l', ')', '.', 'i', 'also', 'get', 'hungry', 'for', 'sugar', '##y', 'foods', 'and', 'want', 'to', 'jerk', 'off', '.', 'what', 'do', 'your', 'med', '##s', 'do', 'to', 'you', '?', 'i', 'think', 'it', 'gives', 'me', 'a', 'different', 'personality', 'as', 'well', ',', 'i', \"'\", 'm', 'much', 'more', 'curt', 'and', 'more', 'rushed', 'to', 'do', 'things', ',', 'although', 'i', 'can', 'spend', 'a', 'lot', 'of', 'time', 'pro', '##cr', '##ast', '##inating', '.']\n",
      "INFO:__main__:Number of tokens: 131\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'your', 'med', '##s', 'do', '(', 'to', 'you', ')', '?', 'i', 'have', 'add', '##eral', '##l', 'and', 'i', 'know', 'what', 'effects', 'it', 'has', 'on', 'me', '.', 'it', 'makes', 'me', 'not', 'tired', ',', 'more', 'active', 'in', 'o', '##cd', 'activities', 'and', 'after', 'taking', 'it', 'for', '3', 'days', 'or', 'more', ',', 'all', 'my', 'emotions', 'are', 'dead', ',', 'i', 'can', 'only', 'have', 'two', 'feelings', '(', 'add', '##eral', '##l', 'and', 'no', 'add', '##eral', '##l', ')', '.', 'i', 'also', 'get', 'hungry', 'for', 'sugar', '##y', 'foods', 'and', 'want', 'to', 'jerk', 'off', '.', 'what', 'do', 'your', 'med', '##s', 'do', 'to', 'you', '?', 'i', 'think', 'it', 'gives', 'me', 'a', 'different', 'personality', 'as', 'well', ',', 'i', \"'\", 'm', 'much', 'more', 'curt', 'and', 'more', 'rushed', 'to', 'do', 'things', ',', 'although', 'i', 'can', 'spend', 'a', 'lot', 'of', 'time', 'pro', '##cr', '##ast', '##inating', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['well', 'that', \"'\", 's', 'de', '##pressing', ',', 'thanks', 'google', '.', '[', 'warning', 'rage', 'inducing', ']']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['well', 'that', \"'\", 's', 'de', '##pressing', ',', 'thanks', 'google', '.', '[', 'warning', 'rage', 'inducing', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['simply', '##no', '##ise', '.', 'com', 'color', 'noise', 'generator', '.']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['simply', '##no', '##ise', '.', 'com', 'color', 'noise', 'generator', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['simply', '##no', '##ise', 'white', 'noise', 'generator']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['simply', '##no', '##ise', 'white', 'noise', 'generator']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', ',', 'a', 'diagnosis']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', ',', 'a', 'diagnosis']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['slug', '##gis', '##h', 'cognitive', 'tempo', 'anybody', 'else', 'have', 'this', '?', 'a', 'sub', '##pop', '##ulation', 'of', 'the', 'ad', '##hd', '-', 'i', 'population', '.']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['slug', '##gis', '##h', 'cognitive', 'tempo', 'anybody', 'else', 'have', 'this', '?', 'a', 'sub', '##pop', '##ulation', 'of', 'the', 'ad', '##hd', '-', 'i', 'population', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['feeling', 'frustrated', 'and', 'disappointed', 'with', 'initial', 'psychiatrist', 'session', '.', 'now', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '.']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['feeling', 'frustrated', 'and', 'disappointed', 'with', 'initial', 'psychiatrist', 'session', '.', 'now', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'taking', 'pro', '##vi', '##gil', '/', 'nu', '##vi', '##gil', 'for', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'taking', 'pro', '##vi', '##gil', '/', 'nu', '##vi', '##gil', 'for', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'it', 'weird', 'to', 'sometimes', 'wish', 'i', 'had', 'a', 'broken', 'leg', 'or', 'something', 'visible', 'wrong', 'with', 'me', '?', 'really', ',', 'the', 'title', 'says', 'it', 'all', '.', 'i', 'am', 'a', '21', '-', 'year', 'old', 'suffering', 'from', 'severe', 'ad', '##hd', ',', 'and', 'sometimes', 'i', 'just', 'wish', 'i', 'had', 'a', 'broken', 'leg', 'or', 'something', ',', 'so', 'that', ',', 'figurative', '##ly', 'speaking', ',', 'people', 'on', 'the', 'bus', 'of', 'life', 'would', 'stand', 'up', 'for', 'me', 'and', 'know', 'that', 'there', 'is', 'actually', 'something', 'wrong', 'with', 'me', '.', 'rather', 'than', 'having', 'this', 'invisible', ',', 'im', '##me', '##asurable', 'thing', 'hanging', 'around', 'me', ',', 'without', 'any', 'way', 'of', 'proving', 'it', 'is', 'real', '.', 'am', 'i', 'the', 'only', 'one', 'feeling', 'like', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 113\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'it', 'weird', 'to', 'sometimes', 'wish', 'i', 'had', 'a', 'broken', 'leg', 'or', 'something', 'visible', 'wrong', 'with', 'me', '?', 'really', ',', 'the', 'title', 'says', 'it', 'all', '.', 'i', 'am', 'a', '21', '-', 'year', 'old', 'suffering', 'from', 'severe', 'ad', '##hd', ',', 'and', 'sometimes', 'i', 'just', 'wish', 'i', 'had', 'a', 'broken', 'leg', 'or', 'something', ',', 'so', 'that', ',', 'figurative', '##ly', 'speaking', ',', 'people', 'on', 'the', 'bus', 'of', 'life', 'would', 'stand', 'up', 'for', 'me', 'and', 'know', 'that', 'there', 'is', 'actually', 'something', 'wrong', 'with', 'me', '.', 'rather', 'than', 'having', 'this', 'invisible', ',', 'im', '##me', '##asurable', 'thing', 'hanging', 'around', 'me', ',', 'without', 'any', 'way', 'of', 'proving', 'it', 'is', 'real', '.', 'am', 'i', 'the', 'only', 'one', 'feeling', 'like', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'find', 'themselves', 'completely', 'eng', '##ross', '##ed', 'in', 'acquiring', 'knowledge', ',', 'sometimes', 'for', 'hours', 'at', 'a', 'time', ',', 'but', 'then', ',', 'after', ',', 'feel', 'like', 'you', 'didn', \"'\", 't', 'retain', 'any', 'of', 'the', 'details', 'others', 'find', 'important', '(', 'names', ',', 'dates', ',', 'places', ')', 'and', 'so', 'your', 'study', 'is', 'essentially', 'worthless', '?']\n",
      "INFO:__main__:Number of tokens: 53\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'find', 'themselves', 'completely', 'eng', '##ross', '##ed', 'in', 'acquiring', 'knowledge', ',', 'sometimes', 'for', 'hours', 'at', 'a', 'time', ',', 'but', 'then', ',', 'after', ',', 'feel', 'like', 'you', 'didn', \"'\", 't', 'retain', 'any', 'of', 'the', 'details', 'others', 'find', 'important', '(', 'names', ',', 'dates', ',', 'places', ')', 'and', 'so', 'your', 'study', 'is', 'essentially', 'worthless', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'describe', 'having', 'ad', '##hd', '?', 'my', 'dad', ',', 'for', 'the', 'longest', 'time', 'didn', \"'\", 't', 'accept', 'that', 'ad', '##hd', 'was', 'a', 'real', 'thing', 'until', 'i', 'moved', 'out', 'and', 'we', 'didn', \"'\", 't', 'talk', 'for', 'awhile', '.', 'i', 'started', 'on', 'st', '##rat', '##tera', 'and', 'then', 'we', 'reunited', 'and', 'he', 'wanted', 'to', 'know', 'what', 'the', 'med', '##s', 'did', ',', 'so', 'i', 'said', 'it', 'was', 'like', 'they', ',', 'in', 'effect', ',', 'removed', 'a', 'perpetual', 'writers', 'block', 'in', 'my', 'mind', ',', 'but', 'it', 'wasn', '##t', 'that', 'i', 'couldn', \"'\", 't', 'come', 'up', 'with', 'ideas', ',', 'just', 'that', 'i', 'have', 'a', 'hard', 'time', 'focusing', 'on', 'them']\n",
      "INFO:__main__:Number of tokens: 104\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'describe', 'having', 'ad', '##hd', '?', 'my', 'dad', ',', 'for', 'the', 'longest', 'time', 'didn', \"'\", 't', 'accept', 'that', 'ad', '##hd', 'was', 'a', 'real', 'thing', 'until', 'i', 'moved', 'out', 'and', 'we', 'didn', \"'\", 't', 'talk', 'for', 'awhile', '.', 'i', 'started', 'on', 'st', '##rat', '##tera', 'and', 'then', 'we', 'reunited', 'and', 'he', 'wanted', 'to', 'know', 'what', 'the', 'med', '##s', 'did', ',', 'so', 'i', 'said', 'it', 'was', 'like', 'they', ',', 'in', 'effect', ',', 'removed', 'a', 'perpetual', 'writers', 'block', 'in', 'my', 'mind', ',', 'but', 'it', 'wasn', '##t', 'that', 'i', 'couldn', \"'\", 't', 'come', 'up', 'with', 'ideas', ',', 'just', 'that', 'i', 'have', 'a', 'hard', 'time', 'focusing', 'on', 'them']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'prescribed', 'st', '##im', '##ula', '##nt', 'treatments', 'since', 'a', 'young', 'age', 'feel', 'this', 'way', '?', 'i', \"'\", 've', 'been', 'on', 'st', '##im', '##ula', '##nt', 'med', '##s', 'pretty', 'much', 'since', 'i', 'started', 'second', 'grade', '.', 'i', 'have', 'tried', 'to', 'stop', 'them', 'a', 'number', 'of', 'times', ',', 'and', 'it', 'always', 'ended', 'in', 'disaster', '(', 'evident', 'by', 'my', 'gp', '##a', 'and', 'job', 'retention', ')', '.', 'i', 'even', 'took', 'off', 'nearly', 'a', 'year', 'of', 'college', 'to', 'try', 'to', 'rule', 'out', 'im', '##med', '##itate', 'withdrawal', 'effects', 'to', 'no', 'avail', '.', 'so', 'the', 'general', 'consensus', 'seems', 'to', 'be', 'that', 'this', 'is', 'evidence', 'to', 'support', 'the', 'efficacy', 'of', 'this', 'treatment', '.', 'but', 'recently', 'i', \"'\", 've', 'been', 'starting', 'to', 'wonder', 'if', 'my', 'inability', 'to', 'work', 'without', 'them', 'is', 'actually', 'evidence', 'of', 'the', 'harm', 'they', \"'\", 've', 'done', '.', 'i', 'have', 'three', 'theories', ',', 'but', 'none', 'of', 'them', 'can', 'really', 'be', 'tested', 'easily', ':', '1', '.', 'what', 'if', 'my', 'problem', 'is', 'that', 'i', 'never', 'actually', 'learned', 'how', 'to', 'work', 'without', 'them', '?', '2', '.', '[', 'prolonged', 'use', 'of', 'st', '##im', '##ula', '##nts', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'amp', '##het', '##amine', '_', 'psycho', '##sis', '#', 'cite', '_', 'note', '-', '0', ')', 'may', 'have', 'changed', 'my', 'brain', 'chemistry', 'or', 'even', 'potent', '##iated', 'underlying', 'problems', '.', '(', 'depression', ',', 'memory', 'problems', ',', 'ap', '##athy', ',', 'addict', '##ive', '/', 'reckless', 'behavior', ')', '3', '.', 'what', 'if', 'the', 'lack', 'of', 'appetite', 'and', 'sleep', 'over', 'the', 'course', 'of', 'my', 'life', 'has', 'stunt', '##ed', 'my', 'physical', ',', 'mental', ',', 'and', 'emotional', 'development', '?', 'i', 'can', 'think', 'of', 'others', ',', 'but', 'you', 'get', 'the', 'idea', '.']\n",
      "INFO:__main__:Number of tokens: 272\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'prescribed', 'st', '##im', '##ula', '##nt', 'treatments', 'since', 'a', 'young', 'age', 'feel', 'this', 'way', '?', 'i', \"'\", 've', 'been', 'on', 'st', '##im', '##ula', '##nt', 'med', '##s', 'pretty', 'much', 'since', 'i', 'started', 'second', 'grade', '.', 'i', 'have', 'tried', 'to', 'stop', 'them', 'a', 'number', 'of', 'times', ',', 'and', 'it', 'always', 'ended', 'in', 'disaster', '(', 'evident', 'by', 'my', 'gp', '##a', 'and', 'job', 'retention', ')', '.', 'i', 'even', 'took', 'off', 'nearly', 'a', 'year', 'of', 'college', 'to', 'try', 'to', 'rule', 'out', 'im', '##med', '##itate', 'withdrawal', 'effects', 'to', 'no', 'avail', '.', 'so', 'the', 'general', 'consensus', 'seems', 'to', 'be', 'that', 'this', 'is', 'evidence', 'to', 'support', 'the', 'efficacy', 'of', 'this', 'treatment', '.', 'but', 'recently', 'i', \"'\", 've', 'been', 'starting', 'to', 'wonder', 'if', 'my', 'inability', 'to', 'work', 'without', 'them', 'is', 'actually', 'evidence', 'of', 'the', 'harm', 'they', \"'\", 've', 'done', '.', 'i', 'have', 'three', 'theories', ',', 'but', 'none', 'of', 'them', 'can', 'really', 'be', 'tested', 'easily', ':', '1', '.', 'what', 'if', 'my', 'problem', 'is', 'that', 'i', 'never', 'actually', 'learned', 'how', 'to', 'work', 'without', 'them', '?', '2', '.', '[', 'prolonged', 'use', 'of', 'st', '##im', '##ula', '##nts', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'amp', '##het', '##amine', '_', 'psycho', '##sis', '#', 'cite', '_', 'note', '-', '0', ')', 'may', 'have', 'changed', 'my', 'brain', 'chemistry', 'or', 'even', 'potent', '##iated', 'underlying', 'problems', '.', '(', 'depression', ',', 'memory', 'problems', ',', 'ap', '##athy', ',', 'addict', '##ive', '/', 'reckless', 'behavior', ')', '3', '.', 'what', 'if', 'the', 'lack', 'of', 'appetite', 'and', 'sleep', 'over', 'the', 'course', 'of', 'my', 'life', 'has', 'stunt', '##ed', 'my', 'physical', ',', 'mental', ',', 'and', 'emotional', 'development', '?', 'i', 'can', 'think', 'of', 'others', ',', 'but', 'you', 'get', 'the', 'idea', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['okay', '.', '.', '.', 'delicate', 'question', '(', 'for', 'the', 'men', 'who', 'take', 'add', '##eral', '##l', 'for', 'ad', '##hd', ')', 'problems', 'with', 'your', '\"', 'little', 'dude', '?', '\"']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['okay', '.', '.', '.', 'delicate', 'question', '(', 'for', 'the', 'men', 'who', 'take', 'add', '##eral', '##l', 'for', 'ad', '##hd', ')', 'problems', 'with', 'your', '\"', 'little', 'dude', '?', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sleep', 'advice', 'for', 'a', 'tired', 'person', 'who', 'can', \"'\", 't', 'fall', 'asleep', '?', 'hey', 'i', 'was', 'wondering', 'if', 'anyone', 'here', 'has', 'initiation', 'ins', '##om', '##nia', '.', 'i', 'can', \"'\", 't', 'seem', 'to', 'fall', 'asleep', 'until', 'around', 'an', 'hour', 'or', 'more', 'after', 'i', 'get', 'into', 'bed', '.', 'my', 'mind', 'is', 'still', 'racing', 'around', 'for', '40', 'minutes', 'and', 'i', 'have', 'no', 'idea', 'how', 'to', 'slow', 'it', 'down', 'and', 'enter', 'sleep', '.', 'worst', 'thing', 'ever', '.', 'does', 'anyone', 'else', 'struggle', 'with', 'falling', 'asleep', '?', 'and', 'if', 'so', 'does', 'anyone', 'have', 'any', 'advice', 'on', 'trying', 'to', 'fall', 'asleep', '?', '*', 'edit', '*', 'thank', 'you', 'for', 'the', 'suggestions', '!', 'i', 'started', 'reading', 'an', 'hour', 'before', 'bed', 'and', 'not', 'going', 'on', 'the', 'computer', 'an', 'hour', 'before', 'bed', '.', 'thank', 'you', 'for', 'the', 'help', 'and', 'stuff', '!']\n",
      "INFO:__main__:Number of tokens: 130\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sleep', 'advice', 'for', 'a', 'tired', 'person', 'who', 'can', \"'\", 't', 'fall', 'asleep', '?', 'hey', 'i', 'was', 'wondering', 'if', 'anyone', 'here', 'has', 'initiation', 'ins', '##om', '##nia', '.', 'i', 'can', \"'\", 't', 'seem', 'to', 'fall', 'asleep', 'until', 'around', 'an', 'hour', 'or', 'more', 'after', 'i', 'get', 'into', 'bed', '.', 'my', 'mind', 'is', 'still', 'racing', 'around', 'for', '40', 'minutes', 'and', 'i', 'have', 'no', 'idea', 'how', 'to', 'slow', 'it', 'down', 'and', 'enter', 'sleep', '.', 'worst', 'thing', 'ever', '.', 'does', 'anyone', 'else', 'struggle', 'with', 'falling', 'asleep', '?', 'and', 'if', 'so', 'does', 'anyone', 'have', 'any', 'advice', 'on', 'trying', 'to', 'fall', 'asleep', '?', '*', 'edit', '*', 'thank', 'you', 'for', 'the', 'suggestions', '!', 'i', 'started', 'reading', 'an', 'hour', 'before', 'bed', 'and', 'not', 'going', 'on', 'the', 'computer', 'an', 'hour', 'before', 'bed', '.', 'thank', 'you', 'for', 'the', 'help', 'and', 'stuff', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['general', 'question', 'to', 'r', '/', 'ad', '##hd', 'hello', ',', 'i', 'wanted', 'to', 'know', 'if', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'ha', '##a', 'its', 'own', 'fa', '##q', '?', 'if', 'not', 'i', 'think', 'it', 'would', 'be', 'a', 'great', 'idea', 'to', 'have', 'one', '.', 'personally', 'i', 'find', 'useful', 'information', 'in', 'every', 'post', 'and', 'in', 'every', 'reply', '.', 'i', 'finally', 'don', \"'\", 't', 'have', 'to', 'say', 'what', 'its', 'like', ',', 'because', 'you', 'all', 'know', 'what', 'its', 'like', '.', 'but', 'i', 'think', 'an', 'fa', '##q', 'might', 'be', 'a', 'great', 'resource', 'for', 'new', 'people', 'looking', 'for', 'information', '.', 'for', 'example', 'common', 'side', 'effects', 'to', 'drugs', '(', 'i', 'know', 'there', \"'\", 's', 'ph', '##arm', '##aco', '##logical', 'sites', 'but', 'personal', 'experiences', 'are', 'far', 'more', 'useful', 'to', 'someone', 'just', 'starting', 'their', 'research', ')', '.', 'some', 'studying', 'tips', 'for', 'people', 'in', 'school', ',', 'links', 'to', 'online', 'lectures', 'people', 'might', 'not', 'know', 'exist', ',', 'etc', '.', 'in', 'my', 'case', 'i', 'accidentally', 'discovered', 'this', 'sub', '##red', '##dit', 'and', 'in', 'my', 'initial', 'post', 'found', 'so', 'much', 'information', 'about', 'how', 'to', 'study', 'more', 'efficiently', ',', 'a', 'whole', 'new', 'way', 'of', 'taking', 'notes', 'that', 'stream', '##lines', 'my', 'studying', 'habits', ',', 'its', 'awesome', '.', 'i', 'think', 'that', 'an', 'fa', '##q', 'section', 'that', 'has', 'a', 'centralized', 'location', 'for', 'the', 'most', 'common', 'question', 'that', 'we', 'all', 'asked', 'ourselves', 'when', 'we', 'all', 'started', 'might', 'be', 'a', 'great', 'resource', 'for', 'people', 'are', 'shy', 'about', 'asking', 'question', ',', 'and', 'who', 'can', \"'\", 't', 'find', 'all', 'the', 'information', 'by', 'just', 'searching', '(', 'run', 'on', 'sentence', ')', 'just', 'a', 'thought', '.']\n",
      "INFO:__main__:Number of tokens: 256\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['general', 'question', 'to', 'r', '/', 'ad', '##hd', 'hello', ',', 'i', 'wanted', 'to', 'know', 'if', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'ha', '##a', 'its', 'own', 'fa', '##q', '?', 'if', 'not', 'i', 'think', 'it', 'would', 'be', 'a', 'great', 'idea', 'to', 'have', 'one', '.', 'personally', 'i', 'find', 'useful', 'information', 'in', 'every', 'post', 'and', 'in', 'every', 'reply', '.', 'i', 'finally', 'don', \"'\", 't', 'have', 'to', 'say', 'what', 'its', 'like', ',', 'because', 'you', 'all', 'know', 'what', 'its', 'like', '.', 'but', 'i', 'think', 'an', 'fa', '##q', 'might', 'be', 'a', 'great', 'resource', 'for', 'new', 'people', 'looking', 'for', 'information', '.', 'for', 'example', 'common', 'side', 'effects', 'to', 'drugs', '(', 'i', 'know', 'there', \"'\", 's', 'ph', '##arm', '##aco', '##logical', 'sites', 'but', 'personal', 'experiences', 'are', 'far', 'more', 'useful', 'to', 'someone', 'just', 'starting', 'their', 'research', ')', '.', 'some', 'studying', 'tips', 'for', 'people', 'in', 'school', ',', 'links', 'to', 'online', 'lectures', 'people', 'might', 'not', 'know', 'exist', ',', 'etc', '.', 'in', 'my', 'case', 'i', 'accidentally', 'discovered', 'this', 'sub', '##red', '##dit', 'and', 'in', 'my', 'initial', 'post', 'found', 'so', 'much', 'information', 'about', 'how', 'to', 'study', 'more', 'efficiently', ',', 'a', 'whole', 'new', 'way', 'of', 'taking', 'notes', 'that', 'stream', '##lines', 'my', 'studying', 'habits', ',', 'its', 'awesome', '.', 'i', 'think', 'that', 'an', 'fa', '##q', 'section', 'that', 'has', 'a', 'centralized', 'location', 'for', 'the', 'most', 'common', 'question', 'that', 'we', 'all', 'asked', 'ourselves', 'when', 'we', 'all', 'started', 'might', 'be', 'a', 'great', 'resource', 'for', 'people', 'are', 'shy', 'about', 'asking', 'question', ',', 'and', 'who', 'can', \"'\", 't', 'find', 'all', 'the', 'information', 'by', 'just', 'searching', '(', 'run', 'on', 'sentence', ')', 'just', 'a', 'thought', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'kinds', 'of', 'jobs', '/', 'careers', 'have', 'all', 'you', 'ad', '##hd', '##ers', 'become', 'involved', 'in', '?', 'if', 'you', \"'\", 're', 'still', 'in', 'school', 'for', 'a', 'degree', ',', 'what', 'are', 'you', 'pursuing', '?', 'just', 'out', 'of', 'curiosity', ':', ')']\n",
      "INFO:__main__:Number of tokens: 38\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'kinds', 'of', 'jobs', '/', 'careers', 'have', 'all', 'you', 'ad', '##hd', '##ers', 'become', 'involved', 'in', '?', 'if', 'you', \"'\", 're', 'still', 'in', 'school', 'for', 'a', 'degree', ',', 'what', 'are', 'you', 'pursuing', '?', 'just', 'out', 'of', 'curiosity', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['symptoms', 'of', 'ad', '##hd', 'can', 'someone', 'tell', 'me', 'what', 'the', 'symptoms', 'of', 'ad', '##hd', 'are', ',', 'i', 'do', 'not', 'know', 'if', 'i', 'have', 'it', 'or', 'not', '.', 'when', 'writing', 'exams', 'i', 'often', 'get', 'distracted', 'by', 'even', 'the', 'smallest', 'sound', 'like', 'the', 'turn', 'of', 'a', 'page', ',', 'and', 'then', 'i', 'can', 'not', 'concentrate', ',', 'when', 'i', 'am', 'writing', 'an', 'essay', 'for', 'example', ',', 'i', 'can', 'sit', 'in', 'front', 'of', 'my', 'computer', 'for', 'hours', 'and', 'not', 'even', 'write', 'down', 'one', 'word', 'because', 'i', 'can', 'not', 'concentrate', 'and', 'other', 'things', 'catch', 'my', 'attention']\n",
      "INFO:__main__:Number of tokens: 90\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['symptoms', 'of', 'ad', '##hd', 'can', 'someone', 'tell', 'me', 'what', 'the', 'symptoms', 'of', 'ad', '##hd', 'are', ',', 'i', 'do', 'not', 'know', 'if', 'i', 'have', 'it', 'or', 'not', '.', 'when', 'writing', 'exams', 'i', 'often', 'get', 'distracted', 'by', 'even', 'the', 'smallest', 'sound', 'like', 'the', 'turn', 'of', 'a', 'page', ',', 'and', 'then', 'i', 'can', 'not', 'concentrate', ',', 'when', 'i', 'am', 'writing', 'an', 'essay', 'for', 'example', ',', 'i', 'can', 'sit', 'in', 'front', 'of', 'my', 'computer', 'for', 'hours', 'and', 'not', 'even', 'write', 'down', 'one', 'word', 'because', 'i', 'can', 'not', 'concentrate', 'and', 'other', 'things', 'catch', 'my', 'attention']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['killer', 'ad', '##hd', 'resource', '.', 'helped', 'me', 'answer', 'al', '##ot', 'of', 'questions', '.']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['killer', 'ad', '##hd', 'resource', '.', 'helped', 'me', 'answer', 'al', '##ot', 'of', 'questions', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'here', 'have', 'trouble', 'hearing', 'the', 'correct', 'lyrics', 'in', 'songs', ',', 'and', 'end', 'up', 'preferring', 'instrumental', '/', 'classical', '/', 'electronic', 'music', 'instead', '?']\n",
      "INFO:__main__:Number of tokens: 25\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'here', 'have', 'trouble', 'hearing', 'the', 'correct', 'lyrics', 'in', 'songs', ',', 'and', 'end', 'up', 'preferring', 'instrumental', '/', 'classical', '/', 'electronic', 'music', 'instead', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '-', 'pi', ':', 'or', 'how', 'the', 'world', 'is', 'ruled', '.', 'free', 'ebook', 'inside']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '-', 'pi', ':', 'or', 'how', 'the', 'world', 'is', 'ruled', '.', 'free', 'ebook', 'inside']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['at', 'what', 'point', 'would', 'a', 'st', '##im', '##ula', '##nt', 'have', 'reverse', 'results', 'for', 'an', 'ad', '##hd', 'brain', '?', 'for', 'those', 'of', 'us', 'on', 'st', '##im', '##ula', '##nts', ',', 'we', 'know', 'that', 'they', \"'\", 're', 'tricky', 'things', ',', 'and', 'for', 'most', 'of', 'us', ',', 'it', 'doesn', \"'\", 't', 'take', 'much', 'to', 'get', 'our', 'brains', 'to', '(', 'somewhat', ')', 'normal', ',', 'for', 'lack', 'of', 'a', 'better', 'term', ',', '\"', 'level', '\"', '.', 'lately', 'i', \"'\", 've', 'been', 'wondering', ',', 'if', 'someone', 'with', 'an', 'ad', '##hd', 'brain', 'works', 'normal', 'on', 'a', 'st', '##im', '##ula', '##nt', ',', '*', '*', 'would', 'it', 'be', 'possible', 'to', 'take', 'more', 'then', 'the', 'amount', 'needed', 'and', 'have', 'the', 'same', 'results', 'as', 'a', 'person', 'without', 'an', 'ad', '##hd', 'brain', '?', 'or', 'would', 'the', 'increase', 'level', 'of', 'the', 'st', '##im', '##ula', '##nt', 'have', 'a', 'negative', 'effect', 'on', 'the', 'brain', '?', '*', '*', 'discuss', '!', '[', 'note', ':', 'i', 'am', 'not', 'condo', '##ning', 'the', 'inappropriate', 'use', 'of', 'prescribed', 'medication', ',', 'i', \"'\", 'm', 'only', 'presenting', 'a', '\"', 'what', 'if', '\"', 'situation', '.', 'please', 'talk', 'to', 'your', 'doctor', 'before', 'changing', 'the', 'dos', '##age', 'of', 'any', 'prescribed', 'medication', '.', 'don', \"'\", 't', 'be', 'dumb', '.', ']']\n",
      "INFO:__main__:Number of tokens: 192\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['at', 'what', 'point', 'would', 'a', 'st', '##im', '##ula', '##nt', 'have', 'reverse', 'results', 'for', 'an', 'ad', '##hd', 'brain', '?', 'for', 'those', 'of', 'us', 'on', 'st', '##im', '##ula', '##nts', ',', 'we', 'know', 'that', 'they', \"'\", 're', 'tricky', 'things', ',', 'and', 'for', 'most', 'of', 'us', ',', 'it', 'doesn', \"'\", 't', 'take', 'much', 'to', 'get', 'our', 'brains', 'to', '(', 'somewhat', ')', 'normal', ',', 'for', 'lack', 'of', 'a', 'better', 'term', ',', '\"', 'level', '\"', '.', 'lately', 'i', \"'\", 've', 'been', 'wondering', ',', 'if', 'someone', 'with', 'an', 'ad', '##hd', 'brain', 'works', 'normal', 'on', 'a', 'st', '##im', '##ula', '##nt', ',', '*', '*', 'would', 'it', 'be', 'possible', 'to', 'take', 'more', 'then', 'the', 'amount', 'needed', 'and', 'have', 'the', 'same', 'results', 'as', 'a', 'person', 'without', 'an', 'ad', '##hd', 'brain', '?', 'or', 'would', 'the', 'increase', 'level', 'of', 'the', 'st', '##im', '##ula', '##nt', 'have', 'a', 'negative', 'effect', 'on', 'the', 'brain', '?', '*', '*', 'discuss', '!', '[', 'note', ':', 'i', 'am', 'not', 'condo', '##ning', 'the', 'inappropriate', 'use', 'of', 'prescribed', 'medication', ',', 'i', \"'\", 'm', 'only', 'presenting', 'a', '\"', 'what', 'if', '\"', 'situation', '.', 'please', 'talk', 'to', 'your', 'doctor', 'before', 'changing', 'the', 'dos', '##age', 'of', 'any', 'prescribed', 'medication', '.', 'don', \"'\", 't', 'be', 'dumb', '.', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'using', 'red', '##dit', 'as', 'an', 'escape', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'using', 'red', '##dit', 'as', 'an', 'escape', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'find', 'they', 'have', 'more', 'emotions', 'after', 'diagnosis', '.']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'find', 'they', 'have', 'more', 'emotions', 'after', 'diagnosis', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'apps', 'do', 'you', 'find', 'useful', 'for', 'ad', '##hd', 'symptoms', '?', 'i', 'bought', 'an', 'iphone', 'and', 'i', 'am', 'wondering', 'if', 'there', 'are', 'any', 'good', 'apps', 'to', 'help', 'overcome', 'some', 'of', 'the', 'symptoms', '.', 'please', 'include', 'how', 'you', 'find', 'the', 'app', 'to', 'be', 'helpful', '.']\n",
      "INFO:__main__:Number of tokens: 44\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'apps', 'do', 'you', 'find', 'useful', 'for', 'ad', '##hd', 'symptoms', '?', 'i', 'bought', 'an', 'iphone', 'and', 'i', 'am', 'wondering', 'if', 'there', 'are', 'any', 'good', 'apps', 'to', 'help', 'overcome', 'some', 'of', 'the', 'symptoms', '.', 'please', 'include', 'how', 'you', 'find', 'the', 'app', 'to', 'be', 'helpful', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'i', 'have', 'ad', '##hd', '.', '.', '.', 'now', 'what', '?', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'and', 'a', 'processing', 'disorder', 'in', '8th', 'grade', 'and', 'was', 'put', 'on', 'st', '##rat', '##tera', '.', 'it', 'did', 'very', 'little', 'for', 'my', 'focus', 'and', 'left', 'me', 'with', 'cr', '##ip', '##pling', 'anxiety', '.', 'the', 'anxiety', 'decreased', 'dramatically', 'after', 'i', 'quit', 'taking', 'it', 'but', 'there', 'are', 'still', 'a', 'number', 'of', 'things', 'that', 'still', 'make', 'me', 'anxious', 'that', 'didn', \"'\", 't', 'before', 'i', 'took', 'st', '##rat', '##tera', '.', 'i', 'am', 'not', 'saying', 'that', 'st', '##rat', '##tera', 'caused', 'my', 'anxiety', 'problems', 'because', 'i', 'definitely', 'had', 'plenty', 'before', 'i', 'started', 'it', 'but', 'it', 'manifested', 'in', 'very', 'different', 'ways', ',', 'usually', 'not', 'socially', 'as', 'it', 'does', 'now', '.', 'when', 'i', 'was', 'first', 'diagnosed', ',', 'they', 'chose', 'st', '##rat', '##tera', 'because', 'i', 'didn', \"'\", 't', 'weigh', 'enough', 'to', 'take', 'a', 'st', '##im', '##ula', '##nt', '.', 'i', 'was', 'right', 'at', 'the', 'border', 'between', 'normal', 'and', 'under', '##weight', 'most', 'of', 'my', 'life', 'and', 'only', 'recently', 'have', 'gained', 'some', 'extra', 'weight', '.', 'i', \"'\", 'm', 'going', 'to', 'guess', 'that', 'st', '##im', '##ula', '##nt', 'drugs', 'are', 'now', 'also', 'an', 'option', 'for', 'me', '.', 'i', 'have', 'an', 'appointment', 'with', 'a', 'psychologist', 'next', 'week', 'since', 'i', 'was', 'told', 'that', 'i', 'have', 'to', 'get', 're', '-', 'diagnosed', 'now', 'that', 'i', 'am', 'an', 'adult', '.', 'i', 'am', '21', 'and', 'in', 'college', '.', 'my', 'grades', 'are', 'fantastic', 'but', 'i', 'don', \"'\", 't', 'feel', 'like', 'that', 'means', 'much', '.', 'i', 'am', 'going', 'through', 'pretty', 'insane', 'amounts', 'of', 'stress', 'and', 'crying', 'fits', 'in', 'between', 'doing', 'homework', 'and', 'studying', 'and', 'i', 'put', 'everything', 'off', 'until', 'the', 'very', 'last', 'minute', '.', 'when', 'it', 'comes', 'to', 'actually', 'reading', 'for', 'a', 'class', ',', 'just', 'forget', 'it', '.', 'i', 'manage', 'all', 'a', \"'\", 's', 'by', 'carefully', 'calculating', 'what', 'assignments', 'matter', 'and', 'trying', 'to', 'balance', 'out', 'what', 'i', 'can', \"'\", 't', 'do', 'well', 'on', 'by', 'going', 'way', 'above', 'and', 'beyond', 'on', 'what', 'i', 'can', '.', 'even', 'still', ',', 'i', 'feel', 'like', 'i', \"'\", 'm', 'cheating', 'myself', '.', 'i', 'don', \"'\", 't', 'know', 'all', 'of', 'the', 'things', 'i', 'should', 'know', '.', 'i', 'am', 'taking', 'a', 'class', 'where', 'i', 'got', 'all', '90s', 'and', 'above', 'on', 'assignments', 'that', 'could', 'be', 'taken', '3', 'times', 'each', 'to', 'boost', 'them', '.', 'i', 'got', 'a', '74', 'on', 'the', 'final', 'which', 'shows', 'me', 'i', 'didn', \"'\", 't', 'learn', 'a', 'damn', 'thing', '.', 'still', 'got', 'an', 'a', 'in', 'the', 'class', '.', 'i', 'keep', 'getting', 'intense', 'bouts', 'of', 'terrible', 'low', 'self', 'esteem', ',', 'feeling', 'like', 'a', 'failure', ',', 'a', 'liar', ',', 'a', 'fake', ',', 'lazy', ',', 'stupid', ',', 'inc', '##omp', '##ete', '##nt', ',', 'whatever', '.', 'my', 'boyfriend', ',', 'whom', 'i', 'live', 'with', ',', 'has', 'fi', '##bro', '##my', '##al', '##gia', '(', 'chronic', 'pain', ',', 'it', 'never', 'goes', 'away', ')', 'and', 'anxiety', 'problems', 'so', 'i', 'feel', 'extra', 'guilty', 'for', 'adding', 'to', 'his', 'stress', '.', 'he', 'also', 'really', 'doesn', \"'\", 't', 'understand', 'ad', '##hd', 'and', 'has', 'said', 'before', 'that', 'he', 'thinks', 'it', 'isn', \"'\", 't', 'real', '.', 'even', 'more', 'than', 'just', 'with', 'school', ',', 'i', 'forget', 'things', 'constantly', 'and', 'get', 'distracted', ',', 'i', \"'\", 've', 'missed', 'important', 'things', 'my', 'boyfriend', 'said', 'to', 'me', 'or', 'forgotten', 'them', ',', 'i', 'can', \"'\", 't', 'seem', 'to', 'make', 'myself', 'clean', 'or', 'do', 'chores', ',', 'my', 'cats', 'would', 'probably', 'star', '##ve', 'half', 'the', 'time', 'if', 'my', 'boyfriend', 'didn', \"'\", 't', 'feed', 'them', '.', 'i', \"'\", 'm', 'at', 'the', 'breaking', 'point', ',', 'basically', '.', 'i', 'need', 'things', 'to', 'improve', 'and', 'soon', '.', 'here', 'are', 'my', 'main', 'questions', ':', 'does', 'anyone', 'have', 'any', 'ideas', 'on', 'which', 'medicines', 'might', 'be', 'best', 'for', 'me', '?', 'i', 'am', 'a', 'little', 'concerned', 'about', 'my', 'anxiety', 'problems', 'and', 'assume', 'i', 'will', 'also', 'get', 'some', 'kind', 'of', 'diagnosis', 'related', 'to', 'that', 'soon', '.', 'i', 'know', 'st', '##im', '##ula', '##nts', 'can', 'increase', 'anxiety', '.', 'am', 'i', 'going', 'to', 'run', 'into', 'problems', 'with', 'the', 'drug', 'shortage', 'i', \"'\", 'm', 'hearing', 'about', '?', 'i', 'have', 'heard', 'of', 'some', 'instances', 'where', 'well', '##bu', '##tri', '##n', 'is', 'used', '.', 'i', \"'\", 've', 'looked', 'into', 'it', 'a', 'little', 'and', 'it', 'seems', 'like', 'it', 'may', 'help', 'most', 'of', 'what', 'i', 'struggle', 'with', 'but', 'i', 'really', 'am', 'not', 'sure', '.', 'has', 'anyone', 'tried', 'well', '##bu', '##tri', '##n', 'for', 'ad', '##hd', 'before', '?', 'does', 'anyone', 'have', 'any', 'other', 'advice', '?', '*', '*', 't', '##l', ';', 'dr', ':', '*', '*', 'i', 'have', 'ad', '##hd', '.', 'st', '##rat', '##tera', 'sucked', '.', 'i', 'have', 'anxiety', 'problems', 'and', 'ad', '##hd', 'has', 'shot', 'my', 'self', '-', 'esteem', 'into', 'the', 'ground', '.', 'what', 'can', 'i', 'take', '?', 'what', 'do', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 743\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['so', 'i', 'have', 'ad', '##hd', '.', '.', '.', 'now', 'what', '?', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'and', 'a', 'processing', 'disorder', 'in', '8th', 'grade', 'and', 'was', 'put', 'on', 'st', '##rat', '##tera', '.', 'it', 'did', 'very', 'little', 'for', 'my', 'focus', 'and', 'left', 'me', 'with', 'cr', '##ip', '##pling', 'anxiety', '.', 'the', 'anxiety', 'decreased', 'dramatically', 'after', 'i', 'quit', 'taking', 'it', 'but', 'there', 'are', 'still', 'a', 'number', 'of', 'things', 'that', 'still', 'make', 'me', 'anxious', 'that', 'didn', \"'\", 't', 'before', 'i', 'took', 'st', '##rat', '##tera', '.', 'i', 'am', 'not', 'saying', 'that', 'st', '##rat', '##tera', 'caused', 'my', 'anxiety', 'problems', 'because', 'i', 'definitely', 'had', 'plenty', 'before', 'i', 'started', 'it', 'but', 'it', 'manifested', 'in', 'very', 'different', 'ways', ',', 'usually', 'not', 'socially', 'as', 'it', 'does', 'now', '.', 'when', 'i', 'was', 'first', 'diagnosed', ',', 'they', 'chose', 'st', '##rat', '##tera', 'because', 'i', 'didn', \"'\", 't', 'weigh', 'enough', 'to', 'take', 'a', 'st', '##im', '##ula', '##nt', '.', 'i', 'was', 'right', 'at', 'the', 'border', 'between', 'normal', 'and', 'under', '##weight', 'most', 'of', 'my', 'life', 'and', 'only', 'recently', 'have', 'gained', 'some', 'extra', 'weight', '.', 'i', \"'\", 'm', 'going', 'to', 'guess', 'that', 'st', '##im', '##ula', '##nt', 'drugs', 'are', 'now', 'also', 'an', 'option', 'for', 'me', '.', 'i', 'have', 'an', 'appointment', 'with', 'a', 'psychologist', 'next', 'week', 'since', 'i', 'was', 'told', 'that', 'i', 'have', 'to', 'get', 're', '-', 'diagnosed', 'now', 'that', 'i', 'am', 'an', 'adult', '.', 'i', 'am', '21', 'and', 'in', 'college', '.', 'my', 'grades', 'are', 'fantastic', 'but', 'i', 'don', \"'\", 't', 'feel', 'like', 'that', 'means', 'much', '.', 'i', 'am', 'going', 'through', 'pretty', 'insane', 'amounts', 'of', 'stress', 'and', 'crying', 'fits', 'in', 'between', 'doing', 'homework', 'and', 'studying', 'and', 'i', 'put', 'everything', 'off', 'until', 'the', 'very', 'last', 'minute', '.', 'when', 'it', 'comes', 'to', 'actually', 'reading', 'for', 'a', 'class', ',', 'just', 'forget', 'it', '.', 'i', 'manage', 'all', 'a', \"'\", 's', 'by', 'carefully', 'calculating', 'what', 'assignments', 'matter', 'and', 'trying', 'to', 'balance', 'out', 'what', 'i', 'can', \"'\", 't', 'do', 'well', 'on', 'by', 'going', 'way', 'above', 'and', 'beyond', 'on', 'what', 'i', 'can', '.', 'even', 'still', ',', 'i', 'feel', 'like', 'i', \"'\", 'm', 'cheating', 'myself', '.', 'i', 'don', \"'\", 't', 'know', 'all', 'of', 'the', 'things', 'i', 'should', 'know', '.', 'i', 'am', 'taking', 'a', 'class', 'where', 'i', 'got', 'all', '90s', 'and', 'above', 'on', 'assignments', 'that', 'could', 'be', 'taken', '3', 'times', 'each', 'to', 'boost', 'them', '.', 'i', 'got', 'a', '74', 'on', 'the', 'final', 'which', 'shows', 'me', 'i', 'didn', \"'\", 't', 'learn', 'a', 'damn', 'thing', '.', 'still', 'got', 'an', 'a', 'in', 'the', 'class', '.', 'i', 'keep', 'getting', 'intense', 'bouts', 'of', 'terrible', 'low', 'self', 'esteem', ',', 'feeling', 'like', 'a', 'failure', ',', 'a', 'liar', ',', 'a', 'fake', ',', 'lazy', ',', 'stupid', ',', 'inc', '##omp', '##ete', '##nt', ',', 'whatever', '.', 'my', 'boyfriend', ',', 'whom', 'i', 'live', 'with', ',', 'has', 'fi', '##bro', '##my', '##al', '##gia', '(', 'chronic', 'pain', ',', 'it', 'never', 'goes', 'away', ')', 'and', 'anxiety', 'problems', 'so', 'i', 'feel', 'extra', 'guilty', 'for', 'adding', 'to', 'his', 'stress', '.', 'he', 'also', 'really', 'doesn', \"'\", 't', 'understand', 'ad', '##hd', 'and', 'has', 'said', 'before', 'that', 'he', 'thinks', 'it', 'isn', \"'\", 't', 'real', '.', 'even', 'more', 'than', 'just', 'with', 'school', ',', 'i', 'forget', 'things', 'constantly', 'and', 'get', 'distracted', ',', 'i', \"'\", 've', 'missed', 'important', 'things', 'my', 'boyfriend', 'said', 'to', 'me'], ['or', 'forgotten', 'them', ',', 'i', 'can', \"'\", 't', 'seem', 'to', 'make', 'myself', 'clean', 'or', 'do', 'chores', ',', 'my', 'cats', 'would', 'probably', 'star', '##ve', 'half', 'the', 'time', 'if', 'my', 'boyfriend', 'didn', \"'\", 't', 'feed', 'them', '.', 'i', \"'\", 'm', 'at', 'the', 'breaking', 'point', ',', 'basically', '.', 'i', 'need', 'things', 'to', 'improve', 'and', 'soon', '.', 'here', 'are', 'my', 'main', 'questions', ':', 'does', 'anyone', 'have', 'any', 'ideas', 'on', 'which', 'medicines', 'might', 'be', 'best', 'for', 'me', '?', 'i', 'am', 'a', 'little', 'concerned', 'about', 'my', 'anxiety', 'problems', 'and', 'assume', 'i', 'will', 'also', 'get', 'some', 'kind', 'of', 'diagnosis', 'related', 'to', 'that', 'soon', '.', 'i', 'know', 'st', '##im', '##ula', '##nts', 'can', 'increase', 'anxiety', '.', 'am', 'i', 'going', 'to', 'run', 'into', 'problems', 'with', 'the', 'drug', 'shortage', 'i', \"'\", 'm', 'hearing', 'about', '?', 'i', 'have', 'heard', 'of', 'some', 'instances', 'where', 'well', '##bu', '##tri', '##n', 'is', 'used', '.', 'i', \"'\", 've', 'looked', 'into', 'it', 'a', 'little', 'and', 'it', 'seems', 'like', 'it', 'may', 'help', 'most', 'of', 'what', 'i', 'struggle', 'with', 'but', 'i', 'really', 'am', 'not', 'sure', '.', 'has', 'anyone', 'tried', 'well', '##bu', '##tri', '##n', 'for', 'ad', '##hd', 'before', '?', 'does', 'anyone', 'have', 'any', 'other', 'advice', '?', '*', '*', 't', '##l', ';', 'dr', ':', '*', '*', 'i', 'have', 'ad', '##hd', '.', 'st', '##rat', '##tera', 'sucked', '.', 'i', 'have', 'anxiety', 'problems', 'and', 'ad', '##hd', 'has', 'shot', 'my', 'self', '-', 'esteem', 'into', 'the', 'ground', '.', 'what', 'can', 'i', 'take', '?', 'what', 'do', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['beating', 'pro', '##cr', '##ast', '##ination']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['beating', 'pro', '##cr', '##ast', '##ination']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'have', 'sensory', 'integration', '?', 'i', 'was', 'diagnosed', 'with', 'add', 'when', 'i', 'was', 'in', 'first', 'grade', 'and', 'have', 'sensory', 'integration', '.', 'i', \"'\", 've', 'been', 'having', 'some', 'trouble', 'with', 'it', 'and', 'was', 'wondering', 'if', 'anyone', 'knows', 'how', 'to', 'help', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 43\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'have', 'sensory', 'integration', '?', 'i', 'was', 'diagnosed', 'with', 'add', 'when', 'i', 'was', 'in', 'first', 'grade', 'and', 'have', 'sensory', 'integration', '.', 'i', \"'\", 've', 'been', 'having', 'some', 'trouble', 'with', 'it', 'and', 'was', 'wondering', 'if', 'anyone', 'knows', 'how', 'to', 'help', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'art', 'of', 'distraction']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'art', 'of', 'distraction']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['graduate', 'student', 'looking', 'for', 'advice', 'hello', 'r', '/', 'ad', '##hd', '!', 'i', 'am', 'a', 'first', 'year', 'graduate', 'student', '\\\\', '(', 'i', \"'\", 'm', 'in', 'my', 'early', '30', \"'\", 's', '\\\\', ')', ',', 'suspected', 'ad', '##hd', 'and', 'i', 'am', 'scheduled', 'to', 'get', 'assessed', '.', '.', '.', '.', 'sometime', 'soon', 'i', 'hope', '.', 'i', \"'\", 'm', 'not', 'here', 'to', 'ask', 'if', 'i', 'have', 'ad', '##hd', ',', 'but', 'how', 'should', 'i', 'go', 'about', 'letting', 'my', 'professors', 'know', 'that', 'something', 'isn', \"'\", 't', 'functioning', 'in', 'a', 'way', 'that', 'i', 'am', 'able', 'to', 'do', 'my', 'work', '.', 'here', 'is', 'my', 'problem', ':', 'i', 'am', 'in', 'a', 'not', 'so', 'good', 'place', 'academic', '##ally', '.', 'my', 'first', 'semester', 'of', 'gr', '##ad', 'school', 'was', 'terrible', '.', 'what', 'worked', 'for', 'me', 'in', 'my', 'under', '##grad', 'doesn', \"'\", 't', 'come', 'close', 'to', 'getting', 'by', 'anymore', '\\\\', '(', 'i', \"'\", 'm', 'in', 'the', 'hard', 'sciences', '\\\\', ')', '.', 'last', 'semester', ',', 'and', 'me', 'going', 'for', 'a', 'repeat', 'this', 'semester', 'despite', 'vow', '##ing', 'to', 'change', 'are', 'among', 'the', 'top', 'reasons', 'for', 'seeking', 'assessment', '.', 'i', 'won', \"'\", 't', 'get', 'into', 'the', 'rest', '.', 'relevant', 'background', ':', '*', 'no', 'official', 'diagnosis', '.', 'it', 'is', 'likely', 'i', 'will', 'be', 'diagnosed', 'ad', '##hd', '-', 'pi', '.', '*', 'preliminary', 'meeting', 'with', 'pc', '##p', 'results', 'in', 'medication', ',', 'just', 'recently', 'started', '.', '*', 'psychiatrist', 'appointment', 'for', 'late', 'april', '.', '*', 'university', 'counseling', 'starting', 'soon', '.', '*', 'spoke', 'with', 'my', 'adviser', 'of', '3', '.', '5', 'years', 'about', 'my', 'concerns', 'and', 'difficulties', '-', '-', 'he', 'is', 'supportive', '*', 'went', 'to', 'under', '##grad', 'at', 'same', 'school', ',', 'i', 'am', 'known', 'and', 'active', 'in', 'the', 'department', '.', 'i', 'am', 'in', 'real', 'danger', 'of', 'guaranteed', 'failing', 'of', 'one', 'of', 'my', 'courses', ',', 'and', 'the', 'other', 'two', 'aren', \"'\", 't', 'going', 'so', 'well', '.', 'of', 'course', ',', 'something', 'needs', 'to', 'be', 'done', ',', 'and', 'i', 'need', 'to', 'do', 'it', 'right', 'now', ':', ')', 'seriously', 'though', ',', 'do', 'i', 'speak', 'with', 'the', 'professor', 'and', 'tell', 'him', 'of', 'my', 'difficulties', ',', 'and', 'i', 'am', 'attempting', 'to', 'get', 'help', '?', '*', '*', 'i', 'don', \"'\", 't', 'have', 'an', 'official', 'diagnosis', '*', '*', 'and', 'i', 'don', \"'\", 't', 'want', 'to', 'get', 'all', 'dramatic', 'for', 'nothing', '.', 'even', 'if', 'i', 'am', 'not', 'diagnosed', ',', 'there', 'has', 'always', 'been', 'problem', 'and', 'within', 'the', 'past', 'few', 'years', 'it', 'has', 'become', 'un', '##mana', '##ge', '##able', '-', '-', 'it', 'wouldn', \"'\", 't', 'be', 'a', 'total', 'waste', '.', 'if', 'i', 'do', 'speak', 'with', 'the', 'professor', ',', 'how', 'do', 'i', 'explain', 'why', 'i', \"'\", 've', 'only', 'half', '-', 'completed', 'homework', 'sets', 'and', 'failed', 'an', 'exam', '?', 'how', 'to', 'explain', 'i', 'am', 'seeking', 'professional', 'help', 'for', 'mental', 'issues', '?', 'thanks', 'for', 'reading', ',', 'it', \"'\", 's', 'been', 'a', 'very', 'stress', '##ful', 'month', '.', 'any', 'advice', 'is', 'appreciated', '.', 'edit', ':', 'format', '##ting', '/', 'spelling', '*', '*', 'update', '*', '*', 'i', 'cornered', 'the', 'professor', 'after', 'class', 'today', '.', 'he', 'is', 'acute', '##ly', 'aware', 'of', 'the', 'need', 'to', 'talk', '.', 'i', 'get', 'the', 'feeling', 'he', 'can', \"'\", 't', 'understand', 'why', 'i', 'don', \"'\", 't', 'get', 'my', 'work', 'done', '(', 'keeps', 'asking', 'if', 'i', 'work', ',', 'etc', '.', '.', '.', ')', '.', 'we', 'meet', 'tomorrow', '.', 'thanks', 'for', 'all', 'the', 'advice', '.']\n",
      "INFO:__main__:Number of tokens: 524\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['graduate', 'student', 'looking', 'for', 'advice', 'hello', 'r', '/', 'ad', '##hd', '!', 'i', 'am', 'a', 'first', 'year', 'graduate', 'student', '\\\\', '(', 'i', \"'\", 'm', 'in', 'my', 'early', '30', \"'\", 's', '\\\\', ')', ',', 'suspected', 'ad', '##hd', 'and', 'i', 'am', 'scheduled', 'to', 'get', 'assessed', '.', '.', '.', '.', 'sometime', 'soon', 'i', 'hope', '.', 'i', \"'\", 'm', 'not', 'here', 'to', 'ask', 'if', 'i', 'have', 'ad', '##hd', ',', 'but', 'how', 'should', 'i', 'go', 'about', 'letting', 'my', 'professors', 'know', 'that', 'something', 'isn', \"'\", 't', 'functioning', 'in', 'a', 'way', 'that', 'i', 'am', 'able', 'to', 'do', 'my', 'work', '.', 'here', 'is', 'my', 'problem', ':', 'i', 'am', 'in', 'a', 'not', 'so', 'good', 'place', 'academic', '##ally', '.', 'my', 'first', 'semester', 'of', 'gr', '##ad', 'school', 'was', 'terrible', '.', 'what', 'worked', 'for', 'me', 'in', 'my', 'under', '##grad', 'doesn', \"'\", 't', 'come', 'close', 'to', 'getting', 'by', 'anymore', '\\\\', '(', 'i', \"'\", 'm', 'in', 'the', 'hard', 'sciences', '\\\\', ')', '.', 'last', 'semester', ',', 'and', 'me', 'going', 'for', 'a', 'repeat', 'this', 'semester', 'despite', 'vow', '##ing', 'to', 'change', 'are', 'among', 'the', 'top', 'reasons', 'for', 'seeking', 'assessment', '.', 'i', 'won', \"'\", 't', 'get', 'into', 'the', 'rest', '.', 'relevant', 'background', ':', '*', 'no', 'official', 'diagnosis', '.', 'it', 'is', 'likely', 'i', 'will', 'be', 'diagnosed', 'ad', '##hd', '-', 'pi', '.', '*', 'preliminary', 'meeting', 'with', 'pc', '##p', 'results', 'in', 'medication', ',', 'just', 'recently', 'started', '.', '*', 'psychiatrist', 'appointment', 'for', 'late', 'april', '.', '*', 'university', 'counseling', 'starting', 'soon', '.', '*', 'spoke', 'with', 'my', 'adviser', 'of', '3', '.', '5', 'years', 'about', 'my', 'concerns', 'and', 'difficulties', '-', '-', 'he', 'is', 'supportive', '*', 'went', 'to', 'under', '##grad', 'at', 'same', 'school', ',', 'i', 'am', 'known', 'and', 'active', 'in', 'the', 'department', '.', 'i', 'am', 'in', 'real', 'danger', 'of', 'guaranteed', 'failing', 'of', 'one', 'of', 'my', 'courses', ',', 'and', 'the', 'other', 'two', 'aren', \"'\", 't', 'going', 'so', 'well', '.', 'of', 'course', ',', 'something', 'needs', 'to', 'be', 'done', ',', 'and', 'i', 'need', 'to', 'do', 'it', 'right', 'now', ':', ')', 'seriously', 'though', ',', 'do', 'i', 'speak', 'with', 'the', 'professor', 'and', 'tell', 'him', 'of', 'my', 'difficulties', ',', 'and', 'i', 'am', 'attempting', 'to', 'get', 'help', '?', '*', '*', 'i', 'don', \"'\", 't', 'have', 'an', 'official', 'diagnosis', '*', '*', 'and', 'i', 'don', \"'\", 't', 'want', 'to', 'get', 'all', 'dramatic', 'for', 'nothing', '.', 'even', 'if', 'i', 'am', 'not', 'diagnosed', ',', 'there', 'has', 'always', 'been', 'problem', 'and', 'within', 'the', 'past', 'few', 'years', 'it', 'has', 'become', 'un', '##mana', '##ge', '##able', '-', '-', 'it', 'wouldn', \"'\", 't', 'be', 'a', 'total', 'waste', '.', 'if', 'i', 'do', 'speak', 'with', 'the', 'professor', ',', 'how', 'do', 'i', 'explain', 'why', 'i', \"'\", 've', 'only', 'half', '-', 'completed', 'homework', 'sets', 'and', 'failed', 'an', 'exam', '?', 'how', 'to', 'explain', 'i', 'am', 'seeking', 'professional', 'help', 'for', 'mental', 'issues', '?', 'thanks', 'for', 'reading', ',', 'it', \"'\", 's', 'been', 'a', 'very', 'stress', '##ful', 'month', '.', 'any', 'advice', 'is', 'appreciated', '.', 'edit', ':', 'format', '##ting', '/', 'spelling', '*', '*', 'update', '*', '*', 'i', 'cornered', 'the', 'professor', 'after', 'class', 'today', '.', 'he', 'is', 'acute', '##ly', 'aware', 'of', 'the', 'need', 'to', 'talk', '.', 'i', 'get', 'the', 'feeling', 'he', 'can', \"'\", 't', 'understand', 'why', 'i', 'don', \"'\", 't', 'get', 'my', 'work', 'done', '(', 'keeps', 'asking', 'if', 'i', 'work', ',', 'etc', '.', '.', '.'], [')', '.', 'we', 'meet', 'tomorrow', '.', 'thanks', 'for', 'all', 'the', 'advice', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['issues', 'getting', 'diagnosed', '?', 'hello', 'r', '/', 'ad', '##hd', '!', 'i', 'am', 'scheduled', 'to', 'see', 'a', 'psychiatrist', 'about', 'the', 'possibility', 'of', 'me', 'having', 'ad', '##hd', '(', 'probably', 'predominantly', 'ina', '##tten', '##tive', ')', 'but', 'am', 'a', 'little', 'nervous', 'about', 'the', 'diagnostic', 'process', '.', 'i', \"'\", 've', 'heard', 'of', 'some', 'people', 'saying', 'they', 'took', 'a', 'test', 'and', 'other', 'saying', 'they', 'just', 'had', 'to', 'answer', 'some', 'questions', '.', 'did', 'anyone', 'experience', 'a', 'diagnosis', 'they', 'felt', 'wasn', \"'\", 't', 'accurate', '?', 'do', 'you', 'think', 'the', 'tests', '(', 'like', 'the', '1', \"'\", 's', 'and', '2', \"'\", 's', 'test', ')', 'are', 'a', 'true', 'measure', 'of', 'whether', 'you', 'have', 'ad', '##hd', '?', 'i', 'guess', 'i', 'am', 'afraid', 'of', 'taking', 'a', 'test', 'like', 'that', 'and', 'not', 'performing', '\"', 'accurately', '\"', 'because', 'i', 'would', 'be', 'under', 'some', 'pressure', '.', 'also', ',', 'a', 'couple', 'of', 'months', 'ago', 'i', 'had', 'one', 'session', 'with', 'a', 'therapist', '.', 'i', 'was', 'there', 'to', 'talk', 'about', 'my', 'social', 'anxiety', 'but', 'at', 'the', 'beginning', 'of', 'the', 'meeting', 'i', 'mentioned', 'exploring', 'the', 'possibility', 'of', 'ad', '##hd', '.', 'we', 'spent', 'most', 'of', 'the', 'hour', 'talking', 'about', 'my', 'social', 'anxiety', 'and', 'he', 'didn', \"'\", 't', 'even', 'ask', 'me', 'anything', 'regarding', 'ad', '##hd', 'but', 'at', 'the', 'end', 'straight', 'up', 'said', 'i', 'don', \"'\", 't', 'have', 'it', 'when', 'i', 'hadn', \"'\", 't', 'even', 'talked', 'about', 'it', '!', 'i', 'left', 'frustrated', 'that', 'i', 'felt', 'i', 'had', 'mis', '##re', '##pres', '##ented', 'myself', '.', 'any', 'advice', 'or', 'comments', 'would', 'be', 'appreciated', '.']\n",
      "INFO:__main__:Number of tokens: 238\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['issues', 'getting', 'diagnosed', '?', 'hello', 'r', '/', 'ad', '##hd', '!', 'i', 'am', 'scheduled', 'to', 'see', 'a', 'psychiatrist', 'about', 'the', 'possibility', 'of', 'me', 'having', 'ad', '##hd', '(', 'probably', 'predominantly', 'ina', '##tten', '##tive', ')', 'but', 'am', 'a', 'little', 'nervous', 'about', 'the', 'diagnostic', 'process', '.', 'i', \"'\", 've', 'heard', 'of', 'some', 'people', 'saying', 'they', 'took', 'a', 'test', 'and', 'other', 'saying', 'they', 'just', 'had', 'to', 'answer', 'some', 'questions', '.', 'did', 'anyone', 'experience', 'a', 'diagnosis', 'they', 'felt', 'wasn', \"'\", 't', 'accurate', '?', 'do', 'you', 'think', 'the', 'tests', '(', 'like', 'the', '1', \"'\", 's', 'and', '2', \"'\", 's', 'test', ')', 'are', 'a', 'true', 'measure', 'of', 'whether', 'you', 'have', 'ad', '##hd', '?', 'i', 'guess', 'i', 'am', 'afraid', 'of', 'taking', 'a', 'test', 'like', 'that', 'and', 'not', 'performing', '\"', 'accurately', '\"', 'because', 'i', 'would', 'be', 'under', 'some', 'pressure', '.', 'also', ',', 'a', 'couple', 'of', 'months', 'ago', 'i', 'had', 'one', 'session', 'with', 'a', 'therapist', '.', 'i', 'was', 'there', 'to', 'talk', 'about', 'my', 'social', 'anxiety', 'but', 'at', 'the', 'beginning', 'of', 'the', 'meeting', 'i', 'mentioned', 'exploring', 'the', 'possibility', 'of', 'ad', '##hd', '.', 'we', 'spent', 'most', 'of', 'the', 'hour', 'talking', 'about', 'my', 'social', 'anxiety', 'and', 'he', 'didn', \"'\", 't', 'even', 'ask', 'me', 'anything', 'regarding', 'ad', '##hd', 'but', 'at', 'the', 'end', 'straight', 'up', 'said', 'i', 'don', \"'\", 't', 'have', 'it', 'when', 'i', 'hadn', \"'\", 't', 'even', 'talked', 'about', 'it', '!', 'i', 'left', 'frustrated', 'that', 'i', 'felt', 'i', 'had', 'mis', '##re', '##pres', '##ented', 'myself', '.', 'any', 'advice', 'or', 'comments', 'would', 'be', 'appreciated', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['health', 'insurance', 'problem', 'update', '!', 'i', 'posted', 'awhile', 'back', 'about', 'my', 'health', 'insurance', 'problems', ',', 'and', 'i', 'wanted', 'to', 'give', 'an', 'update', 'just', 'in', 'case', 'this', 'info', 'can', 'help', 'someone', 'else', 'out', '!', 'i', 'graduated', 'from', 'gr', '##ad', 'school', 'in', 'december', 'and', 'my', 'student', 'health', 'insurance', 'ran', 'out', 'at', 'the', 'beginning', 'of', 'this', 'month', '(', 'february', ')', '.', 'i', \"'\", 'm', 'just', 'too', 'old', 'to', 'go', 'back', 'on', 'my', 'parent', \"'\", 's', 'plan', ',', 'so', 'i', 'decided', 'to', 'purchase', 'a', 'single', 'buyer', 'plan', 'while', 'looking', 'for', 'a', 'job', '.', 'the', 'anthem', 'blue', 'cross', 'sales', 'guy', 'told', 'me', 'over', 'the', 'phone', 'that', 'my', 'ad', '##hd', 'med', '##s', 'would', 'be', 'covered', ',', 'but', 'my', 'application', 'was', 'rejected', 'and', 'they', 'said', 'that', 'the', 'only', 'plan', 'they', 'would', 'offer', 'me', 'would', 'cost', '$', '950', 'a', 'month', ',', 'meaning', 'i', 'was', 'totally', 'un', '-', 'ins', '##urable', 'because', 'i', 'had', 'been', 'treating', 'my', 'ad', '##hd', 'in', 'the', 'previous', '6', 'months', '(', 'they', 'said', 'this', 'was', 'the', 'reason', 'for', 'the', 'price', 'hike', ')', '.', 'they', 'wouldn', \"'\", 't', 'even', 'offer', 'me', 'a', 'plan', 'with', 'no', 'ad', '##hd', 'coverage', 'and', 'everything', 'else', 'covered', ',', 'which', 'would', 'have', 'been', 'at', 'least', '$', '700', 'cheaper', 'a', 'month', '(', 'my', 'med', '##s', 'are', '$', '80', 'out', 'of', 'pocket', 'generic', ')', '.', 'well', ',', 'i', 'applied', 'to', 'this', 'company', ':', 'http', ':', '/', '/', 'www', '.', 'ass', '##ura', '##nt', '.', 'com', '/', 'inc', '/', 'ass', '##ura', '##nt', '/', 'index', '.', 'html', 'and', 'they', 'gave', 'me', 'insurance', 'right', 'away', ',', 'no', 'problems', 'with', 'my', 'ad', '##hd', ',', 'i', 'didn', \"'\", 't', 'even', 'have', 'to', 'list', 'it', 'on', 'the', 'application', 'form', '!', 'they', 'do', 'permanent', 'and', 'temporary', '(', 'up', 'to', '6', '##mont', '##h', ')', 'insurance', 'plans', 'which', 'is', 'great', 'if', 'you', \"'\", 're', 'looking', 'for', 'a', 'job', '.', 'the', 'de', '##du', '##ct', '##ible', 'isn', \"'\", 't', 'great', '-', 'you', 'have', 'to', 'pay', 'the', 'first', '$', '1000', 'of', 'anything', 'you', 'inc', '##ur', '(', 'this', 'includes', 'med', '##s', ',', 'doctor', 'visits', ',', 'etc', ')', 'but', 'after', 'you', 'use', 'up', 'the', '$', '1000', 'they', 'pay', '80', '%', 'of', 'the', 'cost', 'up', 'to', 'a', 'certain', 'amount', 'after', 'which', 'they', 'pay', '100', '%', '.', 'if', 'you', 'can', \"'\", 't', 'get', 'insurance', 'because', 'of', 'your', 'ad', '##hd', ',', 'try', 'ass', '##ura', '##nt', '!']\n",
      "INFO:__main__:Number of tokens: 373\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['health', 'insurance', 'problem', 'update', '!', 'i', 'posted', 'awhile', 'back', 'about', 'my', 'health', 'insurance', 'problems', ',', 'and', 'i', 'wanted', 'to', 'give', 'an', 'update', 'just', 'in', 'case', 'this', 'info', 'can', 'help', 'someone', 'else', 'out', '!', 'i', 'graduated', 'from', 'gr', '##ad', 'school', 'in', 'december', 'and', 'my', 'student', 'health', 'insurance', 'ran', 'out', 'at', 'the', 'beginning', 'of', 'this', 'month', '(', 'february', ')', '.', 'i', \"'\", 'm', 'just', 'too', 'old', 'to', 'go', 'back', 'on', 'my', 'parent', \"'\", 's', 'plan', ',', 'so', 'i', 'decided', 'to', 'purchase', 'a', 'single', 'buyer', 'plan', 'while', 'looking', 'for', 'a', 'job', '.', 'the', 'anthem', 'blue', 'cross', 'sales', 'guy', 'told', 'me', 'over', 'the', 'phone', 'that', 'my', 'ad', '##hd', 'med', '##s', 'would', 'be', 'covered', ',', 'but', 'my', 'application', 'was', 'rejected', 'and', 'they', 'said', 'that', 'the', 'only', 'plan', 'they', 'would', 'offer', 'me', 'would', 'cost', '$', '950', 'a', 'month', ',', 'meaning', 'i', 'was', 'totally', 'un', '-', 'ins', '##urable', 'because', 'i', 'had', 'been', 'treating', 'my', 'ad', '##hd', 'in', 'the', 'previous', '6', 'months', '(', 'they', 'said', 'this', 'was', 'the', 'reason', 'for', 'the', 'price', 'hike', ')', '.', 'they', 'wouldn', \"'\", 't', 'even', 'offer', 'me', 'a', 'plan', 'with', 'no', 'ad', '##hd', 'coverage', 'and', 'everything', 'else', 'covered', ',', 'which', 'would', 'have', 'been', 'at', 'least', '$', '700', 'cheaper', 'a', 'month', '(', 'my', 'med', '##s', 'are', '$', '80', 'out', 'of', 'pocket', 'generic', ')', '.', 'well', ',', 'i', 'applied', 'to', 'this', 'company', ':', 'http', ':', '/', '/', 'www', '.', 'ass', '##ura', '##nt', '.', 'com', '/', 'inc', '/', 'ass', '##ura', '##nt', '/', 'index', '.', 'html', 'and', 'they', 'gave', 'me', 'insurance', 'right', 'away', ',', 'no', 'problems', 'with', 'my', 'ad', '##hd', ',', 'i', 'didn', \"'\", 't', 'even', 'have', 'to', 'list', 'it', 'on', 'the', 'application', 'form', '!', 'they', 'do', 'permanent', 'and', 'temporary', '(', 'up', 'to', '6', '##mont', '##h', ')', 'insurance', 'plans', 'which', 'is', 'great', 'if', 'you', \"'\", 're', 'looking', 'for', 'a', 'job', '.', 'the', 'de', '##du', '##ct', '##ible', 'isn', \"'\", 't', 'great', '-', 'you', 'have', 'to', 'pay', 'the', 'first', '$', '1000', 'of', 'anything', 'you', 'inc', '##ur', '(', 'this', 'includes', 'med', '##s', ',', 'doctor', 'visits', ',', 'etc', ')', 'but', 'after', 'you', 'use', 'up', 'the', '$', '1000', 'they', 'pay', '80', '%', 'of', 'the', 'cost', 'up', 'to', 'a', 'certain', 'amount', 'after', 'which', 'they', 'pay', '100', '%', '.', 'if', 'you', 'can', \"'\", 't', 'get', 'insurance', 'because', 'of', 'your', 'ad', '##hd', ',', 'try', 'ass', '##ura', '##nt', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', \"'\", 't', 'get', 'motivated', '?', 'let', 'me', 'show', 'you', 'some', 'logic', 'to', 'help', 'you', '!']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', \"'\", 't', 'get', 'motivated', '?', 'let', 'me', 'show', 'you', 'some', 'logic', 'to', 'help', 'you', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'know', 'any', 'good', 'ad', '##hd', 'psychiatrist', '##s', 'in', 'la', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'know', 'any', 'good', 'ad', '##hd', 'psychiatrist', '##s', 'in', 'la', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['posted', 'on', '/', 'r', '/', 'true', '##red', '##dit', ',', 'yet', 'another', 'post', 'questioning', 'the', 'validity', 'of', 'ad', '##hd', 'as', 'a', 'mental', 'illness', '.']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['posted', 'on', '/', 'r', '/', 'true', '##red', '##dit', ',', 'yet', 'another', 'post', 'questioning', 'the', 'validity', 'of', 'ad', '##hd', 'as', 'a', 'mental', 'illness', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['gardner', \"'\", 's', 'theory', 'of', 'multiple', 'intelligence', '##s']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['gardner', \"'\", 's', 'theory', 'of', 'multiple', 'intelligence', '##s']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['in', 'your', 'opinion', ',', 'what', 'would', 'you', 'say', 'is', 'the', 'most', 'bother', '##some', 'part', 'about', 'ad', '##hd', '?', 'for', 'me', 'it', \"'\", 's', 'other', 'people', ';', 'the', 'people', 'who', 'are', 'un', '##in', '##formed', 'of', 'what', 'ad', '##hd', 'is', ',', 'and', 'once', 'they', 'discover', 'your', '\"', 'terrible', 'secret', '\"', 'treat', 'you', 'like', 'you', \"'\", 're', 'insane', '.', 'you', 'know', ',', 'the', 'kind', 'of', 'people', 'that', 'ask', 'you', '\"', 'did', 'you', 'take', 'your', 'med', '##s', 'today', '?', '\"', '.', 'anyone', 'else', 'feel', 'the', 'same', 'way', '?', 'if', 'not', ',', 'i', 'was', 'wondering', ',', 'what', 'aspects', 'of', 'ad', '##hd', 'do', 'you', 'guys', 'find', 'most', 'bother', '##some', '?']\n",
      "INFO:__main__:Number of tokens: 104\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['in', 'your', 'opinion', ',', 'what', 'would', 'you', 'say', 'is', 'the', 'most', 'bother', '##some', 'part', 'about', 'ad', '##hd', '?', 'for', 'me', 'it', \"'\", 's', 'other', 'people', ';', 'the', 'people', 'who', 'are', 'un', '##in', '##formed', 'of', 'what', 'ad', '##hd', 'is', ',', 'and', 'once', 'they', 'discover', 'your', '\"', 'terrible', 'secret', '\"', 'treat', 'you', 'like', 'you', \"'\", 're', 'insane', '.', 'you', 'know', ',', 'the', 'kind', 'of', 'people', 'that', 'ask', 'you', '\"', 'did', 'you', 'take', 'your', 'med', '##s', 'today', '?', '\"', '.', 'anyone', 'else', 'feel', 'the', 'same', 'way', '?', 'if', 'not', ',', 'i', 'was', 'wondering', ',', 'what', 'aspects', 'of', 'ad', '##hd', 'do', 'you', 'guys', 'find', 'most', 'bother', '##some', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'updates', 'on', 'the', 'add', '##eral', '##l', 'shortage', '?', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'picking', 'up', 'my', 'prescription', 'tomorrow', '.', 'however', ',', 'i', \"'\", 'm', 'not', 'sure', 'if', 'the', 'nationwide', 'shortage', 'is', 'still', 'in', 'effect', '.', 'has', 'anybody', 'here', 'had', 'any', 'trouble', 'filling', 'their', 'prescription', '##s', 'lately', '?', 'edit', ':', 'thanks', ',', 'everyone', '.', 'i', 'was', 'able', 'to', 'pick', 'up', 'my', 'script', 'today', 'with', 'no', 'problems', '.', 'however', ',', 'the', 'ph', '##arm', '##ac', '##ist', 'said', 'something', 'i', 'thought', 'was', 'funny', ':', '\"', 'you', 'guys', 'are', 'cleaning', 'me', 'out', '.', '\"']\n",
      "INFO:__main__:Number of tokens: 91\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'updates', 'on', 'the', 'add', '##eral', '##l', 'shortage', '?', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'picking', 'up', 'my', 'prescription', 'tomorrow', '.', 'however', ',', 'i', \"'\", 'm', 'not', 'sure', 'if', 'the', 'nationwide', 'shortage', 'is', 'still', 'in', 'effect', '.', 'has', 'anybody', 'here', 'had', 'any', 'trouble', 'filling', 'their', 'prescription', '##s', 'lately', '?', 'edit', ':', 'thanks', ',', 'everyone', '.', 'i', 'was', 'able', 'to', 'pick', 'up', 'my', 'script', 'today', 'with', 'no', 'problems', '.', 'however', ',', 'the', 'ph', '##arm', '##ac', '##ist', 'said', 'something', 'i', 'thought', 'was', 'funny', ':', '\"', 'you', 'guys', 'are', 'cleaning', 'me', 'out', '.', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['afraid', 'to', 'ask', 'my', 'parents', 'to', 'take', 'my', 'to', 'be', 'tested', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['afraid', 'to', 'ask', 'my', 'parents', 'to', 'take', 'my', 'to', 'be', 'tested', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'i', 'have', 'add', '?']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'i', 'have', 'add', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'info', 'post', '!', '(', 'what', 'are', 'you', 'on', '?', ',', 'how', 'long', '?', ',', 'how', 'does', 'it', 'work', 'for', 'you', '?', ')', 'i', \"'\", 'll', 'start', '.', '.', '.', 'i', \"'\", 'm', 'trying', 'add', '##eral', '##l', 'x', '##r', 'for', 'the', 'first', 'time', '(', 'first', 'time', 'on', 'any', 'med', '##s', 'for', 'ad', '##hd', ')', ',', 'it', \"'\", 's', 'been', 'a', 'week', 'and', 'it', \"'\", 's', 'gone', 'from', '\"', 'effective', '\"', 'to', '\"', 'did', 'i', 'even', 'take', 'it', 'today', '?', '\"', 'in', 'the', 'span', 'of', '7', 'days', ',', 'so', 'that', \"'\", 's', 'why', 'i', 'put', 'this', 'post', 'out', 'there', ',', 'lets', 'hear', 'from', 'everybody']\n",
      "INFO:__main__:Number of tokens: 103\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'info', 'post', '!', '(', 'what', 'are', 'you', 'on', '?', ',', 'how', 'long', '?', ',', 'how', 'does', 'it', 'work', 'for', 'you', '?', ')', 'i', \"'\", 'll', 'start', '.', '.', '.', 'i', \"'\", 'm', 'trying', 'add', '##eral', '##l', 'x', '##r', 'for', 'the', 'first', 'time', '(', 'first', 'time', 'on', 'any', 'med', '##s', 'for', 'ad', '##hd', ')', ',', 'it', \"'\", 's', 'been', 'a', 'week', 'and', 'it', \"'\", 's', 'gone', 'from', '\"', 'effective', '\"', 'to', '\"', 'did', 'i', 'even', 'take', 'it', 'today', '?', '\"', 'in', 'the', 'span', 'of', '7', 'days', ',', 'so', 'that', \"'\", 's', 'why', 'i', 'put', 'this', 'post', 'out', 'there', ',', 'lets', 'hear', 'from', 'everybody']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['not', 'listening', 'consciously', ',', 'but', 'then', 'being', 'able', 'to', '\"', 'play', 'back', '\"', 'the', 'audio', 'of', 'the', 'conversation', 'in', 'your', 'head', '.', 'anyone', 'who', 'recognizes', 'this', '?', 'sometimes', 'when', 'i', \"'\", 'm', 'talking', 'to', 'someone', 'but', 'somewhere', 'completely', 'else', 'in', 'my', 'thoughts', ',', 'i', 'subconscious', '##ly', 'record', 'the', 'sound', 'of', 'the', 'conversation', 'like', 'a', 'tape', 'recorder', '.', 'then', 'when', 'i', 'snap', 'back', 'i', 'can', 'listen', 'to', 'this', 'recording', 'in', 'my', 'head', 'to', 'rec', '##ap', 'what', 'the', 'other', 'person', 'just', 'said', '.', 'i', \"'\", 'm', 'curious', 'to', 'know', 'if', 'this', 'is', 'something', 'everyone', 'does', 'or', 'if', 'it', \"'\", 's', 'maybe', 'common', 'to', 'ad', '##hd', 'people']\n",
      "INFO:__main__:Number of tokens: 104\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['not', 'listening', 'consciously', ',', 'but', 'then', 'being', 'able', 'to', '\"', 'play', 'back', '\"', 'the', 'audio', 'of', 'the', 'conversation', 'in', 'your', 'head', '.', 'anyone', 'who', 'recognizes', 'this', '?', 'sometimes', 'when', 'i', \"'\", 'm', 'talking', 'to', 'someone', 'but', 'somewhere', 'completely', 'else', 'in', 'my', 'thoughts', ',', 'i', 'subconscious', '##ly', 'record', 'the', 'sound', 'of', 'the', 'conversation', 'like', 'a', 'tape', 'recorder', '.', 'then', 'when', 'i', 'snap', 'back', 'i', 'can', 'listen', 'to', 'this', 'recording', 'in', 'my', 'head', 'to', 'rec', '##ap', 'what', 'the', 'other', 'person', 'just', 'said', '.', 'i', \"'\", 'm', 'curious', 'to', 'know', 'if', 'this', 'is', 'something', 'everyone', 'does', 'or', 'if', 'it', \"'\", 's', 'maybe', 'common', 'to', 'ad', '##hd', 'people']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['avoiding', 'tolerance', '/', 'withdrawal', '##s', 'for', 'add', '##eral', 'x', '##r', 'started', 'taking', 'r', '##x', 'for', '25', '##mg', 'add', '##eral', '##l', 'x', '##r', 'last', 'week', '.', 'had', 'some', 'experience', 'taking', 'ir', \"'\", 's', 'er', ',', 'informally', ',', 'during', 'college', 'so', 'i', 'knew', 'what', 'to', 'expect', 'in', 'terms', 'of', 'the', 'effects', 'dull', '##ing', 'over', 'time', 'if', 'taken', 'too', 'frequently', '.', 'sure', 'the', 'doctor', 'says', 'take', 'it', 'every', 'day', ',', 'but', 'i', \"'\", 'm', 'horrified', 'by', 'many', 'people', \"'\", 's', 'stories', 'of', 'quick', 'tolerance', ',', 'adverse', 'side', 'effects', ',', 'and', 'monster', 'withdrawal', 'symptoms', '.', '.', '.', 'so', 'last', 'week', 'i', 'took', 'it', '3', 'days', 'in', 'a', 'row', 'then', 'took', 'two', 'days', 'off', 'and', 'felt', 'fine', '.', 'this', 'week', 'i', 'took', 'one', 'yesterday', ',', 'but', 'today', 'i', 'feel', 'absolutely', 'brain', 'dead', 'and', 'dead', 'tired', 'like', 'i', \"'\", 've', 'been', 'hit', 'by', 'a', 'truck', '.', 'i', 'should', 'mention', 'that', 'i', \"'\", 'm', 'diet', '##ing', 'and', 'doing', 'moderate', 'card', '##io', 'exercise', 'but', 'that', 'didn', \"'\", 't', 'interfere', 'with', 'my', 'energy', 'when', 'i', 'took', 'the', 'break', 'last', 'week', '.', 'i', \"'\", 've', 'got', 'a', 'big', 'research', 'project', 'this', 'semester', 'and', 'a', 'few', 'smaller', '20', '-', '30', 'page', 'papers', 'so', 'i', \"'\", 'm', 'hoping', 'to', 'maybe', 'take', 'the', 'add', '##eral', '##l', '3', '-', '4', 'times', 'a', 'week', '.', '.', '.', 'but', 'what', 'can', 'i', 'do', 'to', 'minimize', 'withdrawal', '##s', 'and', 'avoid', 'building', 'up', 'a', 'tolerance', '?', 'should', 'i', 'maybe', 'take', 'a', 'dose', 'and', 'a', 'half', ',', '2', '-', '3', 'days', 'a', 'week', 'instead', '?', 'coffee', 'on', 'the', 'days', 'off', '?', 'looking', 'forward', 'to', 'hearing', 'you', 'guys', \"'\", 'experience', 'with', 'this', '.', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 265\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['avoiding', 'tolerance', '/', 'withdrawal', '##s', 'for', 'add', '##eral', 'x', '##r', 'started', 'taking', 'r', '##x', 'for', '25', '##mg', 'add', '##eral', '##l', 'x', '##r', 'last', 'week', '.', 'had', 'some', 'experience', 'taking', 'ir', \"'\", 's', 'er', ',', 'informally', ',', 'during', 'college', 'so', 'i', 'knew', 'what', 'to', 'expect', 'in', 'terms', 'of', 'the', 'effects', 'dull', '##ing', 'over', 'time', 'if', 'taken', 'too', 'frequently', '.', 'sure', 'the', 'doctor', 'says', 'take', 'it', 'every', 'day', ',', 'but', 'i', \"'\", 'm', 'horrified', 'by', 'many', 'people', \"'\", 's', 'stories', 'of', 'quick', 'tolerance', ',', 'adverse', 'side', 'effects', ',', 'and', 'monster', 'withdrawal', 'symptoms', '.', '.', '.', 'so', 'last', 'week', 'i', 'took', 'it', '3', 'days', 'in', 'a', 'row', 'then', 'took', 'two', 'days', 'off', 'and', 'felt', 'fine', '.', 'this', 'week', 'i', 'took', 'one', 'yesterday', ',', 'but', 'today', 'i', 'feel', 'absolutely', 'brain', 'dead', 'and', 'dead', 'tired', 'like', 'i', \"'\", 've', 'been', 'hit', 'by', 'a', 'truck', '.', 'i', 'should', 'mention', 'that', 'i', \"'\", 'm', 'diet', '##ing', 'and', 'doing', 'moderate', 'card', '##io', 'exercise', 'but', 'that', 'didn', \"'\", 't', 'interfere', 'with', 'my', 'energy', 'when', 'i', 'took', 'the', 'break', 'last', 'week', '.', 'i', \"'\", 've', 'got', 'a', 'big', 'research', 'project', 'this', 'semester', 'and', 'a', 'few', 'smaller', '20', '-', '30', 'page', 'papers', 'so', 'i', \"'\", 'm', 'hoping', 'to', 'maybe', 'take', 'the', 'add', '##eral', '##l', '3', '-', '4', 'times', 'a', 'week', '.', '.', '.', 'but', 'what', 'can', 'i', 'do', 'to', 'minimize', 'withdrawal', '##s', 'and', 'avoid', 'building', 'up', 'a', 'tolerance', '?', 'should', 'i', 'maybe', 'take', 'a', 'dose', 'and', 'a', 'half', ',', '2', '-', '3', 'days', 'a', 'week', 'instead', '?', 'coffee', 'on', 'the', 'days', 'off', '?', 'looking', 'forward', 'to', 'hearing', 'you', 'guys', \"'\", 'experience', 'with', 'this', '.', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'i', 'think', 'of', 'all', 'the', 'annoying', 'prep', '##py', 'girls', 'at', 'my', 'school']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'i', 'think', 'of', 'all', 'the', 'annoying', 'prep', '##py', 'girls', 'at', 'my', 'school']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'this', 'supposed', 'to', 'happen', '?', '(', 'on', '27', '##mg', 'of', 'con', '##ser', '##ta', ')', 'so', 'i', 'have', 'been', 'taken', '27', '##mg', 'of', 'con', '##ser', '##ta', 'daily', 'in', 'the', 'morning', 'for', 'school', '(', 'year', '11', ')', 'but', 'lately', 'i', 'have', 'been', 'having', 'a', 'strange', 'side', 'effect', 'it', 'would', 'seem', ',', 'after', 'school', '4', '-', '5', 'pm', 'i', 'have', 'been', 'becoming', 'overly', 'ir', '##rita', '##ble', 'and', 'angry', ',', 'i', 'have', 'no', 'idea', 'why', ',', 'this', 'is', 'usually', 'followed', 'by', 'increased', 'emotions', 'about', 'everything', 'e', '.', 'g', '.', 'crying', 'so', 'as', 'silly', 'as', 'my', 'story', 'sound', 'i', 'just', 'want', 'to', 'know', 'if', 'this', 'is', 'a', 'common', 'side', 'effect', 'or', 'just', 'what', 'is', 'going', 'on', '?']\n",
      "INFO:__main__:Number of tokens: 113\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'this', 'supposed', 'to', 'happen', '?', '(', 'on', '27', '##mg', 'of', 'con', '##ser', '##ta', ')', 'so', 'i', 'have', 'been', 'taken', '27', '##mg', 'of', 'con', '##ser', '##ta', 'daily', 'in', 'the', 'morning', 'for', 'school', '(', 'year', '11', ')', 'but', 'lately', 'i', 'have', 'been', 'having', 'a', 'strange', 'side', 'effect', 'it', 'would', 'seem', ',', 'after', 'school', '4', '-', '5', 'pm', 'i', 'have', 'been', 'becoming', 'overly', 'ir', '##rita', '##ble', 'and', 'angry', ',', 'i', 'have', 'no', 'idea', 'why', ',', 'this', 'is', 'usually', 'followed', 'by', 'increased', 'emotions', 'about', 'everything', 'e', '.', 'g', '.', 'crying', 'so', 'as', 'silly', 'as', 'my', 'story', 'sound', 'i', 'just', 'want', 'to', 'know', 'if', 'this', 'is', 'a', 'common', 'side', 'effect', 'or', 'just', 'what', 'is', 'going', 'on', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['music', 'that', 'helps', 'you', 'focus', '.', 'hi', 'there', ',', 'i', \"'\", 'm', 'someone', 'who', 'finds', 'immense', 'benefit', 'from', 'listening', 'to', 'good', 'progressive', 'music', 'to', 'help', 'me', 'focus', 'while', 'studying', ',', 'writing', ',', 'working', ',', 'climbing', '-', 'pretty', 'much', 'anything', '-', 'and', 'so', 'i', 'thought', 'i', \"'\", 'd', 'share', 'some', 'music', 'that', 'is', 'basically', 'the', 'no', '##uri', '##sh', '##ment', 'of', 'my', 'soul', '.', 'if', 'you', 'have', 'some', 'music', 'of', 'any', 'genre', 'that', 'you', 'would', 'recommend', 'please', 'share', 'with', 'me', '.', 'the', 'impossible', 'to', 'be', 'unhappy', 'song', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'hi', '##3', '##sl', '##5', '##st', '##fi', '##4', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'j', '##qa', '##8', '##75', '##c', '##f', '##d', '##v', '##m', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'rt', '##vc', '##en', '##zd', '##bf', '##w', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'x', '##j', '##vb', '##h', '##2', '##hc', '##on', '##m', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'c', '##z', '##v', '##k', '##9', '_', 'cy', '##0', '##u', '##q', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'nl', '##jk', '##i', '##5', '##oe', '##j', '-', 'u', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'iv', '##go', '##6', '##vn', '##d', '-', '0', '##e', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'd', '##x', '##ci', '##h', '##fr', '##h', '##f', '##hy', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'w', '##gy', '##m', '##2', '##q', '##b', '##vr', '##rk', '&', 'feature', '=', 'related']\n",
      "INFO:__main__:Number of tokens: 288\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['music', 'that', 'helps', 'you', 'focus', '.', 'hi', 'there', ',', 'i', \"'\", 'm', 'someone', 'who', 'finds', 'immense', 'benefit', 'from', 'listening', 'to', 'good', 'progressive', 'music', 'to', 'help', 'me', 'focus', 'while', 'studying', ',', 'writing', ',', 'working', ',', 'climbing', '-', 'pretty', 'much', 'anything', '-', 'and', 'so', 'i', 'thought', 'i', \"'\", 'd', 'share', 'some', 'music', 'that', 'is', 'basically', 'the', 'no', '##uri', '##sh', '##ment', 'of', 'my', 'soul', '.', 'if', 'you', 'have', 'some', 'music', 'of', 'any', 'genre', 'that', 'you', 'would', 'recommend', 'please', 'share', 'with', 'me', '.', 'the', 'impossible', 'to', 'be', 'unhappy', 'song', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'hi', '##3', '##sl', '##5', '##st', '##fi', '##4', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'j', '##qa', '##8', '##75', '##c', '##f', '##d', '##v', '##m', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'rt', '##vc', '##en', '##zd', '##bf', '##w', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'x', '##j', '##vb', '##h', '##2', '##hc', '##on', '##m', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'c', '##z', '##v', '##k', '##9', '_', 'cy', '##0', '##u', '##q', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'nl', '##jk', '##i', '##5', '##oe', '##j', '-', 'u', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'iv', '##go', '##6', '##vn', '##d', '-', '0', '##e', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'd', '##x', '##ci', '##h', '##fr', '##h', '##f', '##hy', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'w', '##gy', '##m', '##2', '##q', '##b', '##vr', '##rk', '&', 'feature', '=', 'related']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'started', 'med', '##s', 'about', 'a', 'week', 'ago', '.', 'help', 'me', 'with', 'decreased', 'appetite', '?', 'last', 'friday', 'i', 'went', 'to', 'the', 'doctor', 'with', 'my', 'concerns', 'about', 'my', 'ad', '##hd', 'and', 'he', 'prescribed', 'me', 'st', '##rat', '##tera', 'and', 'recommended', 'i', 'see', 'a', 'psychiatrist', 'when', 'i', 'went', 'back', 'to', 'school', '.', 'since', 'i', 'started', 'taking', 'it', 'on', 'friday', ',', 'i', \"'\", 've', 'had', 'absolutely', 'no', 'appetite', '.', 'i', 'think', 'i', \"'\", 've', 'actually', 'felt', 'hungry', 'twice', '.', 'over', 'the', 'weekend', ',', 'i', 'had', 'been', 'smoking', 'weed', 'so', 'i', 'could', 'eat', ',', 'but', 'since', 'st', '##rat', '##tera', 'is', 'breaking', 'the', 'bank', 'as', 'it', 'is', ',', 'i', \"'\", 've', 'given', 'up', 'weed', '.', 'i', 'just', 'tried', 'to', 'eat', 'breakfast', 'so', 'i', 'could', 'take', 'my', 'med', '##s', 'and', 'i', 'couldn', \"'\", 't', 'even', 'eat', '100', 'cal', '##ories', 'before', 'i', 'felt', 'full', 'and', 'not', 'even', '200', 'before', 'i', 'started', 'feeling', 'bad', '.', 'this', 'is', 'affecting', 'a', 'lot', 'of', 'different', 'things', 'in', 'my', 'life', 'because', 'i', \"'\", 'm', 'tired', 'all', 'the', 'time', 'now', '.', 'i', 'like', 'to', 'work', 'out', ',', 'but', 'every', 'time', 'i', \"'\", 've', 'tried', 'i', \"'\", 've', 'ended', 'up', 'almost', 'passing', 'out', 'on', 'a', 'tread', '##mill', 'or', 'almost', 'throwing', 'up', '.', 'i', 'also', 'find', 'that', 'the', 'med', '##s', 'aren', \"'\", 't', 'working', 'the', 'best', 'because', 'i', 'don', \"'\", 't', 'have', 'enough', 'energy', 'to', 'focus', 'most', 'of', 'the', 'time', '.', 'it', 'also', 'has', 'made', 'me', 'extremely', 'ir', '##rita', '##ble', ',', 'headache', '##s', 'have', 'become', 'more', 'frequent', ',', 'and', 'i', \"'\", 'm', 'just', 'generally', 'in', 'a', 'bad', 'mood', '.', 'anyone', 'have', 'any', 'suggestions', 'on', 'how', 'i', 'can', 'eat', 'more', 'or', 'get', 'this', 'medicine', 'to', 'be', 'most', 'beneficial', 'to', 'me', '?']\n",
      "INFO:__main__:Number of tokens: 274\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'started', 'med', '##s', 'about', 'a', 'week', 'ago', '.', 'help', 'me', 'with', 'decreased', 'appetite', '?', 'last', 'friday', 'i', 'went', 'to', 'the', 'doctor', 'with', 'my', 'concerns', 'about', 'my', 'ad', '##hd', 'and', 'he', 'prescribed', 'me', 'st', '##rat', '##tera', 'and', 'recommended', 'i', 'see', 'a', 'psychiatrist', 'when', 'i', 'went', 'back', 'to', 'school', '.', 'since', 'i', 'started', 'taking', 'it', 'on', 'friday', ',', 'i', \"'\", 've', 'had', 'absolutely', 'no', 'appetite', '.', 'i', 'think', 'i', \"'\", 've', 'actually', 'felt', 'hungry', 'twice', '.', 'over', 'the', 'weekend', ',', 'i', 'had', 'been', 'smoking', 'weed', 'so', 'i', 'could', 'eat', ',', 'but', 'since', 'st', '##rat', '##tera', 'is', 'breaking', 'the', 'bank', 'as', 'it', 'is', ',', 'i', \"'\", 've', 'given', 'up', 'weed', '.', 'i', 'just', 'tried', 'to', 'eat', 'breakfast', 'so', 'i', 'could', 'take', 'my', 'med', '##s', 'and', 'i', 'couldn', \"'\", 't', 'even', 'eat', '100', 'cal', '##ories', 'before', 'i', 'felt', 'full', 'and', 'not', 'even', '200', 'before', 'i', 'started', 'feeling', 'bad', '.', 'this', 'is', 'affecting', 'a', 'lot', 'of', 'different', 'things', 'in', 'my', 'life', 'because', 'i', \"'\", 'm', 'tired', 'all', 'the', 'time', 'now', '.', 'i', 'like', 'to', 'work', 'out', ',', 'but', 'every', 'time', 'i', \"'\", 've', 'tried', 'i', \"'\", 've', 'ended', 'up', 'almost', 'passing', 'out', 'on', 'a', 'tread', '##mill', 'or', 'almost', 'throwing', 'up', '.', 'i', 'also', 'find', 'that', 'the', 'med', '##s', 'aren', \"'\", 't', 'working', 'the', 'best', 'because', 'i', 'don', \"'\", 't', 'have', 'enough', 'energy', 'to', 'focus', 'most', 'of', 'the', 'time', '.', 'it', 'also', 'has', 'made', 'me', 'extremely', 'ir', '##rita', '##ble', ',', 'headache', '##s', 'have', 'become', 'more', 'frequent', ',', 'and', 'i', \"'\", 'm', 'just', 'generally', 'in', 'a', 'bad', 'mood', '.', 'anyone', 'have', 'any', 'suggestions', 'on', 'how', 'i', 'can', 'eat', 'more', 'or', 'get', 'this', 'medicine', 'to', 'be', 'most', 'beneficial', 'to', 'me', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['med', '##icated', ',', 'but', 'still', 'feeling', 'lost', 'and', 'hopeless', '.', 'any', 'personal', 'advice', '?', '(', 'warning', ':', 'long', 'post', ')']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['med', '##icated', ',', 'but', 'still', 'feeling', 'lost', 'and', 'hopeless', '.', 'any', 'personal', 'advice', '?', '(', 'warning', ':', 'long', 'post', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'first', 'med', '##s', '-', 'st', '##rat', '##tera', '.', '.', 'stand', '-', 'off', '##ish', 'doctor', '.', '.', 'advice', 'needed', '.', 'i', 'am', 'an', 'adult', '(', '31', ')', 'with', 'pretty', 'bad', 'ad', '##hd', '.', 'i', 'saw', 'a', 'doctor', 'recently', 'and', 'he', 'prescribed', 'st', '##rat', '##tera', '.', 'now', 'i', 'will', 'be', '100', 'percent', 'honest', 'i', 'really', 'wanted', 'to', 'try', 'add', '##eral', ',', 'only', 'for', 'the', 'fact', 'that', 'it', 'seemed', 'like', 'something', 'that', 'would', 'really', 'help', 'me', '.', '.', 'its', 'benefits', 'seemed', 'exactly', 'like', 'what', 'i', 'need', '.', 'i', 'mentioned', 'to', 'the', 'doctor', 'id', 'like', 'to', 'try', 'add', '##eral', 'and', 'i', 'was', 'app', '##re', '##hen', '##sive', 'about', 'trying', 'a', 'somewhat', 'new', 'drug', 'that', 'will', 'be', 'working', 'with', 'my', 'brain', '.', 'plus', 'i', 'hear', 'the', 'side', 'effects', 'are', 'not', 'good', '.', 'his', 'demeanor', 'instantly', 'changed', 'and', 'he', 'basically', 'threw', 'the', 'prescription', 'in', 'my', 'lap', 'and', 'said', '\"', 'i', 'am', 'not', 'writing', 'you', 'a', 'r', '##x', 'for', 'add', '##eral', '.', 'take', 'this', 'and', 'see', 'you', 'in', '30', 'days', '\"', 'and', 'suddenly', 'became', 'mean', '.', 'i', 'am', 'not', 'seeking', 'any', 'drug', 'for', 'abuse', ',', 'i', 'truly', 'want', 'to', 'just', 'get', 'right', 'and', 'stop', 'this', 'problem', 'so', 'i', 'can', 'be', 'the', 'best', 'i', 'can', 'be', '.', 'should', 'i', 'seek', 'another', 'doctor', '?', 'should', 'i', 'try', 'st', '##rate', '##rra', '?', 'i', 'don', '##t', 'want', 'to', 'be', 'any', 'more', 'let', '##har', '##gic', 'than', 'i', 'already', 'am', '.', '.', '.', 'thanks', 'in', 'advance', 'for', 'any', 'insight', 'anyone', 'can', 'give']\n",
      "INFO:__main__:Number of tokens: 239\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'first', 'med', '##s', '-', 'st', '##rat', '##tera', '.', '.', 'stand', '-', 'off', '##ish', 'doctor', '.', '.', 'advice', 'needed', '.', 'i', 'am', 'an', 'adult', '(', '31', ')', 'with', 'pretty', 'bad', 'ad', '##hd', '.', 'i', 'saw', 'a', 'doctor', 'recently', 'and', 'he', 'prescribed', 'st', '##rat', '##tera', '.', 'now', 'i', 'will', 'be', '100', 'percent', 'honest', 'i', 'really', 'wanted', 'to', 'try', 'add', '##eral', ',', 'only', 'for', 'the', 'fact', 'that', 'it', 'seemed', 'like', 'something', 'that', 'would', 'really', 'help', 'me', '.', '.', 'its', 'benefits', 'seemed', 'exactly', 'like', 'what', 'i', 'need', '.', 'i', 'mentioned', 'to', 'the', 'doctor', 'id', 'like', 'to', 'try', 'add', '##eral', 'and', 'i', 'was', 'app', '##re', '##hen', '##sive', 'about', 'trying', 'a', 'somewhat', 'new', 'drug', 'that', 'will', 'be', 'working', 'with', 'my', 'brain', '.', 'plus', 'i', 'hear', 'the', 'side', 'effects', 'are', 'not', 'good', '.', 'his', 'demeanor', 'instantly', 'changed', 'and', 'he', 'basically', 'threw', 'the', 'prescription', 'in', 'my', 'lap', 'and', 'said', '\"', 'i', 'am', 'not', 'writing', 'you', 'a', 'r', '##x', 'for', 'add', '##eral', '.', 'take', 'this', 'and', 'see', 'you', 'in', '30', 'days', '\"', 'and', 'suddenly', 'became', 'mean', '.', 'i', 'am', 'not', 'seeking', 'any', 'drug', 'for', 'abuse', ',', 'i', 'truly', 'want', 'to', 'just', 'get', 'right', 'and', 'stop', 'this', 'problem', 'so', 'i', 'can', 'be', 'the', 'best', 'i', 'can', 'be', '.', 'should', 'i', 'seek', 'another', 'doctor', '?', 'should', 'i', 'try', 'st', '##rate', '##rra', '?', 'i', 'don', '##t', 'want', 'to', 'be', 'any', 'more', 'let', '##har', '##gic', 'than', 'i', 'already', 'am', '.', '.', '.', 'thanks', 'in', 'advance', 'for', 'any', 'insight', 'anyone', 'can', 'give']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['thanks', 'hi', ',', 'i', \"'\", 've', 'posted', 'a', 'couple', 'of', 'times', 'here', ',', 'and', 'lu', '##rked', 'quite', 'a', 'bit', 'more', '.', 'ever', 'since', 'i', 'was', 'young', ',', 'i', \"'\", 've', 'had', 'this', 'problem', 'that', 'i', \"'\", 'd', 'never', 'been', 'able', 'to', 'put', 'my', 'finger', 'on', '.', 'i', 'couldn', \"'\", 't', 'remember', 'things', ',', 'couldn', \"'\", 't', 'stay', 'organized', 'or', 'manage', 'my', 'time', ',', 'couldn', \"'\", 't', '\"', 'buckle', 'down', '\"', 'and', 'get', 'things', 'done', '.', 'after', 'visiting', 'r', '/', 'add', 'and', 'then', 'r', '/', 'ad', '##hd', ',', 'i', 'decided', 'to', 'talk', 'to', 'my', 'doctor', '.', 'i', \"'\", 've', 'been', 'on', 'add', '##eral', '##l', 'for', 'nearly', 'a', 'month', 'now', ',', 'and', 'the', 'change', 'is', 'amazing', '.', 'i', 'still', 'hop', 'around', 'a', 'bit', ',', 'but', 'i', 'can', 'focus', 'when', 'i', 'need', 'to', '.', 'i', 'get', 'things', 'done', '-', 'i', 'wrote', 'my', 'first', 'honest', '-', 'to', '-', 'god', 'program', 'the', 'other', 'day', '.', 'my', 'mood', 'has', 'also', 'gone', 'up', 'im', '##me', '##as', '##ura', '##bly', ';', 'i', 'no', 'longer', 'snap', 'at', 'my', 'friends', 'or', 'family', ',', 'and', 'i', 'no', 'longer', 'wake', 'up', 'in', 'a', 'foul', 'mood', '.', 'i', \"'\", 'm', 'building', 'good', 'habits', 'and', 'getting', 'better', 'at', 'managing', 'my', 'time', 'and', 'tasks', '.', 'hell', ',', 'i', \"'\", 've', 'even', 'lost', '15', 'pounds', '!', 'anyway', ',', 'i', 'just', 'wanted', 'to', 'say', 'thanks', 'for', 'the', 'information', ',', 'and', 'for', 'giving', 'me', 'the', 'nu', '##dge', 'in', 'the', 'right', 'direction', 'i', 'needed', '.']\n",
      "INFO:__main__:Number of tokens: 235\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['thanks', 'hi', ',', 'i', \"'\", 've', 'posted', 'a', 'couple', 'of', 'times', 'here', ',', 'and', 'lu', '##rked', 'quite', 'a', 'bit', 'more', '.', 'ever', 'since', 'i', 'was', 'young', ',', 'i', \"'\", 've', 'had', 'this', 'problem', 'that', 'i', \"'\", 'd', 'never', 'been', 'able', 'to', 'put', 'my', 'finger', 'on', '.', 'i', 'couldn', \"'\", 't', 'remember', 'things', ',', 'couldn', \"'\", 't', 'stay', 'organized', 'or', 'manage', 'my', 'time', ',', 'couldn', \"'\", 't', '\"', 'buckle', 'down', '\"', 'and', 'get', 'things', 'done', '.', 'after', 'visiting', 'r', '/', 'add', 'and', 'then', 'r', '/', 'ad', '##hd', ',', 'i', 'decided', 'to', 'talk', 'to', 'my', 'doctor', '.', 'i', \"'\", 've', 'been', 'on', 'add', '##eral', '##l', 'for', 'nearly', 'a', 'month', 'now', ',', 'and', 'the', 'change', 'is', 'amazing', '.', 'i', 'still', 'hop', 'around', 'a', 'bit', ',', 'but', 'i', 'can', 'focus', 'when', 'i', 'need', 'to', '.', 'i', 'get', 'things', 'done', '-', 'i', 'wrote', 'my', 'first', 'honest', '-', 'to', '-', 'god', 'program', 'the', 'other', 'day', '.', 'my', 'mood', 'has', 'also', 'gone', 'up', 'im', '##me', '##as', '##ura', '##bly', ';', 'i', 'no', 'longer', 'snap', 'at', 'my', 'friends', 'or', 'family', ',', 'and', 'i', 'no', 'longer', 'wake', 'up', 'in', 'a', 'foul', 'mood', '.', 'i', \"'\", 'm', 'building', 'good', 'habits', 'and', 'getting', 'better', 'at', 'managing', 'my', 'time', 'and', 'tasks', '.', 'hell', ',', 'i', \"'\", 've', 'even', 'lost', '15', 'pounds', '!', 'anyway', ',', 'i', 'just', 'wanted', 'to', 'say', 'thanks', 'for', 'the', 'information', ',', 'and', 'for', 'giving', 'me', 'the', 'nu', '##dge', 'in', 'the', 'right', 'direction', 'i', 'needed', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['resource', 'list', ':', 'please', 'add', 'stuff', 'websites', '/', 'general', 'information', ':', 'totally', '##ad', '##d', '.', 'com', 'http', ':', '/', '/', 'www', '.', 'help', '##guide', '.', 'org', '/', 'topics', '/', 'adult', '_', 'ad', '##hd', '.', 'h', '##tm', 'http', ':', '/', '/', 'ia', '##dd', '##vant', '##age', '.', 'com', '/', 'books', '%', '20', '%', '26', '%', '20', '##int', '##ern', '##et', '%', '20', '##res', '##our', '##ces', '.', 'h', '##tm', 'http', ':', '/', '/', 'www', '.', 'edge', '##fo', '##unda', '##tion', '.', 'org', '/', 'blog', '/', '(', 'from', 'an', 'ad', '##hd', 'coach', ')', 'http', ':', '/', '/', 'help', '##4', '##ad', '##hd', '.', 'org', '/', 'http', ':', '/', '/', 'www', '.', 'scattered', '##mind', '##s', '.', 'com', '/', 'about', '.', 'h', '##tm', 'podcast', '##s', 'ad', '##hd', 'weekly', 'http', ':', '/', '/', 'itunes', '.', 'apple', '.', 'com', '/', 'us', '/', 'podcast', '/', 'ad', '##hd', '-', 'weekly', '/', 'id', '##34', '##6', '##24', '##9', '##66', '##9', 'more', 'attention', 'less', 'deficit', 'podcast', 'http', ':', '/', '/', 'itunes', '.', 'apple', '.', 'com', '/', 'us', '/', 'podcast', '/', 'more', '-', 'attention', '-', 'less', '-', 'deficit', '/', 'id', '##31', '##28', '##31', '##48', '##5', 'magazine', 'add', '##itt', '##ude', '(', 'both', 'kids', 'and', 'adults', 'and', 'has', 'tons', 'of', 'resources', 'on', 'their', 'website', ')', 'http', ':', '/', '/', 'www', '.', 'add', '##itude', '##ma', '##g', '.', 'com', '/', 'books', 'ad', '##hd', 'friendly', 'ways', 'to', 'organize', 'your', 'life', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'add', '-', 'friendly', '-', 'ways', '-', 'organize', '-', 'your', '-', 'life', '/', 'd', '##p', '/', '158', '##39', '##13', '##58', '##0', 'delivered', 'from', 'distraction', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'delivered', '-', 'distraction', '-', 'getting', '-', 'attention', '-', 'disorder', '/', 'd', '##p', '/', '03', '##45', '##44', '##23', '##0', '##x', 'driven', 'to', 'distraction', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'driven', '-', 'distraction', '-', 'revised', '-', 'recognizing', '-', 'attention', '/', 'd', '##p', '/', '03', '##0', '##7', '##7', '##43', '##15', '##2', '/', 'ref', '=', 'sr', '_', '1', '_', '1', '?', 's', '=', 'books', '&', 'ie', '=', 'ut', '##f', '##8', '&', 'qi', '##d', '=', '133', '##0', '##6', '##31', '##80', '##9', '&', 'sr', '=', '1', '-', '1', 'cognitive', 'behavioral', 'therapy', 'for', 'adult', 'ad', '##hd', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'cognitive', '-', 'behavioral', '-', 'therapy', '-', 'adult', '-', 'ad', '##hd', '-', 'dysfunction', '/', 'd', '##p', '/', '1609', '##18', '##13', '##1', '##x', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'adult', '-', 'psycho', '##therapy', '-', 'homework', '-', 'planner', '-', 'practice', '##pl', '##anne', '##rs', '/', 'd', '##p', '/', '04', '##7', '##17', '##6', '##34', '##38', '(', 'my', 'therapist', 'gave', 'me', 'a', 'exercise', 'to', 'fill', 'out', 'from', 'this', 'book', 'titled', '“', 'symptoms', 'and', 'fix', '##es', 'for', 'add', '”', ')', 'more', 'attention', 'less', 'deficit', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'more', '-', 'attention', '-', 'less', '-', 'deficit', '-', 'strategies', '/', 'd', '##p', '/', '1886', '##9', '##41', '##7', '##42', '/', 'ref', '=', 'sr', '_', '1', '_', '1', '?', 's', '=', 'books', '&', 'ie', '=', 'ut', '##f', '##8', '&', 'qi', '##d', '=', '133', '##0', '##6', '##32', '##00', '##4', '&', 'sr', '=', '1', '-', '1', 'do', 'i', 'have', 'ad', '##hd', '/', 'and', 'diagnosis', '?', 'http', ':', '/', '/', 'help', '##4', '##ad', '##hd', '.', 'org', '/', 'en', '/', 'treatment', '/', 'scales', '.', '.', '.', 'need', 'help', 'with', 'this', 'one', 'treatment', 'medication', 'chart', 'from', 'web', '##md', 'http', ':', '/', '/', 'www', '.', 'web', '##md', '.', 'com', '/', 'add', '-', 'ad', '##hd', '/', 'ad', '##hd', '-', 'medication', '-', 'chart', 'medication', 'price', 'guide', 'http', ':', '/', '/', 'www', '.', 'consumer', '##re', '##ports', '.', 'org', '/', 'health', '/', 'resources', '/', 'pdf', '/', 'best', '-', 'buy', '-', 'drugs', '/', '2', '##page', '##r', '_', 'ad', '##hd', '.', 'pdf', 'co', '##gm', '##ed', 'treatment', '(', 'non', 'medication', 'treatment', 'using', 'brain', 'training', ')', 'http', ':', '/', '/', 'www', '.', 'co', '##gm', '##ed', '.', 'com', '/', 'cognitive', 'behavioral', 'therapy', '(', 'non', 'medication', 'treatment', ')', 'http', ':', '/', '/', 'www', '.', 'add', '##itude', '##ma', '##g', '.', 'com', '/', 'ad', '##hd', '/', 'article', '/', '91', '##2', '.', 'html', 'managing', 'your', 'life', 'studying', '/', 'r', '/', 'gets', '##tu', '##dy', '##ing', 'love', 'life', 'http', ':', '/', '/', 'www', '.', 'help', '##4', '##ad', '##hd', '.', 'org', '/', 'en', '/', 'living', '/', 're', '##lands', '##oc', '/', 'marriage', 'home', 'organization', 'http', ':', '/', '/', 'www', '.', 'fly', '##lad', '##y', '.', 'net', 'music', '(', 'for', 'concentration', ')', 'http', ':', '/', '/', 'chatter', '##block', '##er', '.', 'com', '/', 'http', ':', '/', '/', 'www', '.', 'calm', '##so', '##und', '.', 'com', '/', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'hi', '##3', '##sl', '##5', '##st', '##fi', '##4', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'j', '##qa', '##8', '##75', '##c', '##f', '##d', '##v', '##m', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'rt', '##vc', '##en', '##zd', '##bf', '##w', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'x', '##j', '##vb', '##h', '##2', '##hc', '##on', '##m', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'c', '##z', '##v', '##k', '##9', '_', 'cy', '##0', '##u', '##q', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'nl', '##jk', '##i', '##5', '##oe', '##j', '-', 'u', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'iv', '##go', '##6', '##vn', '##d', '-', '0', '##e', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'd', '##x', '##ci', '##h', '##fr', '##h', '##f', '##hy', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'w', '##gy', '##m', '##2', '##q', '##b', '##vr', '##rk', '&', 'feature', '=', 'related', 'explosions', 'in', 'the', 'sky', 'edit', ':', 'weird', 'format', '##ting']\n",
      "INFO:__main__:Number of tokens: 944\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['resource', 'list', ':', 'please', 'add', 'stuff', 'websites', '/', 'general', 'information', ':', 'totally', '##ad', '##d', '.', 'com', 'http', ':', '/', '/', 'www', '.', 'help', '##guide', '.', 'org', '/', 'topics', '/', 'adult', '_', 'ad', '##hd', '.', 'h', '##tm', 'http', ':', '/', '/', 'ia', '##dd', '##vant', '##age', '.', 'com', '/', 'books', '%', '20', '%', '26', '%', '20', '##int', '##ern', '##et', '%', '20', '##res', '##our', '##ces', '.', 'h', '##tm', 'http', ':', '/', '/', 'www', '.', 'edge', '##fo', '##unda', '##tion', '.', 'org', '/', 'blog', '/', '(', 'from', 'an', 'ad', '##hd', 'coach', ')', 'http', ':', '/', '/', 'help', '##4', '##ad', '##hd', '.', 'org', '/', 'http', ':', '/', '/', 'www', '.', 'scattered', '##mind', '##s', '.', 'com', '/', 'about', '.', 'h', '##tm', 'podcast', '##s', 'ad', '##hd', 'weekly', 'http', ':', '/', '/', 'itunes', '.', 'apple', '.', 'com', '/', 'us', '/', 'podcast', '/', 'ad', '##hd', '-', 'weekly', '/', 'id', '##34', '##6', '##24', '##9', '##66', '##9', 'more', 'attention', 'less', 'deficit', 'podcast', 'http', ':', '/', '/', 'itunes', '.', 'apple', '.', 'com', '/', 'us', '/', 'podcast', '/', 'more', '-', 'attention', '-', 'less', '-', 'deficit', '/', 'id', '##31', '##28', '##31', '##48', '##5', 'magazine', 'add', '##itt', '##ude', '(', 'both', 'kids', 'and', 'adults', 'and', 'has', 'tons', 'of', 'resources', 'on', 'their', 'website', ')', 'http', ':', '/', '/', 'www', '.', 'add', '##itude', '##ma', '##g', '.', 'com', '/', 'books', 'ad', '##hd', 'friendly', 'ways', 'to', 'organize', 'your', 'life', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'add', '-', 'friendly', '-', 'ways', '-', 'organize', '-', 'your', '-', 'life', '/', 'd', '##p', '/', '158', '##39', '##13', '##58', '##0', 'delivered', 'from', 'distraction', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'delivered', '-', 'distraction', '-', 'getting', '-', 'attention', '-', 'disorder', '/', 'd', '##p', '/', '03', '##45', '##44', '##23', '##0', '##x', 'driven', 'to', 'distraction', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'driven', '-', 'distraction', '-', 'revised', '-', 'recognizing', '-', 'attention', '/', 'd', '##p', '/', '03', '##0', '##7', '##7', '##43', '##15', '##2', '/', 'ref', '=', 'sr', '_', '1', '_', '1', '?', 's', '=', 'books', '&', 'ie', '=', 'ut', '##f', '##8', '&', 'qi', '##d', '=', '133', '##0', '##6', '##31', '##80', '##9', '&', 'sr', '=', '1', '-', '1', 'cognitive', 'behavioral', 'therapy', 'for', 'adult', 'ad', '##hd', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'cognitive', '-', 'behavioral', '-', 'therapy', '-', 'adult', '-', 'ad', '##hd', '-', 'dysfunction', '/', 'd', '##p', '/', '1609', '##18', '##13', '##1', '##x', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'adult', '-', 'psycho', '##therapy', '-', 'homework', '-', 'planner', '-', 'practice', '##pl', '##anne', '##rs', '/', 'd', '##p', '/', '04', '##7', '##17', '##6', '##34', '##38', '(', 'my', 'therapist', 'gave', 'me', 'a', 'exercise', 'to', 'fill', 'out', 'from', 'this', 'book', 'titled', '“', 'symptoms', 'and', 'fix', '##es', 'for', 'add', '”', ')', 'more', 'attention', 'less', 'deficit', 'http', ':', '/', '/', 'www', '.', 'amazon', '.', 'com', '/', 'more', '-', 'attention', '-', 'less', '-', 'deficit', '-', 'strategies', '/', 'd', '##p', '/', '1886', '##9', '##41', '##7', '##42', '/', 'ref', '=', 'sr', '_', '1', '_', '1', '?', 's', '=', 'books', '&', 'ie', '=', 'ut', '##f', '##8', '&', 'qi', '##d', '=', '133', '##0', '##6', '##32', '##00', '##4', '&', 'sr', '=', '1', '-', '1', 'do', 'i', 'have', 'ad', '##hd'], ['/', 'and', 'diagnosis', '?', 'http', ':', '/', '/', 'help', '##4', '##ad', '##hd', '.', 'org', '/', 'en', '/', 'treatment', '/', 'scales', '.', '.', '.', 'need', 'help', 'with', 'this', 'one', 'treatment', 'medication', 'chart', 'from', 'web', '##md', 'http', ':', '/', '/', 'www', '.', 'web', '##md', '.', 'com', '/', 'add', '-', 'ad', '##hd', '/', 'ad', '##hd', '-', 'medication', '-', 'chart', 'medication', 'price', 'guide', 'http', ':', '/', '/', 'www', '.', 'consumer', '##re', '##ports', '.', 'org', '/', 'health', '/', 'resources', '/', 'pdf', '/', 'best', '-', 'buy', '-', 'drugs', '/', '2', '##page', '##r', '_', 'ad', '##hd', '.', 'pdf', 'co', '##gm', '##ed', 'treatment', '(', 'non', 'medication', 'treatment', 'using', 'brain', 'training', ')', 'http', ':', '/', '/', 'www', '.', 'co', '##gm', '##ed', '.', 'com', '/', 'cognitive', 'behavioral', 'therapy', '(', 'non', 'medication', 'treatment', ')', 'http', ':', '/', '/', 'www', '.', 'add', '##itude', '##ma', '##g', '.', 'com', '/', 'ad', '##hd', '/', 'article', '/', '91', '##2', '.', 'html', 'managing', 'your', 'life', 'studying', '/', 'r', '/', 'gets', '##tu', '##dy', '##ing', 'love', 'life', 'http', ':', '/', '/', 'www', '.', 'help', '##4', '##ad', '##hd', '.', 'org', '/', 'en', '/', 'living', '/', 're', '##lands', '##oc', '/', 'marriage', 'home', 'organization', 'http', ':', '/', '/', 'www', '.', 'fly', '##lad', '##y', '.', 'net', 'music', '(', 'for', 'concentration', ')', 'http', ':', '/', '/', 'chatter', '##block', '##er', '.', 'com', '/', 'http', ':', '/', '/', 'www', '.', 'calm', '##so', '##und', '.', 'com', '/', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'hi', '##3', '##sl', '##5', '##st', '##fi', '##4', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'j', '##qa', '##8', '##75', '##c', '##f', '##d', '##v', '##m', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'rt', '##vc', '##en', '##zd', '##bf', '##w', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'x', '##j', '##vb', '##h', '##2', '##hc', '##on', '##m', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'c', '##z', '##v', '##k', '##9', '_', 'cy', '##0', '##u', '##q', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'nl', '##jk', '##i', '##5', '##oe', '##j', '-', 'u', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'iv', '##go', '##6', '##vn', '##d', '-', '0', '##e', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'd', '##x', '##ci', '##h', '##fr', '##h', '##f', '##hy', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'w', '##gy', '##m', '##2', '##q', '##b', '##vr', '##rk', '&', 'feature', '=', 'related', 'explosions', 'in', 'the', 'sky', 'edit', ':', 'weird', 'format', '##ting']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'vs', 'shakespeare', '?', 'hey', 'there', '!', 'i', \"'\", 'm', 'sort', 'of', 'a', 'newcomer', 'to', 'r', '/', 'ad', '##hd', ',', 'but', 'i', 'was', 'diagnosed', 'four', 'years', 'ago', ',', 'around', 'the', 'time', 'i', 'turned', '18', '.', 'ever', 'since', 'then', ',', 'it', 'has', 'been', '25', '##mg', 'of', 'add', '##eral', '##l', 'twice', 'per', 'day', ',', 'and', 'heavy', 'planning', '/', 'calendar', '/', 'schedule', 'work', 'to', 'keep', 'track', 'of', 'assignments', ',', 'meetings', ',', 'and', 'other', 'important', 'tasks', '.', 'i', 'am', 'currently', 'studying', 'english', 'and', 'secondary', 'education', ',', 'and', 'up', 'until', 'now', 'i', 'had', 'been', 'passing', 'the', 'majority', 'of', 'my', 'classes', 'with', 'no', 'question', '.', 'that', 'is', ',', 'until', 'i', 'hit', 'a', 'road', '##block', 'with', 'shakespeare', '.', 'of', 'course', ',', 'i', 'understand', 'that', 'shakespeare', 'is', 'supposed', 'to', 'be', 'difficult', ',', 'and', 'that', 'would', 'definitely', 'be', 'a', 'good', 'justification', 'for', 'it', \"'\", 's', 'placement', 'at', 'a', 'college', 'senior', 'level', '.', 'however', 'what', 'i', 'do', 'not', 'understand', 'is', 'this', ':', 'i', 'have', 'gone', 'through', 'shakespeare', 'plays', 'in', 'my', 'life', ',', '(', 'ot', '##hell', '##o', ',', 'macbeth', ',', 'romeo', 'and', 'juliet', ',', 'hamlet', ',', 'etc', ')', 'and', 'even', 'those', 'were', 'in', 'high', 'school', ',', 'before', 'i', 'was', 'diagnosed', '.', 'this', 'time', 'around', ',', 'i', 'am', 'stopped', 'dead', 'in', 'my', 'tracks', '.', 'i', 'end', 'up', 'reading', 'the', 'same', 'lines', 'over', 'and', 'over', 'and', 'over', 'until', 'i', 'realize', 'that', 'it', \"'\", 's', 'taken', 'me', 'six', 'hours', 'to', 'get', 'through', 'the', 'first', 'act', 'of', 'richard', 'ii', ',', 'and', 'that', 'i', 'don', \"'\", 't', 'remember', 'a', 'damn', 'thing', 'i', 'just', 'spent', 'those', 'six', 'hours', 'reading', '.', 'recently', ',', 'i', 'switched', 'to', 'using', 'audio', '##books', 'and', 'have', 'found', 'a', 'much', 'bigger', 'improvement', 'compared', 'to', 'my', 'own', 'reading', ',', 'but', 'the', 'problem', 'still', 'lies', 'in', 'comprehension', '.', 'i', 'am', 'absolutely', 'terrible', 'with', 'retention', 'and', 'memory', ',', 'and', 'have', 'failed', 'every', 'quiz', 'that', 'i', 'have', 'taken', 'this', 'semester', ',', 'when', 'all', 'around', 'me', 'there', 'are', 'kids', 'that', 'scan', 'the', 'book', 'twenty', 'minutes', 'before', 'the', 'quiz', 'and', 'ace', 'it', '.', 'then', ',', 'after', 'the', 'quiz', ',', 'we', 'go', 'over', 'the', 'answers', 'and', 'i', 'feel', 'like', 'a', 'doo', '##fus', ',', 'because', 'it', \"'\", 's', 'stuff', 'i', 'had', 'read', 'and', 'knew', ',', 'but', 'could', 'not', 'remember', '.', 'anyway', '##s', ',', 'anyone', 'else', 'having', 'trouble', 'with', 'my', 'main', 'man', 'willie', 'sha', '##x', ',', 'and', 'does', 'anyone', 'have', 'any', 'tips', 'that', 'could', 'help', 'a', 'fellow', 'reader', 'out', '?', 'there', \"'\", 's', 'probably', 'not', 'nearly', 'enough', 'information', 'here', 'for', 'me', 'to', 'make', 'my', 'point', ',', 'but', 'i', 'can', 'go', 'more', 'into', 'detail', 'about', 'specific', 'things', 'if', 'anyone', 'asks', '.', 'and', 't', '##l', ';', 'dr', ',', 'shakespeare', 'is', 'difficult', ',', 'audio', '##books', 'help', ',', 'any', 'other', 'tips', '?']\n",
      "INFO:__main__:Number of tokens: 436\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'vs', 'shakespeare', '?', 'hey', 'there', '!', 'i', \"'\", 'm', 'sort', 'of', 'a', 'newcomer', 'to', 'r', '/', 'ad', '##hd', ',', 'but', 'i', 'was', 'diagnosed', 'four', 'years', 'ago', ',', 'around', 'the', 'time', 'i', 'turned', '18', '.', 'ever', 'since', 'then', ',', 'it', 'has', 'been', '25', '##mg', 'of', 'add', '##eral', '##l', 'twice', 'per', 'day', ',', 'and', 'heavy', 'planning', '/', 'calendar', '/', 'schedule', 'work', 'to', 'keep', 'track', 'of', 'assignments', ',', 'meetings', ',', 'and', 'other', 'important', 'tasks', '.', 'i', 'am', 'currently', 'studying', 'english', 'and', 'secondary', 'education', ',', 'and', 'up', 'until', 'now', 'i', 'had', 'been', 'passing', 'the', 'majority', 'of', 'my', 'classes', 'with', 'no', 'question', '.', 'that', 'is', ',', 'until', 'i', 'hit', 'a', 'road', '##block', 'with', 'shakespeare', '.', 'of', 'course', ',', 'i', 'understand', 'that', 'shakespeare', 'is', 'supposed', 'to', 'be', 'difficult', ',', 'and', 'that', 'would', 'definitely', 'be', 'a', 'good', 'justification', 'for', 'it', \"'\", 's', 'placement', 'at', 'a', 'college', 'senior', 'level', '.', 'however', 'what', 'i', 'do', 'not', 'understand', 'is', 'this', ':', 'i', 'have', 'gone', 'through', 'shakespeare', 'plays', 'in', 'my', 'life', ',', '(', 'ot', '##hell', '##o', ',', 'macbeth', ',', 'romeo', 'and', 'juliet', ',', 'hamlet', ',', 'etc', ')', 'and', 'even', 'those', 'were', 'in', 'high', 'school', ',', 'before', 'i', 'was', 'diagnosed', '.', 'this', 'time', 'around', ',', 'i', 'am', 'stopped', 'dead', 'in', 'my', 'tracks', '.', 'i', 'end', 'up', 'reading', 'the', 'same', 'lines', 'over', 'and', 'over', 'and', 'over', 'until', 'i', 'realize', 'that', 'it', \"'\", 's', 'taken', 'me', 'six', 'hours', 'to', 'get', 'through', 'the', 'first', 'act', 'of', 'richard', 'ii', ',', 'and', 'that', 'i', 'don', \"'\", 't', 'remember', 'a', 'damn', 'thing', 'i', 'just', 'spent', 'those', 'six', 'hours', 'reading', '.', 'recently', ',', 'i', 'switched', 'to', 'using', 'audio', '##books', 'and', 'have', 'found', 'a', 'much', 'bigger', 'improvement', 'compared', 'to', 'my', 'own', 'reading', ',', 'but', 'the', 'problem', 'still', 'lies', 'in', 'comprehension', '.', 'i', 'am', 'absolutely', 'terrible', 'with', 'retention', 'and', 'memory', ',', 'and', 'have', 'failed', 'every', 'quiz', 'that', 'i', 'have', 'taken', 'this', 'semester', ',', 'when', 'all', 'around', 'me', 'there', 'are', 'kids', 'that', 'scan', 'the', 'book', 'twenty', 'minutes', 'before', 'the', 'quiz', 'and', 'ace', 'it', '.', 'then', ',', 'after', 'the', 'quiz', ',', 'we', 'go', 'over', 'the', 'answers', 'and', 'i', 'feel', 'like', 'a', 'doo', '##fus', ',', 'because', 'it', \"'\", 's', 'stuff', 'i', 'had', 'read', 'and', 'knew', ',', 'but', 'could', 'not', 'remember', '.', 'anyway', '##s', ',', 'anyone', 'else', 'having', 'trouble', 'with', 'my', 'main', 'man', 'willie', 'sha', '##x', ',', 'and', 'does', 'anyone', 'have', 'any', 'tips', 'that', 'could', 'help', 'a', 'fellow', 'reader', 'out', '?', 'there', \"'\", 's', 'probably', 'not', 'nearly', 'enough', 'information', 'here', 'for', 'me', 'to', 'make', 'my', 'point', ',', 'but', 'i', 'can', 'go', 'more', 'into', 'detail', 'about', 'specific', 'things', 'if', 'anyone', 'asks', '.', 'and', 't', '##l', ';', 'dr', ',', 'shakespeare', 'is', 'difficult', ',', 'audio', '##books', 'help', ',', 'any', 'other', 'tips', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['too', 'scared', 'to', 'go', 'to', 'the', 'doctor', ',', 'any', 'self', 'help', 'tips', '?', 'i', \"'\", 've', 'always', 'had', 'trouble', 'concentrating', 'on', 'tasks', ',', 'always', 'been', 'pro', '##cr', '##ast', '##inating', 'like', 'crazy', ',', 'homework', 'have', 'made', 'me', 'get', 'anxiety', 'attacks', 'and', 'cry', '.', 'i', \"'\", 've', 'never', 'had', 'problems', 'understanding', 'things', ',', 'and', 'i', \"'\", 've', 'often', 'gotten', 'good', 'grades', ',', 'just', 'that', 'all', 'the', 'assignments', 'that', 'meant', 'me', 'producing', 'something', 'gave', 'me', 'lots', 'of', 'anxiety', '.', 'i', \"'\", 've', 'thought', 'for', 'long', 'that', 'it', 'was', 'me', 'being', 'lazy', 'and', 'useless', ',', 'then', 'later', 'realized', 'i', 'was', 'depressed', 'and', 'understood', 'that', 'it', 'could', 'be', 'the', 'reason', 'for', 'it', '.', 'but', 'lately', 'the', 'depression', 'has', 'gotten', 'better', 'since', 'i', \"'\", 've', 'started', 'to', 'take', 'ad', '.', 'but', 'the', 'concentration', 'problems', 'remains', ',', 'even', 'if', 'the', 'anxiety', 'has', 'decreased', 'a', 'little', 'bit', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '.', 'i', \"'\", 'm', 'scared', 'of', 'seeking', 'help', 'for', 'it', ',', 'i', \"'\", 'm', 'terrified', 'the', 'doctor', 'won', \"'\", 't', 'take', 'me', 'seriously', 'and', 'just', 'confirm', 'my', 'fears', 'and', 'tell', 'me', 'it', \"'\", 's', 'normal', 'to', 'lack', 'self', 'discipline', ',', 'or', 'something', '.', 'my', 'bf', 'often', 'also', 'says', 'that', 'people', 'get', 'diagnosed', 'with', 'ad', '##hd', 'way', 'too', 'often', '.', 'i', \"'\", 'm', 'scared', 'he', 'will', 'judge', 'me', ',', 'the', 'same', 'goes', 'for', 'my', 'parents', ',', 'if', 'i', 'try', 'to', 'get', 'help', 'for', 'this', '.', 'this', 'was', 'just', 'some', 'sad', 'ram', '##bling', '.', '.', 'my', 'main', 'question', 'is', ',', 'since', 'i', \"'\", 'm', 'not', 'ready', 'to', 'go', 'to', 'a', 'doctor', 'for', 'this', ',', 'are', 'there', 'any', 'self', 'help', 'methods', 'one', 'can', 'use', 'to', 'stay', 'more', 'focused', '?', 'like', ',', 'cb', '##t', '-', 'methods', '?', 'or', 'something', 'like', 'that', '?']\n",
      "INFO:__main__:Number of tokens: 286\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['too', 'scared', 'to', 'go', 'to', 'the', 'doctor', ',', 'any', 'self', 'help', 'tips', '?', 'i', \"'\", 've', 'always', 'had', 'trouble', 'concentrating', 'on', 'tasks', ',', 'always', 'been', 'pro', '##cr', '##ast', '##inating', 'like', 'crazy', ',', 'homework', 'have', 'made', 'me', 'get', 'anxiety', 'attacks', 'and', 'cry', '.', 'i', \"'\", 've', 'never', 'had', 'problems', 'understanding', 'things', ',', 'and', 'i', \"'\", 've', 'often', 'gotten', 'good', 'grades', ',', 'just', 'that', 'all', 'the', 'assignments', 'that', 'meant', 'me', 'producing', 'something', 'gave', 'me', 'lots', 'of', 'anxiety', '.', 'i', \"'\", 've', 'thought', 'for', 'long', 'that', 'it', 'was', 'me', 'being', 'lazy', 'and', 'useless', ',', 'then', 'later', 'realized', 'i', 'was', 'depressed', 'and', 'understood', 'that', 'it', 'could', 'be', 'the', 'reason', 'for', 'it', '.', 'but', 'lately', 'the', 'depression', 'has', 'gotten', 'better', 'since', 'i', \"'\", 've', 'started', 'to', 'take', 'ad', '.', 'but', 'the', 'concentration', 'problems', 'remains', ',', 'even', 'if', 'the', 'anxiety', 'has', 'decreased', 'a', 'little', 'bit', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '.', 'i', \"'\", 'm', 'scared', 'of', 'seeking', 'help', 'for', 'it', ',', 'i', \"'\", 'm', 'terrified', 'the', 'doctor', 'won', \"'\", 't', 'take', 'me', 'seriously', 'and', 'just', 'confirm', 'my', 'fears', 'and', 'tell', 'me', 'it', \"'\", 's', 'normal', 'to', 'lack', 'self', 'discipline', ',', 'or', 'something', '.', 'my', 'bf', 'often', 'also', 'says', 'that', 'people', 'get', 'diagnosed', 'with', 'ad', '##hd', 'way', 'too', 'often', '.', 'i', \"'\", 'm', 'scared', 'he', 'will', 'judge', 'me', ',', 'the', 'same', 'goes', 'for', 'my', 'parents', ',', 'if', 'i', 'try', 'to', 'get', 'help', 'for', 'this', '.', 'this', 'was', 'just', 'some', 'sad', 'ram', '##bling', '.', '.', 'my', 'main', 'question', 'is', ',', 'since', 'i', \"'\", 'm', 'not', 'ready', 'to', 'go', 'to', 'a', 'doctor', 'for', 'this', ',', 'are', 'there', 'any', 'self', 'help', 'methods', 'one', 'can', 'use', 'to', 'stay', 'more', 'focused', '?', 'like', ',', 'cb', '##t', '-', 'methods', '?', 'or', 'something', 'like', 'that', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['that', \"'\", 's', 'no', 'moon', '.', '.', '.', 'that', \"'\", 's', 'a', 'fa', '##q', 'page', '!', 'it', \"'\", 's', 'finally', 'happened', '!', 'i', \"'\", 've', 'just', 'updated', 'the', 'side', '##bar', 'with', 'a', 'link', 'to', '/', 'r', '/', 'ad', '##hd', \"'\", 's', '[', 'official', 'fa', '##q', '.', ']', '(', 'http', ':', '/', '/', 'code', '.', 'red', '##dit', '.', 'com', '/', 'wi', '##ki', '/', 'help', '/', 'fa', '##q', '##s', '/', 'ad', '##hd', ')', '*', '*', 'the', 'biggest', 'thanks', '*', '*', 'goes', 'out', 'to', 'community', 'member', 'pound', '##nco', '##lon', '##s', 'who', 'was', 'awesome', 'enough', 'to', 'write', 'this', 'fa', '##q', 'for', 'us', '.', 'i', 'also', 'want', 'to', 'thank', 'the', 'various', 'members', 'of', 'the', 'community', 'who', 'have', 'been', 'messaging', 'us', 'in', 'the', 'past', 'few', 'months', 'making', 'recommendations', 'for', 'the', 'fa', '##q', 'and', 'making', 'sure', 'we', 'didn', \"'\", 't', 'forget', 'about', 'it', '.', 'it', 'is', 'still', 'very', 'subject', 'to', 'revision', ',', 'and', 'all', 'feedback', 'is', 'encouraged', '.', 'if', 'i', 'personally', 'told', 'you', 'that', 'something', 'was', 'going', 'to', 'be', 'in', 'this', 'and', 'it', 'isn', \"'\", 't', 'yet', ',', 'well', ',', 'feel', 'free', 'to', 'remind', 'me', '.', 'edit', ':', 'i', \"'\", 've', 'done', 'this', 'just', 'as', 'i', \"'\", 'm', 'running', 'out', 'the', 'door', ',', 'but', 'i', \"'\", 'll', 'be', 'back', 'later', 'to', 'chat', 'with', 'people', 'and', 'maybe', 'spruce', 'the', 'thing', 'up', 'a', 'bit', '.']\n",
      "INFO:__main__:Number of tokens: 216\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['that', \"'\", 's', 'no', 'moon', '.', '.', '.', 'that', \"'\", 's', 'a', 'fa', '##q', 'page', '!', 'it', \"'\", 's', 'finally', 'happened', '!', 'i', \"'\", 've', 'just', 'updated', 'the', 'side', '##bar', 'with', 'a', 'link', 'to', '/', 'r', '/', 'ad', '##hd', \"'\", 's', '[', 'official', 'fa', '##q', '.', ']', '(', 'http', ':', '/', '/', 'code', '.', 'red', '##dit', '.', 'com', '/', 'wi', '##ki', '/', 'help', '/', 'fa', '##q', '##s', '/', 'ad', '##hd', ')', '*', '*', 'the', 'biggest', 'thanks', '*', '*', 'goes', 'out', 'to', 'community', 'member', 'pound', '##nco', '##lon', '##s', 'who', 'was', 'awesome', 'enough', 'to', 'write', 'this', 'fa', '##q', 'for', 'us', '.', 'i', 'also', 'want', 'to', 'thank', 'the', 'various', 'members', 'of', 'the', 'community', 'who', 'have', 'been', 'messaging', 'us', 'in', 'the', 'past', 'few', 'months', 'making', 'recommendations', 'for', 'the', 'fa', '##q', 'and', 'making', 'sure', 'we', 'didn', \"'\", 't', 'forget', 'about', 'it', '.', 'it', 'is', 'still', 'very', 'subject', 'to', 'revision', ',', 'and', 'all', 'feedback', 'is', 'encouraged', '.', 'if', 'i', 'personally', 'told', 'you', 'that', 'something', 'was', 'going', 'to', 'be', 'in', 'this', 'and', 'it', 'isn', \"'\", 't', 'yet', ',', 'well', ',', 'feel', 'free', 'to', 'remind', 'me', '.', 'edit', ':', 'i', \"'\", 've', 'done', 'this', 'just', 'as', 'i', \"'\", 'm', 'running', 'out', 'the', 'door', ',', 'but', 'i', \"'\", 'll', 'be', 'back', 'later', 'to', 'chat', 'with', 'people', 'and', 'maybe', 'spruce', 'the', 'thing', 'up', 'a', 'bit', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['demographics', 'of', '/', 'r', '/', 'ad', '##hd', 'survey', '[', '10', 'questions', 'long', ']']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['demographics', 'of', '/', 'r', '/', 'ad', '##hd', 'survey', '[', '10', 'questions', 'long', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['concert', '##a', 'gen', '.', 'w', '/', 'out', 'insurance', '.', 'asking', '$', '170', 'for', 'a', 'month', '.', 'what', '!', '?', 'anywhere', 'that', 'actually', 'has', 'it', 'reasonably', 'priced', '?']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['concert', '##a', 'gen', '.', 'w', '/', 'out', 'insurance', '.', 'asking', '$', '170', 'for', 'a', 'month', '.', 'what', '!', '?', 'anywhere', 'that', 'actually', 'has', 'it', 'reasonably', 'priced', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['27', 'f', 'and', 'diagnosed', 'as', 'ad', '##hd', 'ina', '##tten', '##tive', 'type', 'awhile', 'back', 'i', 'was', 'also', 'diagnosed', 'with', 'd', '##yst', '##hy', '##mia', '.', 'i', 'hate', 'taking', 'medication', ',', 'but', 'after', 'some', 'thinking', ',', 'am', 'now', 'considering', 'taking', 'medication', 'of', 'the', 'ad', '##hd', '.', 'the', 'doc', 'suggested', 'two', 'medications', ',', 'she', 'wrote', 'them', 'down', 'on', 'a', 'sheet', 'because', 'i', 'told', 'i', 'wanted', 'to', 'look', 'them', 'up', 'before', 'making', 'a', 'decision', ',', 'but', '.', '.', '.', 'i', 'lost', 'the', 'f', '*', 'ck', '##ing', 'sheet', '.', '*', 'face', '##pal', '##m', '*', 'anyway', '##s', ',', 'there', 'two', 'medications', 'i', 'was', 'thinking', 'about', ',', 'one', 'was', 'a', 'st', '##im', '##ula', '##nt', 'and', 'the', 'other', 'had', 'no', 'st', '##im', '##ula', '##nt', ',', 'and', 'i', 'opted', 'for', 'the', 'one', 'that', 'wasn', \"'\", 't', 'a', 'st', '##im', '##ula', '##nt', '.', 'i', 'am', 'still', 'on', 'pa', '##xi', '##l', 'for', 'd', '##yst', '##hy', '##mia', '.', 'i', 'am', 'asking', 'for', 'an', 'opinion', 'from', 'ad', '##hd', 'red', '##dit', '##ors', ',', 'you', 'think', 'what', 'i', 'doing', 'is', 'a', 'good', 'idea', ',', 'or', 'should', 'i', 'wait', 'until', 'i', 'am', 'off', 'the', 'pa', '##xi', '##l', 'until', 'starting', 'the', 'other', 'medication', '?']\n",
      "INFO:__main__:Number of tokens: 186\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['27', 'f', 'and', 'diagnosed', 'as', 'ad', '##hd', 'ina', '##tten', '##tive', 'type', 'awhile', 'back', 'i', 'was', 'also', 'diagnosed', 'with', 'd', '##yst', '##hy', '##mia', '.', 'i', 'hate', 'taking', 'medication', ',', 'but', 'after', 'some', 'thinking', ',', 'am', 'now', 'considering', 'taking', 'medication', 'of', 'the', 'ad', '##hd', '.', 'the', 'doc', 'suggested', 'two', 'medications', ',', 'she', 'wrote', 'them', 'down', 'on', 'a', 'sheet', 'because', 'i', 'told', 'i', 'wanted', 'to', 'look', 'them', 'up', 'before', 'making', 'a', 'decision', ',', 'but', '.', '.', '.', 'i', 'lost', 'the', 'f', '*', 'ck', '##ing', 'sheet', '.', '*', 'face', '##pal', '##m', '*', 'anyway', '##s', ',', 'there', 'two', 'medications', 'i', 'was', 'thinking', 'about', ',', 'one', 'was', 'a', 'st', '##im', '##ula', '##nt', 'and', 'the', 'other', 'had', 'no', 'st', '##im', '##ula', '##nt', ',', 'and', 'i', 'opted', 'for', 'the', 'one', 'that', 'wasn', \"'\", 't', 'a', 'st', '##im', '##ula', '##nt', '.', 'i', 'am', 'still', 'on', 'pa', '##xi', '##l', 'for', 'd', '##yst', '##hy', '##mia', '.', 'i', 'am', 'asking', 'for', 'an', 'opinion', 'from', 'ad', '##hd', 'red', '##dit', '##ors', ',', 'you', 'think', 'what', 'i', 'doing', 'is', 'a', 'good', 'idea', ',', 'or', 'should', 'i', 'wait', 'until', 'i', 'am', 'off', 'the', 'pa', '##xi', '##l', 'until', 'starting', 'the', 'other', 'medication', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['specific', 'food', 'craving', '##s', 'when', 'add', '##eral', '##l', 'wears', 'off', '?', 'i', \"'\", 'm', 'taking', '15', 'mg', '##s', 'add', '##eral', '##l', 'twice', 'a', 'day', '.', 'i', 'know', 'it', \"'\", 's', 'normal', 'to', 'experience', 'a', 'loss', 'of', 'appetite', 'on', 'add', '##eral', '##l', '(', 'i', \"'\", 've', 'lost', 'a', 'few', 'pounds', 'while', 'on', 'it', ')', 'but', 'when', 'it', 'wears', 'off', ',', 'i', 'have', 'intense', 'craving', '##s', 'for', 'the', 'most', 'random', 'foods', '-', 'cocoa', 'puff', '##s', ',', 'pizza', ',', 'and', 'let', '##tu', '##ce', 'especially', '.', 'any', 'other', 'food', 'just', 'seems', 'gross', '.', 'when', 'i', \"'\", 'm', 'not', 'taking', 'add', '##eral', '##l', ',', 'i', \"'\", 'll', 'eat', 'with', 'more', 'variety', '.', 'i', 'also', 'have', 'to', 'deal', 'with', 'mood', 'changes', 'and', 'fatigue', 'when', 'it', 'wears', 'off', '.', 'does', 'anyone', 'else', 'experience', 'this', 'or', 'is', 'it', 'just', 'me', '?']\n",
      "INFO:__main__:Number of tokens: 133\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['specific', 'food', 'craving', '##s', 'when', 'add', '##eral', '##l', 'wears', 'off', '?', 'i', \"'\", 'm', 'taking', '15', 'mg', '##s', 'add', '##eral', '##l', 'twice', 'a', 'day', '.', 'i', 'know', 'it', \"'\", 's', 'normal', 'to', 'experience', 'a', 'loss', 'of', 'appetite', 'on', 'add', '##eral', '##l', '(', 'i', \"'\", 've', 'lost', 'a', 'few', 'pounds', 'while', 'on', 'it', ')', 'but', 'when', 'it', 'wears', 'off', ',', 'i', 'have', 'intense', 'craving', '##s', 'for', 'the', 'most', 'random', 'foods', '-', 'cocoa', 'puff', '##s', ',', 'pizza', ',', 'and', 'let', '##tu', '##ce', 'especially', '.', 'any', 'other', 'food', 'just', 'seems', 'gross', '.', 'when', 'i', \"'\", 'm', 'not', 'taking', 'add', '##eral', '##l', ',', 'i', \"'\", 'll', 'eat', 'with', 'more', 'variety', '.', 'i', 'also', 'have', 'to', 'deal', 'with', 'mood', 'changes', 'and', 'fatigue', 'when', 'it', 'wears', 'off', '.', 'does', 'anyone', 'else', 'experience', 'this', 'or', 'is', 'it', 'just', 'me', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'taken', 'a', 't', '.', 'o', '.', 'v', '.', 'a', '.', 'test', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'taken', 'a', 't', '.', 'o', '.', 'v', '.', 'a', '.', 'test', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['butterfly']\n",
      "INFO:__main__:Number of tokens: 1\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['butterfly']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'find', 'they', 'lose', 'focus', 'the', 'moment', 'you', 'ing', '##est', 'a', 'car', '##b', 'heavy', 'meal', '?', 'if', 'i', 'have', 'a', 'lot', 'of', 'protein', 'and', 'vegetables', 'i', 'feel', 'great', 'all', 'day', 'but', 'like', 'if', 'i', 'have', 'wa', '##ffle', '##s', 'or', 'pancakes', 'my', 'eyelids', 'get', 'heavy', 'and', 'i', 'sort', '##a', 'slow', 'down', 'and', 'i', 'zone', 'out', 'quite', 'a', 'bit', '.']\n",
      "INFO:__main__:Number of tokens: 60\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'find', 'they', 'lose', 'focus', 'the', 'moment', 'you', 'ing', '##est', 'a', 'car', '##b', 'heavy', 'meal', '?', 'if', 'i', 'have', 'a', 'lot', 'of', 'protein', 'and', 'vegetables', 'i', 'feel', 'great', 'all', 'day', 'but', 'like', 'if', 'i', 'have', 'wa', '##ffle', '##s', 'or', 'pancakes', 'my', 'eyelids', 'get', 'heavy', 'and', 'i', 'sort', '##a', 'slow', 'down', 'and', 'i', 'zone', 'out', 'quite', 'a', 'bit', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['awake', 'for', 'the', 'first', 'time', 'with', 'v', '##y', '##van', '##se', 'hello', 'all', '.', 'according', 'to', 'my', 'psychological', 'profile', ',', 'i', \"'\", 'm', 'a', '22', 'year', ',', '5', 'month', 'old', 'caucasian', 'male', '.', 'whatever', 'that', 'means', '.', 'after', 'a', 'couple', 'of', 'months', 'of', 'tests', ',', 'refer', '##als', ',', 'and', 'mostly', 'waiting', ',', 'i', 'have', 'been', 'diagnosed', 'with', 'the', 'combination', 'sub', '##type', 'of', 'ad', '##hd', '.', 'i', 'was', 'prescribed', '40', '##mg', 'v', '##y', '##van', '##se', 'daily', 'for', 'the', 'next', 'month', '.', 'i', 'have', 'finished', 'more', 'projects', 'today', 'than', 'i', 'ever', 'have', 'in', 'one', 'day', ',', 'and', 'i', 'feel', 'eu', '##ph', '##oric', '.', 'i', 'feel', 'less', 'restless', 'than', 'i', 'ever', 'have', '.', 'most', 'of', 'all', ',', 'i', 'feel', 'like', 'i', 'can', 'navigate', 'inter', '##personal', 'relationships', 'better', '.', 'to', 'keep', 'it', 'short', '(', 'since', 'you', 'all', 'have', 'ad', '##hd', ':', 'p', ')', ',', 'i', 'can', 'only', 'define', 'how', 'i', 'feel', 'as', 'awake', 'for', 'the', 'first', 'time', '.', 'i', 'look', 'forward', 'to', 'contributing', 'to', 'this', 'sub', '##red', '##dit', ',', 'but', 'most', 'of', 'all', 'i', 'look', 'forward', 'to', 'a', 'long', 'career', 'of', 'lurking', ':', ')', '-', 'mania', '##cal', '##mania', '(', 'i', 'suppose', 'mel', '##low', '##mel', '##od', '##y', 'would', 'be', 'more', 'appropriate', 'now', ')']\n",
      "INFO:__main__:Number of tokens: 197\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['awake', 'for', 'the', 'first', 'time', 'with', 'v', '##y', '##van', '##se', 'hello', 'all', '.', 'according', 'to', 'my', 'psychological', 'profile', ',', 'i', \"'\", 'm', 'a', '22', 'year', ',', '5', 'month', 'old', 'caucasian', 'male', '.', 'whatever', 'that', 'means', '.', 'after', 'a', 'couple', 'of', 'months', 'of', 'tests', ',', 'refer', '##als', ',', 'and', 'mostly', 'waiting', ',', 'i', 'have', 'been', 'diagnosed', 'with', 'the', 'combination', 'sub', '##type', 'of', 'ad', '##hd', '.', 'i', 'was', 'prescribed', '40', '##mg', 'v', '##y', '##van', '##se', 'daily', 'for', 'the', 'next', 'month', '.', 'i', 'have', 'finished', 'more', 'projects', 'today', 'than', 'i', 'ever', 'have', 'in', 'one', 'day', ',', 'and', 'i', 'feel', 'eu', '##ph', '##oric', '.', 'i', 'feel', 'less', 'restless', 'than', 'i', 'ever', 'have', '.', 'most', 'of', 'all', ',', 'i', 'feel', 'like', 'i', 'can', 'navigate', 'inter', '##personal', 'relationships', 'better', '.', 'to', 'keep', 'it', 'short', '(', 'since', 'you', 'all', 'have', 'ad', '##hd', ':', 'p', ')', ',', 'i', 'can', 'only', 'define', 'how', 'i', 'feel', 'as', 'awake', 'for', 'the', 'first', 'time', '.', 'i', 'look', 'forward', 'to', 'contributing', 'to', 'this', 'sub', '##red', '##dit', ',', 'but', 'most', 'of', 'all', 'i', 'look', 'forward', 'to', 'a', 'long', 'career', 'of', 'lurking', ':', ')', '-', 'mania', '##cal', '##mania', '(', 'i', 'suppose', 'mel', '##low', '##mel', '##od', '##y', 'would', 'be', 'more', 'appropriate', 'now', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'many', 'of', 'you', 'find', 'that', 'you', 'suffer', 'most', 'from', 'hyper', 'focus', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'many', 'of', 'you', 'find', 'that', 'you', 'suffer', 'most', 'from', 'hyper', 'focus', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 've', 'been', 'focusing', 'on', 'maintaining', 'healthy', 'lifestyle', 'habits', 'for', 'treatment', '.', 'one', 'that', 'seems', 'to', 'be', 'really', 'helping', 'is', 'eating', 'an', 'egg', 'each', 'morning', '.']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 've', 'been', 'focusing', 'on', 'maintaining', 'healthy', 'lifestyle', 'habits', 'for', 'treatment', '.', 'one', 'that', 'seems', 'to', 'be', 'really', 'helping', 'is', 'eating', 'an', 'egg', 'each', 'morning', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'took', 'my', 'prescribed', '15', 'mg', 'dex', '##ed', '##rine', 'spans', '##ule', 'before', 'my', 'afternoon', 'exam', ',', 'and', 'when', 'my', 'friend', 'asked', 'me', 'how', 'it', 'went', ',', 'i', 'sent', 'him', 'this', '.', 'friend', ':', 'how', 'was', 'the', 'mid', '##ter', '##m', '?', 'massive', '_', 'tunes', ':', 'easy', 'as', 'pie', '.', 'how', 'can', 'it', 'easy', 'as', 'pie', 'you', 'ask', '?', 'have', 'you', 'ever', 'eaten', 'an', 'oven', 'baked', 'apple', 'cr', '##umble', 'apple', 'pie', '?', 'well', ',', 'writing', 'that', 'mid', '##ter', '##m', 'was', 'like', 'eating', 'a', 'delicious', 'slice', 'of', 'some', 'mother', '##fu', '##cking', 'apple', 'cr', '##umble', 'apple', 'pie', '.', 'friend', ':', 'lo', '##l', 'massive', '_', 'tunes', ':', 'you', 'look', 'at', 'the', 'pie', ',', 'then', 'you', 'question', 'its', 'legitimacy', 'of', 'delicious', '##ness', 'by', 'staring', 'at', 'it', '.', 'you', 'seriously', 'doubt', 'that', 'the', 'slice', 'of', 'pie', 'will', 'be', 'good', 'because', 'you', \"'\", 've', 'eaten', 'a', 'lot', 'of', 'pie', 'in', 'the', 'past', 'and', 'you', \"'\", 're', 'not', 'sure', 'you', 'want', 'to', 'cr', '##umble', 'beliefs', 'of', 'what', 'a', 'pie', 'should', 'taste', 'like', '.', 'massive', '_', 'tunes', ':', 'you', 'proceed', 'to', 'take', 'a', 'bite', 'and', 'all', 'your', 'worries', 'and', 'doubts', 'drift', 'away', '.', 'an', 'hour', 'and', 'a', 'half', 'later', ',', 'pie', 'is', 'finished', 'and', 'you', \"'\", 're', 'wondering', ',', 'why', 'couldn', \"'\", 't', 'all', 'the', 'pie', '##s', 'i', \"'\", 've', 'had', 'before', 'could', 'be', 'this', 'delicious', '?', 'massive', '_', 'tunes', ':', 'i', 'contra', '##dict', '##ed', 'myself', 'when', 'i', 'said', 'all', 'pie', '##s', 'were', 'delicious', '.', 'i', \"'\", 've', 'been', 'consistently', 'supplied', 'shitty', 'pie', 'over', 'the', 'last', '4', 'years', '.']\n",
      "INFO:__main__:Number of tokens: 249\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'took', 'my', 'prescribed', '15', 'mg', 'dex', '##ed', '##rine', 'spans', '##ule', 'before', 'my', 'afternoon', 'exam', ',', 'and', 'when', 'my', 'friend', 'asked', 'me', 'how', 'it', 'went', ',', 'i', 'sent', 'him', 'this', '.', 'friend', ':', 'how', 'was', 'the', 'mid', '##ter', '##m', '?', 'massive', '_', 'tunes', ':', 'easy', 'as', 'pie', '.', 'how', 'can', 'it', 'easy', 'as', 'pie', 'you', 'ask', '?', 'have', 'you', 'ever', 'eaten', 'an', 'oven', 'baked', 'apple', 'cr', '##umble', 'apple', 'pie', '?', 'well', ',', 'writing', 'that', 'mid', '##ter', '##m', 'was', 'like', 'eating', 'a', 'delicious', 'slice', 'of', 'some', 'mother', '##fu', '##cking', 'apple', 'cr', '##umble', 'apple', 'pie', '.', 'friend', ':', 'lo', '##l', 'massive', '_', 'tunes', ':', 'you', 'look', 'at', 'the', 'pie', ',', 'then', 'you', 'question', 'its', 'legitimacy', 'of', 'delicious', '##ness', 'by', 'staring', 'at', 'it', '.', 'you', 'seriously', 'doubt', 'that', 'the', 'slice', 'of', 'pie', 'will', 'be', 'good', 'because', 'you', \"'\", 've', 'eaten', 'a', 'lot', 'of', 'pie', 'in', 'the', 'past', 'and', 'you', \"'\", 're', 'not', 'sure', 'you', 'want', 'to', 'cr', '##umble', 'beliefs', 'of', 'what', 'a', 'pie', 'should', 'taste', 'like', '.', 'massive', '_', 'tunes', ':', 'you', 'proceed', 'to', 'take', 'a', 'bite', 'and', 'all', 'your', 'worries', 'and', 'doubts', 'drift', 'away', '.', 'an', 'hour', 'and', 'a', 'half', 'later', ',', 'pie', 'is', 'finished', 'and', 'you', \"'\", 're', 'wondering', ',', 'why', 'couldn', \"'\", 't', 'all', 'the', 'pie', '##s', 'i', \"'\", 've', 'had', 'before', 'could', 'be', 'this', 'delicious', '?', 'massive', '_', 'tunes', ':', 'i', 'contra', '##dict', '##ed', 'myself', 'when', 'i', 'said', 'all', 'pie', '##s', 'were', 'delicious', '.', 'i', \"'\", 've', 'been', 'consistently', 'supplied', 'shitty', 'pie', 'over', 'the', 'last', '4', 'years', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'bipolar']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'bipolar']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', 'some', 'input', 'from', 'experienced', 'add', '##eral', '##l', 'users', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', 'some', 'input', 'from', 'experienced', 'add', '##eral', '##l', 'users', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['computer', 'program', '##ing', 'and', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['computer', 'program', '##ing', 'and', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'ad', '##hd', '-', 'pi', 'and', 'i', 'can', \"'\", 't', 'manage', 'money', 'at', 'all', '.', 'it', \"'\", 's', 'a', 'huge', 'issue', '.', 'anyone', 'else', 'have', 'money', 'problems', 'or', 'have', 'tips', 'on', 'how', 'to', 'save', '?', 'i', 'can', \"'\", 't', 'manage', 'money', 'at', 'all', '.', 'it', 'seems', 'like', 'every', 'time', 'i', 'get', 'any', 'amount', 'of', 'money', ',', 'small', 'or', 'large', ',', 'i', 'spend', 'it', 'immediately', '.', 'i', 'always', 'try', 'and', 'budget', 'my', 'money', ',', 'and', 'plan', 'on', 'saving', 'it', ',', 'but', 'then', 'i', 'just', 'end', 'up', 'spontaneously', 'buying', 'something', 'fr', '##ivo', '##lous', 'or', 'unnecessary', '.', 'its', 'so', 'frustrating', ',', 'and', 'i', 'am', 'always', 'dangerously', 'low', 'on', 'money', '.', 'does', 'anyone', 'else', 'have', 'this', 'problem', 'of', 'managing', 'their', 'money', 'and', 'budget', '##ing', 'efficiently', '?', 'anyone', 'have', 'any', 'tips', 'or', 'strategies', 'that', 'work', 'for', 'them', '?']\n",
      "INFO:__main__:Number of tokens: 134\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'ad', '##hd', '-', 'pi', 'and', 'i', 'can', \"'\", 't', 'manage', 'money', 'at', 'all', '.', 'it', \"'\", 's', 'a', 'huge', 'issue', '.', 'anyone', 'else', 'have', 'money', 'problems', 'or', 'have', 'tips', 'on', 'how', 'to', 'save', '?', 'i', 'can', \"'\", 't', 'manage', 'money', 'at', 'all', '.', 'it', 'seems', 'like', 'every', 'time', 'i', 'get', 'any', 'amount', 'of', 'money', ',', 'small', 'or', 'large', ',', 'i', 'spend', 'it', 'immediately', '.', 'i', 'always', 'try', 'and', 'budget', 'my', 'money', ',', 'and', 'plan', 'on', 'saving', 'it', ',', 'but', 'then', 'i', 'just', 'end', 'up', 'spontaneously', 'buying', 'something', 'fr', '##ivo', '##lous', 'or', 'unnecessary', '.', 'its', 'so', 'frustrating', ',', 'and', 'i', 'am', 'always', 'dangerously', 'low', 'on', 'money', '.', 'does', 'anyone', 'else', 'have', 'this', 'problem', 'of', 'managing', 'their', 'money', 'and', 'budget', '##ing', 'efficiently', '?', 'anyone', 'have', 'any', 'tips', 'or', 'strategies', 'that', 'work', 'for', 'them', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'am', 'a', 'male', 'who', 'just', 'turned', '20', '.', 'i', 'suspect', 'i', 'suffer', 'from', 'ad', '##hd', '.', 'what', 'is', 'the', 'most', 'logical', 'thing', 'to', 'do', 'now', '?', 'short', 'backs', '##tory', '.', 'did', 'well', 'in', 'elementary', '##sch', '##ool', ',', 'quite', 'hyper', '##active', 'but', 'never', 'a', 'trouble', 'for', 'teachers', 'and', 'parents', '.', 'i', 'could', 'control', '##l', 'myself', 'and', 'did', 'well', 'in', 'school', '.', 'often', 'joking', '##ly', 'called', 'the', '\"', 'caf', '##fe', '##ine', 'child', '\"', '.', 'hi', '##gs', '##cho', '##ol', 'my', 'grades', 'started', 'falling', 'in', 'subjects', 'that', 'didn', \"'\", 't', 'interests', 'me', 'to', 'either', 'a', 'fail', 'or', 'a', 'barely', 'pass', '.', 'highest', 'grade', 'in', 'almost', 'every', 'science', '-', 'real', '##ated', 'subject', 'such', 'as', 'math', ',', 'physics', 'or', 'chemistry', '.', 'i', 'did', 'managed', 'to', 'get', 'into', 'a', 'good', 'university', 'reading', 'physics', 'solely', 'based', 'on', 'my', 'math', 'and', 'physics', 'knowledge', '.', 'not', 'as', 'hyper', '##active', 'as', 'i', 'used', 'to', 'be', 'though', '.', 'in', 'first', 'year', 'of', 'university', 'now', 'studying', 'physics', '.', 'things', 'are', 'not', 'looking', 'too', 'good', 'now', 'as', 'suddenly', 'things', 'are', 'hard', 'and', 'i', 'never', 'had', 'to', 'study', 'more', 'than', '1', '##h', 'at', 'one', 'time', '.', '(', 'reason', 'why', 'i', 'failed', 'classes', 'that', 'required', 'me', 'to', 'write', 'essays', 'and', 'similar', 'stuff', ')', '.', 'it', 'hit', 'me', 'a', 'few', 'weeks', 'ago', 'i', 'may', 'have', 'ad', '##hd', 'and', 'after', 'reading', '[', 'this', ']', '(', 'http', ':', '/', '/', 'www', '.', 'psychology', '##to', '##day', '.', 'com', '/', 'blog', '/', 'the', '-', 'mysteries', '-', 'add', '/', '2011', '##0', '##8', '/', 'the', '-', 'mysteries', '-', 'add', '-', 'and', '-', 'high', '-', 'iq', ')', 'article', 'made', 'me', 'even', 'more', 'sure', 'as', 'almost', '100', '%', 'of', 'the', 'things', 'mentioned', 'in', 'that', 'is', 'me', '.', 'what', 'is', 'the', 'best', 'way', 'to', 'confirm', 'this', '?', 'just', 'go', 'to', 'a', 'psychiatrist', '?', 'i', 'have', 'no', 'idea', 'what', 'to', 'do', 'now', 'and', 'how', 'i', 'should', 'get', 'help', 'before', 'it', \"'\", 's', 'too', 'late', '.', 'as', 'things', 'looks', 'now', 'my', 'first', 'year', 'at', 'the', 'university', 'is', 'too', 'late', 'to', 'save', 'but', 'if', 'i', 'can', 'get', 'help', 'i', 'can', 'maybe', 'catch', 'up', 'during', 'the', 'summer', '.']\n",
      "INFO:__main__:Number of tokens: 339\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'am', 'a', 'male', 'who', 'just', 'turned', '20', '.', 'i', 'suspect', 'i', 'suffer', 'from', 'ad', '##hd', '.', 'what', 'is', 'the', 'most', 'logical', 'thing', 'to', 'do', 'now', '?', 'short', 'backs', '##tory', '.', 'did', 'well', 'in', 'elementary', '##sch', '##ool', ',', 'quite', 'hyper', '##active', 'but', 'never', 'a', 'trouble', 'for', 'teachers', 'and', 'parents', '.', 'i', 'could', 'control', '##l', 'myself', 'and', 'did', 'well', 'in', 'school', '.', 'often', 'joking', '##ly', 'called', 'the', '\"', 'caf', '##fe', '##ine', 'child', '\"', '.', 'hi', '##gs', '##cho', '##ol', 'my', 'grades', 'started', 'falling', 'in', 'subjects', 'that', 'didn', \"'\", 't', 'interests', 'me', 'to', 'either', 'a', 'fail', 'or', 'a', 'barely', 'pass', '.', 'highest', 'grade', 'in', 'almost', 'every', 'science', '-', 'real', '##ated', 'subject', 'such', 'as', 'math', ',', 'physics', 'or', 'chemistry', '.', 'i', 'did', 'managed', 'to', 'get', 'into', 'a', 'good', 'university', 'reading', 'physics', 'solely', 'based', 'on', 'my', 'math', 'and', 'physics', 'knowledge', '.', 'not', 'as', 'hyper', '##active', 'as', 'i', 'used', 'to', 'be', 'though', '.', 'in', 'first', 'year', 'of', 'university', 'now', 'studying', 'physics', '.', 'things', 'are', 'not', 'looking', 'too', 'good', 'now', 'as', 'suddenly', 'things', 'are', 'hard', 'and', 'i', 'never', 'had', 'to', 'study', 'more', 'than', '1', '##h', 'at', 'one', 'time', '.', '(', 'reason', 'why', 'i', 'failed', 'classes', 'that', 'required', 'me', 'to', 'write', 'essays', 'and', 'similar', 'stuff', ')', '.', 'it', 'hit', 'me', 'a', 'few', 'weeks', 'ago', 'i', 'may', 'have', 'ad', '##hd', 'and', 'after', 'reading', '[', 'this', ']', '(', 'http', ':', '/', '/', 'www', '.', 'psychology', '##to', '##day', '.', 'com', '/', 'blog', '/', 'the', '-', 'mysteries', '-', 'add', '/', '2011', '##0', '##8', '/', 'the', '-', 'mysteries', '-', 'add', '-', 'and', '-', 'high', '-', 'iq', ')', 'article', 'made', 'me', 'even', 'more', 'sure', 'as', 'almost', '100', '%', 'of', 'the', 'things', 'mentioned', 'in', 'that', 'is', 'me', '.', 'what', 'is', 'the', 'best', 'way', 'to', 'confirm', 'this', '?', 'just', 'go', 'to', 'a', 'psychiatrist', '?', 'i', 'have', 'no', 'idea', 'what', 'to', 'do', 'now', 'and', 'how', 'i', 'should', 'get', 'help', 'before', 'it', \"'\", 's', 'too', 'late', '.', 'as', 'things', 'looks', 'now', 'my', 'first', 'year', 'at', 'the', 'university', 'is', 'too', 'late', 'to', 'save', 'but', 'if', 'i', 'can', 'get', 'help', 'i', 'can', 'maybe', 'catch', 'up', 'during', 'the', 'summer', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['not', 'sure', 'what', 'to', 'do', '.', 'i', 'came', 'across', 'something', 'about', 'ad', '##hd', '(', 'the', '\"', 'ina', '##tten', '##tive', '\"', 'type', 'specifically', ',', 'i', 'think', 'it', \"'\", 's', 'referred', 'to', 'as', 'ad', '##hd', '-', 'pi', ')', 'purely', 'by', 'chance', 'not', 'long', 'ago', 'and', 'it', 'hit', 'me', 'pretty', 'hard', ',', 'i', \"'\", 've', 'been', 'noticing', 'a', 'lot', 'of', 'stuff', 'about', 'myself', 'the', 'last', 'few', 'years', 'but', 'it', 'never', 'occurred', 'to', 'me', 'that', 'i', 'could', 'actually', 'be', 'add', '/', 'ad', '##hd', '.', '.', '.', 'except', 'it', 'described', 'me', 'so', 'spot', 'on', 'that', 'i', \"'\", 'm', '*', 'nearly', '*', 'convinced', 'i', 'have', 'it', 'for', 'sure', '.', 'i', 'really', 'don', \"'\", 't', 'like', 'to', 'dia', '##gno', '##se', 'myself', 'because', 'i', \"'\", 'm', 'not', 'a', 'doctor', '/', 'psychiatrist', ',', 'but', 'here', \"'\", 's', 'where', 'the', 'problems', 'start', 'coming', 'in', '.', 'i', \"'\", 'm', 'turning', '17', 'in', 'a', 'little', 'over', 'a', 'month', ',', 'and', 'i', \"'\", 've', 'been', 'planning', 'on', 'enlist', '##ing', 'in', 'the', 'military', 'at', '18', 'ever', 'since', 'i', 'was', '12', '.', 'the', 'military', 'allows', 'people', 'with', 'ad', '##hd', 'to', 'join', ',', 'but', 'only', 'if', 'they', 'get', 'a', 'wai', '##ver', ',', 'and', 'they', \"'\", 're', 'only', 'eligible', 'for', 'a', 'wai', '##ver', 'if', 'they', \"'\", 've', 'been', 'off', 'any', 'ad', '##hd', 'medication', 'for', 'at', 'least', '1', 'year', '.', 'so', 'if', 'i', 'did', 'get', 'diagnosed', 'with', 'ad', '##hd', ',', 'it', 'would', 'go', 'on', 'my', 'medical', 'record', ',', 'and', 'even', 'if', 'i', 'did', 'decide', 'to', 'try', 'out', 'a', 'treatment', 'for', 'it', 'i', 'would', 'either', 'have', 'to', 'push', 'back', 'my', 'plans', 'or', 'only', 'take', 'the', 'med', '##s', 'for', 'about', 'a', 'month', '.', 'plus', 'the', 'military', 'is', 'facing', 'cuts', 'and', 'can', 'be', 'more', 'selective', 'in', 'who', 'it', 'recruits', 'now', '.', 'i', \"'\", 'm', 'worried', 'having', 'something', 'like', 'ad', '##hd', 'on', 'my', 'record', 'might', 'be', 'a', 'bigger', 'b', '##lot', '##ch', 'than', 'it', 'was', 'a', 'few', 'years', 'ago', ',', 'when', 'the', 'military', 'was', 'still', 'gun', '##g', '-', 'ho', 'let', \"'\", 's', 'recruit', 'the', 'shit', 'out', 'of', 'people', 'fuck', 'yeah', 'wars', '.', 'it', 'doesn', \"'\", 't', 'really', 'seem', 'worth', 'it', ',', 'but', 'here', \"'\", 's', 'the', 'thing', '.', 'my', 'life', 'pretty', 'much', 'went', 'off', 'track', 'in', 'the', '6th', '-', '7th', 'grade', '.', 'i', 'went', 'from', 'just', 'gliding', 'through', 'school', 'to', 'failing', 'for', 'the', 'first', 'time', 'in', 'my', 'life', 'and', 'it', 'was', 'such', 'a', 'sudden', 'turn', '.', 'the', 'work', 'didn', \"'\", 't', 'get', 'that', 'much', 'harder', '.', 'i', 'was', 'pretty', 'confident', 'in', 'my', 'ability', 'to', 'ace', 'school', '##work', 'because', 'i', \"'\", 'd', 'always', 'been', 'in', 'the', 'gifted', 'classes', ',', 'told', 'i', 'was', 'really', 'smart', ',', 'on', 'the', 'yearly', 'iq', 'tests', 'they', 'made', 'us', 'take', 'i', 'always', 'scored', 'in', 'the', '130', '##s', ',', 'etc', '.', 'and', 'then', 'it', 'was', 'like', 'my', 'mind', 'started', 'working', 'at', 'a', 'much', 'slower', 'pace', '.', 'it', 'would', 'take', 'me', 'forever', 'to', 'recall', 'things', ',', 'i', 'was', 'forgetting', 'stuff', 'and', 'just', 'not', 'being', 'able', 'to', 'remember', 'it', 'for', 'the', 'life', 'of', 'me', 'for', 'short', 'periods', 'of', 'time', ',', 'i', 'constantly', 'lost', 'focus', 'on', 'what', 'i', 'was', 'doing', 'and', 'it', 'was', 'like', 'walking', 'up', 'a', 'steep', 'incline', 'in', 'muddy', ',', 'snowy', ',', 'liquid', 'quick', '##san', '##d', 'just', 'to', 'try', 'and', 'make', 'myself', 'just', 'do', 'my', 'school', '##work', 'at', 'all', '.', 'i', 'ended', 'up', 'not', 'turning', 'in', 'a', 'lot', 'of', 'work', 'for', 'awhile', 'and', 'fell', 'into', 'a', 'pretty', 'deep', 'depression', '.', 'my', 'parents', '/', 'counselor', '##s', '/', 'etc', '.', 'thought', 'i', 'was', 'getting', 'bull', '##ied', 'or', 'something', ',', 'but', 'i', 'was', 'just', 'really', 'depressed', 'because', 'i', 'felt', 'like', 'a', 'complete', 'failure', 'and', 'like', 'everything', 'about', 'me', 'being', 'oh', 'so', 'smart', 'had', 'been', 'a', 'far', '##ce', '.', 'i', 'felt', 'like', 'it', 'was', 'easier', 'to', 'give', 'up', 'completely', 'rather', 'than', 'be', 'a', 'failure', ',', 'and', 'i', 'didn', \"'\", 't', 'tell', 'anyone', 'about', 'this', 'stuff', 'because', 'i', 'thought', 'the', 'options', 'were', 'let', 'them', 'wonder', 'why', 'i', \"'\", 'm', 'depressed', 'or', 'admit', 'i', \"'\", 'm', 'not', 'as', 'smart', 'as', 'they', 'thought', 'and', 'let', 'them', 'all', 'down', '.', 'anyway', ',', 'i', \"'\", 've', 'never', 'really', 'recovered', 'from', 'that', 'depression', ',', 'only', 'cope', '##d', 'with', 'it', 'better', 'over', 'the', 'last', 'few', 'years', 'and', 'tried', 'to', 'accept', 'that', 'i', \"'\", 'm', 'just', 'not', 'as', 'smart', 'as', 'i', 'thought', 'i', 'was', '.', 'but', 'i', \"'\", 've', 'always', 'felt', 'like', 'something', 'was', 'straight', 'up', '*', 'off', '*', 'about', 'the', 'way', 'i', 'felt', 'dumb', ',', 'because', 'i', 'never', 'felt', 'like', 'i', 'was', '\"', 'dumb', '\"', 'exactly', ',', 'more', 'like', 'something', 'was', 'blocking', 'me', 'from', 'being', '\"', 'smart', '.', '\"', 'mostly', 'with', 'memory', 'retrieval', 'and', 'my', 'cognition', '.', 'i', \"'\", 've', 'been', 'talking', 'to', 'my', 'mom', 'about', 'how', 'my', 'memory', 'feels', 'so', 'bog', '##ged', 'down', 'sometimes', 'for', 'at', 'least', 'a', 'year', 'or', 'two', ',', 'but', 'neither', 'of', 'us', 'thought', 'of', 'anything', 'except', 'eating', 'better', '(', 'which', 'i', 'could', 'do', ',', 'honestly', ')', '.', 'if', 'i', 'do', 'have', 'ad', '##hd', ',', 'and', 'there', \"'\", 's', 'a', 'treatment', ',', 'it', 'would', 'be', 'incredible', '.', 'it', 'could', 'potentially', 'change', 'my', 'life', ',', 'and', 'change', 'my', 'plans', 'regarding', 'the', 'military', '(', 'college', 'first', ')', '.', 'i', 'never', 'really', 'wanted', 'to', 'go', 'to', 'college', 'first', ',', 'but', 'if', 'there', \"'\", 's', 'something', 'wrong', 'with', 'me', 'and', 'there', \"'\", 's', 'a', 'way', 'to', 'manage', 'it', 'better', ',', 'i', 'might', 'have', 'a', 'different', 'outlook', 'on', 'life', 'than', 'i', \"'\", 've', 'had', 'previously', ',', 'i', 'don', \"'\", 't', 'know', '.', 'but', 'i', \"'\", 'm', 'quite', 'serious', 'about', 'the', 'military', 'and', 'potentially', 'fucking', 'that', 'up', 'gives', 'me', 'great', 'pause', 'about', 'doing', 'anything', '.', 'some', 'other', 'things', 'about', 'myself', 'that', 'res', '##onate', 'with', 'what', 'i', \"'\", 've', 'read', 'about', 'ad', '##hd', 'pi', ':', '1', '.', 'i', 'fail', 'to', 'follow', 'instructions', 'and', 'finish', 'school', '##work', ',', 'chores', ',', 'etc', '.', 'which', 'really', 'makes', 'me', 'feel', 'like', 'a', 'po', '##s', 'sometimes', '(', 'especially', 'since', 'my', 'mom', 'has', 'her', 'own', 'medical', 'problems', 'right', 'now', 'and', 'relies', 'on', 'me', 'to', 'do', 'a', 'lot', '-', 'i', '*', '*', 'really', '*', '*', 'mean', 'to', 'do', 'stuff', 'that', 'takes', 'me', 'forever', 'to', 'do', 'when', 'it', 'shouldn', \"'\", 't', ')', '.', '2', '.', 'i', 'have', 'trouble', 'organizing', 'activities', '.', 'i', \"'\", 've', 'thought', 'about', 'this', 'a', 'lot', 'in', 'the', 'past', '.', 'it', \"'\", 's', 'one', 'of', 'the', 'things', 'that', 'ann', '##oys', 'me', 'so', 'much', ',', 'it', 'seems', 'like', 'a', 'lot', 'of', 'the', 'time', 'when', 'i', 'try', 'to', 'organize', 'an', 'activity', 'i', 'just', 'can', \"'\", 't', 'get', 'it', 'for', 'some', 'reason', '.', 'it', 'takes', 'me', 'an', 'in', '##ord', '##inate', 'amount', 'of', 'time', ',', 'and', 'i', \"'\", 'm', 'pretty', 'into', 'programming', 'so', 'i', 'run', 'into', 'this', 'problem', 'with', 'projects', 'often', '.', '3', '.', 'i', 'am', 'incredibly', 'forget', '##ful', 'in', 'everyday', 'activities', '.', 'i', 'wish', 'i', 'could', 'give', 'a', 'list', 'but', 'i', 'feel', 'like', 'that', 'would', 'look', 'like', 'cognitive', 'bias', 'and', 'if', 'it', 'didn', \"'\", 't', 'it', 'would', 'be', 'much', 'too', 'long', '.', 'although', 'i', 'realize', 'all', 'of', 'this', 'could', 'be', 'cognitive', 'bias', 'which', 'is', 'why', 'i', 'don', \"'\", 't', 'want', 'to', 'dia', '##gno', '##se', 'myself', '.', 'but', 'i', \"'\", 'm', 'constantly', 'frustrated', 'by', 'this', '.', '4', '.', 'i', 'don', \"'\", 't', 'know', 'what', \"'\", 'appearing', 'to', 'not', 'listen', 'when', 'spoken', 'to', \"'\", 'means', 'exactly', ',', 'but', 'i', 'do', 'have', 'trouble', 'listening', 'sometimes', '.', 'i', 'don', \"'\", 't', 'know', 'if', 'i', \"'\", 'd', 'consider', 'that', 'a', 'sy', '##mpt', '##om', 'of', 'anything', 'though', ',', 'i', \"'\", 've', 'always', 'been', 'that', 'way', '.', 'anyway', '.', '.', '.', 'i', 'don', \"'\", 't', 'know', 'exactly', 'what', 'i', \"'\", 'm', 'looking', 'for', 'here', ',', 'i', 'guess', 'a', 'lot', 'of', 'what', 'i', \"'\", 've', 'told', 'you', 'guys', 'is', 'only', 'useful', 'to', 'a', 'psychiatrist', ',', 'but', 'i', 'think', 'typing', 'all', 'of', 'that', 'out', 'helped', 'me', '.', 'i', 'guess', 'i', \"'\", 'm', 'leaning', 'towards', 'going', 'to', 'see', 'a', 'doc', '/', 'psychiatrist', '.', 'what', 'was', 'it', 'like', 'getting', 'diagnosed', ',', 'and', 'what', \"'\", 's', 'it', 'been', 'like', 'post', '-', 'diagnosis', '?', 'have', 'you', 'had', 'much', 'success', 'with', 'your', 'treatment', '?', 'what', 'are', 'side', 'effects', 'or', 'long', 'term', 'effects', 'you', \"'\", 've', 'had', '?', 'if', 'getting', 'diagnosed', 'with', 'ad', '##hd', 'would', 'potentially', 'mess', 'up', 'something', 'you', \"'\", 've', 'wanted', 'to', 'do', 'for', 'a', 'long', 'time', ',', 'would', 'you', 'still', 'think', 'it', 'was', 'worth', 'it', '?', 'thanks', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 1344\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['not', 'sure', 'what', 'to', 'do', '.', 'i', 'came', 'across', 'something', 'about', 'ad', '##hd', '(', 'the', '\"', 'ina', '##tten', '##tive', '\"', 'type', 'specifically', ',', 'i', 'think', 'it', \"'\", 's', 'referred', 'to', 'as', 'ad', '##hd', '-', 'pi', ')', 'purely', 'by', 'chance', 'not', 'long', 'ago', 'and', 'it', 'hit', 'me', 'pretty', 'hard', ',', 'i', \"'\", 've', 'been', 'noticing', 'a', 'lot', 'of', 'stuff', 'about', 'myself', 'the', 'last', 'few', 'years', 'but', 'it', 'never', 'occurred', 'to', 'me', 'that', 'i', 'could', 'actually', 'be', 'add', '/', 'ad', '##hd', '.', '.', '.', 'except', 'it', 'described', 'me', 'so', 'spot', 'on', 'that', 'i', \"'\", 'm', '*', 'nearly', '*', 'convinced', 'i', 'have', 'it', 'for', 'sure', '.', 'i', 'really', 'don', \"'\", 't', 'like', 'to', 'dia', '##gno', '##se', 'myself', 'because', 'i', \"'\", 'm', 'not', 'a', 'doctor', '/', 'psychiatrist', ',', 'but', 'here', \"'\", 's', 'where', 'the', 'problems', 'start', 'coming', 'in', '.', 'i', \"'\", 'm', 'turning', '17', 'in', 'a', 'little', 'over', 'a', 'month', ',', 'and', 'i', \"'\", 've', 'been', 'planning', 'on', 'enlist', '##ing', 'in', 'the', 'military', 'at', '18', 'ever', 'since', 'i', 'was', '12', '.', 'the', 'military', 'allows', 'people', 'with', 'ad', '##hd', 'to', 'join', ',', 'but', 'only', 'if', 'they', 'get', 'a', 'wai', '##ver', ',', 'and', 'they', \"'\", 're', 'only', 'eligible', 'for', 'a', 'wai', '##ver', 'if', 'they', \"'\", 've', 'been', 'off', 'any', 'ad', '##hd', 'medication', 'for', 'at', 'least', '1', 'year', '.', 'so', 'if', 'i', 'did', 'get', 'diagnosed', 'with', 'ad', '##hd', ',', 'it', 'would', 'go', 'on', 'my', 'medical', 'record', ',', 'and', 'even', 'if', 'i', 'did', 'decide', 'to', 'try', 'out', 'a', 'treatment', 'for', 'it', 'i', 'would', 'either', 'have', 'to', 'push', 'back', 'my', 'plans', 'or', 'only', 'take', 'the', 'med', '##s', 'for', 'about', 'a', 'month', '.', 'plus', 'the', 'military', 'is', 'facing', 'cuts', 'and', 'can', 'be', 'more', 'selective', 'in', 'who', 'it', 'recruits', 'now', '.', 'i', \"'\", 'm', 'worried', 'having', 'something', 'like', 'ad', '##hd', 'on', 'my', 'record', 'might', 'be', 'a', 'bigger', 'b', '##lot', '##ch', 'than', 'it', 'was', 'a', 'few', 'years', 'ago', ',', 'when', 'the', 'military', 'was', 'still', 'gun', '##g', '-', 'ho', 'let', \"'\", 's', 'recruit', 'the', 'shit', 'out', 'of', 'people', 'fuck', 'yeah', 'wars', '.', 'it', 'doesn', \"'\", 't', 'really', 'seem', 'worth', 'it', ',', 'but', 'here', \"'\", 's', 'the', 'thing', '.', 'my', 'life', 'pretty', 'much', 'went', 'off', 'track', 'in', 'the', '6th', '-', '7th', 'grade', '.', 'i', 'went', 'from', 'just', 'gliding', 'through', 'school', 'to', 'failing', 'for', 'the', 'first', 'time', 'in', 'my', 'life', 'and', 'it', 'was', 'such', 'a', 'sudden', 'turn', '.', 'the', 'work', 'didn', \"'\", 't', 'get', 'that', 'much', 'harder', '.', 'i', 'was', 'pretty', 'confident', 'in', 'my', 'ability', 'to', 'ace', 'school', '##work', 'because', 'i', \"'\", 'd', 'always', 'been', 'in', 'the', 'gifted', 'classes', ',', 'told', 'i', 'was', 'really', 'smart', ',', 'on', 'the', 'yearly', 'iq', 'tests', 'they', 'made', 'us', 'take', 'i', 'always', 'scored', 'in', 'the', '130', '##s', ',', 'etc', '.', 'and', 'then', 'it', 'was', 'like', 'my', 'mind', 'started', 'working', 'at', 'a', 'much', 'slower', 'pace', '.', 'it', 'would', 'take', 'me', 'forever', 'to', 'recall', 'things', ',', 'i', 'was', 'forgetting', 'stuff', 'and', 'just', 'not', 'being', 'able', 'to', 'remember', 'it', 'for', 'the', 'life', 'of', 'me', 'for', 'short', 'periods', 'of', 'time', ',', 'i', 'constantly', 'lost', 'focus', 'on', 'what', 'i', 'was', 'doing', 'and', 'it', 'was', 'like', 'walking', 'up', 'a', 'steep', 'incline', 'in', 'muddy', ',', 'snowy', ',', 'liquid', 'quick'], ['##san', '##d', 'just', 'to', 'try', 'and', 'make', 'myself', 'just', 'do', 'my', 'school', '##work', 'at', 'all', '.', 'i', 'ended', 'up', 'not', 'turning', 'in', 'a', 'lot', 'of', 'work', 'for', 'awhile', 'and', 'fell', 'into', 'a', 'pretty', 'deep', 'depression', '.', 'my', 'parents', '/', 'counselor', '##s', '/', 'etc', '.', 'thought', 'i', 'was', 'getting', 'bull', '##ied', 'or', 'something', ',', 'but', 'i', 'was', 'just', 'really', 'depressed', 'because', 'i', 'felt', 'like', 'a', 'complete', 'failure', 'and', 'like', 'everything', 'about', 'me', 'being', 'oh', 'so', 'smart', 'had', 'been', 'a', 'far', '##ce', '.', 'i', 'felt', 'like', 'it', 'was', 'easier', 'to', 'give', 'up', 'completely', 'rather', 'than', 'be', 'a', 'failure', ',', 'and', 'i', 'didn', \"'\", 't', 'tell', 'anyone', 'about', 'this', 'stuff', 'because', 'i', 'thought', 'the', 'options', 'were', 'let', 'them', 'wonder', 'why', 'i', \"'\", 'm', 'depressed', 'or', 'admit', 'i', \"'\", 'm', 'not', 'as', 'smart', 'as', 'they', 'thought', 'and', 'let', 'them', 'all', 'down', '.', 'anyway', ',', 'i', \"'\", 've', 'never', 'really', 'recovered', 'from', 'that', 'depression', ',', 'only', 'cope', '##d', 'with', 'it', 'better', 'over', 'the', 'last', 'few', 'years', 'and', 'tried', 'to', 'accept', 'that', 'i', \"'\", 'm', 'just', 'not', 'as', 'smart', 'as', 'i', 'thought', 'i', 'was', '.', 'but', 'i', \"'\", 've', 'always', 'felt', 'like', 'something', 'was', 'straight', 'up', '*', 'off', '*', 'about', 'the', 'way', 'i', 'felt', 'dumb', ',', 'because', 'i', 'never', 'felt', 'like', 'i', 'was', '\"', 'dumb', '\"', 'exactly', ',', 'more', 'like', 'something', 'was', 'blocking', 'me', 'from', 'being', '\"', 'smart', '.', '\"', 'mostly', 'with', 'memory', 'retrieval', 'and', 'my', 'cognition', '.', 'i', \"'\", 've', 'been', 'talking', 'to', 'my', 'mom', 'about', 'how', 'my', 'memory', 'feels', 'so', 'bog', '##ged', 'down', 'sometimes', 'for', 'at', 'least', 'a', 'year', 'or', 'two', ',', 'but', 'neither', 'of', 'us', 'thought', 'of', 'anything', 'except', 'eating', 'better', '(', 'which', 'i', 'could', 'do', ',', 'honestly', ')', '.', 'if', 'i', 'do', 'have', 'ad', '##hd', ',', 'and', 'there', \"'\", 's', 'a', 'treatment', ',', 'it', 'would', 'be', 'incredible', '.', 'it', 'could', 'potentially', 'change', 'my', 'life', ',', 'and', 'change', 'my', 'plans', 'regarding', 'the', 'military', '(', 'college', 'first', ')', '.', 'i', 'never', 'really', 'wanted', 'to', 'go', 'to', 'college', 'first', ',', 'but', 'if', 'there', \"'\", 's', 'something', 'wrong', 'with', 'me', 'and', 'there', \"'\", 's', 'a', 'way', 'to', 'manage', 'it', 'better', ',', 'i', 'might', 'have', 'a', 'different', 'outlook', 'on', 'life', 'than', 'i', \"'\", 've', 'had', 'previously', ',', 'i', 'don', \"'\", 't', 'know', '.', 'but', 'i', \"'\", 'm', 'quite', 'serious', 'about', 'the', 'military', 'and', 'potentially', 'fucking', 'that', 'up', 'gives', 'me', 'great', 'pause', 'about', 'doing', 'anything', '.', 'some', 'other', 'things', 'about', 'myself', 'that', 'res', '##onate', 'with', 'what', 'i', \"'\", 've', 'read', 'about', 'ad', '##hd', 'pi', ':', '1', '.', 'i', 'fail', 'to', 'follow', 'instructions', 'and', 'finish', 'school', '##work', ',', 'chores', ',', 'etc', '.', 'which', 'really', 'makes', 'me', 'feel', 'like', 'a', 'po', '##s', 'sometimes', '(', 'especially', 'since', 'my', 'mom', 'has', 'her', 'own', 'medical', 'problems', 'right', 'now', 'and', 'relies', 'on', 'me', 'to', 'do', 'a', 'lot', '-', 'i', '*', '*', 'really', '*', '*', 'mean', 'to', 'do', 'stuff', 'that', 'takes', 'me', 'forever', 'to', 'do', 'when', 'it', 'shouldn', \"'\", 't', ')', '.', '2', '.', 'i', 'have', 'trouble', 'organizing', 'activities', '.', 'i', \"'\", 've', 'thought', 'about', 'this', 'a', 'lot', 'in', 'the', 'past', '.', 'it', \"'\", 's', 'one', 'of', 'the', 'things', 'that', 'ann', '##oys', 'me', 'so', 'much', ',', 'it'], ['seems', 'like', 'a', 'lot', 'of', 'the', 'time', 'when', 'i', 'try', 'to', 'organize', 'an', 'activity', 'i', 'just', 'can', \"'\", 't', 'get', 'it', 'for', 'some', 'reason', '.', 'it', 'takes', 'me', 'an', 'in', '##ord', '##inate', 'amount', 'of', 'time', ',', 'and', 'i', \"'\", 'm', 'pretty', 'into', 'programming', 'so', 'i', 'run', 'into', 'this', 'problem', 'with', 'projects', 'often', '.', '3', '.', 'i', 'am', 'incredibly', 'forget', '##ful', 'in', 'everyday', 'activities', '.', 'i', 'wish', 'i', 'could', 'give', 'a', 'list', 'but', 'i', 'feel', 'like', 'that', 'would', 'look', 'like', 'cognitive', 'bias', 'and', 'if', 'it', 'didn', \"'\", 't', 'it', 'would', 'be', 'much', 'too', 'long', '.', 'although', 'i', 'realize', 'all', 'of', 'this', 'could', 'be', 'cognitive', 'bias', 'which', 'is', 'why', 'i', 'don', \"'\", 't', 'want', 'to', 'dia', '##gno', '##se', 'myself', '.', 'but', 'i', \"'\", 'm', 'constantly', 'frustrated', 'by', 'this', '.', '4', '.', 'i', 'don', \"'\", 't', 'know', 'what', \"'\", 'appearing', 'to', 'not', 'listen', 'when', 'spoken', 'to', \"'\", 'means', 'exactly', ',', 'but', 'i', 'do', 'have', 'trouble', 'listening', 'sometimes', '.', 'i', 'don', \"'\", 't', 'know', 'if', 'i', \"'\", 'd', 'consider', 'that', 'a', 'sy', '##mpt', '##om', 'of', 'anything', 'though', ',', 'i', \"'\", 've', 'always', 'been', 'that', 'way', '.', 'anyway', '.', '.', '.', 'i', 'don', \"'\", 't', 'know', 'exactly', 'what', 'i', \"'\", 'm', 'looking', 'for', 'here', ',', 'i', 'guess', 'a', 'lot', 'of', 'what', 'i', \"'\", 've', 'told', 'you', 'guys', 'is', 'only', 'useful', 'to', 'a', 'psychiatrist', ',', 'but', 'i', 'think', 'typing', 'all', 'of', 'that', 'out', 'helped', 'me', '.', 'i', 'guess', 'i', \"'\", 'm', 'leaning', 'towards', 'going', 'to', 'see', 'a', 'doc', '/', 'psychiatrist', '.', 'what', 'was', 'it', 'like', 'getting', 'diagnosed', ',', 'and', 'what', \"'\", 's', 'it', 'been', 'like', 'post', '-', 'diagnosis', '?', 'have', 'you', 'had', 'much', 'success', 'with', 'your', 'treatment', '?', 'what', 'are', 'side', 'effects', 'or', 'long', 'term', 'effects', 'you', \"'\", 've', 'had', '?', 'if', 'getting', 'diagnosed', 'with', 'ad', '##hd', 'would', 'potentially', 'mess', 'up', 'something', 'you', \"'\", 've', 'wanted', 'to', 'do', 'for', 'a', 'long', 'time', ',', 'would', 'you', 'still', 'think', 'it', 'was', 'worth', 'it', '?', 'thanks', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['trying', 'desperately', 'to', 'find', 'sand', '##oz', '/', 'e', '##on', 'add', '##eral', '##l', 'in', 'san', 'francisco', 'recently', 'switched', 'insurance', 'from', 'kaiser', 'and', 'had', '*', 'no', '*', 'idea', 'how', 'hard', 'it', 'was', 'to', 'get', 'medication', 'without', 'it', '.', 'at', 'kaiser', ',', 'you', 'hand', 'over', 'the', 'hard', 'copy', 'and', 'they', 'hand', 'you', 'back', 'the', 'requested', 'amount', '.', 'no', 'issues', ',', 'no', 'questions', '.', 'they', 'also', 'carry', 'the', 'best', 'generic', '-', 'sand', '##oz', '.', 'i', 'tried', 'wal', '##gree', '##ns', 'first', '(', 'i', \"'\", 'm', 'on', 'blue', 'shield', ')', 'and', 'after', 'spending', 'a', 'good', 'deal', 'of', 'time', 'being', 'smug', 'and', 'treating', 'me', 'like', 'some', 'par', '##iah', '(', '\"', 'this', 'much', '?', '\"', ')', ',', 'i', 'was', 'informed', 'that', 'i', 'would', 'only', 'be', 'able', 'to', 'acquire', 'the', 'core', '##pha', '##rma', 'generic', '.', 'yu', '##p', ',', 'not', 'gonna', 'happen', '.', '.', '.', 'if', 'you', 'live', 'in', 'or', 'around', 'san', 'francisco', 'and', 'know', 'where', 'i', 'can', 'find', 'this', 'generic', 'brand', 'please', 'let', 'me', 'know', '.', 'i', 'have', 'one', 'day', \"'\", 's', 'dos', '##age', 'left', '.']\n",
      "INFO:__main__:Number of tokens: 167\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['trying', 'desperately', 'to', 'find', 'sand', '##oz', '/', 'e', '##on', 'add', '##eral', '##l', 'in', 'san', 'francisco', 'recently', 'switched', 'insurance', 'from', 'kaiser', 'and', 'had', '*', 'no', '*', 'idea', 'how', 'hard', 'it', 'was', 'to', 'get', 'medication', 'without', 'it', '.', 'at', 'kaiser', ',', 'you', 'hand', 'over', 'the', 'hard', 'copy', 'and', 'they', 'hand', 'you', 'back', 'the', 'requested', 'amount', '.', 'no', 'issues', ',', 'no', 'questions', '.', 'they', 'also', 'carry', 'the', 'best', 'generic', '-', 'sand', '##oz', '.', 'i', 'tried', 'wal', '##gree', '##ns', 'first', '(', 'i', \"'\", 'm', 'on', 'blue', 'shield', ')', 'and', 'after', 'spending', 'a', 'good', 'deal', 'of', 'time', 'being', 'smug', 'and', 'treating', 'me', 'like', 'some', 'par', '##iah', '(', '\"', 'this', 'much', '?', '\"', ')', ',', 'i', 'was', 'informed', 'that', 'i', 'would', 'only', 'be', 'able', 'to', 'acquire', 'the', 'core', '##pha', '##rma', 'generic', '.', 'yu', '##p', ',', 'not', 'gonna', 'happen', '.', '.', '.', 'if', 'you', 'live', 'in', 'or', 'around', 'san', 'francisco', 'and', 'know', 'where', 'i', 'can', 'find', 'this', 'generic', 'brand', 'please', 'let', 'me', 'know', '.', 'i', 'have', 'one', 'day', \"'\", 's', 'dos', '##age', 'left', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'a', 'university', '-', 'student', 'and', 'i', 'suspect', 'i', 'suffer', 'from', 'ad', '##hd', '.', 'where', 'should', 'i', 'start', 'to', 'get', 'help', '?']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'a', 'university', '-', 'student', 'and', 'i', 'suspect', 'i', 'suffer', 'from', 'ad', '##hd', '.', 'where', 'should', 'i', 'start', 'to', 'get', 'help', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['essay', 'i', 'have', 'to', 'write', '.', 'can', \"'\", 't', 'i', 'hate', 'to', 'ran', '##t', 'here', '.', 'but', 'i', 'need', 'some', 'words', 'of', 'encouragement', '.', 'i', 'have', 'an', 'essay', 'due', 'on', 'tuesday', '.', 'its', 'supposed', 'to', 'be', '6', 'pages', '.', 'i', 'have', 'a', 'rough', 'idea', 'of', 'what', 'i', 'want', 'to', 'write', '.', 'but', 'i', \"'\", 'm', 'just', 'kind', 'of', 'facing', 'a', 'block', 'and', 'doing', 'anything', 'but', 'the', 'paper', '.', 'i', \"'\", 'm', 'beginning', 'to', 'lose', 'hope', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'take', 'concert', '##a', '27', 'right', 'now', ',', 'but', 'i', 'might', 'tomorrow', '.']\n",
      "INFO:__main__:Number of tokens: 94\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['essay', 'i', 'have', 'to', 'write', '.', 'can', \"'\", 't', 'i', 'hate', 'to', 'ran', '##t', 'here', '.', 'but', 'i', 'need', 'some', 'words', 'of', 'encouragement', '.', 'i', 'have', 'an', 'essay', 'due', 'on', 'tuesday', '.', 'its', 'supposed', 'to', 'be', '6', 'pages', '.', 'i', 'have', 'a', 'rough', 'idea', 'of', 'what', 'i', 'want', 'to', 'write', '.', 'but', 'i', \"'\", 'm', 'just', 'kind', 'of', 'facing', 'a', 'block', 'and', 'doing', 'anything', 'but', 'the', 'paper', '.', 'i', \"'\", 'm', 'beginning', 'to', 'lose', 'hope', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'take', 'concert', '##a', '27', 'right', 'now', ',', 'but', 'i', 'might', 'tomorrow', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['4', 'keys', 'to', 'being', 'successful', 'with', 'ad', '##hd', 'and', 'money']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['4', 'keys', 'to', 'being', 'successful', 'with', 'ad', '##hd', 'and', 'money']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'your', 'position', 'on', 'medication', '?', 'last', 'semester', ',', 'i', 'did', 'a', 'report', 'on', 'ad', '##hd', 'for', 'my', 'english', 'class', ',', 'and', 'it', 'got', 'me', 'thinking', 'about', 'my', 'medication', 'and', 'how', 'i', 'deal', 'with', 'my', 'ad', '##hd', '.', 'i', 'take', 'concert', '##a', 'and', 'have', 'for', 'about', '10', '+', 'years', 'now', '.', 'i', \"'\", 'm', 'curious', 'as', 'to', 'what', 'red', '##dit', 'takes', 'and', 'how', 'they', 'feel', 'about', 'it', '.', 'edit', ':', 'i', 'really', 'like', 'what', 'i', 'see', 'here', ';', 'mixed', 'views', ',', 'pro', '##s', 'and', 'con', '##s', '.', 'i', 'think', 'i', \"'\", 'm', 'going', 'to', 'post', 'my', 'series', 'of', 'papers', 'i', 'did', 'discussing', 'medication', 'use', 'of', 'kids', 'with', 'ad', '##hd', 'and', 'add', '.', 'keep', 'in', 'mind', 'though', 'that', 'i', 'am', 'a', 'fray', 'an', 'engineering', 'student', 'who', 'hated', 'english', ',', 'so', 'i', 'kinda', 'rushed', 'through', 'the', 'assignment', ',', 'so', 'forgive', 'me', 'if', 'it', \"'\", 's', 'not', 'the', 'greatest', '.']\n",
      "INFO:__main__:Number of tokens: 148\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'your', 'position', 'on', 'medication', '?', 'last', 'semester', ',', 'i', 'did', 'a', 'report', 'on', 'ad', '##hd', 'for', 'my', 'english', 'class', ',', 'and', 'it', 'got', 'me', 'thinking', 'about', 'my', 'medication', 'and', 'how', 'i', 'deal', 'with', 'my', 'ad', '##hd', '.', 'i', 'take', 'concert', '##a', 'and', 'have', 'for', 'about', '10', '+', 'years', 'now', '.', 'i', \"'\", 'm', 'curious', 'as', 'to', 'what', 'red', '##dit', 'takes', 'and', 'how', 'they', 'feel', 'about', 'it', '.', 'edit', ':', 'i', 'really', 'like', 'what', 'i', 'see', 'here', ';', 'mixed', 'views', ',', 'pro', '##s', 'and', 'con', '##s', '.', 'i', 'think', 'i', \"'\", 'm', 'going', 'to', 'post', 'my', 'series', 'of', 'papers', 'i', 'did', 'discussing', 'medication', 'use', 'of', 'kids', 'with', 'ad', '##hd', 'and', 'add', '.', 'keep', 'in', 'mind', 'though', 'that', 'i', 'am', 'a', 'fray', 'an', 'engineering', 'student', 'who', 'hated', 'english', ',', 'so', 'i', 'kinda', 'rushed', 'through', 'the', 'assignment', ',', 'so', 'forgive', 'me', 'if', 'it', \"'\", 's', 'not', 'the', 'greatest', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'i', 'do', '/', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'i', 'do', '/', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'insurance', 'company', 'will', 'not', 'approve', 'a', 'claim', 'for', '60', '40', '##mg', 'pills', 'for', 'an', '80', '##mg', 'dose', ',', 'but', 'will', 'approve', 'a', 'script', 'for', '30', '10', '##mg', 'pills', ',', 'and', 'another', 'script', 'for', '30', '70', '##mg', 'pills', '.', 'because', 'it', \"'\", 's', 'only', 'approved', 'for', 'once', '-', 'daily', 'dos', '##ing', '.', 'this', 'is', 'the', 'third', 'time', 'i', \"'\", 'll', 'have', 'to', 'go', 'back', 'to', 'my', 'doctor', 'and', 'get', 'a', 'new', 'prescription', ';', 'last', 'week', 'he', 'wrote', 'me', 'a', 'prescription', 'dated', 'for', 'february', '1st', '.', '*', 'head', '##des', '##k', '*', '(', 'the', 'drug', 'company', 'does', 'not', 'make', 'an', '80', '##mg', 'pill', '.', ')']\n",
      "INFO:__main__:Number of tokens: 103\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'insurance', 'company', 'will', 'not', 'approve', 'a', 'claim', 'for', '60', '40', '##mg', 'pills', 'for', 'an', '80', '##mg', 'dose', ',', 'but', 'will', 'approve', 'a', 'script', 'for', '30', '10', '##mg', 'pills', ',', 'and', 'another', 'script', 'for', '30', '70', '##mg', 'pills', '.', 'because', 'it', \"'\", 's', 'only', 'approved', 'for', 'once', '-', 'daily', 'dos', '##ing', '.', 'this', 'is', 'the', 'third', 'time', 'i', \"'\", 'll', 'have', 'to', 'go', 'back', 'to', 'my', 'doctor', 'and', 'get', 'a', 'new', 'prescription', ';', 'last', 'week', 'he', 'wrote', 'me', 'a', 'prescription', 'dated', 'for', 'february', '1st', '.', '*', 'head', '##des', '##k', '*', '(', 'the', 'drug', 'company', 'does', 'not', 'make', 'an', '80', '##mg', 'pill', '.', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['e', '-', 'books', 'on', 'tablets', 'fight', 'digital', 'distraction', '##s', '(', 'x', '-', 'post', 'from', 'r', '/', 'kind', '##le', ')']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['e', '-', 'books', 'on', 'tablets', 'fight', 'digital', 'distraction', '##s', '(', 'x', '-', 'post', 'from', 'r', '/', 'kind', '##le', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'symptoms', '.', '.', '.', 'any', 'thoughts', '?', 'hello', 'everyone', ',', 'i', \"'\", 'm', 'here', 'to', 'say', ',', 'basically', ',', 'that', 'i', \"'\", 'm', 'tired', 'of', 'being', 'tired', '.', 'i', 'lack', 'motivation', 'and', 'i', \"'\", 'm', 'often', 'fatigue', '##d', '(', 'especially', 'in', 'the', 'morning', ')', '.', 'i', 'have', 'intense', 'urges', 'to', 'stop', 'whatever', 'it', 'is', 'i', \"'\", 'm', 'doing', 'after', 'a', 'couple', 'minutes', 'and', 'go', 'watch', 'tv', 'or', 'something', 'else', 'that', \"'\", 's', 'easy', 'and', 'stimulating', '.', 'it', \"'\", 's', 'a', 'very', 'intense', 'urge', 'like', 'my', 'body', 'and', 'mind', 'cannot', 'stand', 'it', 'anymore', 'and', 'i', 'just', 'have', 'to', 'stand', 'up', 'and', 'leave', '.', 'if', 'i', \"'\", 'm', 'able', 'to', 'do', 'something', ',', 'lets', 'say', 'read', 'a', 'book', ',', 'for', 'more', 'than', 'a', 'few', 'minutes', 'i', 'will', 'begin', 'day', '##dre', '##ami', '##ng', 'and', 'thus', 'won', \"'\", 't', 'be', 'able', 'to', 'retain', 'anything', 'that', 'i', \"'\", 've', 'read', '.', 'i', 'feel', 'like', 'everything', 'is', 'hazy', ',', 'like', 'i', \"'\", 'm', 'looking', 'at', 'life', 'through', 'a', 'fog', '.', 'i', \"'\", 'm', 'extremely', 'intro', '##sp', '##ect', '##ive', ',', 'analytical', ',', 'intro', '##verted', ',', 'self', '-', 'conscious', 'which', 'can', 'lead', 'to', 'bouts', 'of', 'anxiety', 'and', 'depression', 'that', 'can', 'last', 'for', 'a', 'few', 'days', 'or', 'weeks', 'at', 'a', 'time', '(', 'my', 'blood', 'work', 'is', 'fine', 'i', 'do', 'not', 'have', 'any', 'major', 'im', '##balance', '##s', 'or', 'thyroid', 'issues', ')', '.', 'i', \"'\", 'm', 'generally', 'di', '##sor', '##gan', '##ized', 'and', 'lose', 'things', 'constantly', '(', 'mostly', 'because', 'i', 'have', 'no', 'desire', 'to', 'put', 'things', 'away', ',', 'clean', 'my', 'room', 'etc', '.', ')', 'i', 'have', 'taken', 'anti', '-', 'de', '##press', '##ants', 'before', 'as', 'an', 'under', '##grad', 'for', 'my', 'anxiety', '(', 'which', 'mostly', 'stemmed', 'from', 'missing', 'class', 'and', 'pro', '##cr', '##ast', '##inating', 'on', 'projects', ')', 'but', 'those', 'didn', \"'\", 't', 'do', 'too', 'much', '.', 'i', 'was', 'able', 'to', 'graduate', 'somehow', 'though', '.', 'i', 'tried', 'taking', 'concert', '##a', 'last', 'year', 'but', 'just', 'like', 'the', 'anti', '-', 'de', '##press', '##ants', 'i', 'almost', 'felt', 'more', 'tired', 'and', 'un', '##fo', '##cus', '##ed', 'on', 'it', '.', 'i', \"'\", 'am', 'in', 'no', 'sense', 'hyper', '##active', 'though', '.', 'i', 'have', 'been', 'doing', 'some', 'research', 'and', 'it', 'seems', 'like', 'ad', '##hd', '-', 'pi', 'really', 'speaks', 'true', 'to', 'me', '.', 'as', 'well', 'as', 'slug', '##gis', '##h', 'cognitive', 'tempo', '.', 'the', 'biggest', 'issue', 'though', 'is', 'really', 'my', 'lack', 'of', 'motivation', ',', 'my', 'fatigue', ',', 'and', 'pro', '##cr', '##ast', '##ination', '.', 'i', 'will', 'be', 'seeing', 'a', 'psychologist', 'later', 'next', 'week', 'to', 'discuss', 'all', 'this', 'but', 'i', 'was', 'just', 'curious', 'what', 'this', 'community', 'had', 'to', 'say', ',', 'to', 'see', 'if', 'any', 'of', 'this', 'sounded', 'familiar', 'to', 'you', '.', 'thanks', 'for', 'your', 'thoughts', 'everyone', '.']\n",
      "INFO:__main__:Number of tokens: 431\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'symptoms', '.', '.', '.', 'any', 'thoughts', '?', 'hello', 'everyone', ',', 'i', \"'\", 'm', 'here', 'to', 'say', ',', 'basically', ',', 'that', 'i', \"'\", 'm', 'tired', 'of', 'being', 'tired', '.', 'i', 'lack', 'motivation', 'and', 'i', \"'\", 'm', 'often', 'fatigue', '##d', '(', 'especially', 'in', 'the', 'morning', ')', '.', 'i', 'have', 'intense', 'urges', 'to', 'stop', 'whatever', 'it', 'is', 'i', \"'\", 'm', 'doing', 'after', 'a', 'couple', 'minutes', 'and', 'go', 'watch', 'tv', 'or', 'something', 'else', 'that', \"'\", 's', 'easy', 'and', 'stimulating', '.', 'it', \"'\", 's', 'a', 'very', 'intense', 'urge', 'like', 'my', 'body', 'and', 'mind', 'cannot', 'stand', 'it', 'anymore', 'and', 'i', 'just', 'have', 'to', 'stand', 'up', 'and', 'leave', '.', 'if', 'i', \"'\", 'm', 'able', 'to', 'do', 'something', ',', 'lets', 'say', 'read', 'a', 'book', ',', 'for', 'more', 'than', 'a', 'few', 'minutes', 'i', 'will', 'begin', 'day', '##dre', '##ami', '##ng', 'and', 'thus', 'won', \"'\", 't', 'be', 'able', 'to', 'retain', 'anything', 'that', 'i', \"'\", 've', 'read', '.', 'i', 'feel', 'like', 'everything', 'is', 'hazy', ',', 'like', 'i', \"'\", 'm', 'looking', 'at', 'life', 'through', 'a', 'fog', '.', 'i', \"'\", 'm', 'extremely', 'intro', '##sp', '##ect', '##ive', ',', 'analytical', ',', 'intro', '##verted', ',', 'self', '-', 'conscious', 'which', 'can', 'lead', 'to', 'bouts', 'of', 'anxiety', 'and', 'depression', 'that', 'can', 'last', 'for', 'a', 'few', 'days', 'or', 'weeks', 'at', 'a', 'time', '(', 'my', 'blood', 'work', 'is', 'fine', 'i', 'do', 'not', 'have', 'any', 'major', 'im', '##balance', '##s', 'or', 'thyroid', 'issues', ')', '.', 'i', \"'\", 'm', 'generally', 'di', '##sor', '##gan', '##ized', 'and', 'lose', 'things', 'constantly', '(', 'mostly', 'because', 'i', 'have', 'no', 'desire', 'to', 'put', 'things', 'away', ',', 'clean', 'my', 'room', 'etc', '.', ')', 'i', 'have', 'taken', 'anti', '-', 'de', '##press', '##ants', 'before', 'as', 'an', 'under', '##grad', 'for', 'my', 'anxiety', '(', 'which', 'mostly', 'stemmed', 'from', 'missing', 'class', 'and', 'pro', '##cr', '##ast', '##inating', 'on', 'projects', ')', 'but', 'those', 'didn', \"'\", 't', 'do', 'too', 'much', '.', 'i', 'was', 'able', 'to', 'graduate', 'somehow', 'though', '.', 'i', 'tried', 'taking', 'concert', '##a', 'last', 'year', 'but', 'just', 'like', 'the', 'anti', '-', 'de', '##press', '##ants', 'i', 'almost', 'felt', 'more', 'tired', 'and', 'un', '##fo', '##cus', '##ed', 'on', 'it', '.', 'i', \"'\", 'am', 'in', 'no', 'sense', 'hyper', '##active', 'though', '.', 'i', 'have', 'been', 'doing', 'some', 'research', 'and', 'it', 'seems', 'like', 'ad', '##hd', '-', 'pi', 'really', 'speaks', 'true', 'to', 'me', '.', 'as', 'well', 'as', 'slug', '##gis', '##h', 'cognitive', 'tempo', '.', 'the', 'biggest', 'issue', 'though', 'is', 'really', 'my', 'lack', 'of', 'motivation', ',', 'my', 'fatigue', ',', 'and', 'pro', '##cr', '##ast', '##ination', '.', 'i', 'will', 'be', 'seeing', 'a', 'psychologist', 'later', 'next', 'week', 'to', 'discuss', 'all', 'this', 'but', 'i', 'was', 'just', 'curious', 'what', 'this', 'community', 'had', 'to', 'say', ',', 'to', 'see', 'if', 'any', 'of', 'this', 'sounded', 'familiar', 'to', 'you', '.', 'thanks', 'for', 'your', 'thoughts', 'everyone', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'the', 'perception', 'of', 'add', 'different', 'in', 'the', 'uk', '?', 'i', 'know', 'they', 'have', 'a', 'different', 'health', 'policy', ',', 'so', 'i', 'just', 'can', \"'\", 't', 'help', 'but', 'be', 'curious', '.', 'like', ',', 'is', 'it', 'treated', 'the', 'same', '?', 'do', 'people', 'think', 'it', 'doesn', \"'\", 't', 'exist', 'over', 'there', '?', 'i', \"'\", 'm', 'just', 'curious', '.', 'edit', ':', 'i', 'guess', 'i', 'should', 'include', 'australia', ',', 'too', '.']\n",
      "INFO:__main__:Number of tokens: 66\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'the', 'perception', 'of', 'add', 'different', 'in', 'the', 'uk', '?', 'i', 'know', 'they', 'have', 'a', 'different', 'health', 'policy', ',', 'so', 'i', 'just', 'can', \"'\", 't', 'help', 'but', 'be', 'curious', '.', 'like', ',', 'is', 'it', 'treated', 'the', 'same', '?', 'do', 'people', 'think', 'it', 'doesn', \"'\", 't', 'exist', 'over', 'there', '?', 'i', \"'\", 'm', 'just', 'curious', '.', 'edit', ':', 'i', 'guess', 'i', 'should', 'include', 'australia', ',', 'too', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['started', 'concert', '##a', 'on', 'friday', '.', 'haven', \"'\", 't', 'taken', 'drugs', 'in', '10', 'years', '.', 'hi', 'ra', '##dd', '##it', ',', 'i', 'took', 'add', '##eral', '##l', 'and', 'later', 'concert', '##a', 'about', 'ten', 'or', 'so', 'years', 'ago', '(', 'when', 'concert', '##a', 'was', 'new', ')', '.', 'recently', ',', 'due', 'to', 'issues', 'in', 'my', 'school', 'and', 'work', 'life', ',', 'i', 'went', 'back', 'onto', 'it', '(', '18', '##mg', 'once', 'a', 'day', ')', '.', 'perhaps', 'it', \"'\", 's', 'simply', 'a', 'place', '##bo', 'effect', ',', 'but', 'i', 'feel', 'like', 'i', \"'\", 've', 'been', 'much', 'more', 'productive', 'than', 'usual', ',', 'and', 'i', \"'\", 've', 'been', 'sleeping', 'a', 'bit', 'better', '(', 'i', \"'\", 'm', 'usually', 'completely', 'tired', 'during', 'the', 'day', ',', 'but', 'super', 'en', '##er', '##gi', '##zed', 'in', 'the', 'evening', '-', 'making', 'it', 'difficult', 'to', 'sleep', ')', '.', 'i', 'haven', \"'\", 't', 'had', 'any', 'adverse', 'effects', ';', 'i', 'notice', 'my', 'mood', 'has', 'picked', 'up', 'a', 'bit', ',', 'but', 'i', 'mostly', 'attribute', 'that', 'to', 'feeling', 'good', 'about', 'myself', 'and', 'the', 'fact', 'that', 'i', \"'\", 'm', 'able', 'to', 'get', 'more', 'done', 'than', 'i', 'was', 'before', '.', 'i', 'noticed', 'that', 'concert', '##a', 'can', 'lead', 'to', 'sleep', 'issues', ';', 'has', 'anyone', 'here', 'experienced', 'that', '?', 'how', 'did', 'you', 'manage', 'with', 'it', '?', 'i', 'believe', 'that', 'keeping', 'a', 'good', 'routine', 'will', 'help', 'prevent', 'sleep', 'issues', '.', 'previously', ',', 'i', 'never', 'experienced', 'any', ',', 'but', 'i', 'happened', 'to', 'be', 'on', 'a', 'combination', 'of', 'concert', 'and', 'ce', '##le', '##xa', '(', 'concert', '##a', 'in', 'the', 'morning', ',', 'and', 'ce', '##le', '##xa', 'at', 'night', 'to', 'help', 'me', 'sleep', ')', '.', 'given', 'that', 'it', \"'\", 's', 'an', 'issue', 'i', 'face', 'already', 'without', 'the', 'medication', ',', 'i', \"'\", 'm', 'simply', 'concerned', 'that', 'it', 'can', 'grow', 'worse', '?']\n",
      "INFO:__main__:Number of tokens: 277\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['started', 'concert', '##a', 'on', 'friday', '.', 'haven', \"'\", 't', 'taken', 'drugs', 'in', '10', 'years', '.', 'hi', 'ra', '##dd', '##it', ',', 'i', 'took', 'add', '##eral', '##l', 'and', 'later', 'concert', '##a', 'about', 'ten', 'or', 'so', 'years', 'ago', '(', 'when', 'concert', '##a', 'was', 'new', ')', '.', 'recently', ',', 'due', 'to', 'issues', 'in', 'my', 'school', 'and', 'work', 'life', ',', 'i', 'went', 'back', 'onto', 'it', '(', '18', '##mg', 'once', 'a', 'day', ')', '.', 'perhaps', 'it', \"'\", 's', 'simply', 'a', 'place', '##bo', 'effect', ',', 'but', 'i', 'feel', 'like', 'i', \"'\", 've', 'been', 'much', 'more', 'productive', 'than', 'usual', ',', 'and', 'i', \"'\", 've', 'been', 'sleeping', 'a', 'bit', 'better', '(', 'i', \"'\", 'm', 'usually', 'completely', 'tired', 'during', 'the', 'day', ',', 'but', 'super', 'en', '##er', '##gi', '##zed', 'in', 'the', 'evening', '-', 'making', 'it', 'difficult', 'to', 'sleep', ')', '.', 'i', 'haven', \"'\", 't', 'had', 'any', 'adverse', 'effects', ';', 'i', 'notice', 'my', 'mood', 'has', 'picked', 'up', 'a', 'bit', ',', 'but', 'i', 'mostly', 'attribute', 'that', 'to', 'feeling', 'good', 'about', 'myself', 'and', 'the', 'fact', 'that', 'i', \"'\", 'm', 'able', 'to', 'get', 'more', 'done', 'than', 'i', 'was', 'before', '.', 'i', 'noticed', 'that', 'concert', '##a', 'can', 'lead', 'to', 'sleep', 'issues', ';', 'has', 'anyone', 'here', 'experienced', 'that', '?', 'how', 'did', 'you', 'manage', 'with', 'it', '?', 'i', 'believe', 'that', 'keeping', 'a', 'good', 'routine', 'will', 'help', 'prevent', 'sleep', 'issues', '.', 'previously', ',', 'i', 'never', 'experienced', 'any', ',', 'but', 'i', 'happened', 'to', 'be', 'on', 'a', 'combination', 'of', 'concert', 'and', 'ce', '##le', '##xa', '(', 'concert', '##a', 'in', 'the', 'morning', ',', 'and', 'ce', '##le', '##xa', 'at', 'night', 'to', 'help', 'me', 'sleep', ')', '.', 'given', 'that', 'it', \"'\", 's', 'an', 'issue', 'i', 'face', 'already', 'without', 'the', 'medication', ',', 'i', \"'\", 'm', 'simply', 'concerned', 'that', 'it', 'can', 'grow', 'worse', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['concert', '##a', 'vs', '.', 'rita', '##lin', 'vs', '.', 'v', '##y', '##van', '##se', 'has', 'anyone', 'around', 'here', 'had', 'experiences', 'with', 'all', 'three', 'of', 'these', '?', 'i', \"'\", 'm', 'seeing', 'a', 'psychiatrist', 'tomorrow', 'where', 'i', \"'\", 'm', 'likely', 'to', 'be', 'diagnosed', 'with', 'ad', '##hd', '-', 'pi', '.', 'i', 'have', 'been', 'informally', 'diagnosed', 'with', 'it', 'by', 'my', 'primary', 'care', 'physician', 'last', 'summer', 'and', 'was', 'put', 'on', 'concert', '##a', 'which', 'did', 'next', 'to', 'nothing', 'for', 'my', 'ina', '##tten', '##tive', '##ness', ',', 'let', '##har', '##gy', ',', 'lack', 'of', 'motivation', 'and', 'repetitive', 'loop', '##y', 'mind', 'cl', '##utter', '(', 'my', 'main', 'symptoms', ')', '.', 'i', 'know', 'every', 'reacts', 'to', 'prescription', '##s', 'differently', 'and', 'i', \"'\", 'll', 'eventually', 'get', 'what', 'i', 'needed', 'dialed', 'in', '.', 'but', 'is', 'it', 'common', 'for', ',', 'say', ',', 'concert', '##a', ',', 'to', 'have', 'zero', 'affect', 'and', 'then', 'have', 'v', '##y', '##van', '##se', 'or', 'another', 'similar', 'drug', 'have', 'an', 'actual', 'affect', '?', 'what', 'were', 'your', 'experiences', 'in', 'finding', 'the', 'right', 'medication', 'for', 'your', 'pi', '?']\n",
      "INFO:__main__:Number of tokens: 161\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['concert', '##a', 'vs', '.', 'rita', '##lin', 'vs', '.', 'v', '##y', '##van', '##se', 'has', 'anyone', 'around', 'here', 'had', 'experiences', 'with', 'all', 'three', 'of', 'these', '?', 'i', \"'\", 'm', 'seeing', 'a', 'psychiatrist', 'tomorrow', 'where', 'i', \"'\", 'm', 'likely', 'to', 'be', 'diagnosed', 'with', 'ad', '##hd', '-', 'pi', '.', 'i', 'have', 'been', 'informally', 'diagnosed', 'with', 'it', 'by', 'my', 'primary', 'care', 'physician', 'last', 'summer', 'and', 'was', 'put', 'on', 'concert', '##a', 'which', 'did', 'next', 'to', 'nothing', 'for', 'my', 'ina', '##tten', '##tive', '##ness', ',', 'let', '##har', '##gy', ',', 'lack', 'of', 'motivation', 'and', 'repetitive', 'loop', '##y', 'mind', 'cl', '##utter', '(', 'my', 'main', 'symptoms', ')', '.', 'i', 'know', 'every', 'reacts', 'to', 'prescription', '##s', 'differently', 'and', 'i', \"'\", 'll', 'eventually', 'get', 'what', 'i', 'needed', 'dialed', 'in', '.', 'but', 'is', 'it', 'common', 'for', ',', 'say', ',', 'concert', '##a', ',', 'to', 'have', 'zero', 'affect', 'and', 'then', 'have', 'v', '##y', '##van', '##se', 'or', 'another', 'similar', 'drug', 'have', 'an', 'actual', 'affect', '?', 'what', 'were', 'your', 'experiences', 'in', 'finding', 'the', 'right', 'medication', 'for', 'your', 'pi', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##eral', '##l', 'vs', '.', 'ad', '##hd', '-', 'a', 'little', 'bit', 'of', 'perspective', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##eral', '##l', 'vs', '.', 'ad', '##hd', '-', 'a', 'little', 'bit', 'of', 'perspective', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'x', '##r', ',', 'what', 'should', 'i', 'know', '.', 'i', 'also', 'take', 'ce', '##le', '##xa', ',', 'does', 'anyone', 'else', 'have', 'this', 'combination', '?']\n",
      "INFO:__main__:Number of tokens: 25\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'x', '##r', ',', 'what', 'should', 'i', 'know', '.', 'i', 'also', 'take', 'ce', '##le', '##xa', ',', 'does', 'anyone', 'else', 'have', 'this', 'combination', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['skeptical', 'psychiatrist', 'scared', 'me', 'away', 'from', 'treatment']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['skeptical', 'psychiatrist', 'scared', 'me', 'away', 'from', 'treatment']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'appointment', 'tomorrow', ',', 'do', 'i', 'tell', 'them', 'i', 'smoke', 'weed', '?', 'hi', 'r', '/', 'ad', '##hd', '.', 'i', 'have', 'an', 'appointment', 'with', 'a', 'new', 'psychiatrist', 'tomorrow', 'to', 'talk', 'about', 'ad', '##hd', 'and', 'social', 'anxiety', '.', 'in', 'the', 'new', 'patient', 'forms', 'there', \"'\", 's', 'one', 'question', 'asking', 'if', 'i', 'currently', 'use', 'recreational', 'or', 'street', 'drugs', '.', 'i', 'smoke', '(', 'technically', 'va', '##pe', ')', 'marijuana', 'every', 'once', 'in', 'a', 'while', 'and', 'am', 'not', 'sure', 'whether', 'to', 'answer', 'truth', '##fully', '.', 'i', 'have', 'read', 'a', 'few', 'stories', 'on', 'forums', 'in', 'which', 'the', 'doc', 'immediately', 'labeled', 'the', 'patient', 'as', 'drug', 'seeking', 'or', 'would', 'not', 'pre', '##scribe', 'st', '##im', '##ula', '##nts', 'because', 'of', 'the', 'pot', 'smoking', '.', 'i', 'of', 'course', 'want', 'the', 'best', 'treatment', 'for', 'me', 'but', 'i', 'do', 'not', 'want', 'to', 'lie', 'either', '.', 'any', 'advice', '?', '*', '*', 'edit', '*', '*', ':', 'thanks', 'all', ',', 'i', 'will', 'bring', 'it', 'up', 'the', 'fact', 'i', 'do', 'it', 'occasionally', 'but', 'also', 'mention', 'i', 'don', \"'\", 't', 'and', 'don', \"'\", 't', 'want', 'to', 'rely', 'on', 'it', '.', '*', '*', 'update', '*', '*', ':', 'i', 'had', 'my', 'appointment', 'this', 'afternoon', 'and', 'was', 'honest', 'about', 'my', 'marijuana', 'use', '.', 'my', 'psychiatrist', 'didn', \"'\", 't', 'seem', 'to', 'mind', 'but', 'suggested', 'i', 'stop', 'or', 'reduce', 'my', 'intake', 'of', 'that', 'and', 'alcohol', '.', 'thank', 'all', 'for', 'your', 'advice', '.', 'turns', 'out', 'i', 'have', 'ad', '##hd', 'like', 'i', 'thought', '.', '.', '.', 'welcome', 'to', 'the', 'club', '!']\n",
      "INFO:__main__:Number of tokens: 235\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', 'appointment', 'tomorrow', ',', 'do', 'i', 'tell', 'them', 'i', 'smoke', 'weed', '?', 'hi', 'r', '/', 'ad', '##hd', '.', 'i', 'have', 'an', 'appointment', 'with', 'a', 'new', 'psychiatrist', 'tomorrow', 'to', 'talk', 'about', 'ad', '##hd', 'and', 'social', 'anxiety', '.', 'in', 'the', 'new', 'patient', 'forms', 'there', \"'\", 's', 'one', 'question', 'asking', 'if', 'i', 'currently', 'use', 'recreational', 'or', 'street', 'drugs', '.', 'i', 'smoke', '(', 'technically', 'va', '##pe', ')', 'marijuana', 'every', 'once', 'in', 'a', 'while', 'and', 'am', 'not', 'sure', 'whether', 'to', 'answer', 'truth', '##fully', '.', 'i', 'have', 'read', 'a', 'few', 'stories', 'on', 'forums', 'in', 'which', 'the', 'doc', 'immediately', 'labeled', 'the', 'patient', 'as', 'drug', 'seeking', 'or', 'would', 'not', 'pre', '##scribe', 'st', '##im', '##ula', '##nts', 'because', 'of', 'the', 'pot', 'smoking', '.', 'i', 'of', 'course', 'want', 'the', 'best', 'treatment', 'for', 'me', 'but', 'i', 'do', 'not', 'want', 'to', 'lie', 'either', '.', 'any', 'advice', '?', '*', '*', 'edit', '*', '*', ':', 'thanks', 'all', ',', 'i', 'will', 'bring', 'it', 'up', 'the', 'fact', 'i', 'do', 'it', 'occasionally', 'but', 'also', 'mention', 'i', 'don', \"'\", 't', 'and', 'don', \"'\", 't', 'want', 'to', 'rely', 'on', 'it', '.', '*', '*', 'update', '*', '*', ':', 'i', 'had', 'my', 'appointment', 'this', 'afternoon', 'and', 'was', 'honest', 'about', 'my', 'marijuana', 'use', '.', 'my', 'psychiatrist', 'didn', \"'\", 't', 'seem', 'to', 'mind', 'but', 'suggested', 'i', 'stop', 'or', 'reduce', 'my', 'intake', 'of', 'that', 'and', 'alcohol', '.', 'thank', 'all', 'for', 'your', 'advice', '.', 'turns', 'out', 'i', 'have', 'ad', '##hd', 'like', 'i', 'thought', '.', '.', '.', 'welcome', 'to', 'the', 'club', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'seven', 'deadly', 'sins', ':', 'wrath', '&', 'the', 'imp', '##ulsive', 'temper']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'seven', 'deadly', 'sins', ':', 'wrath', '&', 'the', 'imp', '##ulsive', 'temper']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ia', '##ma', 'producer', 'of', 'a', 'documentary', 'on', 'ad', '/', 'hd', 'looking', 'for', 'connections', 'located', 'in', 'po', '##ugh', '##kee', '##ps', '##ie', ',', 'ny', '.', 'will', 'travel', 'to', 'neighboring', 'states', ',', 'and', 'people', 'within', 'reasonable', 'driving', 'distances', 'from', 'our', 'area', '.', 'if', 'you', 'are', 'interested', ',', 'please', 'e', '-', 'mail', 'learning', '##in', '##pro', '##gre', '##ss', '@', 'hot', '##mail', '.', 'com', 'more', 'info', 'will', 'be', 'provided']\n",
      "INFO:__main__:Number of tokens: 63\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ia', '##ma', 'producer', 'of', 'a', 'documentary', 'on', 'ad', '/', 'hd', 'looking', 'for', 'connections', 'located', 'in', 'po', '##ugh', '##kee', '##ps', '##ie', ',', 'ny', '.', 'will', 'travel', 'to', 'neighboring', 'states', ',', 'and', 'people', 'within', 'reasonable', 'driving', 'distances', 'from', 'our', 'area', '.', 'if', 'you', 'are', 'interested', ',', 'please', 'e', '-', 'mail', 'learning', '##in', '##pro', '##gre', '##ss', '@', 'hot', '##mail', '.', 'com', 'more', 'info', 'will', 'be', 'provided']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['when', 'the', 'ad', '##hd', 'med', '##s', 'kick', 'in']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['when', 'the', 'ad', '##hd', 'med', '##s', 'kick', 'in']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'failing', 'get', 'easier', '?']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'failing', 'get', 'easier', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hesitation', 'and', 'ad', '##hd', '.', 'i', 'just', 'had', 'a', 'bit', 'of', 'a', 'revelation', 'today', ',', 'and', 'i', 'wondered', 'if', 'anyone', 'had', 'similar', 'stories', 'to', 'share', '.', 'as', 'i', 'get', 'older', '(', 'i', \"'\", 'm', '19', ')', ',', 'i', 'find', 'that', 'my', 'ability', 'to', 'do', 'anything', 'other', 'than', 'brows', '##e', 'the', 'internet', 'decreases', '.', 'it', 'has', 'become', 'a', 'really', 'big', 'problem', ',', 'with', 'school', ',', 'chores', ',', 'finding', 'a', 'job', ',', 'etc', '.', 'i', 'can', \"'\", 't', '*', 'start', '*', 'anything', '.', 'i', 'thought', 'it', 'was', 'ad', '##hd', ',', 'but', 'i', 'realized', 'today', 'that', 'it', \"'\", 's', 'an', '*', 'effect', '*', 'of', 'ad', '##hd', '.', 'i', 'was', 'cleaning', 'up', 'after', 'the', 'cats', ',', 'a', 'good', '20', '-', 'minute', 'task', ',', 'which', 'i', 'do', 'every', 'day', 'without', 'hesitation', '.', 'i', 'was', 'wondering', 'to', 'myself', 'how', 'it', 'was', 'i', 'could', 'do', 'this', 'but', 'neglect', 'to', 'clean', 'my', 'absolute', 'train', '##wr', '##eck', 'of', 'a', 'room', 'after', 'long', ',', 'painful', 'months', 'of', 'trying', '.', 'i', 'realized', 'it', 'was', 'because', 'i', '*', 'need', '*', 'to', 'clean', 'up', 'after', 'the', 'cats', ',', 'and', 'i', 'don', \"'\", 't', 'allow', 'myself', 'to', 'hesitate', '.', 'my', 'theory', 'is', ',', 'over', 'the', 'years', 'i', \"'\", 've', 'learned', 'to', 'restrain', 'myself', '.', 'i', 'don', \"'\", 't', 'speak', 'when', 'i', 'want', 'to', ',', 'i', 'don', \"'\", 't', 'do', 'things', 'when', 'i', 'want', 'to', '.', 'i', \"'\", 've', 'got', 'this', 'hesitation', '/', 'slow', '##ness', 'filter', 'over', 'everything', ',', 'because', 'otherwise', 'i', 'can', \"'\", 't', 'function', 'without', 'being', 'all', 'over', 'the', 'place', '.', 'but', 'now', 'it', 'keeps', 'me', 'from', 'doing', '*', 'anything', '*', '.', 'i', \"'\", 'm', 'not', 'a', 'funny', 'person', 'anymore', 'because', 'i', 'make', 'myself', 'slow', 'down', 'to', 'think', 'about', 'that', 'qui', '##p', 'i', 'was', 'about', 'to', 'throw', 'into', 'the', 'conversation', ',', 'saying', 'it', 'bela', '##ted', '##ly', '.', 'instead', 'of', 'just', 'jumping', 'into', 'cleaning', ',', 'i', 'would', 'stop', 'and', 'think', 'and', 'plan', '.', 'because', 'planning', 'was', 'something', 'i', 'was', 'never', 'good', 'at', '.', 'it', \"'\", 's', 'why', 'my', 'ad', '##hd', 'dad', 'is', 'poor', 'and', 'unsuccessful', '.', 'it', 'allows', 'me', 'to', 'work', 'out', 'which', 'parts', 'of', 'the', 'thing', 'i', \"'\", 'm', 'about', 'to', 'undertake', 'are', 'essential', '.', 'except', 'i', 'over', '##di', '##d', 'it', '.', 'now', 'i', 'plan', 'and', 'plan', 'and', 'plan', 'and', 'never', 'do', 'anything', '.', 'because', 'doubts', 'creep', 'into', 'my', 'mind', '.', 'depression', 'and', 'hopeless', '##ness', 'take', 'over', ',', 'because', 'i', \"'\", 've', 'conditioned', 'myself', 'to', 'accept', 'failure', '.', 'i', \"'\", 'm', 'now', 'taking', 'a', 'break', 'from', 'cleaning', 'my', 'room', ',', 'which', 'i', 'managed', 'to', 'start', 'after', 'realizing', 'that', 'all', 'i', 'had', 'to', 'do', 'was', 'make', 'it', 'a', '*', 'need', '*', 'instead', 'of', 'a', 'possibility', 'in', 'my', 'mind', '.', 'i', 'jumped', 'into', 'it', ',', 'i', \"'\", 'm', 'doing', 'things', 'in', 'the', 'order', 'they', 'occur', 'to', 'me', ',', 'and', 'i', \"'\", 'm', 'getting', 'it', 'done', '.', 'planning', 'doesn', \"'\", 't', 'work', 'for', 'me', ',', 'because', 'i', 'can', \"'\", 't', 'do', 'things', 'in', 'a', 'pre', '##set', 'order', '.', 'it', \"'\", 's', 'good', 'that', 'i', 'did', 'plan', 'a', 'while', 'ago', ',', 'because', 'now', 'i', \"'\", 'm', 'not', 'going', 'to', ',', 'say', ',', 'attempt', 'to', 'organize', 'my', 'books', '##hel', '##f', 'before', 'i', \"'\", 've', 'even', 'gotten', 'all', 'of', 'my', 'books', 'off', 'the', 'floor', '.', 'i', 'could', 'have', 'easily', 'done', 'something', 'like', 'that', '.', 'i', 'have', 'those', 'kinds', 'of', 'essential', 'things', 'in', 'the', 'back', 'of', 'my', 'mind', 'because', 'i', 'did', 'plan', 'ahead', ',', 'but', 'now', 'i', \"'\", 'm', 'allowing', 'myself', 'to', 'do', 'what', 'i', 'want', '.', 'i', \"'\", 'm', 'following', 'my', 'w', '##him', '##s', '.', 'and', 'for', 'the', 'first', 'time', 'in', 'a', 'long', 'time', ',', 'i', 'don', \"'\", 't', 'feel', 'useless', '.', 'what', 'i', \"'\", 'm', 'realizing', 'is', 'that', 'i', 'am', 'useless', 'if', 'i', 'try', 'to', 'follow', 'a', 'rule', 'book', ';', 'i', 'can', 'only', 'move', 'forward', 'if', 'i', 'do', 'it', 'in', 'my', 'own', 'style', '.', 'and', 'i', 'think', 'it', \"'\", 's', 'important', 'that', 'every', 'ad', '##hd', '##er', 'know', 'that', 'about', 'themselves', '.', '*', '*', 't', '##l', ';', 'dr', '.', 'i', 'learned', 'that', 'you', 'need', 'to', 'plan', 'ahead', ',', 'but', 'when', 'it', 'comes', 'to', 'it', ',', 'follow', 'your', 'w', '##him', '##s', 'while', 'keeping', 'the', 'bare', 'minimum', 'of', 'essential', 'tasks', 'in', 'the', 'back', 'of', 'your', 'mind', '.', 'you', 'must', 'follow', 'your', 'wants', 'and', 'not', 'hesitate', 'in', 'order', 'to', 'move', 'forward', 'at', 'all', '.', '*', '*', 'hopefully', 'others', 'can', 'relate', '.', 'anyone', 'else', 'ever', 'had', 'this', 'problem', 'with', 'hesitation', 'or', 'a', 'similar', 'revelation', 'about', 'their', 'ad', '##hd', '?', 'edit', ':', 'also', ',', 'i', 'think', 'that', 'when', 'i', 'ignore', 'my', 'wishes', ',', 'they', 'sit', 'in', 'the', 'forefront', 'of', 'my', 'mind', 'bothering', 'me', 'and', 'distracting', 'me', 'from', 'whatever', 'i', 'was', 'trying', 'to', 'force', 'myself', 'to', 'do', '.', 'just', 'a', 'thought', 'i', 'wanted', 'to', 'add', '.', 'i', 'hope', 'the', 'ad', '##hd', 'community', 'doesn', \"'\", 't', 'take', 'this', 'whole', 'thing', 'as', 'self', '-', 'serving', ',', 'i', 'just', 'felt', 'it', 'was', 'an', 'extremely', 'important', 'revelation', 'and', 'was', 'hoping', 'it', 'might', 'help', 'someone', 'else', 'too', '.']\n",
      "INFO:__main__:Number of tokens: 808\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['hesitation', 'and', 'ad', '##hd', '.', 'i', 'just', 'had', 'a', 'bit', 'of', 'a', 'revelation', 'today', ',', 'and', 'i', 'wondered', 'if', 'anyone', 'had', 'similar', 'stories', 'to', 'share', '.', 'as', 'i', 'get', 'older', '(', 'i', \"'\", 'm', '19', ')', ',', 'i', 'find', 'that', 'my', 'ability', 'to', 'do', 'anything', 'other', 'than', 'brows', '##e', 'the', 'internet', 'decreases', '.', 'it', 'has', 'become', 'a', 'really', 'big', 'problem', ',', 'with', 'school', ',', 'chores', ',', 'finding', 'a', 'job', ',', 'etc', '.', 'i', 'can', \"'\", 't', '*', 'start', '*', 'anything', '.', 'i', 'thought', 'it', 'was', 'ad', '##hd', ',', 'but', 'i', 'realized', 'today', 'that', 'it', \"'\", 's', 'an', '*', 'effect', '*', 'of', 'ad', '##hd', '.', 'i', 'was', 'cleaning', 'up', 'after', 'the', 'cats', ',', 'a', 'good', '20', '-', 'minute', 'task', ',', 'which', 'i', 'do', 'every', 'day', 'without', 'hesitation', '.', 'i', 'was', 'wondering', 'to', 'myself', 'how', 'it', 'was', 'i', 'could', 'do', 'this', 'but', 'neglect', 'to', 'clean', 'my', 'absolute', 'train', '##wr', '##eck', 'of', 'a', 'room', 'after', 'long', ',', 'painful', 'months', 'of', 'trying', '.', 'i', 'realized', 'it', 'was', 'because', 'i', '*', 'need', '*', 'to', 'clean', 'up', 'after', 'the', 'cats', ',', 'and', 'i', 'don', \"'\", 't', 'allow', 'myself', 'to', 'hesitate', '.', 'my', 'theory', 'is', ',', 'over', 'the', 'years', 'i', \"'\", 've', 'learned', 'to', 'restrain', 'myself', '.', 'i', 'don', \"'\", 't', 'speak', 'when', 'i', 'want', 'to', ',', 'i', 'don', \"'\", 't', 'do', 'things', 'when', 'i', 'want', 'to', '.', 'i', \"'\", 've', 'got', 'this', 'hesitation', '/', 'slow', '##ness', 'filter', 'over', 'everything', ',', 'because', 'otherwise', 'i', 'can', \"'\", 't', 'function', 'without', 'being', 'all', 'over', 'the', 'place', '.', 'but', 'now', 'it', 'keeps', 'me', 'from', 'doing', '*', 'anything', '*', '.', 'i', \"'\", 'm', 'not', 'a', 'funny', 'person', 'anymore', 'because', 'i', 'make', 'myself', 'slow', 'down', 'to', 'think', 'about', 'that', 'qui', '##p', 'i', 'was', 'about', 'to', 'throw', 'into', 'the', 'conversation', ',', 'saying', 'it', 'bela', '##ted', '##ly', '.', 'instead', 'of', 'just', 'jumping', 'into', 'cleaning', ',', 'i', 'would', 'stop', 'and', 'think', 'and', 'plan', '.', 'because', 'planning', 'was', 'something', 'i', 'was', 'never', 'good', 'at', '.', 'it', \"'\", 's', 'why', 'my', 'ad', '##hd', 'dad', 'is', 'poor', 'and', 'unsuccessful', '.', 'it', 'allows', 'me', 'to', 'work', 'out', 'which', 'parts', 'of', 'the', 'thing', 'i', \"'\", 'm', 'about', 'to', 'undertake', 'are', 'essential', '.', 'except', 'i', 'over', '##di', '##d', 'it', '.', 'now', 'i', 'plan', 'and', 'plan', 'and', 'plan', 'and', 'never', 'do', 'anything', '.', 'because', 'doubts', 'creep', 'into', 'my', 'mind', '.', 'depression', 'and', 'hopeless', '##ness', 'take', 'over', ',', 'because', 'i', \"'\", 've', 'conditioned', 'myself', 'to', 'accept', 'failure', '.', 'i', \"'\", 'm', 'now', 'taking', 'a', 'break', 'from', 'cleaning', 'my', 'room', ',', 'which', 'i', 'managed', 'to', 'start', 'after', 'realizing', 'that', 'all', 'i', 'had', 'to', 'do', 'was', 'make', 'it', 'a', '*', 'need', '*', 'instead', 'of', 'a', 'possibility', 'in', 'my', 'mind', '.', 'i', 'jumped', 'into', 'it', ',', 'i', \"'\", 'm', 'doing', 'things', 'in', 'the', 'order', 'they', 'occur', 'to', 'me', ',', 'and', 'i', \"'\", 'm', 'getting', 'it', 'done', '.', 'planning', 'doesn', \"'\", 't', 'work', 'for', 'me', ',', 'because', 'i', 'can', \"'\", 't', 'do', 'things', 'in', 'a', 'pre', '##set', 'order', '.', 'it', \"'\", 's', 'good', 'that', 'i', 'did', 'plan', 'a', 'while', 'ago', ',', 'because', 'now', 'i', \"'\", 'm', 'not', 'going', 'to', ',', 'say', ',', 'attempt', 'to', 'organize', 'my', 'books', '##hel', '##f'], ['before', 'i', \"'\", 've', 'even', 'gotten', 'all', 'of', 'my', 'books', 'off', 'the', 'floor', '.', 'i', 'could', 'have', 'easily', 'done', 'something', 'like', 'that', '.', 'i', 'have', 'those', 'kinds', 'of', 'essential', 'things', 'in', 'the', 'back', 'of', 'my', 'mind', 'because', 'i', 'did', 'plan', 'ahead', ',', 'but', 'now', 'i', \"'\", 'm', 'allowing', 'myself', 'to', 'do', 'what', 'i', 'want', '.', 'i', \"'\", 'm', 'following', 'my', 'w', '##him', '##s', '.', 'and', 'for', 'the', 'first', 'time', 'in', 'a', 'long', 'time', ',', 'i', 'don', \"'\", 't', 'feel', 'useless', '.', 'what', 'i', \"'\", 'm', 'realizing', 'is', 'that', 'i', 'am', 'useless', 'if', 'i', 'try', 'to', 'follow', 'a', 'rule', 'book', ';', 'i', 'can', 'only', 'move', 'forward', 'if', 'i', 'do', 'it', 'in', 'my', 'own', 'style', '.', 'and', 'i', 'think', 'it', \"'\", 's', 'important', 'that', 'every', 'ad', '##hd', '##er', 'know', 'that', 'about', 'themselves', '.', '*', '*', 't', '##l', ';', 'dr', '.', 'i', 'learned', 'that', 'you', 'need', 'to', 'plan', 'ahead', ',', 'but', 'when', 'it', 'comes', 'to', 'it', ',', 'follow', 'your', 'w', '##him', '##s', 'while', 'keeping', 'the', 'bare', 'minimum', 'of', 'essential', 'tasks', 'in', 'the', 'back', 'of', 'your', 'mind', '.', 'you', 'must', 'follow', 'your', 'wants', 'and', 'not', 'hesitate', 'in', 'order', 'to', 'move', 'forward', 'at', 'all', '.', '*', '*', 'hopefully', 'others', 'can', 'relate', '.', 'anyone', 'else', 'ever', 'had', 'this', 'problem', 'with', 'hesitation', 'or', 'a', 'similar', 'revelation', 'about', 'their', 'ad', '##hd', '?', 'edit', ':', 'also', ',', 'i', 'think', 'that', 'when', 'i', 'ignore', 'my', 'wishes', ',', 'they', 'sit', 'in', 'the', 'forefront', 'of', 'my', 'mind', 'bothering', 'me', 'and', 'distracting', 'me', 'from', 'whatever', 'i', 'was', 'trying', 'to', 'force', 'myself', 'to', 'do', '.', 'just', 'a', 'thought', 'i', 'wanted', 'to', 'add', '.', 'i', 'hope', 'the', 'ad', '##hd', 'community', 'doesn', \"'\", 't', 'take', 'this', 'whole', 'thing', 'as', 'self', '-', 'serving', ',', 'i', 'just', 'felt', 'it', 'was', 'an', 'extremely', 'important', 'revelation', 'and', 'was', 'hoping', 'it', 'might', 'help', 'someone', 'else', 'too', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['creativity', 'and', 'ad', '##hd', '?', 'out', 'of', 'most', 'of', 'my', 'friends', ',', 'i', 'feel', 'like', 'i', \"'\", 'm', 'one', 'of', 'the', 'most', 'creative', '.', 'ever', 'since', 'i', 'was', 'little', 'i', 'used', 'to', 'be', 'great', 'at', 'lying', 'because', 'i', 'could', 'make', 'a', 'bel', '##ie', '##vable', 'story', 'up', 'on', 'the', 'spot', '.', 'i', 'have', 'a', 'lot', 'of', 'creative', '##ness', 'when', 'it', 'comes', 'to', 'art', ',', 'i', 'just', 'can', 'never', 'get', 'my', 'hand', 'to', 'go', 'where', 'my', 'mind', 'is', '.', 'i', 'almost', 'always', 'can', 'create', '/', 'design', 'useful', 'tools', 'to', 'help', 'me', 'out', '(', 'though', 'i', 'rarely', 'get', 'around', 'to', 'making', 'them', ')', '.', 'is', 'anyone', 'else', 'with', 'ad', '##hd', 'really', 'creative', '?', 'does', 'ad', '##hd', 'have', 'a', 'link', 'to', 'creativity', 'or', 'am', 'i', 'just', 'pulling', 'this', 'out', 'of', 'my', 'g', '##lu', '##te', '##us', 'maximus', '?']\n",
      "INFO:__main__:Number of tokens: 133\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['creativity', 'and', 'ad', '##hd', '?', 'out', 'of', 'most', 'of', 'my', 'friends', ',', 'i', 'feel', 'like', 'i', \"'\", 'm', 'one', 'of', 'the', 'most', 'creative', '.', 'ever', 'since', 'i', 'was', 'little', 'i', 'used', 'to', 'be', 'great', 'at', 'lying', 'because', 'i', 'could', 'make', 'a', 'bel', '##ie', '##vable', 'story', 'up', 'on', 'the', 'spot', '.', 'i', 'have', 'a', 'lot', 'of', 'creative', '##ness', 'when', 'it', 'comes', 'to', 'art', ',', 'i', 'just', 'can', 'never', 'get', 'my', 'hand', 'to', 'go', 'where', 'my', 'mind', 'is', '.', 'i', 'almost', 'always', 'can', 'create', '/', 'design', 'useful', 'tools', 'to', 'help', 'me', 'out', '(', 'though', 'i', 'rarely', 'get', 'around', 'to', 'making', 'them', ')', '.', 'is', 'anyone', 'else', 'with', 'ad', '##hd', 'really', 'creative', '?', 'does', 'ad', '##hd', 'have', 'a', 'link', 'to', 'creativity', 'or', 'am', 'i', 'just', 'pulling', 'this', 'out', 'of', 'my', 'g', '##lu', '##te', '##us', 'maximus', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['felt', 'like', 'i', 'was', 'being', 'judged', 'from', 'the', 'get', 'go', '.', '.', '.', '.', 'i', \"'\", 'm', 'a', 'first', 'year', 'medical', 'student', '.', 'i', \"'\", 've', 'had', 'si', '##gin', '##ific', '##ant', 'problems', 'concentrating', 'and', 'started', 'seeing', 'an', 'on', 'campus', 'therapist', 'once', 'a', 'week', 'about', '2', 'months', 'ago', '.', 'i', 'like', 'going', 'there', ',', 'but', 'with', 'the', 'large', 'amount', 'of', 'abuse', 'of', 'ad', '##hd', 'drugs', 'in', 'medical', 'school', 'they', 'are', 'hesitant', 'to', 'pre', '##scribe', 'you', 'anything', 'without', 'having', 'an', 'ad', '##hd', 'eva', '##l', 'from', 'a', 'psychologist', 'which', 'can', 'run', 'up', 'to', '$', '800', '.', 'i', 'can', \"'\", 't', 'afford', 'this', ',', 'my', 'therapist', 'really', 'thinks', 'i', 'may', 'have', 'ad', '##hd', 'and', 'told', 'me', 'to', 'just', 'make', 'an', 'appointment', 'with', 'a', 'family', 'practice', 'physician', 'in', 'town', 'and', 'tell', 'him', 'my', 'story', 'and', 'i', 'may', 'be', 'able', 'to', 'get', 'a', 'prescription', 'that', 'way', '.', 'so', 'i', 'walk', 'into', 'the', 'doctor', \"'\", 's', 'office', 'and', 'tell', 'the', 'secretary', 'i', 'wanted', 'to', 'make', 'an', 'appointment', '.', 'she', 'asks', 'what', 'for', 'and', 'i', 'say', 'just', 'a', 'general', 'check', '-', 'up', ',', 'i', 'would', 'just', 'tell', 'the', 'doctor', 'what', 'the', 'real', 'problem', 'was', 'when', 'i', 'talked', 'to', 'him', '.', 'well', 'she', 'starts', 'typing', 'and', 'i', 'say', ',', '\"', 'well', 'if', 'you', 'want', 'you', 'can', 'put', 'on', 'there', 'that', 'i', \"'\", 'm', 'having', 'a', 'really', 'hard', 'time', 'focusing', '.', '\"', 'she', 'then', 'says', '\"', 'oh', 'so', 'are', 'you', 'saying', 'you', 'have', 'add', '?', '\"', 'and', 'i', 'say', '\"', 'well', 'i', \"'\", 'm', 'not', 'sure', 'that', \"'\", 's', 'why', 'i', \"'\", 'm', 'here', '.', '\"', 'she', 'then', 'says', '\"', 'well', 'if', 'you', 'are', 'trying', 'to', 'get', 'add', '##eral', '##l', 'or', 'something', 'like', 'that', 'they', 'are', 'really', 'cracking', 'down', 'on', 'that', '.', '\"', 'she', 'said', 'this', 'pretty', 'loudly', 'and', 'it', 'made', 'me', 'feel', 'like', 'they', 'thought', 'i', 'was', 'fa', '##king', 'and', 'just', 'trying', 'to', 'abuse', 'the', 'drug', 'before', 'even', 'talking', 'to', 'me', '.', 'anyone', 'else', 'feel', 'like', 'this', '?', 'like', 'they', 'have', 'to', 'prove', 'their', 'ad', '##hd', '?', 'maybe', 'it', \"'\", 's', 'just', 'because', 'i', \"'\", 'm', 'a', 'medical', 'student', 'and', 'everyone', 'says', 'they', 'have', 'it', '.', 'but', 'now', 'i', \"'\", 'm', 'nervous', 'about', 'going', 'cause', 'id', '##k', 'if', 'the', 'doc', 'is', 'gonna', 'seriously', 'evaluate', 'me', 'or', 'just', 'assume', 'i', \"'\", 'm', 'lying', 'and', 'trying', 'to', 'get', 'ahead', 'in', 'school', '.']\n",
      "INFO:__main__:Number of tokens: 381\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['felt', 'like', 'i', 'was', 'being', 'judged', 'from', 'the', 'get', 'go', '.', '.', '.', '.', 'i', \"'\", 'm', 'a', 'first', 'year', 'medical', 'student', '.', 'i', \"'\", 've', 'had', 'si', '##gin', '##ific', '##ant', 'problems', 'concentrating', 'and', 'started', 'seeing', 'an', 'on', 'campus', 'therapist', 'once', 'a', 'week', 'about', '2', 'months', 'ago', '.', 'i', 'like', 'going', 'there', ',', 'but', 'with', 'the', 'large', 'amount', 'of', 'abuse', 'of', 'ad', '##hd', 'drugs', 'in', 'medical', 'school', 'they', 'are', 'hesitant', 'to', 'pre', '##scribe', 'you', 'anything', 'without', 'having', 'an', 'ad', '##hd', 'eva', '##l', 'from', 'a', 'psychologist', 'which', 'can', 'run', 'up', 'to', '$', '800', '.', 'i', 'can', \"'\", 't', 'afford', 'this', ',', 'my', 'therapist', 'really', 'thinks', 'i', 'may', 'have', 'ad', '##hd', 'and', 'told', 'me', 'to', 'just', 'make', 'an', 'appointment', 'with', 'a', 'family', 'practice', 'physician', 'in', 'town', 'and', 'tell', 'him', 'my', 'story', 'and', 'i', 'may', 'be', 'able', 'to', 'get', 'a', 'prescription', 'that', 'way', '.', 'so', 'i', 'walk', 'into', 'the', 'doctor', \"'\", 's', 'office', 'and', 'tell', 'the', 'secretary', 'i', 'wanted', 'to', 'make', 'an', 'appointment', '.', 'she', 'asks', 'what', 'for', 'and', 'i', 'say', 'just', 'a', 'general', 'check', '-', 'up', ',', 'i', 'would', 'just', 'tell', 'the', 'doctor', 'what', 'the', 'real', 'problem', 'was', 'when', 'i', 'talked', 'to', 'him', '.', 'well', 'she', 'starts', 'typing', 'and', 'i', 'say', ',', '\"', 'well', 'if', 'you', 'want', 'you', 'can', 'put', 'on', 'there', 'that', 'i', \"'\", 'm', 'having', 'a', 'really', 'hard', 'time', 'focusing', '.', '\"', 'she', 'then', 'says', '\"', 'oh', 'so', 'are', 'you', 'saying', 'you', 'have', 'add', '?', '\"', 'and', 'i', 'say', '\"', 'well', 'i', \"'\", 'm', 'not', 'sure', 'that', \"'\", 's', 'why', 'i', \"'\", 'm', 'here', '.', '\"', 'she', 'then', 'says', '\"', 'well', 'if', 'you', 'are', 'trying', 'to', 'get', 'add', '##eral', '##l', 'or', 'something', 'like', 'that', 'they', 'are', 'really', 'cracking', 'down', 'on', 'that', '.', '\"', 'she', 'said', 'this', 'pretty', 'loudly', 'and', 'it', 'made', 'me', 'feel', 'like', 'they', 'thought', 'i', 'was', 'fa', '##king', 'and', 'just', 'trying', 'to', 'abuse', 'the', 'drug', 'before', 'even', 'talking', 'to', 'me', '.', 'anyone', 'else', 'feel', 'like', 'this', '?', 'like', 'they', 'have', 'to', 'prove', 'their', 'ad', '##hd', '?', 'maybe', 'it', \"'\", 's', 'just', 'because', 'i', \"'\", 'm', 'a', 'medical', 'student', 'and', 'everyone', 'says', 'they', 'have', 'it', '.', 'but', 'now', 'i', \"'\", 'm', 'nervous', 'about', 'going', 'cause', 'id', '##k', 'if', 'the', 'doc', 'is', 'gonna', 'seriously', 'evaluate', 'me', 'or', 'just', 'assume', 'i', \"'\", 'm', 'lying', 'and', 'trying', 'to', 'get', 'ahead', 'in', 'school', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['med', '##ida', '##te', 'cd', 'and', 'sc', '##oc', '##ial', 'anxiety', 'i', 'have', 'severe', 'sc', '##oc', '##ial', '##y', 'anxiety', 'but', 'only', 'when', 'i', 'take', 'my', 'medicine', ',', 'med', '##ida', '##te', 'cd', ',', 'it', 'helps', 'me', 'focus', 'but', 'i', 'can', \"'\", 't', 'talk', 'to', 'people', 'but', 'when', 'i', 'am', 'off', 'my', 'med', '##s', 'i', 'am', 'funny', 'and', 'happy', 'what', 'should', 'i', 'do', 'to', 'talk', 'to', 'people', 'and', 'get', 'them', 'to', 'hang', 'out', 'with', 'me', '?']\n",
      "INFO:__main__:Number of tokens: 72\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['med', '##ida', '##te', 'cd', 'and', 'sc', '##oc', '##ial', 'anxiety', 'i', 'have', 'severe', 'sc', '##oc', '##ial', '##y', 'anxiety', 'but', 'only', 'when', 'i', 'take', 'my', 'medicine', ',', 'med', '##ida', '##te', 'cd', ',', 'it', 'helps', 'me', 'focus', 'but', 'i', 'can', \"'\", 't', 'talk', 'to', 'people', 'but', 'when', 'i', 'am', 'off', 'my', 'med', '##s', 'i', 'am', 'funny', 'and', 'happy', 'what', 'should', 'i', 'do', 'to', 'talk', 'to', 'people', 'and', 'get', 'them', 'to', 'hang', 'out', 'with', 'me', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'just', 'got', 'diagnosed', 'yesterday', '.', 'anything', 'i', 'should', 'know', '?', 'so', 'i', 'went', 'to', 'a', 'therapist', 'yesterday', ',', 'and', 'was', 'tested', 'for', 'ad', '##hd', ',', 'and', 'was', 'told', 'that', 'all', 'signs', 'point', 'to', 'a', 'fairly', 'definite', 'yes', '.', 'he', 'recommended', 'me', 'to', 'someone', 'who', 'would', 'be', 'able', 'to', 'give', 'me', 'a', 'prescription', 'for', 'something', '.', '(', 'i', 'don', \"'\", 't', 'remember', 'if', 'he', 'said', 'what', 'it', 'was', 'or', 'not', '.', ')', 'i', \"'\", 'm', 'going', 'back', 'to', 'see', 'him', 'on', 'the', '18th', ',', 'but', 'i', \"'\", 'm', 'wondering', 'if', 'you', 'guys', 'have', 'any', 'advice', '/', 'tips', '/', 'warnings', 'now', 'that', 'i', 'know', 'i', 'have', 'ad', '##hd', ',', 'and', 'will', 'be', 'taking', 'medication', 'for', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 116\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'just', 'got', 'diagnosed', 'yesterday', '.', 'anything', 'i', 'should', 'know', '?', 'so', 'i', 'went', 'to', 'a', 'therapist', 'yesterday', ',', 'and', 'was', 'tested', 'for', 'ad', '##hd', ',', 'and', 'was', 'told', 'that', 'all', 'signs', 'point', 'to', 'a', 'fairly', 'definite', 'yes', '.', 'he', 'recommended', 'me', 'to', 'someone', 'who', 'would', 'be', 'able', 'to', 'give', 'me', 'a', 'prescription', 'for', 'something', '.', '(', 'i', 'don', \"'\", 't', 'remember', 'if', 'he', 'said', 'what', 'it', 'was', 'or', 'not', '.', ')', 'i', \"'\", 'm', 'going', 'back', 'to', 'see', 'him', 'on', 'the', '18th', ',', 'but', 'i', \"'\", 'm', 'wondering', 'if', 'you', 'guys', 'have', 'any', 'advice', '/', 'tips', '/', 'warnings', 'now', 'that', 'i', 'know', 'i', 'have', 'ad', '##hd', ',', 'and', 'will', 'be', 'taking', 'medication', 'for', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anybody', 'else', '?']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anybody', 'else', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['let', \"'\", 's', 'talk', 'about', 'the', '\"', 'chatter', '\"', '.', 'first', 'of', 'all', ',', 'i', 'want', 'to', 'that', 'i', \"'\", 've', 'never', 'been', 'diagnosed', '(', 'but', 'always', 'had', 'my', 'suspicions', ')', ',', 'but', 'i', \"'\", 'm', 'trying', 'to', 'figure', 'out', 'this', 'mysterious', 'thing', 'people', 'describe', 'as', '\"', 'the', 'chatter', '\"', '.', 'one', 'multiple', 'occasions', 'during', 'the', 'day', ',', 'and', 'especially', 'at', 'night', ',', 'my', 'internal', \"'\", 'voice', \"'\", 'seems', 'to', 'change', '.', 'not', 'just', 'a', 'little', '.', '.', '.', 'i', \"'\", 'm', 'talking', 'about', 'drastically', '!', 'from', 'simple', 'accent', 'changes', 'to', 'rec', '##iting', 'phrases', 'in', 'broken', 'foreign', 'languages', '.', 'sometimes', 'it', \"'\", 's', 'musical', 'rhythms', ',', 'notes', ',', 'or', 'just', 'plain', 'collections', 'of', 'noises', '.', 'i', 'could', 'maybe', 'describe', 'it', 'like', 'a', 'radio', 'tuned', 'to', 'the', 'most', 'worst', 'possible', 'station', '.', 'but', 'that', \"'\", 's', 'not', 'all', '.', 'it', 'loops', '.', '.', '.', '.', 'endless', '##ly', '.', 'i', \"'\", 'm', 'trying', 'to', 'figure', 'out', 'if', 'that', \"'\", 's', 'just', 'my', 'brains', 'way', 'of', 'figuring', 'things', 'out', ',', 'if', 'it', \"'\", 's', 'going', 'through', 'the', 'garbage', 'of', 'the', 'days', 'experiences', 'trying', 'to', 'grab', 'onto', 'something', 'important', 'from', 'them', '.', 'i', 'wouldn', \"'\", 't', 'think', 'much', 'of', 'it', 'i', 'suppose', ',', 'but', 'it', 'can', 'stop', 'me', 'dead', 'in', 'my', 'tracks', 'no', 'matter', 'what', 'i', \"'\", 'm', 'trying', 'to', 'do', ',', 'especially', 'trying', 'to', 'fall', 'asleep', '.', 'thoughts', '?', 'insights', '?']\n",
      "INFO:__main__:Number of tokens: 228\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['let', \"'\", 's', 'talk', 'about', 'the', '\"', 'chatter', '\"', '.', 'first', 'of', 'all', ',', 'i', 'want', 'to', 'that', 'i', \"'\", 've', 'never', 'been', 'diagnosed', '(', 'but', 'always', 'had', 'my', 'suspicions', ')', ',', 'but', 'i', \"'\", 'm', 'trying', 'to', 'figure', 'out', 'this', 'mysterious', 'thing', 'people', 'describe', 'as', '\"', 'the', 'chatter', '\"', '.', 'one', 'multiple', 'occasions', 'during', 'the', 'day', ',', 'and', 'especially', 'at', 'night', ',', 'my', 'internal', \"'\", 'voice', \"'\", 'seems', 'to', 'change', '.', 'not', 'just', 'a', 'little', '.', '.', '.', 'i', \"'\", 'm', 'talking', 'about', 'drastically', '!', 'from', 'simple', 'accent', 'changes', 'to', 'rec', '##iting', 'phrases', 'in', 'broken', 'foreign', 'languages', '.', 'sometimes', 'it', \"'\", 's', 'musical', 'rhythms', ',', 'notes', ',', 'or', 'just', 'plain', 'collections', 'of', 'noises', '.', 'i', 'could', 'maybe', 'describe', 'it', 'like', 'a', 'radio', 'tuned', 'to', 'the', 'most', 'worst', 'possible', 'station', '.', 'but', 'that', \"'\", 's', 'not', 'all', '.', 'it', 'loops', '.', '.', '.', '.', 'endless', '##ly', '.', 'i', \"'\", 'm', 'trying', 'to', 'figure', 'out', 'if', 'that', \"'\", 's', 'just', 'my', 'brains', 'way', 'of', 'figuring', 'things', 'out', ',', 'if', 'it', \"'\", 's', 'going', 'through', 'the', 'garbage', 'of', 'the', 'days', 'experiences', 'trying', 'to', 'grab', 'onto', 'something', 'important', 'from', 'them', '.', 'i', 'wouldn', \"'\", 't', 'think', 'much', 'of', 'it', 'i', 'suppose', ',', 'but', 'it', 'can', 'stop', 'me', 'dead', 'in', 'my', 'tracks', 'no', 'matter', 'what', 'i', \"'\", 'm', 'trying', 'to', 'do', ',', 'especially', 'trying', 'to', 'fall', 'asleep', '.', 'thoughts', '?', 'insights', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['concert', '##a', 'er', 'just', 'not', 'working', '.', '.', '.', 'anyone', 'else', '?', 'i', 'recently', 'diagnosed', 'with', 'adult', 'ad', '##hd', 'and', 'was', 'put', 'on', 'concert', '##a', 'er', '72', '##mg', '.', 'what', 'is', 'weird', 'is', 'that', 'the', 'drug', 'has', 'very', 'little', 'to', 'no', 'effect', 'on', 'me', 'recently', ',', 'even', 'after', '3', '+', 'hours', 'i', 'feel', 'nothing', '.', 'yet', ',', 'if', 'i', 'take', 'a', '10', '##mg', 'rita', '##lin', 'ir', 'i', 'feel', 'some', 'effects', 'within', 'an', 'hour', 'that', 'last', 'about', '2', 'hr', '##s', '.', 'without', 'any', 'benefits', 'i', 'do', 'still', 'get', 'the', 'concert', '##a', 'crash', 'and', 'feel', 'very', 'dr', '##ows', '##y', 'and', 'zombie', 'like', 'after', 'about', '8', 'hr', '##s', 'of', 'taking', 'the', 'drug', '.', 'honestly', ',', 'if', 'it', 'wasn', \"'\", 't', 'for', 'the', 'crash', 'i', 'would', 'be', 'convinced', 'i', 'received', 'place', '##bos', '.', 'i', 'used', 'to', 'take', 'add', '##eral', '##l', 'and', 'it', 'seems', 'that', 'a', 'significantly', 'lower', 'dose', '(', '20', '##mg', 'x', '##r', ')', 'lasted', 'a', 'good', 'portion', 'of', 'the', 'day', 'and', 'had', 'little', 'to', 'no', 'crash', '.', 'has', 'anyone', 'else', 'experienced', 'minimal', 'effectiveness', 'of', 'this', 'drug', '?']\n",
      "INFO:__main__:Number of tokens: 174\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['concert', '##a', 'er', 'just', 'not', 'working', '.', '.', '.', 'anyone', 'else', '?', 'i', 'recently', 'diagnosed', 'with', 'adult', 'ad', '##hd', 'and', 'was', 'put', 'on', 'concert', '##a', 'er', '72', '##mg', '.', 'what', 'is', 'weird', 'is', 'that', 'the', 'drug', 'has', 'very', 'little', 'to', 'no', 'effect', 'on', 'me', 'recently', ',', 'even', 'after', '3', '+', 'hours', 'i', 'feel', 'nothing', '.', 'yet', ',', 'if', 'i', 'take', 'a', '10', '##mg', 'rita', '##lin', 'ir', 'i', 'feel', 'some', 'effects', 'within', 'an', 'hour', 'that', 'last', 'about', '2', 'hr', '##s', '.', 'without', 'any', 'benefits', 'i', 'do', 'still', 'get', 'the', 'concert', '##a', 'crash', 'and', 'feel', 'very', 'dr', '##ows', '##y', 'and', 'zombie', 'like', 'after', 'about', '8', 'hr', '##s', 'of', 'taking', 'the', 'drug', '.', 'honestly', ',', 'if', 'it', 'wasn', \"'\", 't', 'for', 'the', 'crash', 'i', 'would', 'be', 'convinced', 'i', 'received', 'place', '##bos', '.', 'i', 'used', 'to', 'take', 'add', '##eral', '##l', 'and', 'it', 'seems', 'that', 'a', 'significantly', 'lower', 'dose', '(', '20', '##mg', 'x', '##r', ')', 'lasted', 'a', 'good', 'portion', 'of', 'the', 'day', 'and', 'had', 'little', 'to', 'no', 'crash', '.', 'has', 'anyone', 'else', 'experienced', 'minimal', 'effectiveness', 'of', 'this', 'drug', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'no', 'longer', 'feels', 'effective', 'after', 'using', 'it', 'for', 'a', 'month', 'i', \"'\", 've', 'been', 'using', 'add', '##eral', '##l', '(', '20', '##mg', 'twice', 'a', 'day', ',', 'not', 'x', '##r', ')', 'for', 'a', 'month', 'now', ',', 'and', 'the', 'first', '2', '-', '3', 'weeks', 'it', 'was', 'amazing', ',', 'but', 'the', 'effectiveness', 'seems', 'to', 'have', 'd', '##wind', '##led', '.', 'i', \"'\", 'm', 'certainly', 'a', 'bit', 'better', 'than', 'i', 'was', 'without', 'the', 'medicine', ',', 'but', 'i', 'still', 'find', 'it', 'extremely', 'hard', 'to', 'focus', 'on', 'what', 'i', 'need', 'to', 'focus', 'on', '.', 'should', 'i', 'consider', 'asking', 'my', 'doctor', 'to', 'increase', 'my', 'strength', ',', 'try', 'the', 'x', '##r', 'version', 'or', 'perhaps', 'a', 'different', 'drug', 'entirely', 'like', 'v', '##y', '##van', '##se', '?']\n",
      "INFO:__main__:Number of tokens: 117\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'no', 'longer', 'feels', 'effective', 'after', 'using', 'it', 'for', 'a', 'month', 'i', \"'\", 've', 'been', 'using', 'add', '##eral', '##l', '(', '20', '##mg', 'twice', 'a', 'day', ',', 'not', 'x', '##r', ')', 'for', 'a', 'month', 'now', ',', 'and', 'the', 'first', '2', '-', '3', 'weeks', 'it', 'was', 'amazing', ',', 'but', 'the', 'effectiveness', 'seems', 'to', 'have', 'd', '##wind', '##led', '.', 'i', \"'\", 'm', 'certainly', 'a', 'bit', 'better', 'than', 'i', 'was', 'without', 'the', 'medicine', ',', 'but', 'i', 'still', 'find', 'it', 'extremely', 'hard', 'to', 'focus', 'on', 'what', 'i', 'need', 'to', 'focus', 'on', '.', 'should', 'i', 'consider', 'asking', 'my', 'doctor', 'to', 'increase', 'my', 'strength', ',', 'try', 'the', 'x', '##r', 'version', 'or', 'perhaps', 'a', 'different', 'drug', 'entirely', 'like', 'v', '##y', '##van', '##se', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['adult', 'add', 'or', 'a', 'deficiency', '?', 'i', 'was', 'diagnosed', 'with', 'adult', 'add', 'around', '6', 'months', 'ago', '.', 'i', 'was', 'put', 'on', 'rita', '##lin', 'and', 'the', 'dose', 'gradually', 'increased', 'to', '80', 'mg', '/', 'day', '.', 'i', 'feel', 'that', 'my', 'add', 'could', 'be', 'related', 'to', 'some', 'vitamin', 'or', 'mineral', 'deficiency', '.', 'particularly', 'because', 'i', 'had', 'b1', '##2', 'deficiency', 'a', 'year', 'before', 'i', 'got', 'diagnosed', 'with', 'add', '.', 'are', 'their', 'any', 'recommended', 'tests', 'or', 'ass', '##ays', 'for', 'vitamin', '##s', 'or', 'minerals', 'that', 'i', 'could', 'ask', 'my', 'psychiatrist', 'to', 'pre', '##scribe', 'so', 'that', 'we', 'could', 'eliminate', 'that', 'possibility', '?', 'i', 'would', 'appreciate', 'any', 'ideas', ',', 'since', 'i', 'have', 'to', 'visit', 'my', 'ps', '##ych', 'in', 'a', 'week', 'to', 'renew', 'my', 'prescription', '.']\n",
      "INFO:__main__:Number of tokens: 117\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['adult', 'add', 'or', 'a', 'deficiency', '?', 'i', 'was', 'diagnosed', 'with', 'adult', 'add', 'around', '6', 'months', 'ago', '.', 'i', 'was', 'put', 'on', 'rita', '##lin', 'and', 'the', 'dose', 'gradually', 'increased', 'to', '80', 'mg', '/', 'day', '.', 'i', 'feel', 'that', 'my', 'add', 'could', 'be', 'related', 'to', 'some', 'vitamin', 'or', 'mineral', 'deficiency', '.', 'particularly', 'because', 'i', 'had', 'b1', '##2', 'deficiency', 'a', 'year', 'before', 'i', 'got', 'diagnosed', 'with', 'add', '.', 'are', 'their', 'any', 'recommended', 'tests', 'or', 'ass', '##ays', 'for', 'vitamin', '##s', 'or', 'minerals', 'that', 'i', 'could', 'ask', 'my', 'psychiatrist', 'to', 'pre', '##scribe', 'so', 'that', 'we', 'could', 'eliminate', 'that', 'possibility', '?', 'i', 'would', 'appreciate', 'any', 'ideas', ',', 'since', 'i', 'have', 'to', 'visit', 'my', 'ps', '##ych', 'in', 'a', 'week', 'to', 'renew', 'my', 'prescription', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'here', 'have', 'personal', 'experience', 'with', 'st', '##rat', '##tera', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'here', 'have', 'personal', 'experience', 'with', 'st', '##rat', '##tera', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'my', 'med', \"'\", 's', '!', '!', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '3', 'years', 'ago', '.', '.', '.', 'i', 'never', 'got', 'on', 'my', 'med', '##ice', '##ne', '.', '.', 'now', 'im', 'trying', 'to', 'get', 'it', 'and', 'my', 'doctor', 'says', 'i', 'have', 'to', 'be', 'diagnosed', 'again', '.', '.', '.', 'can', '##t', 'afford', 'that', 'now', '!', 'im', '54', 'years', 'old', 'and', 'need', 'to', 'get', 'my', 'life', 'in', 'order', '.', '.', '.', 'im', 'living', 'in', 'a', 'college', 'town', 'and', 'have', 'heard', 'that', 'its', 'hard', 'to', 'get', 'those', 'pills', 'cause', 'the', 'students', 'are', 'abu', '##sing', 'the', '##rm', '.', '.', 'do', 'i', 'have', 'to', 'go', 'to', 'another', 'town', 'to', 'get', 'help', '?']\n",
      "INFO:__main__:Number of tokens: 108\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'my', 'med', \"'\", 's', '!', '!', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '3', 'years', 'ago', '.', '.', '.', 'i', 'never', 'got', 'on', 'my', 'med', '##ice', '##ne', '.', '.', 'now', 'im', 'trying', 'to', 'get', 'it', 'and', 'my', 'doctor', 'says', 'i', 'have', 'to', 'be', 'diagnosed', 'again', '.', '.', '.', 'can', '##t', 'afford', 'that', 'now', '!', 'im', '54', 'years', 'old', 'and', 'need', 'to', 'get', 'my', 'life', 'in', 'order', '.', '.', '.', 'im', 'living', 'in', 'a', 'college', 'town', 'and', 'have', 'heard', 'that', 'its', 'hard', 'to', 'get', 'those', 'pills', 'cause', 'the', 'students', 'are', 'abu', '##sing', 'the', '##rm', '.', '.', 'do', 'i', 'have', 'to', 'go', 'to', 'another', 'town', 'to', 'get', 'help', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['took', 'concert', '##a', '13', 'hours', 'ago', ',', 'ok', 'to', 'smoke', 'weed', '?']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['took', 'concert', '##a', '13', 'hours', 'ago', ',', 'ok', 'to', 'smoke', 'weed', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['adult', 'add', \"'\", 'co', '-', 'mor', '##bid', '##ities', \"'\", '?', 'here', 'is', 'a', 'list', 'of', 'symptoms', 'i', 'experience', 'and', 'some', 'characteristics', ',', 'apart', 'from', 'my', 'add', '.', 'does', 'this', 'ring', 'a', 'bell', '?', 'is', 'it', 'kind', 'of', 'a', 'pattern', 'with', 'other', 'add', 'suffer', '##ers', 'too', '?', '1', '.', 'mild', 'depression', 'and', 'anxiety', '2', '.', 'mild', 'ib', '##s', '3', '.', 'unable', 'to', 'go', 'to', 'bed', 'early', '.', 'delayed', 'sleep', 'phase', '?', 'not', 'sure', '.', 'but', 'certainly', 'a', 'night', 'owl', '4', '.', 'occasional', \"'\", 'brain', '-', 'fog', \"'\", '5', '.', 'intro', '##verted', ',', 'mb', '##ti', '=', 'int', '##p', '6', '.', 'can', \"'\", 't', 'keep', 'track', 'of', 'finances', ',', 'administrative', 'tasks', ',', 'etc', '7', '.', 'easily', 'distracted', 'when', 'focused', 'on', 'something', '8', '.', 'can', 'only', 'work', 'on', 'one', 'thing', 'at', 'a', 'time', 'with', 'focus', ',', 'that', 'too', 'only', 'after', 'taking', 'rita', '##lin', '9', '.', '(', 'lately', ')', 'forget', '##fulness', '/', 'memory', 'issues', 'edit', ':', 'just', 'to', 'add', 'that', 'given', 'the', 'responses', ',', 'i', 'feel', 'there', 'could', 'be', 'an', 'mb', '##ti', 'connection', '.', 'several', 'int', '##ps', 'identify', 'with', 'all', 'of', 'the', 'symptoms', 'above']\n",
      "INFO:__main__:Number of tokens: 179\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['adult', 'add', \"'\", 'co', '-', 'mor', '##bid', '##ities', \"'\", '?', 'here', 'is', 'a', 'list', 'of', 'symptoms', 'i', 'experience', 'and', 'some', 'characteristics', ',', 'apart', 'from', 'my', 'add', '.', 'does', 'this', 'ring', 'a', 'bell', '?', 'is', 'it', 'kind', 'of', 'a', 'pattern', 'with', 'other', 'add', 'suffer', '##ers', 'too', '?', '1', '.', 'mild', 'depression', 'and', 'anxiety', '2', '.', 'mild', 'ib', '##s', '3', '.', 'unable', 'to', 'go', 'to', 'bed', 'early', '.', 'delayed', 'sleep', 'phase', '?', 'not', 'sure', '.', 'but', 'certainly', 'a', 'night', 'owl', '4', '.', 'occasional', \"'\", 'brain', '-', 'fog', \"'\", '5', '.', 'intro', '##verted', ',', 'mb', '##ti', '=', 'int', '##p', '6', '.', 'can', \"'\", 't', 'keep', 'track', 'of', 'finances', ',', 'administrative', 'tasks', ',', 'etc', '7', '.', 'easily', 'distracted', 'when', 'focused', 'on', 'something', '8', '.', 'can', 'only', 'work', 'on', 'one', 'thing', 'at', 'a', 'time', 'with', 'focus', ',', 'that', 'too', 'only', 'after', 'taking', 'rita', '##lin', '9', '.', '(', 'lately', ')', 'forget', '##fulness', '/', 'memory', 'issues', 'edit', ':', 'just', 'to', 'add', 'that', 'given', 'the', 'responses', ',', 'i', 'feel', 'there', 'could', 'be', 'an', 'mb', '##ti', 'connection', '.', 'several', 'int', '##ps', 'identify', 'with', 'all', 'of', 'the', 'symptoms', 'above']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'coffee', 'lovers', 'recommend', 'good', 'de', '-', 'caf', '?', 'caf', '##fe', '##ine', 'basically', 'cancel', '##s', 'out', 'my', 'med', '##s', 'but', 'i', 'love', 'coffee', ',', 'anyone', 'care', 'to', 'share', '?']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'coffee', 'lovers', 'recommend', 'good', 'de', '-', 'caf', '?', 'caf', '##fe', '##ine', 'basically', 'cancel', '##s', 'out', 'my', 'med', '##s', 'but', 'i', 'love', 'coffee', ',', 'anyone', 'care', 'to', 'share', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['herbal', 'supplements', '?', 'i', \"'\", 'm', '23', ',', 'have', 'ad', '##hd', ',', 'and', 'my', 'main', 'problems', 'are', 'focusing', 'and', 'at', '##ten', '##tive', '##ness', '.', 'my', 'hyper', '##act', '##ivity', 'can', 'be', 'an', 'issue', ',', 'but', 'most', 'don', \"'\", 't', 'notice', 'it', '.', 'it', \"'\", 's', 'mainly', 'just', 'a', 'massive', 'need', 'to', 'fi', '##dget', '.', 'either', 'way', ',', 'my', 'work', 'and', 'relationship', 'is', 'suffering', 'because', 'of', 'it', '.', 'my', 'relationship', 'i', 'don', \"'\", 't', 'care', 'too', 'much', 'about', ',', 'but', 'because', 'of', 'the', 'work', 'i', 'do', 'it', 'is', 'essential', 'that', 'i', 'focus', ',', 'listen', 'to', 'customers', ',', 'and', 'think', 'slowly', '.', 'i', \"'\", 've', 'tried', 'going', 'without', 'my', 'medicine', ',', 'and', 'if', 'i', 'concentrate', 'it', 'works', ';', 'but', 'as', 'most', 'of', 'you', 'well', 'know', 'concentrating', 'isn', \"'\", 't', 'that', 'easy', 'to', 'do', '.', 'i', \"'\", 'm', 'trying', 'to', 'grow', 'up', ',', 'provide', 'for', 'my', 'family', ',', 'and', 'impress', 'my', 'cow', '##or', '##kers', 'and', 'i', 'really', 'can', \"'\", 't', 'seem', 'do', 'that', 'without', 'some', 'kind', 'of', 'help', '.', 'my', 'primary', 'care', 'physician', 'is', 'in', 'another', 'town', 'far', 'away', 'which', 'i', 'simply', 'don', \"'\", 't', 'have', 'time', 'to', 'go', 'to', 'because', 'i', 'work', '40', '+', 'hours', 'a', 'week', ',', 'and', 'my', 'only', 'real', 'day', 'off', 'is', 'sunday', '.', 'even', 'when', 'i', 'have', 'a', 'day', 'off', 'of', 'work', ',', 'i', 'have', 'a', '5', 'month', '-', 'old', 'and', 'can', \"'\", 't', 'drag', 'him', 'to', 'the', 'doctor', 'with', 'me', 'for', 'an', 'appointment', 'that', 'usually', 'takes', '2', '+', 'hours', '.', 'so', 'i', 'was', 'wondering', 'if', 'there', 'are', 'any', 'herbal', 'supplements', 'i', 'could', 'try', 'for', 'now', 'that', 'could', 'help', 'with', 'focus', 'or', 'at', '##ten', '##tive', '##ness', '.', 'if', 'anyone', 'here', 'has', 'had', 'a', 'good', 'experience', 'with', 'supplements', ',', 'any', 'advice', 'would', 'be', 'apr', '##pe', '##cia', '##ted', '.', 'thanks', 'so', 'much', '.']\n",
      "INFO:__main__:Number of tokens: 292\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['herbal', 'supplements', '?', 'i', \"'\", 'm', '23', ',', 'have', 'ad', '##hd', ',', 'and', 'my', 'main', 'problems', 'are', 'focusing', 'and', 'at', '##ten', '##tive', '##ness', '.', 'my', 'hyper', '##act', '##ivity', 'can', 'be', 'an', 'issue', ',', 'but', 'most', 'don', \"'\", 't', 'notice', 'it', '.', 'it', \"'\", 's', 'mainly', 'just', 'a', 'massive', 'need', 'to', 'fi', '##dget', '.', 'either', 'way', ',', 'my', 'work', 'and', 'relationship', 'is', 'suffering', 'because', 'of', 'it', '.', 'my', 'relationship', 'i', 'don', \"'\", 't', 'care', 'too', 'much', 'about', ',', 'but', 'because', 'of', 'the', 'work', 'i', 'do', 'it', 'is', 'essential', 'that', 'i', 'focus', ',', 'listen', 'to', 'customers', ',', 'and', 'think', 'slowly', '.', 'i', \"'\", 've', 'tried', 'going', 'without', 'my', 'medicine', ',', 'and', 'if', 'i', 'concentrate', 'it', 'works', ';', 'but', 'as', 'most', 'of', 'you', 'well', 'know', 'concentrating', 'isn', \"'\", 't', 'that', 'easy', 'to', 'do', '.', 'i', \"'\", 'm', 'trying', 'to', 'grow', 'up', ',', 'provide', 'for', 'my', 'family', ',', 'and', 'impress', 'my', 'cow', '##or', '##kers', 'and', 'i', 'really', 'can', \"'\", 't', 'seem', 'do', 'that', 'without', 'some', 'kind', 'of', 'help', '.', 'my', 'primary', 'care', 'physician', 'is', 'in', 'another', 'town', 'far', 'away', 'which', 'i', 'simply', 'don', \"'\", 't', 'have', 'time', 'to', 'go', 'to', 'because', 'i', 'work', '40', '+', 'hours', 'a', 'week', ',', 'and', 'my', 'only', 'real', 'day', 'off', 'is', 'sunday', '.', 'even', 'when', 'i', 'have', 'a', 'day', 'off', 'of', 'work', ',', 'i', 'have', 'a', '5', 'month', '-', 'old', 'and', 'can', \"'\", 't', 'drag', 'him', 'to', 'the', 'doctor', 'with', 'me', 'for', 'an', 'appointment', 'that', 'usually', 'takes', '2', '+', 'hours', '.', 'so', 'i', 'was', 'wondering', 'if', 'there', 'are', 'any', 'herbal', 'supplements', 'i', 'could', 'try', 'for', 'now', 'that', 'could', 'help', 'with', 'focus', 'or', 'at', '##ten', '##tive', '##ness', '.', 'if', 'anyone', 'here', 'has', 'had', 'a', 'good', 'experience', 'with', 'supplements', ',', 'any', 'advice', 'would', 'be', 'apr', '##pe', '##cia', '##ted', '.', 'thanks', 'so', 'much', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'sanctuary', 'of', 'focus']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'sanctuary', 'of', 'focus']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'was', 'i', 'just', 'thinking', 'about', '?', 'do', 'you', 'ever', 'lose', 'your', 'train', 'of', 'thought', ',', 'then', 'try', 'to', 're', '##tra', '##ce', 'it', ',', 'or', 'just', '\"', 'shut', 'down', '\"', 'as', 'a', 'result', '?', 'i', 'have', 'a', 'lot', 'on', 'my', 'mind', 'lately', '-', 'work', 'deadline', '##s', ',', 'bills', ',', 'my', 'social', '/', 'love', 'life', ',', 'business', 'trip', 'next', 'week', ',', 'vacation', 'after', 'that', '-', 'not', 'to', 'mention', 'a', 'few', 'things', 'i', \"'\", 'm', 'behind', 'on', '.', 'earlier', 'this', 'morning', 'i', 'was', 'working', 'on', 'some', 'sql', 'code', 'for', 'work', '.', 'within', 'a', 'few', 'seconds', 'i', 'got', 'distracted', 'by', ':', '2', 'instant', 'messages', ',', 'something', 'going', 'on', 'outside', 'my', 'window', ',', 'my', 'upstairs', 'neighbor', 'dropped', 'something', ',', 'another', 'neighbor', 'was', 'just', 'leaving', 'through', 'the', 'front', 'door', ',', 'checking', 'my', 'calendar', 'concerning', 'the', 'business', 'trip', 'next', 'week', ',', 'something', 'on', 'the', 'tv', 'that', 'caught', 'my', 'attention', ',', 'at', 'least', 'one', 'other', 'thing', 'i', 'can', \"'\", 't', 'remember', ',', 'and', 'then', 'back', 'to', 'the', 'code', '.', 'that', 'all', 'literally', 'happened', 'within', 'just', 'a', 'few', 'seconds', '.', 'then', 'i', 'found', 'myself', 'thinking', ',', '\"', 'now', 'what', 'was', 'i', 'just', 'thinking', 'about', '?', '\"', 'i', 'had', '[', '\"', 'tip', 'of', 'the', 'tongue', 'syndrome', '\"', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'tip', '_', 'of', '_', 'the', '_', 'tongue', ')', 'about', 'my', 'own', 'stream', 'of', 'consciousness', '.', 'and', 'i', 'had', 'a', 'very', 'distinct', 'feeling', 'of', 'overwhelming', 'hopeless', '##ness', 'in', 'the', 'face', 'of', 'straightening', 'it', 'all', 'out', '.', 'this', 'was', 'about', 'an', 'hour', 'after', 'i', 'took', 'my', 'morning', 'dose', 'of', 'add', '##eral', '##l', '.', 'i', 'believe', 'i', 'actually', 'had', 'a', 'mini', 'panic', 'attack', '.', 'i', 'couldn', \"'\", 't', 'focus', ',', 'my', 'heart', 'was', 'racing', 'and', 'all', 'i', 'cared', 'about', 'was', 'what', 'stupid', 'trivial', 'thing', 'was', 'just', 'on', 'my', 'mind', 'a', 'few', 'moments', 'prior', ',', 'because', 'it', 'seemed', 'like', 'the', 'most', 'important', 'thing', 'in', 'the', 'world', '.', 'even', 'though', 'i', 'have', 'a', 'ton', 'of', 'work', 'to', 'do', ',', 'i', 'just', 'didn', \"'\", 't', 'care', 'anymore', '.', 'i', 'walked', 'away', 'from', 'my', 'work', 'computer', ',', 'poured', 'myself', 'a', 'drink', ',', 'took', 'a', 'shower', ',', 'and', 'i', \"'\", 've', 'been', 'laying', 'in', 'bed', 'on', 'the', 'internet', 'ever', 'since', '.', 'now', ',', 'when', 'i', 'step', 'back', 'and', 'think', 'about', 'it', 'logical', '##ly', ',', 'i', 'realize', 'the', 'stream', 'of', 'consciousness', 'isn', \"'\", 't', 'really', 'that', 'important', '.', 'i', 'have', 'a', 'to', '-', 'do', 'list', 'and', 'never', 'really', 'forget', 'the', 'important', 'things', 'anyway', '.', 'but', 'in', 'the', 'moment', ',', 'that', '\"', 'what', 'was', 'i', 'just', 'thinking', 'about', '\"', 'feeling', 'is', 'so', 'incredibly', 'powerful', ',', 'it', 'completely', 'de', '##ple', '##tes', 'my', 'motivation', 'for', 'anything', 'and', 'everything', '.', 'all', 'consuming', '.', 'it', \"'\", 's', 'been', 'happening', 'to', 'me', 'for', 'years', ',', 'and', 'was', 'originally', 'why', 'i', 'sought', 'help', 'in', 'the', 'first', 'place', 'and', 'was', 'subsequently', 'diagnosed', 'with', 'add', 'just', 'over', 'a', 'year', 'ago', '.', 'whenever', 'a', 'thought', 'does', 'pop', 'back', 'into', 'my', 'head', ',', 'it', \"'\", 's', 'always', 'something', 'trivial', '.', '\"', 'oh', ',', 'i', 'was', 'thinking', 'of', 'a', 'quote', 'from', 'a', 'movie', ',', '\"', 'or', '\"', 'oh', 'yeah', ',', 'i', 'was', 'going', 'to', 'see', 'if', 'new', 'socks', 'are', 'on', 'sale', '.', '\"', 'so', '.', '.', '.', 'sound', 'familiar', '?', 'does', 'this', 'happen', 'to', 'anyone', 'else', '?']\n",
      "INFO:__main__:Number of tokens: 539\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['what', 'was', 'i', 'just', 'thinking', 'about', '?', 'do', 'you', 'ever', 'lose', 'your', 'train', 'of', 'thought', ',', 'then', 'try', 'to', 're', '##tra', '##ce', 'it', ',', 'or', 'just', '\"', 'shut', 'down', '\"', 'as', 'a', 'result', '?', 'i', 'have', 'a', 'lot', 'on', 'my', 'mind', 'lately', '-', 'work', 'deadline', '##s', ',', 'bills', ',', 'my', 'social', '/', 'love', 'life', ',', 'business', 'trip', 'next', 'week', ',', 'vacation', 'after', 'that', '-', 'not', 'to', 'mention', 'a', 'few', 'things', 'i', \"'\", 'm', 'behind', 'on', '.', 'earlier', 'this', 'morning', 'i', 'was', 'working', 'on', 'some', 'sql', 'code', 'for', 'work', '.', 'within', 'a', 'few', 'seconds', 'i', 'got', 'distracted', 'by', ':', '2', 'instant', 'messages', ',', 'something', 'going', 'on', 'outside', 'my', 'window', ',', 'my', 'upstairs', 'neighbor', 'dropped', 'something', ',', 'another', 'neighbor', 'was', 'just', 'leaving', 'through', 'the', 'front', 'door', ',', 'checking', 'my', 'calendar', 'concerning', 'the', 'business', 'trip', 'next', 'week', ',', 'something', 'on', 'the', 'tv', 'that', 'caught', 'my', 'attention', ',', 'at', 'least', 'one', 'other', 'thing', 'i', 'can', \"'\", 't', 'remember', ',', 'and', 'then', 'back', 'to', 'the', 'code', '.', 'that', 'all', 'literally', 'happened', 'within', 'just', 'a', 'few', 'seconds', '.', 'then', 'i', 'found', 'myself', 'thinking', ',', '\"', 'now', 'what', 'was', 'i', 'just', 'thinking', 'about', '?', '\"', 'i', 'had', '[', '\"', 'tip', 'of', 'the', 'tongue', 'syndrome', '\"', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'tip', '_', 'of', '_', 'the', '_', 'tongue', ')', 'about', 'my', 'own', 'stream', 'of', 'consciousness', '.', 'and', 'i', 'had', 'a', 'very', 'distinct', 'feeling', 'of', 'overwhelming', 'hopeless', '##ness', 'in', 'the', 'face', 'of', 'straightening', 'it', 'all', 'out', '.', 'this', 'was', 'about', 'an', 'hour', 'after', 'i', 'took', 'my', 'morning', 'dose', 'of', 'add', '##eral', '##l', '.', 'i', 'believe', 'i', 'actually', 'had', 'a', 'mini', 'panic', 'attack', '.', 'i', 'couldn', \"'\", 't', 'focus', ',', 'my', 'heart', 'was', 'racing', 'and', 'all', 'i', 'cared', 'about', 'was', 'what', 'stupid', 'trivial', 'thing', 'was', 'just', 'on', 'my', 'mind', 'a', 'few', 'moments', 'prior', ',', 'because', 'it', 'seemed', 'like', 'the', 'most', 'important', 'thing', 'in', 'the', 'world', '.', 'even', 'though', 'i', 'have', 'a', 'ton', 'of', 'work', 'to', 'do', ',', 'i', 'just', 'didn', \"'\", 't', 'care', 'anymore', '.', 'i', 'walked', 'away', 'from', 'my', 'work', 'computer', ',', 'poured', 'myself', 'a', 'drink', ',', 'took', 'a', 'shower', ',', 'and', 'i', \"'\", 've', 'been', 'laying', 'in', 'bed', 'on', 'the', 'internet', 'ever', 'since', '.', 'now', ',', 'when', 'i', 'step', 'back', 'and', 'think', 'about', 'it', 'logical', '##ly', ',', 'i', 'realize', 'the', 'stream', 'of', 'consciousness', 'isn', \"'\", 't', 'really', 'that', 'important', '.', 'i', 'have', 'a', 'to', '-', 'do', 'list', 'and', 'never', 'really', 'forget', 'the', 'important', 'things', 'anyway', '.', 'but', 'in', 'the', 'moment', ',', 'that', '\"', 'what', 'was', 'i', 'just', 'thinking', 'about', '\"', 'feeling', 'is', 'so', 'incredibly', 'powerful', ',', 'it', 'completely', 'de', '##ple', '##tes', 'my', 'motivation', 'for', 'anything', 'and', 'everything', '.', 'all', 'consuming', '.', 'it', \"'\", 's', 'been', 'happening', 'to', 'me', 'for', 'years', ',', 'and', 'was', 'originally', 'why', 'i', 'sought', 'help', 'in', 'the', 'first', 'place', 'and', 'was', 'subsequently', 'diagnosed', 'with', 'add', 'just', 'over', 'a', 'year', 'ago', '.', 'whenever', 'a', 'thought', 'does', 'pop', 'back', 'into', 'my', 'head', ',', 'it', \"'\", 's', 'always', 'something', 'trivial', '.', '\"', 'oh', ',', 'i', 'was', 'thinking', 'of', 'a', 'quote', 'from', 'a', 'movie', ',', '\"', 'or', '\"', 'oh', 'yeah', ','], ['i', 'was', 'going', 'to', 'see', 'if', 'new', 'socks', 'are', 'on', 'sale', '.', '\"', 'so', '.', '.', '.', 'sound', 'familiar', '?', 'does', 'this', 'happen', 'to', 'anyone', 'else', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', ':', 'getting', 'help', ',', 'asking', 'for', 'yours', 'i', 'have', 'work', 'i', 'have', 'to', 'do', 'today', '.', 'obviously', ',', 'i', \"'\", 'm', 'not', 'doing', 'it', '.', 'right', 'as', 'i', 'had', 'the', 'program', 'opened', 'i', 'needed', 'to', 'do', 'work', 'in', ',', 'i', 'decided', 'to', 'see', 'what', 'the', 'ad', '##hd', 'sub', '##red', '##dit', 'was', 'like', '.', 'after', 'lurking', 'a', 'good', '90', 'minutes', 'on', 'here', ',', 'here', \"'\", 's', 'my', 'first', 'post', '.', 'i', 'wanna', 'say', ',', 'this', 'is', 'a', 'great', 'community', 'and', 'the', 'information', 'here', 'is', 'simply', 'wonderful', ',', 'putting', 'a', 'lot', 'of', 'my', 'anxiety', 'at', 'ease', '.', ':', ')', ':', ')', 'i', 'was', 'diagnosed', 'when', 'i', 'was', 'in', 'elementary', 'school', '.', 'typical', 'ad', '##hd', 'story', ':', 'high', 'iq', ',', 'excelled', 'in', 'middle', 'school', 'and', 'highs', '##cho', '##ol', '(', 'without', 'med', '##s', ')', '.', 'got', 'into', 'good', 'engineering', 'school', ',', 'but', 'without', 'the', 'rigid', 'home', 'structure', ',', 'i', \"'\", 'm', 'struggling', ',', 'in', 'all', 'of', 'the', 'typical', 'ways', '.', 'currently', 'not', 'taking', 'medication', ',', 'but', 'i', \"'\", 've', 'been', 'talking', 'to', 'a', 'doctor', ',', 'taken', 'a', 'test', 'to', 'help', 'figure', 'out', 'the', 'medication', 'i', 'need', '.', 'hopefully', 'this', 'is', 'soon', ',', 'but', 'not', 'for', 'at', 'least', '4', '-', '5', 'days', 'because', 'he', \"'\", 's', 'on', 'vacation', '.', 'my', 'question', 'for', 'you', ',', 'ra', '##dd', '##it', ',', 'is', 'i', 'have', 'some', 'work', 'to', 'do', 'in', 'the', 'mean', 'time', ',', 'and', 'its', 'really', 'hard', 'to', 'stay', 'focused', '.', 'any', 'advice', 'on', 'how', 'to', 'stay', 'motivated', 'and', 'focused', 'without', 'medication', '?', 'typically', 'i', \"'\", 'll', 'med', '##icate', 'with', 'ample', 'amounts', 'of', 'caf', '##fe', '##ine', ',', 'looking', 'for', 'alternatives', '.']\n",
      "INFO:__main__:Number of tokens: 265\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', ':', 'getting', 'help', ',', 'asking', 'for', 'yours', 'i', 'have', 'work', 'i', 'have', 'to', 'do', 'today', '.', 'obviously', ',', 'i', \"'\", 'm', 'not', 'doing', 'it', '.', 'right', 'as', 'i', 'had', 'the', 'program', 'opened', 'i', 'needed', 'to', 'do', 'work', 'in', ',', 'i', 'decided', 'to', 'see', 'what', 'the', 'ad', '##hd', 'sub', '##red', '##dit', 'was', 'like', '.', 'after', 'lurking', 'a', 'good', '90', 'minutes', 'on', 'here', ',', 'here', \"'\", 's', 'my', 'first', 'post', '.', 'i', 'wanna', 'say', ',', 'this', 'is', 'a', 'great', 'community', 'and', 'the', 'information', 'here', 'is', 'simply', 'wonderful', ',', 'putting', 'a', 'lot', 'of', 'my', 'anxiety', 'at', 'ease', '.', ':', ')', ':', ')', 'i', 'was', 'diagnosed', 'when', 'i', 'was', 'in', 'elementary', 'school', '.', 'typical', 'ad', '##hd', 'story', ':', 'high', 'iq', ',', 'excelled', 'in', 'middle', 'school', 'and', 'highs', '##cho', '##ol', '(', 'without', 'med', '##s', ')', '.', 'got', 'into', 'good', 'engineering', 'school', ',', 'but', 'without', 'the', 'rigid', 'home', 'structure', ',', 'i', \"'\", 'm', 'struggling', ',', 'in', 'all', 'of', 'the', 'typical', 'ways', '.', 'currently', 'not', 'taking', 'medication', ',', 'but', 'i', \"'\", 've', 'been', 'talking', 'to', 'a', 'doctor', ',', 'taken', 'a', 'test', 'to', 'help', 'figure', 'out', 'the', 'medication', 'i', 'need', '.', 'hopefully', 'this', 'is', 'soon', ',', 'but', 'not', 'for', 'at', 'least', '4', '-', '5', 'days', 'because', 'he', \"'\", 's', 'on', 'vacation', '.', 'my', 'question', 'for', 'you', ',', 'ra', '##dd', '##it', ',', 'is', 'i', 'have', 'some', 'work', 'to', 'do', 'in', 'the', 'mean', 'time', ',', 'and', 'its', 'really', 'hard', 'to', 'stay', 'focused', '.', 'any', 'advice', 'on', 'how', 'to', 'stay', 'motivated', 'and', 'focused', 'without', 'medication', '?', 'typically', 'i', \"'\", 'll', 'med', '##icate', 'with', 'ample', 'amounts', 'of', 'caf', '##fe', '##ine', ',', 'looking', 'for', 'alternatives', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'notice', 'weed', 'helps', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'notice', 'weed', 'helps', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['v', '##y', '##van', '##se', 'and', 'anxiety', 'i', 'have', 'been', 'discussing', 'medications', 'with', 'my', 'psychiatrist', 'and', 'will', 'likely', 'get', 'a', 'prescription', 'next', 'session', '.', 'she', 'has', 'expressed', 'hesitation', 'in', 'pre', '##sc', '##ri', '##bing', 'me', 'an', 'amp', '##het', '##amine', '(', 'i', 'took', 'concert', '##a', 'previously', 'with', 'no', 'effect', ')', 'because', 'of', 'the', 'potential', 'of', 'ex', '##ace', '##rba', '##ting', 'my', '(', 'low', '-', 'mild', ')', 'anxiety', '.', 'i', 'have', 'read', 'through', 'a', 'lot', 'of', 'the', 'posts', 'around', 'here', 'and', 'have', 'noticed', 'somewhat', 'of', 'a', 'split', 'between', 'those', 'whose', 'anxiety', 'was', 'helped', 'and', 'and', 'those', 'whose', 'anxiety', 'was', 'worsened', 'with', 'v', '##y', '##van', '##se', 'and', 'other', 'amp', '##het', '##amine', '##s', '.', 'i', \"'\", 'm', 'looking', 'to', 'minimize', 'the', 'effects', 'of', 'this', 'trial', 'and', 'error', 'period', 'and', 'wanted', 'to', 'ask', 'which', 'amp', '##het', '##amine', 'was', 'better', 'for', 'everyone', \"'\", 's', 'anxiety', '(', 'if', 'you', \"'\", 've', 'taken', 'more', 'than', 'one', 'type', ')', '.', 'i', 'know', 'everyone', 'reacts', 'differently', ',', 'but', 'i', 'thought', 'it', 'couldn', \"'\", 't', 'hurt', 'to', 'go', 'with', 'the', 'majority', 'decision', 'here', 'if', 'i', \"'\", 'm', 'asked', 'what', 'i', 'would', 'like', 'to', 'try', '(', 'does', 'that', 'even', 'happen', ',', 'id', '##k', '?', ')', '.', 'so', '!', 'which', 'med', '##s', 'helped', 'and', 'which', 'med', '##s', 'worsened', 'your', 'anxiety', '?', 'edit', '-', 'i', 'should', 'maybe', 'add', 'that', 'i', \"'\", 'am', 'of', 'the', 'ina', '##tten', '##tive', 'type', '.', 'not', 'hyper', 'active', 'at', 'all', '.', 'very', 'let', '##har', '##gic', 'and', 'intro', '##verted', '.']\n",
      "INFO:__main__:Number of tokens: 236\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['v', '##y', '##van', '##se', 'and', 'anxiety', 'i', 'have', 'been', 'discussing', 'medications', 'with', 'my', 'psychiatrist', 'and', 'will', 'likely', 'get', 'a', 'prescription', 'next', 'session', '.', 'she', 'has', 'expressed', 'hesitation', 'in', 'pre', '##sc', '##ri', '##bing', 'me', 'an', 'amp', '##het', '##amine', '(', 'i', 'took', 'concert', '##a', 'previously', 'with', 'no', 'effect', ')', 'because', 'of', 'the', 'potential', 'of', 'ex', '##ace', '##rba', '##ting', 'my', '(', 'low', '-', 'mild', ')', 'anxiety', '.', 'i', 'have', 'read', 'through', 'a', 'lot', 'of', 'the', 'posts', 'around', 'here', 'and', 'have', 'noticed', 'somewhat', 'of', 'a', 'split', 'between', 'those', 'whose', 'anxiety', 'was', 'helped', 'and', 'and', 'those', 'whose', 'anxiety', 'was', 'worsened', 'with', 'v', '##y', '##van', '##se', 'and', 'other', 'amp', '##het', '##amine', '##s', '.', 'i', \"'\", 'm', 'looking', 'to', 'minimize', 'the', 'effects', 'of', 'this', 'trial', 'and', 'error', 'period', 'and', 'wanted', 'to', 'ask', 'which', 'amp', '##het', '##amine', 'was', 'better', 'for', 'everyone', \"'\", 's', 'anxiety', '(', 'if', 'you', \"'\", 've', 'taken', 'more', 'than', 'one', 'type', ')', '.', 'i', 'know', 'everyone', 'reacts', 'differently', ',', 'but', 'i', 'thought', 'it', 'couldn', \"'\", 't', 'hurt', 'to', 'go', 'with', 'the', 'majority', 'decision', 'here', 'if', 'i', \"'\", 'm', 'asked', 'what', 'i', 'would', 'like', 'to', 'try', '(', 'does', 'that', 'even', 'happen', ',', 'id', '##k', '?', ')', '.', 'so', '!', 'which', 'med', '##s', 'helped', 'and', 'which', 'med', '##s', 'worsened', 'your', 'anxiety', '?', 'edit', '-', 'i', 'should', 'maybe', 'add', 'that', 'i', \"'\", 'am', 'of', 'the', 'ina', '##tten', '##tive', 'type', '.', 'not', 'hyper', 'active', 'at', 'all', '.', 'very', 'let', '##har', '##gic', 'and', 'intro', '##verted', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'leading', 'expert', 'in', 'the', 'field', '.', 'dr', '.', 'russell', 'bark', '##ley', 'best', 'stream', '!']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'leading', 'expert', 'in', 'the', 'field', '.', 'dr', '.', 'russell', 'bark', '##ley', 'best', 'stream', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['interesting', 'view', 'on', 'per', '##se', '##vera', '##tion', '.']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['interesting', 'view', 'on', 'per', '##se', '##vera', '##tion', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ideas', 'for', 'high', '-', 'stimulus', 'ho', '##bbies', '?', 'i', 'am', 'a', '21', '##yo', 'recently', 'diagnosed', 'with', 'attention', 'deficit', ',', 'and', 'am', 'currently', 'taking', 'v', '##y', '##van', '##se', '&', 'exploring', 'suitable', 'lifestyle', 'adjustments', '(', 'due', 'to', 'related', 'anxiety', ',', 'etc', '.', ')', 'with', 'a', 'therapist', 'as', 'well', '.', 'one', 'idea', 'was', 'to', 'set', 'aside', 'time', 'and', '/', 'or', 'an', 'activity', 'just', 'for', 'myself', ',', 'and', 'that', 'i', '100', '%', 'enjoy', ',', 'but', 'this', 'is', 'difficult', '.', 'i', 'enjoy', 'my', 'job', 'and', 'my', 'school', '##work', ',', 'but', 'i', 'find', 'the', 'pressure', 'to', 'to', 'keep', 'up', 'or', 'succeed', 'is', 'a', 'large', 'part', 'of', 'that', 'enjoyment', 'and', 'motivation', '.', 'other', 'than', 'that', ',', 'my', 'favorite', 'activities', 'are', 'skiing', 'and', 'zip', '##lining', '(', 'yep', ',', 'adrenaline', '-', 'chasing', 'physical', 'activities', ')', ',', 'which', 'are', 'impossible', 'to', 'do', 'regularly', 'due', 'to', 'time', ',', 'money', ',', 'and', 'distance', '.', 'i', 'am', 'contemplating', 'dance', 'or', 'hu', '##la', '-', 'hoop', '##ing', 'classes', ',', 'but', 'i', 'am', 'very', 'self', '-', 'conscious', 'in', 'groups', ',', 'especially', 'when', 'being', 'bad', 'at', 'things', '(', 'working', 'on', 'that', ')', '.', 'also', 'thinking', 'about', 'getting', 'back', 'into', 'ice', 'skating', ',', 'but', 'the', 'cost', ',', 'travel', 'time', ',', '&', 'hereditary', 'joint', 'problems', 'are', 'major', 'con', '##s', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', 'i', 'need', 'something', 'stimulating', 'solely', 'for', 'enjoyment', 'that', 'won', \"'\", 't', 'involved', 'being', 'judged', 'on', 'merit', '(', 'sex', 'and', 'video', 'games', 'don', \"'\", 't', 'count', ')', '.', 'does', 'anyone', 'here', 'have', 'any', 'suggestions', 'for', 'replacement', 'ho', '##bbies', 'for', 'me', '?', 'or', ',', 'if', 'anyone', 'has', 'any', 'personal', 'stories', 'about', 'successfully', 'finding', 'ho', '##bbies', ',', 'i', 'would', 'love', 'to', 'hear', 'them', '!']\n",
      "INFO:__main__:Number of tokens: 270\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ideas', 'for', 'high', '-', 'stimulus', 'ho', '##bbies', '?', 'i', 'am', 'a', '21', '##yo', 'recently', 'diagnosed', 'with', 'attention', 'deficit', ',', 'and', 'am', 'currently', 'taking', 'v', '##y', '##van', '##se', '&', 'exploring', 'suitable', 'lifestyle', 'adjustments', '(', 'due', 'to', 'related', 'anxiety', ',', 'etc', '.', ')', 'with', 'a', 'therapist', 'as', 'well', '.', 'one', 'idea', 'was', 'to', 'set', 'aside', 'time', 'and', '/', 'or', 'an', 'activity', 'just', 'for', 'myself', ',', 'and', 'that', 'i', '100', '%', 'enjoy', ',', 'but', 'this', 'is', 'difficult', '.', 'i', 'enjoy', 'my', 'job', 'and', 'my', 'school', '##work', ',', 'but', 'i', 'find', 'the', 'pressure', 'to', 'to', 'keep', 'up', 'or', 'succeed', 'is', 'a', 'large', 'part', 'of', 'that', 'enjoyment', 'and', 'motivation', '.', 'other', 'than', 'that', ',', 'my', 'favorite', 'activities', 'are', 'skiing', 'and', 'zip', '##lining', '(', 'yep', ',', 'adrenaline', '-', 'chasing', 'physical', 'activities', ')', ',', 'which', 'are', 'impossible', 'to', 'do', 'regularly', 'due', 'to', 'time', ',', 'money', ',', 'and', 'distance', '.', 'i', 'am', 'contemplating', 'dance', 'or', 'hu', '##la', '-', 'hoop', '##ing', 'classes', ',', 'but', 'i', 'am', 'very', 'self', '-', 'conscious', 'in', 'groups', ',', 'especially', 'when', 'being', 'bad', 'at', 'things', '(', 'working', 'on', 'that', ')', '.', 'also', 'thinking', 'about', 'getting', 'back', 'into', 'ice', 'skating', ',', 'but', 'the', 'cost', ',', 'travel', 'time', ',', '&', 'hereditary', 'joint', 'problems', 'are', 'major', 'con', '##s', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', 'i', 'need', 'something', 'stimulating', 'solely', 'for', 'enjoyment', 'that', 'won', \"'\", 't', 'involved', 'being', 'judged', 'on', 'merit', '(', 'sex', 'and', 'video', 'games', 'don', \"'\", 't', 'count', ')', '.', 'does', 'anyone', 'here', 'have', 'any', 'suggestions', 'for', 'replacement', 'ho', '##bbies', 'for', 'me', '?', 'or', ',', 'if', 'anyone', 'has', 'any', 'personal', 'stories', 'about', 'successfully', 'finding', 'ho', '##bbies', ',', 'i', 'would', 'love', 'to', 'hear', 'them', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'someone', 'with', 'ad', '##hd', ',', 'ama', '!', 'the', 'title', 'says', 'it', 'all', '.', 'i', 'have', 'ad', '##hd', ',', 'ask', 'me', 'anything', '!', 'need', 'verification', '?', 'i', \"'\", 'll', 'get', 'around', 'to', 'it', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 38\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'someone', 'with', 'ad', '##hd', ',', 'ama', '!', 'the', 'title', 'says', 'it', 'all', '.', 'i', 'have', 'ad', '##hd', ',', 'ask', 'me', 'anything', '!', 'need', 'verification', '?', 'i', \"'\", 'll', 'get', 'around', 'to', 'it', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['advice', 'for', 'add', '##eral', '##l', 'acquisition', '.', 'i', \"'\", 've', 'been', 'having', 'bad', 'problems', 'that', 'few', 'years', 'paying', 'attention', 'and', 'getting', 'things', 'done', 'on', 'time', 'coupled', 'with', 'anxiety', '.', 'i', 's', '##na', '##gged', 'one', 'of', 'friend', \"'\", 's', 'add', '##eral', '##l', 'and', 'holy', 'fuck', 'it', 'worked', '!', 'it', 'didn', \"'\", 't', 'completely', 'fix', 'all', 'my', 'problems', 'but', 'i', 'felt', 'calm', 'focused', 'and', 'interested', 'in', 'people', 'than', 'i', 'have', 'been', 'in', 'years', '.', 'i', 'am', 'going', 'to', 'a', 'psychologist', 'next', 'week', '.', 'how', 'do', 'acquire', 'a', 'refer', '##ral', 'to', 'psychiatrist', 'for', 'a', 'prescription', 'without', 'sounding', 'sounding', 'like', 'a', 'total', 'drug', 'addict', '?']\n",
      "INFO:__main__:Number of tokens: 101\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['advice', 'for', 'add', '##eral', '##l', 'acquisition', '.', 'i', \"'\", 've', 'been', 'having', 'bad', 'problems', 'that', 'few', 'years', 'paying', 'attention', 'and', 'getting', 'things', 'done', 'on', 'time', 'coupled', 'with', 'anxiety', '.', 'i', 's', '##na', '##gged', 'one', 'of', 'friend', \"'\", 's', 'add', '##eral', '##l', 'and', 'holy', 'fuck', 'it', 'worked', '!', 'it', 'didn', \"'\", 't', 'completely', 'fix', 'all', 'my', 'problems', 'but', 'i', 'felt', 'calm', 'focused', 'and', 'interested', 'in', 'people', 'than', 'i', 'have', 'been', 'in', 'years', '.', 'i', 'am', 'going', 'to', 'a', 'psychologist', 'next', 'week', '.', 'how', 'do', 'acquire', 'a', 'refer', '##ral', 'to', 'psychiatrist', 'for', 'a', 'prescription', 'without', 'sounding', 'sounding', 'like', 'a', 'total', 'drug', 'addict', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'relax', '?']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'relax', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['outs', '##mart', '##ing', 'yourself', 'for', 'success']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['outs', '##mart', '##ing', 'yourself', 'for', 'success']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['have', 'any', 'of', 'you', 'received', 'any', 'public', 'assistance', ',', 'or', 'was', 'it', 'all', 'from', 'private', 'insurance', 'or', 'your', 'own', 'funds', '?', 'in', 'the', 'past', ',', 'i', \"'\", 've', 'called', 'around', 'to', 'public', 'facilities', ',', 'and', 'drew', 'a', 'blank', 'on', 'resources', 'for', 'folks', 'with', 'ad', '##hd', 'in', 'my', 'area', '(', 'so', 'cal', ')', '.', 'specifically', ',', 'i', 'called', 'the', 'los', 'angeles', 'county', 'department', 'of', 'mental', 'health', '.', 'from', 'the', 'person', 'i', 'spoke', 'to', 'on', 'the', 'phone', ',', 'i', 'got', 'the', 'impression', 'they', 'don', \"'\", 't', 'even', 'test', 'for', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 89\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['have', 'any', 'of', 'you', 'received', 'any', 'public', 'assistance', ',', 'or', 'was', 'it', 'all', 'from', 'private', 'insurance', 'or', 'your', 'own', 'funds', '?', 'in', 'the', 'past', ',', 'i', \"'\", 've', 'called', 'around', 'to', 'public', 'facilities', ',', 'and', 'drew', 'a', 'blank', 'on', 'resources', 'for', 'folks', 'with', 'ad', '##hd', 'in', 'my', 'area', '(', 'so', 'cal', ')', '.', 'specifically', ',', 'i', 'called', 'the', 'los', 'angeles', 'county', 'department', 'of', 'mental', 'health', '.', 'from', 'the', 'person', 'i', 'spoke', 'to', 'on', 'the', 'phone', ',', 'i', 'got', 'the', 'impression', 'they', 'don', \"'\", 't', 'even', 'test', 'for', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['thinking', 'of', 'switching', 'from', 'add', '##eral', '##l', '25', '##mg', 'x', '##r', 'to', 'v', '##y', '##van', '##se', '.', 'has', 'anyone', 'else', 'made', 'this', 'switch', '?']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['thinking', 'of', 'switching', 'from', 'add', '##eral', '##l', '25', '##mg', 'x', '##r', 'to', 'v', '##y', '##van', '##se', '.', 'has', 'anyone', 'else', 'made', 'this', 'switch', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['25', '##m', 'no', 'friends', ',', 'no', 'ho', '##bbies', ',', 'trouble', 'making', 'my', 'life', 'interesting', 'again', '.', 'it', 'triggers', 'my', 'anxiety', 'to', 'even', 'think', 'about', 'writing', 'what', 'i', 'am', 'writing', 'now', '.', 'i', 'usually', 'erase', 'everything', 'before', 'posting', '.', 'i', 'still', 'doubt', 'whether', 'i', 'am', 'posting', 'in', 'the', 'right', 'sub', '##red', '##dit', 'but', 'this', 'time', ',', 'i', 'will', 'do', 'it', 'and', 'i', 'will', 'post', 'it', 'because', 'no', 'one', 'i', 'know', 'have', 'ad', '##hd', 'and', 'i', 'would', 'really', 'appreciate', 'any', 'feedback', 'or', 'similar', 'stories', '.', 'i', 'live', 'with', 'ad', '##hd', '-', 'pi', ',', 'was', 'diagnosed', 'in', '2008', ',', 'i', 'am', 'on', 'add', '##eral', '##l', 'x', '##r', '30', '##mg', 'since', 'middle', 'of', '2010', ',', 'but', 'can', \"'\", 't', 'afford', 'therapy', 'yet', '.', 'i', 'have', 'little', 'to', 'no', 'interest', 'in', 'my', 'success', 'at', 'school', '.', 'i', 'couldn', \"'\", 't', 'care', 'less', 'about', 'computer', 'science', '.', 'i', 'work', 'in', 'retail', 'and', ',', 'even', 'though', 'i', 'really', 'love', 'my', 'job', ',', 'i', 'fear', 'that', 'i', 'will', 'get', 'stuck', 'in', 'it', 'forever', 'if', 'i', 'quit', 'school', '.', 'i', 'alien', '##ated', 'and', 'lost', 'all', 'my', 'friends', 'a', 'few', 'years', 'ago', 'and', ',', 'while', 'i', 'still', 'have', 'some', 'acquaintances', ',', 'i', 'have', 'trouble', 'finding', 'people', 'who', 'i', 'can', 'relate', 'to', '.', 'when', 'i', 'do', ',', 'i', 'often', 'feel', 'i', 'am', 'way', 'more', 'interested', 'than', 'they', 'are', '.', 'i', 'feel', 'cl', '##ing', '##y', '.', 'when', 'i', 'get', 'back', 'from', 'school', '/', 'work', 'i', 'mostly', 'brows', '##e', 'red', '##dit', '.', 'it', 'used', 'to', 'be', 'facebook', ',', 'but', 'i', 'deleted', 'it', '.', 'when', 'i', 'am', 'in', 'a', 'relationship', ',', 'i', 'like', 'to', 'cook', ',', 'shop', ',', 'hike', ',', 'go', 'to', 'the', 'movies', 'and', 'much', 'much', 'more', '.', 'single', ',', 'i', 'can', \"'\", 't', 'find', 'the', 'will', 'to', 'do', 'any', 'of', 'that', '.', 'i', 'have', 'no', 'daily', 'ho', '##bbies', ',', 'no', 'projects', 'and', 'i', 'haven', \"'\", 't', 'del', '##ved', 'into', 'any', 'of', 'my', 'interests', '.', 'i', 'am', 'having', 'trouble', 'getting', 'interested', 'in', 'my', 'own', 'well', '-', 'being', '.', '*', '*', 'edit', ':', 'consensus', '*', '*', '*', 'get', 'psychiatrist', '*', 'talk', 'to', 'psychiatrist', 'about', 'possibility', 'of', 'depression', 'and', 'anxiety', '*', 'ask', 'about', 'possible', 'medication', '*', 'get', 'therapy', ',', 'it', \"'\", 's', 'worth', 'it', '*', 're', '##kind', '##le', 'passion', 'with', 'whatever', 'i', 'used', 'to', 'be', 'passionate', 'about', '/', 'find', 'new', 'ho', '##bbies', '*', 'meet', 'people', 'through', 'said', 'ho', '##bbies', 'thank', 'you', ',', 'you', 'guys', 'are', 'awesome', '.', 'i', 'really', 'should', 'interact', 'with', 'the', 'internet', 'more', ',', 'it', 'feels', 'great', '.']\n",
      "INFO:__main__:Number of tokens: 405\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['25', '##m', 'no', 'friends', ',', 'no', 'ho', '##bbies', ',', 'trouble', 'making', 'my', 'life', 'interesting', 'again', '.', 'it', 'triggers', 'my', 'anxiety', 'to', 'even', 'think', 'about', 'writing', 'what', 'i', 'am', 'writing', 'now', '.', 'i', 'usually', 'erase', 'everything', 'before', 'posting', '.', 'i', 'still', 'doubt', 'whether', 'i', 'am', 'posting', 'in', 'the', 'right', 'sub', '##red', '##dit', 'but', 'this', 'time', ',', 'i', 'will', 'do', 'it', 'and', 'i', 'will', 'post', 'it', 'because', 'no', 'one', 'i', 'know', 'have', 'ad', '##hd', 'and', 'i', 'would', 'really', 'appreciate', 'any', 'feedback', 'or', 'similar', 'stories', '.', 'i', 'live', 'with', 'ad', '##hd', '-', 'pi', ',', 'was', 'diagnosed', 'in', '2008', ',', 'i', 'am', 'on', 'add', '##eral', '##l', 'x', '##r', '30', '##mg', 'since', 'middle', 'of', '2010', ',', 'but', 'can', \"'\", 't', 'afford', 'therapy', 'yet', '.', 'i', 'have', 'little', 'to', 'no', 'interest', 'in', 'my', 'success', 'at', 'school', '.', 'i', 'couldn', \"'\", 't', 'care', 'less', 'about', 'computer', 'science', '.', 'i', 'work', 'in', 'retail', 'and', ',', 'even', 'though', 'i', 'really', 'love', 'my', 'job', ',', 'i', 'fear', 'that', 'i', 'will', 'get', 'stuck', 'in', 'it', 'forever', 'if', 'i', 'quit', 'school', '.', 'i', 'alien', '##ated', 'and', 'lost', 'all', 'my', 'friends', 'a', 'few', 'years', 'ago', 'and', ',', 'while', 'i', 'still', 'have', 'some', 'acquaintances', ',', 'i', 'have', 'trouble', 'finding', 'people', 'who', 'i', 'can', 'relate', 'to', '.', 'when', 'i', 'do', ',', 'i', 'often', 'feel', 'i', 'am', 'way', 'more', 'interested', 'than', 'they', 'are', '.', 'i', 'feel', 'cl', '##ing', '##y', '.', 'when', 'i', 'get', 'back', 'from', 'school', '/', 'work', 'i', 'mostly', 'brows', '##e', 'red', '##dit', '.', 'it', 'used', 'to', 'be', 'facebook', ',', 'but', 'i', 'deleted', 'it', '.', 'when', 'i', 'am', 'in', 'a', 'relationship', ',', 'i', 'like', 'to', 'cook', ',', 'shop', ',', 'hike', ',', 'go', 'to', 'the', 'movies', 'and', 'much', 'much', 'more', '.', 'single', ',', 'i', 'can', \"'\", 't', 'find', 'the', 'will', 'to', 'do', 'any', 'of', 'that', '.', 'i', 'have', 'no', 'daily', 'ho', '##bbies', ',', 'no', 'projects', 'and', 'i', 'haven', \"'\", 't', 'del', '##ved', 'into', 'any', 'of', 'my', 'interests', '.', 'i', 'am', 'having', 'trouble', 'getting', 'interested', 'in', 'my', 'own', 'well', '-', 'being', '.', '*', '*', 'edit', ':', 'consensus', '*', '*', '*', 'get', 'psychiatrist', '*', 'talk', 'to', 'psychiatrist', 'about', 'possibility', 'of', 'depression', 'and', 'anxiety', '*', 'ask', 'about', 'possible', 'medication', '*', 'get', 'therapy', ',', 'it', \"'\", 's', 'worth', 'it', '*', 're', '##kind', '##le', 'passion', 'with', 'whatever', 'i', 'used', 'to', 'be', 'passionate', 'about', '/', 'find', 'new', 'ho', '##bbies', '*', 'meet', 'people', 'through', 'said', 'ho', '##bbies', 'thank', 'you', ',', 'you', 'guys', 'are', 'awesome', '.', 'i', 'really', 'should', 'interact', 'with', 'the', 'internet', 'more', ',', 'it', 'feels', 'great', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'canada', '/', 'methyl', '##ph', '##eni', '##date', ']', 'i', 'have', 'very', 'low', 'income', '(', '$', '0', '/', 'month', ')', 'and', 'no', 'prescription', 'plan', '.', 'where', 'is', 'the', 'cheap', '##est', 'place', 'to', 'get', 'my', 'script', 'filled', '?']\n",
      "INFO:__main__:Number of tokens: 36\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'canada', '/', 'methyl', '##ph', '##eni', '##date', ']', 'i', 'have', 'very', 'low', 'income', '(', '$', '0', '/', 'month', ')', 'and', 'no', 'prescription', 'plan', '.', 'where', 'is', 'the', 'cheap', '##est', 'place', 'to', 'get', 'my', 'script', 'filled', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['newly', 'diagnosed', '.', 'are', 'the', 'med', '##s', 'working', '?', 'hi', ',', 'i', 'have', 'been', 'lurking', 'for', 'a', 'while', 'now', ',', 'but', 'i', 'feel', 'like', 'i', 'need', 'some', 'questions', 'answered', '.', 'i', 'was', 'diagnosed', 'about', '5', 'weeks', 'ago', '.', 'i', 'had', 'a', 'melt', 'down', 'at', 'school', 'and', 'had', 'to', 'take', 'some', 'time', 'away', '.', 'the', 'school', 'councillor', 'gave', 'me', 'a', 'test', 'and', 'said', 'that', 'it', 'was', 'quite', 'likely', 'that', 'i', 'have', 'ad', '##hd', '.', 'she', 'managed', 'to', 'get', 'me', 'an', 'emergency', 'session', 'with', 'a', 'ps', '##ych', 'and', 'she', 'agreed', 'that', 'i', 'likely', 'have', 'ad', '##hd', 'as', 'well', 'as', 'anxiety', 'and', 'depression', 'issues', '.', 'she', 'made', 'me', 'wait', 'a', 'few', 'weeks', 'to', 'start', 'med', '##s', 'and', 'on', 'friday', 'i', 'was', 'given', 'a', 'prescription', 'for', 'dex', '##ed', '##rine', '5', 'mg', 'per', 'day', '.', 'i', 'took', 'one', 'that', 'morning', 'and', 'felt', 'nothing', '.', 'i', 'took', 'one', 'yesterday', 'and', 'had', 'to', 'use', 'an', 'energy', 'drink', 'to', 'get', 'me', 'going', '.', 'i', 'have', 'taken', 'another', 'on', 'today', 'and', 'i', 'can', \"'\", 't', 'say', 'that', 'i', 'really', 'feel', 'anything', '.', 'what', 'am', 'i', 'supposed', 'to', 'feel', '?', 'what', 'is', 'supposed', 'to', 'happen', '?', 'i', 'really', 'just', 'sort', 'of', 'feel', 'let', 'down', '.', 'i', 'have', 'heard', 'about', 'how', 'the', 'med', '##s', 'can', 'turn', 'your', 'life', 'around', '.', 'i', 'am', 'not', 'experiencing', 'that', '.', 'anyone', 'have', 'any', 'input', '?']\n",
      "INFO:__main__:Number of tokens: 220\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['newly', 'diagnosed', '.', 'are', 'the', 'med', '##s', 'working', '?', 'hi', ',', 'i', 'have', 'been', 'lurking', 'for', 'a', 'while', 'now', ',', 'but', 'i', 'feel', 'like', 'i', 'need', 'some', 'questions', 'answered', '.', 'i', 'was', 'diagnosed', 'about', '5', 'weeks', 'ago', '.', 'i', 'had', 'a', 'melt', 'down', 'at', 'school', 'and', 'had', 'to', 'take', 'some', 'time', 'away', '.', 'the', 'school', 'councillor', 'gave', 'me', 'a', 'test', 'and', 'said', 'that', 'it', 'was', 'quite', 'likely', 'that', 'i', 'have', 'ad', '##hd', '.', 'she', 'managed', 'to', 'get', 'me', 'an', 'emergency', 'session', 'with', 'a', 'ps', '##ych', 'and', 'she', 'agreed', 'that', 'i', 'likely', 'have', 'ad', '##hd', 'as', 'well', 'as', 'anxiety', 'and', 'depression', 'issues', '.', 'she', 'made', 'me', 'wait', 'a', 'few', 'weeks', 'to', 'start', 'med', '##s', 'and', 'on', 'friday', 'i', 'was', 'given', 'a', 'prescription', 'for', 'dex', '##ed', '##rine', '5', 'mg', 'per', 'day', '.', 'i', 'took', 'one', 'that', 'morning', 'and', 'felt', 'nothing', '.', 'i', 'took', 'one', 'yesterday', 'and', 'had', 'to', 'use', 'an', 'energy', 'drink', 'to', 'get', 'me', 'going', '.', 'i', 'have', 'taken', 'another', 'on', 'today', 'and', 'i', 'can', \"'\", 't', 'say', 'that', 'i', 'really', 'feel', 'anything', '.', 'what', 'am', 'i', 'supposed', 'to', 'feel', '?', 'what', 'is', 'supposed', 'to', 'happen', '?', 'i', 'really', 'just', 'sort', 'of', 'feel', 'let', 'down', '.', 'i', 'have', 'heard', 'about', 'how', 'the', 'med', '##s', 'can', 'turn', 'your', 'life', 'around', '.', 'i', 'am', 'not', 'experiencing', 'that', '.', 'anyone', 'have', 'any', 'input', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['depression', ',', 'ad', '##hd', ',', 'ins', '##om', '##nia', '-', 'which', 'is', 'it', '?', 'i', \"'\", 've', 'been', 'having', 'problems', 'concentrating', 'at', 'work', '.', 'low', 'motivation', ',', 'lack', 'of', 'interest', 'in', 'things', '.', 'but', 'i', \"'\", 've', 'also', 'noticed', 'i', 'cannot', 'focus', 'very', 'well', '.', 'my', 'working', 'memory', 'and', 'attention', 'span', 'are', 'not', 'so', 'great', '.', 'it', 'has', 'become', 'very', 'difficult', 'for', 'me', 'to', 'focus', 'on', 'my', 'work', '(', 'programming', 'software', ')', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', '7', 'and', 'prescribed', 'rita', '##lin', '.', 'i', 'went', 'off', 'the', 'med', '##s', 'at', '11', 'years', 'old', 'because', 'i', 'was', 'sick', 'of', 'being', 'bull', '##ied', 'for', 'taking', '\"', 'mental', 'pills', '\"', 'etc', '.', 'i', 'am', 'now', '23', '(', 'male', ')', '.', 'the', 'flu', '##ox', '##eti', '##ne', 'and', 'ci', '##tal', '##op', '##ram', 'ssr', '##is', 'i', \"'\", 've', 'tried', 'have', 'some', 'bad', 'side', 'effects', '(', 'muscle', 'tension', ',', 'an', '##org', '##as', '##mia', ',', 'low', 'li', '##bid', '##o', ')', 'and', 'they', 'have', 'not', 'helped', 'with', 'the', 'concentration', '.', 'my', 'mood', 'is', 'lifted', 'but', 'i', 'still', 'find', 'myself', 'unable', 'to', 'sleep', 'after', 'an', 'un', '##pro', '##ductive', 'day', 'at', 'work', '.', 'i', 'am', 'fairly', 'close', 'to', 'losing', 'my', 'job', '.', 'this', 'seems', 'to', 'happen', 'with', 'just', 'about', 'every', 'new', 'challenge', 'i', 'take', 'up', '.', 'i', 'can', 'stick', 'at', 'it', 'for', 'a', 'while', '(', '~', '6', '##mont', '##hs', ')', 'and', 'it', 'then', 'becomes', 'boring', 'and', 'i', 'simply', 'cannot', 'focus', 'on', 'it', '.', 'i', 'get', 'distracted', 'easily', ',', 'but', 'i', 'wouldn', \"'\", 't', 'say', 'i', \"'\", 'm', 'hyper', '##active', '.', 'obviously', 'depression', 'and', 'ins', '##om', '##nia', 'are', 'also', 'causes', 'of', 'these', 'work', 'problems', '.', 'it', \"'\", 's', 'hard', 'to', 'isolate', 'one', 'condition', 'from', 'the', 'other', '.', 'my', 'doctor', 'seems', 'to', 'be', 'in', 'favour', 'of', 'treating', 'the', 'sleep', '/', 'depression', 'first', ',', 'but', 'so', 'far', 'this', 'has', 'not', 'been', 'working', '.', 'i', 'am', 'seeing', 'a', 'psychiatrist', 'tomorrow', ',', 'so', 'hopefully', 'i', 'can', 'get', 'a', 'prescription', '.', 'however', 'i', 'thought', 'i', 'would', 'get', 'a', 'second', 'opinion', 'on', 'weather', 'this', 'is', 'the', 'right', 'way', 'to', 'solve', 'my', 'problems', '.', '*', '*', 'is', 'there', 'anyone', 'here', 'who', 'is', 'now', 'on', 'rita', '##lin', '(', 'or', 'similar', 'med', '##s', ')', '*', '*', 'and', 'has', 'been', 'through', 'a', 'similar', 'process', '.', 'did', 'it', 'help', 'your', 'productivity', 'and', 'life', 'in', 'general', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', 'can', \"'\", 't', 'focus', 'at', 'work', ',', 'depression', 'med', '##s', '(', 'ssr', '##is', ')', 'not', 'working', ',', 'had', 'ad', '##hd', 'as', 'a', 'child', '.', 'would', 'rita', '##lin', 'help', 'me', 'focus', '?']\n",
      "INFO:__main__:Number of tokens: 416\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['depression', ',', 'ad', '##hd', ',', 'ins', '##om', '##nia', '-', 'which', 'is', 'it', '?', 'i', \"'\", 've', 'been', 'having', 'problems', 'concentrating', 'at', 'work', '.', 'low', 'motivation', ',', 'lack', 'of', 'interest', 'in', 'things', '.', 'but', 'i', \"'\", 've', 'also', 'noticed', 'i', 'cannot', 'focus', 'very', 'well', '.', 'my', 'working', 'memory', 'and', 'attention', 'span', 'are', 'not', 'so', 'great', '.', 'it', 'has', 'become', 'very', 'difficult', 'for', 'me', 'to', 'focus', 'on', 'my', 'work', '(', 'programming', 'software', ')', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', '7', 'and', 'prescribed', 'rita', '##lin', '.', 'i', 'went', 'off', 'the', 'med', '##s', 'at', '11', 'years', 'old', 'because', 'i', 'was', 'sick', 'of', 'being', 'bull', '##ied', 'for', 'taking', '\"', 'mental', 'pills', '\"', 'etc', '.', 'i', 'am', 'now', '23', '(', 'male', ')', '.', 'the', 'flu', '##ox', '##eti', '##ne', 'and', 'ci', '##tal', '##op', '##ram', 'ssr', '##is', 'i', \"'\", 've', 'tried', 'have', 'some', 'bad', 'side', 'effects', '(', 'muscle', 'tension', ',', 'an', '##org', '##as', '##mia', ',', 'low', 'li', '##bid', '##o', ')', 'and', 'they', 'have', 'not', 'helped', 'with', 'the', 'concentration', '.', 'my', 'mood', 'is', 'lifted', 'but', 'i', 'still', 'find', 'myself', 'unable', 'to', 'sleep', 'after', 'an', 'un', '##pro', '##ductive', 'day', 'at', 'work', '.', 'i', 'am', 'fairly', 'close', 'to', 'losing', 'my', 'job', '.', 'this', 'seems', 'to', 'happen', 'with', 'just', 'about', 'every', 'new', 'challenge', 'i', 'take', 'up', '.', 'i', 'can', 'stick', 'at', 'it', 'for', 'a', 'while', '(', '~', '6', '##mont', '##hs', ')', 'and', 'it', 'then', 'becomes', 'boring', 'and', 'i', 'simply', 'cannot', 'focus', 'on', 'it', '.', 'i', 'get', 'distracted', 'easily', ',', 'but', 'i', 'wouldn', \"'\", 't', 'say', 'i', \"'\", 'm', 'hyper', '##active', '.', 'obviously', 'depression', 'and', 'ins', '##om', '##nia', 'are', 'also', 'causes', 'of', 'these', 'work', 'problems', '.', 'it', \"'\", 's', 'hard', 'to', 'isolate', 'one', 'condition', 'from', 'the', 'other', '.', 'my', 'doctor', 'seems', 'to', 'be', 'in', 'favour', 'of', 'treating', 'the', 'sleep', '/', 'depression', 'first', ',', 'but', 'so', 'far', 'this', 'has', 'not', 'been', 'working', '.', 'i', 'am', 'seeing', 'a', 'psychiatrist', 'tomorrow', ',', 'so', 'hopefully', 'i', 'can', 'get', 'a', 'prescription', '.', 'however', 'i', 'thought', 'i', 'would', 'get', 'a', 'second', 'opinion', 'on', 'weather', 'this', 'is', 'the', 'right', 'way', 'to', 'solve', 'my', 'problems', '.', '*', '*', 'is', 'there', 'anyone', 'here', 'who', 'is', 'now', 'on', 'rita', '##lin', '(', 'or', 'similar', 'med', '##s', ')', '*', '*', 'and', 'has', 'been', 'through', 'a', 'similar', 'process', '.', 'did', 'it', 'help', 'your', 'productivity', 'and', 'life', 'in', 'general', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', 'can', \"'\", 't', 'focus', 'at', 'work', ',', 'depression', 'med', '##s', '(', 'ssr', '##is', ')', 'not', 'working', ',', 'had', 'ad', '##hd', 'as', 'a', 'child', '.', 'would', 'rita', '##lin', 'help', 'me', 'focus', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'taking', 'away', 'distraction', '##s', 'from', 'some', 'one', 'that', 'has', 'add', 'distract', 'the', 'person', 'more', 'or', 'less', '?']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'taking', 'away', 'distraction', '##s', 'from', 'some', 'one', 'that', 'has', 'add', 'distract', 'the', 'person', 'more', 'or', 'less', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 'medication', 'next', 'week']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 'medication', 'next', 'week']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anybody', 'else', '?', 'my', 'roommate', 'and', 'i', 'frequently', 'use', 'add', '##eral', '##l', 'and', 'study', 'together', '.', 'he', 'can', 'take', 'a', '30', '##mg', 'instant', 'and', 'then', '30', 'minutes', 'later', 'eat', 'a', '3', 'course', 'meal', '.', 'i', 'on', 'the', 'other', 'hand', ',', 'take', '1', '/', '4', 'of', 'a', '30', '##mg', 'and', 'within', '60', 'seconds', 'my', 'heart', 'rate', 'is', 'up', ',', 'my', 'eyes', 'are', 'wide', ',', 'my', 'lungs', 'and', 'nasal', 'pass', '##ges', 'feel', 'like', 'they', 'have', 'never', 'been', 'more', 'open', 'in', 'my', 'life', ',', 'i', 'can', '##t', 'even', 'stand', 'the', 'thought', 'of', 'food', 'for', 'at', 'least', 'a', 'day', ',', 'and', 'sleep', 'is', 'not', 'necessary', 'for', '2', 'days', '.', 'i', 'have', 'been', 'this', 'sensitive', 'to', '7', '.', '5', '##mg', 'of', 'add', '##eral', '##l', 'for', '5', 'years', 'now', '.', 'from', 'what', 'i', 'have', 'read', 'this', 'isn', '##t', 'normal', 'and', 'i', 'should', 'have', 'a', 'tolerance', 'by', 'now', '.', 'i', 'was', 'diagnosed', 'extreme', '##ml', '##y', 'mild', 'ad', '##hd', 'when', 'i', 'was', 'in', 'elementary', 'school', 'about', '15', 'years', 'ago', 'but', 'i', 'never', 'took', 'any', 'med', '##s', 'until', 'college', '.']\n",
      "INFO:__main__:Number of tokens: 171\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anybody', 'else', '?', 'my', 'roommate', 'and', 'i', 'frequently', 'use', 'add', '##eral', '##l', 'and', 'study', 'together', '.', 'he', 'can', 'take', 'a', '30', '##mg', 'instant', 'and', 'then', '30', 'minutes', 'later', 'eat', 'a', '3', 'course', 'meal', '.', 'i', 'on', 'the', 'other', 'hand', ',', 'take', '1', '/', '4', 'of', 'a', '30', '##mg', 'and', 'within', '60', 'seconds', 'my', 'heart', 'rate', 'is', 'up', ',', 'my', 'eyes', 'are', 'wide', ',', 'my', 'lungs', 'and', 'nasal', 'pass', '##ges', 'feel', 'like', 'they', 'have', 'never', 'been', 'more', 'open', 'in', 'my', 'life', ',', 'i', 'can', '##t', 'even', 'stand', 'the', 'thought', 'of', 'food', 'for', 'at', 'least', 'a', 'day', ',', 'and', 'sleep', 'is', 'not', 'necessary', 'for', '2', 'days', '.', 'i', 'have', 'been', 'this', 'sensitive', 'to', '7', '.', '5', '##mg', 'of', 'add', '##eral', '##l', 'for', '5', 'years', 'now', '.', 'from', 'what', 'i', 'have', 'read', 'this', 'isn', '##t', 'normal', 'and', 'i', 'should', 'have', 'a', 'tolerance', 'by', 'now', '.', 'i', 'was', 'diagnosed', 'extreme', '##ml', '##y', 'mild', 'ad', '##hd', 'when', 'i', 'was', 'in', 'elementary', 'school', 'about', '15', 'years', 'ago', 'but', 'i', 'never', 'took', 'any', 'med', '##s', 'until', 'college', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['not', 'saying', 'i', 'completely', 'agree', 'but', 'i', 'want', 'some', 'of', 'your', 'opinions', 'of', 'this', 'article', 'on', 'the', 'dangers', 'of', 'v', '##y', '##van', '##se', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['not', 'saying', 'i', 'completely', 'agree', 'but', 'i', 'want', 'some', 'of', 'your', 'opinions', 'of', 'this', 'article', 'on', 'the', 'dangers', 'of', 'v', '##y', '##van', '##se', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'this', 'podcast', 'have', 'any', 'validity', '?', 'all', 'opinions', 'welcome', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'this', 'podcast', 'have', 'any', 'validity', '?', 'all', 'opinions', 'welcome', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'you', 'do', 'to', 'focus', '?', 'for', 'those', 'of', 'you', 'who', 'do', 'not', 'currently', 'take', 'medication', ',', 'what', 'do', 'you', 'do', 'to', 'stay', 'focused', 'on', 'say', ',', 'an', 'essay', '?', 'i', \"'\", 've', 'been', 'sitting', 'here', 'fore', '45', 'minutes', ',', 'saying', 'i', 'was', 'going', 'to', 'start', 'this', 'paper', ',', 'and', 'i', 'have', 'yet', 'to', 'do', 'so', '.']\n",
      "INFO:__main__:Number of tokens: 58\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'you', 'do', 'to', 'focus', '?', 'for', 'those', 'of', 'you', 'who', 'do', 'not', 'currently', 'take', 'medication', ',', 'what', 'do', 'you', 'do', 'to', 'stay', 'focused', 'on', 'say', ',', 'an', 'essay', '?', 'i', \"'\", 've', 'been', 'sitting', 'here', 'fore', '45', 'minutes', ',', 'saying', 'i', 'was', 'going', 'to', 'start', 'this', 'paper', ',', 'and', 'i', 'have', 'yet', 'to', 'do', 'so', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'went', 'from', 'add', '##eral', '##l', 'x', '##r', 'to', 'dex', '##tro', '##amp', '##het', '##amine', 'and', 'now', 'around', '8', '##hr', '##s', 'in', 'i', 'consistently', 'get', 'wicked', 'headache', ',', 'da', '##e', '?']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'went', 'from', 'add', '##eral', '##l', 'x', '##r', 'to', 'dex', '##tro', '##amp', '##het', '##amine', 'and', 'now', 'around', '8', '##hr', '##s', 'in', 'i', 'consistently', 'get', 'wicked', 'headache', ',', 'da', '##e', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'to', 'tell', 'relatives', 'that', 'you', 'were', 'diagnosed', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'to', 'tell', 'relatives', 'that', 'you', 'were', 'diagnosed', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'got', 'put', 'on', '18', '##mg', 'concert', '##a', '.', 'don', \"'\", 't', 'feel', 'any', 'effect', '.', 'i', 'am', 'a', '19', 'year', 'old', 'average', 'sized', 'male', 'who', 'just', 'got', 'put', 'on', '18', '##mg', 'today', '.', 'it', 'has', 'been', 'several', 'hours', 'and', 'i', 'have', 'felt', 'no', 'different', '.', 'would', 'it', 'be', 'ok', 'if', 'i', 'took', 'two', 'tomorrow', 'morning', '>']\n",
      "INFO:__main__:Number of tokens: 57\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'got', 'put', 'on', '18', '##mg', 'concert', '##a', '.', 'don', \"'\", 't', 'feel', 'any', 'effect', '.', 'i', 'am', 'a', '19', 'year', 'old', 'average', 'sized', 'male', 'who', 'just', 'got', 'put', 'on', '18', '##mg', 'today', '.', 'it', 'has', 'been', 'several', 'hours', 'and', 'i', 'have', 'felt', 'no', 'different', '.', 'would', 'it', 'be', 'ok', 'if', 'i', 'took', 'two', 'tomorrow', 'morning', '>']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['on', 'add', '##eral', '##l', ',', 'went', 'to', 'gym', 'today', 'and', 'lifted', 'harder', ',', 'ran', 'longer', ',', 'et', 'ce', '##tera', '.', 'gym', 'closed', 'before', 'i', 'became', 'tired', '.', 'has', 'anyone', 'else', 'had', 'a', 'similar', 'experience', '?', 'i', 'sort', 'of', 'just', 'kept', 'going', 'and', 'going', '.']\n",
      "INFO:__main__:Number of tokens: 44\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['on', 'add', '##eral', '##l', ',', 'went', 'to', 'gym', 'today', 'and', 'lifted', 'harder', ',', 'ran', 'longer', ',', 'et', 'ce', '##tera', '.', 'gym', 'closed', 'before', 'i', 'became', 'tired', '.', 'has', 'anyone', 'else', 'had', 'a', 'similar', 'experience', '?', 'i', 'sort', 'of', 'just', 'kept', 'going', 'and', 'going', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'only', 'problem', 'i', \"'\", 've', 'found', 'with', 'day', '##tra', '##na', '(', 'patches', ')', '.', '.', '.', '.', '.', '.', 'is', 'when', 'its', '2', 'am', ',', 'and', 'i', 'realize', 'that', 'i', 'forgot', 'to', 'take', 'the', 'damn', 'patch', 'off', '.', 'i', \"'\", 'll', 'be', 'up', 'for', 'another', 'couple', 'hours', 'if', 'anyone', 'wants', 'to', 'chat', '.', 'also', ',', 'i', 'can', \"'\", 't', 'believe', 'i', \"'\", 'm', 'only', 'just', 'joining', 'this', 'sub', '##red', '##dit', ',', 'i', 'guess', 'i', 'was', 'to', 'distracted', 'elsewhere', 'to', 'think', 'about', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 84\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'only', 'problem', 'i', \"'\", 've', 'found', 'with', 'day', '##tra', '##na', '(', 'patches', ')', '.', '.', '.', '.', '.', '.', 'is', 'when', 'its', '2', 'am', ',', 'and', 'i', 'realize', 'that', 'i', 'forgot', 'to', 'take', 'the', 'damn', 'patch', 'off', '.', 'i', \"'\", 'll', 'be', 'up', 'for', 'another', 'couple', 'hours', 'if', 'anyone', 'wants', 'to', 'chat', '.', 'also', ',', 'i', 'can', \"'\", 't', 'believe', 'i', \"'\", 'm', 'only', 'just', 'joining', 'this', 'sub', '##red', '##dit', ',', 'i', 'guess', 'i', 'was', 'to', 'distracted', 'elsewhere', 'to', 'think', 'about', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'was', 'prescribed', 'pro', '##vi', '##gil', 'which', 'worked', 'perfectly', 'for', 'my', 'ad', '##hd', 'when', 'i', 'was', '15', ',', 'has', 'anyone', 'else', 'had', 'any', 'experience', 'with', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'was', 'prescribed', 'pro', '##vi', '##gil', 'which', 'worked', 'perfectly', 'for', 'my', 'ad', '##hd', 'when', 'i', 'was', '15', ',', 'has', 'anyone', 'else', 'had', 'any', 'experience', 'with', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ia', '##ma', 'producer', 'of', 'a', 'documentary', 'on', 'ad', '/', 'hd', 'looking', 'for', 'connections', 'located', 'in', 'po', '##ugh', '##kee', '##ps', '##ie', ',', 'ny', '.', 'will', 'travel', 'to', 'neighboring', 'states', ',', 'and', 'people', 'within', 'reasonable', 'driving', 'distances', 'from', 'our', 'area', '.', 'if', 'you', 'are', 'interested', ',', 'please', 'e', '-', 'mail', 'learning', '##in', '##pro', '##gre', '##ss', '@', 'hot', '##mail', '.', 'com', 'more', 'info', 'will', 'be', 'provided']\n",
      "INFO:__main__:Number of tokens: 63\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ia', '##ma', 'producer', 'of', 'a', 'documentary', 'on', 'ad', '/', 'hd', 'looking', 'for', 'connections', 'located', 'in', 'po', '##ugh', '##kee', '##ps', '##ie', ',', 'ny', '.', 'will', 'travel', 'to', 'neighboring', 'states', ',', 'and', 'people', 'within', 'reasonable', 'driving', 'distances', 'from', 'our', 'area', '.', 'if', 'you', 'are', 'interested', ',', 'please', 'e', '-', 'mail', 'learning', '##in', '##pro', '##gre', '##ss', '@', 'hot', '##mail', '.', 'com', 'more', 'info', 'will', 'be', 'provided']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'ad', '##hd', 'community', 'in', 'the', 'making', '!']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'ad', '##hd', 'community', 'in', 'the', 'making', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['when', 'off', 'add', '##eral', '##l', 'i', 'am', 'extremely', 'distant', 'and', 'hazy', '.', 'is', 'this', 'an', 'effect', 'or', 'how', 'things', 'were', 'before', 'add', '##eral', '##l', 'without', 'realizing', 'it', '?', 'has', 'anyone', 'else', 'noticed', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 34\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['when', 'off', 'add', '##eral', '##l', 'i', 'am', 'extremely', 'distant', 'and', 'hazy', '.', 'is', 'this', 'an', 'effect', 'or', 'how', 'things', 'were', 'before', 'add', '##eral', '##l', 'without', 'realizing', 'it', '?', 'has', 'anyone', 'else', 'noticed', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'characteristics', 'in', 'canadian', 'aboriginal', 'children', '(', '2006', ')']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'characteristics', 'in', 'canadian', 'aboriginal', 'children', '(', '2006', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['over', 'time', 'my', 'med', '##s', 'took', 'away', 'all', 'of', 'my', 'energy', '.', '.', '.', 'any', 'tips', '?', 'my', 'concert', '##a', 'at', 'first', 'gave', 'me', 'energy', ',', 'then', 'took', 'it', 'away', '.', 'i', 'quit', ',', 'and', 'i', \"'\", 'm', 'looking', 'for', 'advice', '.', 'pre', '-', 'diagnosis', ':', 'i', 'wake', 'up', 'at', 'noon', '.', 'i', 'would', 'usually', 'need', 'caf', '##fe', '##ine', 'for', 'energy', 'before', 'the', 'gym', '(', '1', '/', '3', 'of', 'a', 'red', '##bu', '##ll', ')', '.', 'i', \"'\", 'd', 'then', 'go', 'to', 'class', 'for', '4', 'hours', ',', 'then', 'have', 'energy', 'to', 'do', 'home', '##ow', '##or', '##k', 'until', '1a', '##m', '.', 'i', 'worked', 'at', 'a', 'super', 'slow', 'pace', 'since', 'i', 'was', 'always', 'distracted', ',', 'but', 'i', 're', '##lia', '##bly', 'had', 'energy', 'go', 'do', 'it', '.', 'first', 'month', 'with', 'concert', '##a', ':', 'amazing', '!', 'i', 'didn', \"'\", 't', 'need', 'caf', '##fe', '##ine', 'at', 'the', 'gym', ',', 'and', 'the', 'effects', 'on', 'my', 'focus', 'made', 'my', 'homework', '/', 'studying', 'easier', '.', 'only', 'issue', 'is', 'that', 'it', 'would', 'keep', 'my', 'up', 'until', '3a', '##m', '.', '.', '.', 'i', 'still', 'aimed', 'for', '8', '.', '5', 'hours', 'of', 'sleep', 'though', '.', 'second', 'month', ':', 'i', 'noticed', 'i', 'started', 'losing', 'focus', 'around', '9', ',', 'which', 'throughout', 'the', 'month', 'rec', '##eded', 'to', '6', '##pm', '.', 'i', 'also', 'become', 'tired', 'around', 'then', 'and', 'couldn', \"'\", 't', 'get', 'anything', 'done', '.', 'my', 'doctor', 'gave', 'me', 'rita', '##lin', 'as', 'a', 'booster', 'to', 'take', 'around', '5', '.', 'now', ',', 'i', 'could', 'continue', 'working', 'to', '1', ',', 'but', 'couldn', \"'\", 't', 'sleep', 'till', '3', 'so', 'i', 'started', 'taking', 'sleeping', 'pills', '(', 'ben', '##ned', '##ryl', 'basically', ')', '.', 'this', 'pattern', 'continued', ',', 'and', 'eventually', 'i', 'had', 'to', 'quit', 'going', 'to', 'the', 'gym', 'since', 'i', 'never', 'had', 'energy', '.', 'instead', 'of', 'feeling', 'good', 'when', 'pushing', 'myself', ',', 'each', 'push', 'on', 'weights', 'made', 'me', 'more', 'tired', '.', 'i', 'was', 'dependent', 'on', 'taking', 'the', 'medicine', ',', 'without', 'it', 'i', 'felt', 'like', 'i', 'couldn', \"'\", 't', 'focus', 'and', 'had', 'something', 'missing', '.', 'no', 'craving', '##s', 'or', 'withdrawal', 'symptoms', ',', 'but', 'weird', '.', 'third', 'month', ':', 'she', 'gave', 'my', 'add', '##eral', '##l', 'x', '##r', 'to', 'try', 'instead', 'since', 'concert', '##a', 'stopped', 'working', 'and', 'back', '##fire', '##d', '.', 'instead', ',', 'i', 'just', 'stopped', 'taking', 'everything', '.', '5', 'days', 'in', 'now', '.', 'i', \"'\", 'm', 'clear', '##head', '##ed', ',', 'sleeping', 'normal', ',', 'but', 'back', 'to', 'working', 'slowly', 'and', 'distracted', '.', 'i', \"'\", 'm', 'afraid', 'of', 'falling', 'in', 'love', 'with', 'another', 'medicine', 'that', 'will', 'make', 'me', 'super', 'tired', '.', 'its', 'nice', 'to', 'not', 'feel', 'like', 'i', 'need', 'medication', 'anymore', ',', 'i', 'feel', 'clear', '##head', '##ed', 'and', 'normal', ',', 'which', 'is', 'very', 'surprising', '.', 'concert', '##a', 'felt', 'like', 'a', 'mandatory', 'magic', 'fix', 'for', 'a', 'while', '.', 'do', 'you', 'have', 'any', 'advice', '?', 'i', 'can', \"'\", 't', 'figure', 'out', 'why', 'my', 'med', '##s', 'back', '##fire', '##d', 'and', 'made', 'me', 'so', 'tired', '.', 'should', 'i', 'start', 'taking', 'the', 'add', '##eral', '##l', 'x', '##r', '?', 'i', 'wish', 'i', 'had', 'that', 'focus', 'but', 'can', \"'\", 't', 'afford', 'to', 'lose', 'energy', 'like', 'that', 'again', '.', 'edit', ':', 'to', 'clarify', ',', 'the', 'concert', '##a', 'was', '18', '##mg', 'and', 'add', '##eral', '##l', 'x', '##r', 'is', '10', '##mg', '.']\n",
      "INFO:__main__:Number of tokens: 516\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['over', 'time', 'my', 'med', '##s', 'took', 'away', 'all', 'of', 'my', 'energy', '.', '.', '.', 'any', 'tips', '?', 'my', 'concert', '##a', 'at', 'first', 'gave', 'me', 'energy', ',', 'then', 'took', 'it', 'away', '.', 'i', 'quit', ',', 'and', 'i', \"'\", 'm', 'looking', 'for', 'advice', '.', 'pre', '-', 'diagnosis', ':', 'i', 'wake', 'up', 'at', 'noon', '.', 'i', 'would', 'usually', 'need', 'caf', '##fe', '##ine', 'for', 'energy', 'before', 'the', 'gym', '(', '1', '/', '3', 'of', 'a', 'red', '##bu', '##ll', ')', '.', 'i', \"'\", 'd', 'then', 'go', 'to', 'class', 'for', '4', 'hours', ',', 'then', 'have', 'energy', 'to', 'do', 'home', '##ow', '##or', '##k', 'until', '1a', '##m', '.', 'i', 'worked', 'at', 'a', 'super', 'slow', 'pace', 'since', 'i', 'was', 'always', 'distracted', ',', 'but', 'i', 're', '##lia', '##bly', 'had', 'energy', 'go', 'do', 'it', '.', 'first', 'month', 'with', 'concert', '##a', ':', 'amazing', '!', 'i', 'didn', \"'\", 't', 'need', 'caf', '##fe', '##ine', 'at', 'the', 'gym', ',', 'and', 'the', 'effects', 'on', 'my', 'focus', 'made', 'my', 'homework', '/', 'studying', 'easier', '.', 'only', 'issue', 'is', 'that', 'it', 'would', 'keep', 'my', 'up', 'until', '3a', '##m', '.', '.', '.', 'i', 'still', 'aimed', 'for', '8', '.', '5', 'hours', 'of', 'sleep', 'though', '.', 'second', 'month', ':', 'i', 'noticed', 'i', 'started', 'losing', 'focus', 'around', '9', ',', 'which', 'throughout', 'the', 'month', 'rec', '##eded', 'to', '6', '##pm', '.', 'i', 'also', 'become', 'tired', 'around', 'then', 'and', 'couldn', \"'\", 't', 'get', 'anything', 'done', '.', 'my', 'doctor', 'gave', 'me', 'rita', '##lin', 'as', 'a', 'booster', 'to', 'take', 'around', '5', '.', 'now', ',', 'i', 'could', 'continue', 'working', 'to', '1', ',', 'but', 'couldn', \"'\", 't', 'sleep', 'till', '3', 'so', 'i', 'started', 'taking', 'sleeping', 'pills', '(', 'ben', '##ned', '##ryl', 'basically', ')', '.', 'this', 'pattern', 'continued', ',', 'and', 'eventually', 'i', 'had', 'to', 'quit', 'going', 'to', 'the', 'gym', 'since', 'i', 'never', 'had', 'energy', '.', 'instead', 'of', 'feeling', 'good', 'when', 'pushing', 'myself', ',', 'each', 'push', 'on', 'weights', 'made', 'me', 'more', 'tired', '.', 'i', 'was', 'dependent', 'on', 'taking', 'the', 'medicine', ',', 'without', 'it', 'i', 'felt', 'like', 'i', 'couldn', \"'\", 't', 'focus', 'and', 'had', 'something', 'missing', '.', 'no', 'craving', '##s', 'or', 'withdrawal', 'symptoms', ',', 'but', 'weird', '.', 'third', 'month', ':', 'she', 'gave', 'my', 'add', '##eral', '##l', 'x', '##r', 'to', 'try', 'instead', 'since', 'concert', '##a', 'stopped', 'working', 'and', 'back', '##fire', '##d', '.', 'instead', ',', 'i', 'just', 'stopped', 'taking', 'everything', '.', '5', 'days', 'in', 'now', '.', 'i', \"'\", 'm', 'clear', '##head', '##ed', ',', 'sleeping', 'normal', ',', 'but', 'back', 'to', 'working', 'slowly', 'and', 'distracted', '.', 'i', \"'\", 'm', 'afraid', 'of', 'falling', 'in', 'love', 'with', 'another', 'medicine', 'that', 'will', 'make', 'me', 'super', 'tired', '.', 'its', 'nice', 'to', 'not', 'feel', 'like', 'i', 'need', 'medication', 'anymore', ',', 'i', 'feel', 'clear', '##head', '##ed', 'and', 'normal', ',', 'which', 'is', 'very', 'surprising', '.', 'concert', '##a', 'felt', 'like', 'a', 'mandatory', 'magic', 'fix', 'for', 'a', 'while', '.', 'do', 'you', 'have', 'any', 'advice', '?', 'i', 'can', \"'\", 't', 'figure', 'out', 'why', 'my', 'med', '##s', 'back', '##fire', '##d', 'and', 'made', 'me', 'so', 'tired', '.', 'should', 'i', 'start', 'taking', 'the', 'add', '##eral', '##l', 'x', '##r', '?', 'i', 'wish', 'i', 'had', 'that', 'focus', 'but', 'can', \"'\", 't', 'afford', 'to', 'lose', 'energy', 'like', 'that', 'again', '.', 'edit', ':', 'to', 'clarify', ',', 'the', 'concert', '##a', 'was', '18', '##mg', 'and', 'add', '##eral', '##l', 'x', '##r'], ['is', '10', '##mg', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', 'ever', 'feel', 'resentment', 'about', 'your', 'ad', '##hd', '?', 'all', 'growing', 'up', 'i', 'was', 'told', 'by', 'my', 'mom', 'that', 'ad', '##hd', 'is', 'just', 'an', 'excuse', 'for', 'la', '##zine', '##ss', 'and', 'i', 'always', 'felt', 'like', 'i', 'had', 'it', ',', 'but', 'felt', 'guilty', 'about', 'it', 'and', 'didn', \"'\", 't', 'get', 'tested', 'until', '22', 'because', 'she', 'told', 'me', 'it', 'wasn', \"'\", 't', 'real', '.', 'i', 'finally', 'had', 'enough', 'and', 'was', 'tested', 'during', 'my', 'last', 'year', 'of', 'college', 'and', 'was', 'diagnosed', 'with', 'ina', '##tten', '##tive', 'ad', '##hd', '.', 'i', 'know', 'my', 'mom', 'only', 'wants', 'what', 'is', 'best', 'for', 'me', ',', 'but', 'i', 'still', 'feel', 'a', 'slight', 'resentment', 'about', 'her', 'not', 'supporting', 'me', 'when', 'i', 'told', 'her', 'i', 'wanted', 'to', 'get', 'tested', 'in', 'high', 'school', '.', 'i', 'feel', 'like', 'i', 'got', 'by', 'through', 'high', 'school', 'and', 'college', 'on', 'my', 'intelligence', ',', 'but', 'feel', 'i', 'could', 'have', 'done', 'so', 'much', 'better', 'had', 'i', 'gotten', 'tested', 'earlier', '.', 'i', 'dunn', '##o', ',', 'maybe', 'i', \"'\", 'm', 'in', 'the', 'wrong', 'for', 'feeling', 'a', 'little', 'resentment', ',', 'but', 'i', 'can', \"'\", 't', 'help', 'it', '.', ':', '/', 'anyone', 'else', 'feel', 'a', 'kind', 'of', 'resentment', 'about', 'not', 'being', 'able', 'to', 'be', 'tested', 'earlier', '?', '?']\n",
      "INFO:__main__:Number of tokens: 196\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', 'ever', 'feel', 'resentment', 'about', 'your', 'ad', '##hd', '?', 'all', 'growing', 'up', 'i', 'was', 'told', 'by', 'my', 'mom', 'that', 'ad', '##hd', 'is', 'just', 'an', 'excuse', 'for', 'la', '##zine', '##ss', 'and', 'i', 'always', 'felt', 'like', 'i', 'had', 'it', ',', 'but', 'felt', 'guilty', 'about', 'it', 'and', 'didn', \"'\", 't', 'get', 'tested', 'until', '22', 'because', 'she', 'told', 'me', 'it', 'wasn', \"'\", 't', 'real', '.', 'i', 'finally', 'had', 'enough', 'and', 'was', 'tested', 'during', 'my', 'last', 'year', 'of', 'college', 'and', 'was', 'diagnosed', 'with', 'ina', '##tten', '##tive', 'ad', '##hd', '.', 'i', 'know', 'my', 'mom', 'only', 'wants', 'what', 'is', 'best', 'for', 'me', ',', 'but', 'i', 'still', 'feel', 'a', 'slight', 'resentment', 'about', 'her', 'not', 'supporting', 'me', 'when', 'i', 'told', 'her', 'i', 'wanted', 'to', 'get', 'tested', 'in', 'high', 'school', '.', 'i', 'feel', 'like', 'i', 'got', 'by', 'through', 'high', 'school', 'and', 'college', 'on', 'my', 'intelligence', ',', 'but', 'feel', 'i', 'could', 'have', 'done', 'so', 'much', 'better', 'had', 'i', 'gotten', 'tested', 'earlier', '.', 'i', 'dunn', '##o', ',', 'maybe', 'i', \"'\", 'm', 'in', 'the', 'wrong', 'for', 'feeling', 'a', 'little', 'resentment', ',', 'but', 'i', 'can', \"'\", 't', 'help', 'it', '.', ':', '/', 'anyone', 'else', 'feel', 'a', 'kind', 'of', 'resentment', 'about', 'not', 'being', 'able', 'to', 'be', 'tested', 'earlier', '?', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['putting', 'together', 'a', 'presentation', 'on', 'reducing', 'the', 'stigma', 'about', 'ad', '##hd', 'i', \"'\", 'm', 'putting', 'together', 'the', 'power', 'point', 'part', 'of', 'it', 'and', 'i', \"'\", 'm', 'unsure', 'how', 'to', 'effectively', 'portray', 'key', 'facts', 'and', 'reasoning', '##s', '.', 'i', 'feel', 'like', 'pictures', 'are', 'too', 'stereo', '##typical', ',', 'and', 'words', 'aren', \"'\", 't', 'enough', '.', 'anybody', 'have', 'any', 'ideas', 'on', 'tack', '##ling', 'this', '?', 'maybe', 'other', 'presentation', 'ideas', '?', '(', 'i', 'don', \"'\", 't', 'have', 'to', 'stick', 'with', 'a', 'power', 'point', ')']\n",
      "INFO:__main__:Number of tokens: 80\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['putting', 'together', 'a', 'presentation', 'on', 'reducing', 'the', 'stigma', 'about', 'ad', '##hd', 'i', \"'\", 'm', 'putting', 'together', 'the', 'power', 'point', 'part', 'of', 'it', 'and', 'i', \"'\", 'm', 'unsure', 'how', 'to', 'effectively', 'portray', 'key', 'facts', 'and', 'reasoning', '##s', '.', 'i', 'feel', 'like', 'pictures', 'are', 'too', 'stereo', '##typical', ',', 'and', 'words', 'aren', \"'\", 't', 'enough', '.', 'anybody', 'have', 'any', 'ideas', 'on', 'tack', '##ling', 'this', '?', 'maybe', 'other', 'presentation', 'ideas', '?', '(', 'i', 'don', \"'\", 't', 'have', 'to', 'stick', 'with', 'a', 'power', 'point', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'the', 'worst', 'time', 'you', \"'\", 've', 'ever', 'had', 'explaining', 'your', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'the', 'worst', 'time', 'you', \"'\", 've', 'ever', 'had', 'explaining', 'your', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['at', '##t', '##n', 'ad', '##hd', 'pc', 'gamer', '##s', ':', 'just', 'created', '/', 'r', '/', 'ad', '##hd', 'steam', 'group', '/', 'r', '/', 'ad', '##hd', 'hyper', '##fo', '##cus', '##ed', 'gamer', '##s', 'i', 'just', 'created', 'a', 'steam', 'gaming', 'group', 'with', 'the', 'hope', 'to', 'start', 'a', 'little', 'community', ',', 'chat', 'about', 'ad', '##hd', ',', 'and', 'last', 'but', 'not', 'least', '.', '.', '.', 'play', 'some', 'amazing', 'video', 'games', '!', 'video', 'games', 'have', 'always', 'been', 'one', 'of', 'the', 'only', 'things', 'i', 'could', 'actually', 'pay', 'attention', 'towards', '.', 'i', 'often', 'hyper', '##fo', '##cus', 'and', 'forget', 'to', 'move', 'for', '6', 'hours', 'while', 'immersed', 'in', 'a', 'game', '.', 'i', 'am', 'sure', 'there', 'are', 'a', 'lot', 'of', 'ad', '##hd', 'gamer', '##s', 'out', 'there', ',', 'so', 'i', 'figured', 'might', 'as', 'well', 'attempt', 'to', 'expand', 'the', 'community', 'over', 'there', '.', 'i', 'play', 'a', 'lot', 'of', 't', '##f', '##2', ',', 'but', 'own', 'over', '100', 'games', '(', 'and', 'played', 'maybe', '13', 'of', 'them', ')', 'due', 'to', 'imp', '##ulsive', 'steam', 'sales', 'purchases', '.', 'my', 'steam', 'name', 'is', '*', '*', 'ad', '##hd', 'santa', '*', '*', '(', 'please', 'add', 'me', '!', ')', 'group', 'name', ':', '*', '*', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'hyper', '##fo', '##cus', '##ed', 'gamer', '##s', '*', '*', 'ur', '##l', ':', '*', '*', '[', 'http', ':', '/', '/', 'steam', '##com', '##mun', '##ity', '.', 'com', '/', 'groups', '/', 'ad', '##hd', '##red', '##dit', ']', '(', 'http', ':', '/', '/', 'steam', '##com', '##mun', '##ity', '.', 'com', '/', 'groups', '/', 'ad', '##hd', '##red', '##dit', ')', '*', '*', 'the', 'group', 'is', 'private', 'so', 'i', 'will', 'have', 'to', 'invite', 'you', '.', 'either', 'message', 'me', 'on', 'steam', ',', 'pm', 'on', 'red', '##dit', ',', 'or', 'leave', 'your', 'steam', 'name', 'in', 'comments', 'if', 'interested', '.', 'i', 'figured', 'it', 'would', 'be', 'a', 'fun', 'place', 'to', 'play', 'games', 'together', ',', 'chat', 'about', 'our', 'ad', '##hd', ',', 'and', 'maybe', 'keep', 'each', 'other', 'accountable', 'so', 'we', 'don', \"'\", 't', 'lose', '14', 'hours', 'by', 'playing', 'sky', '##rim', 'on', 'a', 'weekend', '.', 'can', \"'\", 't', 'wait', 'to', 'meet', '/', 'play', 'with', 'some', 'of', 'you', '!', '!', '!']\n",
      "INFO:__main__:Number of tokens: 336\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['at', '##t', '##n', 'ad', '##hd', 'pc', 'gamer', '##s', ':', 'just', 'created', '/', 'r', '/', 'ad', '##hd', 'steam', 'group', '/', 'r', '/', 'ad', '##hd', 'hyper', '##fo', '##cus', '##ed', 'gamer', '##s', 'i', 'just', 'created', 'a', 'steam', 'gaming', 'group', 'with', 'the', 'hope', 'to', 'start', 'a', 'little', 'community', ',', 'chat', 'about', 'ad', '##hd', ',', 'and', 'last', 'but', 'not', 'least', '.', '.', '.', 'play', 'some', 'amazing', 'video', 'games', '!', 'video', 'games', 'have', 'always', 'been', 'one', 'of', 'the', 'only', 'things', 'i', 'could', 'actually', 'pay', 'attention', 'towards', '.', 'i', 'often', 'hyper', '##fo', '##cus', 'and', 'forget', 'to', 'move', 'for', '6', 'hours', 'while', 'immersed', 'in', 'a', 'game', '.', 'i', 'am', 'sure', 'there', 'are', 'a', 'lot', 'of', 'ad', '##hd', 'gamer', '##s', 'out', 'there', ',', 'so', 'i', 'figured', 'might', 'as', 'well', 'attempt', 'to', 'expand', 'the', 'community', 'over', 'there', '.', 'i', 'play', 'a', 'lot', 'of', 't', '##f', '##2', ',', 'but', 'own', 'over', '100', 'games', '(', 'and', 'played', 'maybe', '13', 'of', 'them', ')', 'due', 'to', 'imp', '##ulsive', 'steam', 'sales', 'purchases', '.', 'my', 'steam', 'name', 'is', '*', '*', 'ad', '##hd', 'santa', '*', '*', '(', 'please', 'add', 'me', '!', ')', 'group', 'name', ':', '*', '*', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'hyper', '##fo', '##cus', '##ed', 'gamer', '##s', '*', '*', 'ur', '##l', ':', '*', '*', '[', 'http', ':', '/', '/', 'steam', '##com', '##mun', '##ity', '.', 'com', '/', 'groups', '/', 'ad', '##hd', '##red', '##dit', ']', '(', 'http', ':', '/', '/', 'steam', '##com', '##mun', '##ity', '.', 'com', '/', 'groups', '/', 'ad', '##hd', '##red', '##dit', ')', '*', '*', 'the', 'group', 'is', 'private', 'so', 'i', 'will', 'have', 'to', 'invite', 'you', '.', 'either', 'message', 'me', 'on', 'steam', ',', 'pm', 'on', 'red', '##dit', ',', 'or', 'leave', 'your', 'steam', 'name', 'in', 'comments', 'if', 'interested', '.', 'i', 'figured', 'it', 'would', 'be', 'a', 'fun', 'place', 'to', 'play', 'games', 'together', ',', 'chat', 'about', 'our', 'ad', '##hd', ',', 'and', 'maybe', 'keep', 'each', 'other', 'accountable', 'so', 'we', 'don', \"'\", 't', 'lose', '14', 'hours', 'by', 'playing', 'sky', '##rim', 'on', 'a', 'weekend', '.', 'can', \"'\", 't', 'wait', 'to', 'meet', '/', 'play', 'with', 'some', 'of', 'you', '!', '!', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['undergoing', 'a', 'ps', '##ych', 'eva', '##l', 'for', 'ad', '##hd', 'soon', ',', 'what', 'should', 'i', 'expect', '?']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['undergoing', 'a', 'ps', '##ych', 'eva', '##l', 'for', 'ad', '##hd', 'soon', ',', 'what', 'should', 'i', 'expect', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['seeing', 'some', 'ad', '##hd', '-', 'like', 'symptoms', 'hi', '.', 'i', \"'\", 'm', 'in', 'high', 'school', '.', 'i', 'feel', 'like', 'i', 'might', 'have', 'ad', '##hd', ',', 'and', 'this', 'seems', 'like', 'a', 'good', 'place', 'to', 'ask', 'about', 'it', '.', 'my', 'grades', 'are', '\"', 'fair', '\"', '.', 'i', 'go', 'to', 'a', 'good', 'school', ',', 'and', 'i', 'rank', 'usually', 'around', 'the', 'middle', 'of', 'the', 'class', 'gp', '##a', '-', 'wise', '.', 'right', 'now', 'i', \"'\", 'm', 'only', 'in', 'basic', 'classes', ',', 'but', 'i', 'get', 'high', 'grades', 'in', 'most', 'of', 'them', ',', 'which', 'keeps', 'my', 'overall', 'gp', '##a', 'up', '.', 'however', ',', 'it', 'isn', \"'\", 't', 'my', 'grades', ',', 'it', \"'\", 's', 'my', 'attention', 'span', 'and', 'my', 'memory', '.', 'every', 'day', 'i', 'struggle', 'with', 'homework', ':', 'it', 'can', 'take', 'me', '5', 'hours', 'to', 'do', '2', 'hours', 'of', 'work', 'because', 'i', 'get', 'distracted', 'so', 'easily', '.', 'my', 'memory', 'is', 'also', 'very', 'bad', ';', 'i', 'have', 'to', 'write', 'down', 'everything', 'in', 'my', 'planner', ',', 'and', 'if', 'i', 'miss', 'something', ',', 'i', 'forget', 'about', 'it', 'completely', '.', 'i', 'also', 'frequently', 'lose', 'things', '.', 'i', 'leave', 'pencil', '##s', 'everywhere', 'and', 'i', \"'\", 'm', 'always', 'mis', '##pl', '##acing', 'things', '.', 'in', 'school', 'in', 'most', 'classes', 'i', 'drift', 'in', 'and', 'out', 'of', 'attention', '.', 'the', 'only', 'classes', 'where', 'i', 'stay', 'at', '##ten', '##tive', 'are', 'the', 'ones', 'that', 'demand', 'constant', 'attention', ':', 'math', ',', 'physics', ',', 'etc', '.', 'i', 'do', 'well', 'in', 'those', ',', 'but', 'struggle', 'through', 'language', 'classes', 'and', 'classes', 'that', 'involve', 'memo', '##rization', '.', 'edit', ':', 'oh', ',', 'there', 'are', 'some', 'things', 'that', 'i', 'can', 'get', 'really', 'focused', 'on', '.', 'for', 'example', ',', 'if', 'i', 'work', 'at', 'my', 'computer', 'programming', 'for', 'maybe', '15', 'minutes', ',', 'i', \"'\", 'll', 'get', 'incredibly', 'involved', 'in', 'it', '.', 'i', \"'\", 'm', 'like', 'that', 'with', 'a', 'few', 'things', '.', 'should', 'i', 'seek', 'medical', 'attention', 'for', 'ad', '##hd', ',', 'or', 'is', 'this', 'possibly', 'something', 'else', '?', 'or', 'is', 'this', 'normal', '?']\n",
      "INFO:__main__:Number of tokens: 314\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['seeing', 'some', 'ad', '##hd', '-', 'like', 'symptoms', 'hi', '.', 'i', \"'\", 'm', 'in', 'high', 'school', '.', 'i', 'feel', 'like', 'i', 'might', 'have', 'ad', '##hd', ',', 'and', 'this', 'seems', 'like', 'a', 'good', 'place', 'to', 'ask', 'about', 'it', '.', 'my', 'grades', 'are', '\"', 'fair', '\"', '.', 'i', 'go', 'to', 'a', 'good', 'school', ',', 'and', 'i', 'rank', 'usually', 'around', 'the', 'middle', 'of', 'the', 'class', 'gp', '##a', '-', 'wise', '.', 'right', 'now', 'i', \"'\", 'm', 'only', 'in', 'basic', 'classes', ',', 'but', 'i', 'get', 'high', 'grades', 'in', 'most', 'of', 'them', ',', 'which', 'keeps', 'my', 'overall', 'gp', '##a', 'up', '.', 'however', ',', 'it', 'isn', \"'\", 't', 'my', 'grades', ',', 'it', \"'\", 's', 'my', 'attention', 'span', 'and', 'my', 'memory', '.', 'every', 'day', 'i', 'struggle', 'with', 'homework', ':', 'it', 'can', 'take', 'me', '5', 'hours', 'to', 'do', '2', 'hours', 'of', 'work', 'because', 'i', 'get', 'distracted', 'so', 'easily', '.', 'my', 'memory', 'is', 'also', 'very', 'bad', ';', 'i', 'have', 'to', 'write', 'down', 'everything', 'in', 'my', 'planner', ',', 'and', 'if', 'i', 'miss', 'something', ',', 'i', 'forget', 'about', 'it', 'completely', '.', 'i', 'also', 'frequently', 'lose', 'things', '.', 'i', 'leave', 'pencil', '##s', 'everywhere', 'and', 'i', \"'\", 'm', 'always', 'mis', '##pl', '##acing', 'things', '.', 'in', 'school', 'in', 'most', 'classes', 'i', 'drift', 'in', 'and', 'out', 'of', 'attention', '.', 'the', 'only', 'classes', 'where', 'i', 'stay', 'at', '##ten', '##tive', 'are', 'the', 'ones', 'that', 'demand', 'constant', 'attention', ':', 'math', ',', 'physics', ',', 'etc', '.', 'i', 'do', 'well', 'in', 'those', ',', 'but', 'struggle', 'through', 'language', 'classes', 'and', 'classes', 'that', 'involve', 'memo', '##rization', '.', 'edit', ':', 'oh', ',', 'there', 'are', 'some', 'things', 'that', 'i', 'can', 'get', 'really', 'focused', 'on', '.', 'for', 'example', ',', 'if', 'i', 'work', 'at', 'my', 'computer', 'programming', 'for', 'maybe', '15', 'minutes', ',', 'i', \"'\", 'll', 'get', 'incredibly', 'involved', 'in', 'it', '.', 'i', \"'\", 'm', 'like', 'that', 'with', 'a', 'few', 'things', '.', 'should', 'i', 'seek', 'medical', 'attention', 'for', 'ad', '##hd', ',', 'or', 'is', 'this', 'possibly', 'something', 'else', '?', 'or', 'is', 'this', 'normal', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hit', 'rock', 'bottom', 'today', 'don', \"'\", 't', 'you', 'love', 'it', 'when', 'you', 'try', 'your', 'hardest', 'at', 'something', 'and', 'do', 'poorly', '?', 'on', 'the', 'other', '-', 'hand', 'when', 'you', 'care', 'less', 'you', 'perform', 'better', '?', 'i', 'studied', 'about', '24', '+', 'hours', 'for', 'a', 'mid', '##ter', '##m', ',', 'and', 'got', 'a', '20', 'fucking', '7', '(', 'excuse', 'my', 'lang', '##au', '##ge', ')', '.', 'the', 'mid', '##ter', '##m', 'was', 'in', 'a', 'cs', 'class', '.', 'i', 'blank', '##ed', '.', 'i', 'couldn', \"'\", 't', 'focus', ',', 'and', 'i', 'couldn', \"'\", 't', 'answer', 'half', 'the', 'damn', 'questions', '.', 'every', 'single', 'major', 'test', 'this', 'is', 'a', 'common', 'occur', '##ence', '.', 'this', 'is', 'even', 'with', 'extended', 'test', 'times', '.', 'anyway', '##s', ',', 'just', 'a', 'ran', '##t', '.', 'had', 'terrible', 'thoughts', 'for', 'a', 'while', ',', 'but', 'i', 're', '-', 'cooperate', '##d', 'and', 'am', 'doing', 'fine', '.', 'scheduled', 'a', 'meeting', 'with', 'my', 'professor', ',', 'and', 'am', 'going', 'to', 'look', 'at', 'my', 'options', '.', 'luckily', ',', 'my', 'final', 'can', 'replace', 'my', 'mid', '##ter', '##m', '.', 'it', 'isn', \"'\", 't', 'like', 'i', 'don', \"'\", 't', 'know', 'the', 'stuff', '.', 'i', 'have', 'gotten', 'an', 'a', 'on', 'every', 'self', '-', 'paced', 'project', 'i', 'have', 'been', 'assigned', '.', 'does', 'anyone', 'have', 'any', 'advice', 'on', 'what', 'i', 'can', 'do', '?', 'i', 'have', 'ad', '##hd', ',', 'take', '200', '##mg', 'of', 'lam', '##ic', '##tal', 'x', '##r', ',', 'and', '20', 'mg', 'of', 'add', '##eral', '##l', 'x', '##r', 'in', 'case', 'you', 'were', 'wondering', '.', 'any', 'help', 'would', 'be', 'appreciated', '.', 'thanks', '.']\n",
      "INFO:__main__:Number of tokens: 241\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hit', 'rock', 'bottom', 'today', 'don', \"'\", 't', 'you', 'love', 'it', 'when', 'you', 'try', 'your', 'hardest', 'at', 'something', 'and', 'do', 'poorly', '?', 'on', 'the', 'other', '-', 'hand', 'when', 'you', 'care', 'less', 'you', 'perform', 'better', '?', 'i', 'studied', 'about', '24', '+', 'hours', 'for', 'a', 'mid', '##ter', '##m', ',', 'and', 'got', 'a', '20', 'fucking', '7', '(', 'excuse', 'my', 'lang', '##au', '##ge', ')', '.', 'the', 'mid', '##ter', '##m', 'was', 'in', 'a', 'cs', 'class', '.', 'i', 'blank', '##ed', '.', 'i', 'couldn', \"'\", 't', 'focus', ',', 'and', 'i', 'couldn', \"'\", 't', 'answer', 'half', 'the', 'damn', 'questions', '.', 'every', 'single', 'major', 'test', 'this', 'is', 'a', 'common', 'occur', '##ence', '.', 'this', 'is', 'even', 'with', 'extended', 'test', 'times', '.', 'anyway', '##s', ',', 'just', 'a', 'ran', '##t', '.', 'had', 'terrible', 'thoughts', 'for', 'a', 'while', ',', 'but', 'i', 're', '-', 'cooperate', '##d', 'and', 'am', 'doing', 'fine', '.', 'scheduled', 'a', 'meeting', 'with', 'my', 'professor', ',', 'and', 'am', 'going', 'to', 'look', 'at', 'my', 'options', '.', 'luckily', ',', 'my', 'final', 'can', 'replace', 'my', 'mid', '##ter', '##m', '.', 'it', 'isn', \"'\", 't', 'like', 'i', 'don', \"'\", 't', 'know', 'the', 'stuff', '.', 'i', 'have', 'gotten', 'an', 'a', 'on', 'every', 'self', '-', 'paced', 'project', 'i', 'have', 'been', 'assigned', '.', 'does', 'anyone', 'have', 'any', 'advice', 'on', 'what', 'i', 'can', 'do', '?', 'i', 'have', 'ad', '##hd', ',', 'take', '200', '##mg', 'of', 'lam', '##ic', '##tal', 'x', '##r', ',', 'and', '20', 'mg', 'of', 'add', '##eral', '##l', 'x', '##r', 'in', 'case', 'you', 'were', 'wondering', '.', 'any', 'help', 'would', 'be', 'appreciated', '.', 'thanks', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', '8', '-', 'year', '-', 'old', 'son', 'has', 'been', 'diagnosed', 'with', 'ad', '##hd', '.', 'need', 'some', 'advice', '.', '.', '.', 'well', ',', 'my', '8', '-', 'year', '-', 'old', 'son', 'has', 'been', 'diagnosed', 'by', 'a', 'child', '-', 'psychologist', 'as', 'ad', '##hd', '.', 'the', 'psychologist', 'recommended', 'we', 'discuss', 'with', 'the', 'pediatric', '##ian', ',', 'and', 'the', 'pediatric', '##ian', 'is', 'likely', 'to', 'recommend', 'rita', '##lin', '(', 'or', 'some', 'equivalent', 'medication', ')', '.', 'i', \"'\", 'm', 'neutral', 'on', 'this', '(', 'though', 'i', 'would', 'prefer', 'not', 'to', 'med', '##icate', 'him', 'if', 'we', 'can', 'avoid', 'it', ')', ',', 'but', 'my', 'wife', 'is', 'dead', '-', 'set', 'against', '\"', 'drug', '##ging', 'our', 'child', '\"', 'and', '\"', 'changing', 'his', 'personality', '\"', '.', 'i', 'don', \"'\", 't', 'feel', 'like', 'we', 'have', 'enough', 'information', 'to', 'make', 'an', 'informed', 'decision', ',', 'and', 'every', 'resource', 'i', \"'\", 've', 'found', 'on', 'the', 'internet', 'is', 'pretty', 'clearly', 'pushing', 'some', 'agenda', 'or', 'other', '.', 'i', \"'\", 'm', 'at', 'the', 'end', 'of', 'my', 'rope', 'here', '.', 'is', 'rita', '##lin', '(', 'or', 'whatever', ')', '\"', 'safe', '\"', '?', 'are', 'there', 'long', '-', 'term', 'side', 'effects', '?', 'i', \"'\", 've', 'read', 'about', 'higher', 'risk', 'of', 'suicide', ',', 'heart', 'failure', ',', 'psycho', '##sis', ',', 'and', 'everything', 'else', 'from', 'hairy', 'palms', 'to', 'hysterical', 'blindness', '.', 'i', 'have', 'no', 'idea', 'what', 'to', 'believe', 'any', 'more', '.', 'to', 'my', 'un', '##train', '##ed', 'eye', ',', 'my', 'son', 'doesn', \"'\", 't', 'seem', 'to', 'be', 'hyper', '##active', ',', 'but', 'he', 'does', 'have', 'massive', 'problems', 'concentrating', 'long', 'enough', 'to', 'finish', 'homework', 'or', 'other', 'tasks', '.', 'edit', ':', 'guys', ',', 'thank', 'you', 'so', 'much', 'for', 'all', 'your', 'advice', '.', 'especially', 'those', 'of', 'you', 'who', 'have', 'lived', 'with', 'it', 'since', 'a', 'young', 'age', '.', 'i', 'really', 'appreciate', 'it', '!', 'for', 'now', ',', 'since', 'it', 'is', 'a', 've', '##rr', '##rry', '*', 'mild', '*', 'case', ',', 'we', 'are', 'taking', 'the', 'diagnosis', 'to', 'his', 'school', 'to', 'get', 'him', 'a', 'modified', 'ie', '##p', ',', 'and', 'talking', 'with', 'his', 'pediatric', '##ian', '.', 'i', 'think', 'for', 'now', 'we', 'will', 'hold', 'off', 'on', 'the', 'medication', ',', 'but', 'after', 'a', 'lot', 'of', 'discussion', ',', 'we', \"'\", 're', 'definitely', 'more', 'ready', 'for', 'that', ',', 'when', 'the', 'time', 'comes', '.', 'thanks', 'again', ',', 'ra', '##dd', '##it', '!', 'this', 'is', 'becoming', 'a', 'real', 'source', 'of', 'friction', 'between', 'my', 'wife', 'and', 'i', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'being', 'open', '-', 'minded', 'about', 'potential', 'solutions', ',', 'and', 'she', 'feels', 'that', 'i', \"'\", 'm', 'a', 'monster', 'who', 'is', 'trying', 'to', '\"', 'give', 'drugs', 'to', 'babies', '\"', '.', 'please', 'help', '!', '(', 'prefer', '##ably', 'with', 'medical', 'information', ',', 'and', 'not', 'marital', 'advice', ':', '-', ')']\n",
      "INFO:__main__:Number of tokens: 422\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', '8', '-', 'year', '-', 'old', 'son', 'has', 'been', 'diagnosed', 'with', 'ad', '##hd', '.', 'need', 'some', 'advice', '.', '.', '.', 'well', ',', 'my', '8', '-', 'year', '-', 'old', 'son', 'has', 'been', 'diagnosed', 'by', 'a', 'child', '-', 'psychologist', 'as', 'ad', '##hd', '.', 'the', 'psychologist', 'recommended', 'we', 'discuss', 'with', 'the', 'pediatric', '##ian', ',', 'and', 'the', 'pediatric', '##ian', 'is', 'likely', 'to', 'recommend', 'rita', '##lin', '(', 'or', 'some', 'equivalent', 'medication', ')', '.', 'i', \"'\", 'm', 'neutral', 'on', 'this', '(', 'though', 'i', 'would', 'prefer', 'not', 'to', 'med', '##icate', 'him', 'if', 'we', 'can', 'avoid', 'it', ')', ',', 'but', 'my', 'wife', 'is', 'dead', '-', 'set', 'against', '\"', 'drug', '##ging', 'our', 'child', '\"', 'and', '\"', 'changing', 'his', 'personality', '\"', '.', 'i', 'don', \"'\", 't', 'feel', 'like', 'we', 'have', 'enough', 'information', 'to', 'make', 'an', 'informed', 'decision', ',', 'and', 'every', 'resource', 'i', \"'\", 've', 'found', 'on', 'the', 'internet', 'is', 'pretty', 'clearly', 'pushing', 'some', 'agenda', 'or', 'other', '.', 'i', \"'\", 'm', 'at', 'the', 'end', 'of', 'my', 'rope', 'here', '.', 'is', 'rita', '##lin', '(', 'or', 'whatever', ')', '\"', 'safe', '\"', '?', 'are', 'there', 'long', '-', 'term', 'side', 'effects', '?', 'i', \"'\", 've', 'read', 'about', 'higher', 'risk', 'of', 'suicide', ',', 'heart', 'failure', ',', 'psycho', '##sis', ',', 'and', 'everything', 'else', 'from', 'hairy', 'palms', 'to', 'hysterical', 'blindness', '.', 'i', 'have', 'no', 'idea', 'what', 'to', 'believe', 'any', 'more', '.', 'to', 'my', 'un', '##train', '##ed', 'eye', ',', 'my', 'son', 'doesn', \"'\", 't', 'seem', 'to', 'be', 'hyper', '##active', ',', 'but', 'he', 'does', 'have', 'massive', 'problems', 'concentrating', 'long', 'enough', 'to', 'finish', 'homework', 'or', 'other', 'tasks', '.', 'edit', ':', 'guys', ',', 'thank', 'you', 'so', 'much', 'for', 'all', 'your', 'advice', '.', 'especially', 'those', 'of', 'you', 'who', 'have', 'lived', 'with', 'it', 'since', 'a', 'young', 'age', '.', 'i', 'really', 'appreciate', 'it', '!', 'for', 'now', ',', 'since', 'it', 'is', 'a', 've', '##rr', '##rry', '*', 'mild', '*', 'case', ',', 'we', 'are', 'taking', 'the', 'diagnosis', 'to', 'his', 'school', 'to', 'get', 'him', 'a', 'modified', 'ie', '##p', ',', 'and', 'talking', 'with', 'his', 'pediatric', '##ian', '.', 'i', 'think', 'for', 'now', 'we', 'will', 'hold', 'off', 'on', 'the', 'medication', ',', 'but', 'after', 'a', 'lot', 'of', 'discussion', ',', 'we', \"'\", 're', 'definitely', 'more', 'ready', 'for', 'that', ',', 'when', 'the', 'time', 'comes', '.', 'thanks', 'again', ',', 'ra', '##dd', '##it', '!', 'this', 'is', 'becoming', 'a', 'real', 'source', 'of', 'friction', 'between', 'my', 'wife', 'and', 'i', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'being', 'open', '-', 'minded', 'about', 'potential', 'solutions', ',', 'and', 'she', 'feels', 'that', 'i', \"'\", 'm', 'a', 'monster', 'who', 'is', 'trying', 'to', '\"', 'give', 'drugs', 'to', 'babies', '\"', '.', 'please', 'help', '!', '(', 'prefer', '##ably', 'with', 'medical', 'information', ',', 'and', 'not', 'marital', 'advice', ':', '-', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['r', '/', 'ad', '##hd', 'i', 'could', 'really', 'use', 'some', 'advice']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['r', '/', 'ad', '##hd', 'i', 'could', 'really', 'use', 'some', 'advice']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['parents', 'are', 'extremely', 'nervous', 'about', 'me', 'taking', 'add', '##eral', '##l', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['parents', 'are', 'extremely', 'nervous', 'about', 'me', 'taking', 'add', '##eral', '##l', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'and', 'lam', '##ic', '##tal', '?', 'does', 'anyone', 'take', 'this', 'combination', '?', 'i', 'take', '20', 'mg', 'x', '##r', 'add', '##eral', '##l', 'and', '200', 'mg', 'of', 'lam', '##ic', '##tal', 'x', '##r', '(', 'which', 'is', 'ridiculous', '##ly', 'ex', '##pen', '##isi', '##ve', '.', 'i', 'am', 'wondering', 'what', 'your', 'experiences', 'are', '.']\n",
      "INFO:__main__:Number of tokens: 50\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'and', 'lam', '##ic', '##tal', '?', 'does', 'anyone', 'take', 'this', 'combination', '?', 'i', 'take', '20', 'mg', 'x', '##r', 'add', '##eral', '##l', 'and', '200', 'mg', 'of', 'lam', '##ic', '##tal', 'x', '##r', '(', 'which', 'is', 'ridiculous', '##ly', 'ex', '##pen', '##isi', '##ve', '.', 'i', 'am', 'wondering', 'what', 'your', 'experiences', 'are', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', 'i', 'would', 'be', 'an', 'awesome', '/', 'horrible', 'wizard', '!']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', 'i', 'would', 'be', 'an', 'awesome', '/', 'horrible', 'wizard', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'used', 'to', 'listen', 'to', 'this', 'song', '20', 'times', 'before', 'realizing', 'that', 'the', 'lyrics', 'where', 'about', 'a', '.', 'd', '.', 'd', '.', '(', 'i', 'speak', 'french', ')']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'used', 'to', 'listen', 'to', 'this', 'song', '20', 'times', 'before', 'realizing', 'that', 'the', 'lyrics', 'where', 'about', 'a', '.', 'd', '.', 'd', '.', '(', 'i', 'speak', 'french', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['seeing', 'a', 'ne', '##uro', '##psy', '##cho', '##logist', 'for', 'an', 'ad', '##hd', 'eva', '##l', 'on', 'tuesday', ',', 'what', 'should', 'i', 'expect', '?', 'my', 'family', 'practice', 'doc', 'referred', 'me', 'to', 'a', 'psychologist', 'because', 'he', 'thinks', 'i', 'have', 'a', 'moderate', 'case', 'of', 'ad', '##hd', '.', 'for', 'those', 'of', 'you', 'who', 'have', 'undergone', 'these', 'or', 'are', 'familiar', 'with', 'them', ',', 'what', 'should', 'i', 'expect', '?', 'is', 'this', 'mostly', 'a', 'question', 'and', 'answer', 'type', 'evaluation', 'or', 'what', '?']\n",
      "INFO:__main__:Number of tokens: 73\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['seeing', 'a', 'ne', '##uro', '##psy', '##cho', '##logist', 'for', 'an', 'ad', '##hd', 'eva', '##l', 'on', 'tuesday', ',', 'what', 'should', 'i', 'expect', '?', 'my', 'family', 'practice', 'doc', 'referred', 'me', 'to', 'a', 'psychologist', 'because', 'he', 'thinks', 'i', 'have', 'a', 'moderate', 'case', 'of', 'ad', '##hd', '.', 'for', 'those', 'of', 'you', 'who', 'have', 'undergone', 'these', 'or', 'are', 'familiar', 'with', 'them', ',', 'what', 'should', 'i', 'expect', '?', 'is', 'this', 'mostly', 'a', 'question', 'and', 'answer', 'type', 'evaluation', 'or', 'what', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['job', 'interviews', ',', 'explaining', 'poor', 'grades', '?', 'graduating', 'engineer', 'here', '.', 'looking', 'for', 'advice', 'on', 'how', 'to', 'explain', '/', 'how', 'do', 'deal', 'with', 'the', 'subject', 'of', 'poor', 'grades', '.', 'i', \"'\", 'm', 'graduating', 'with', 'a', '2', '.', '3', '/', '4', '.', '0', 'gp', '##a', '.', 'i', 'personally', 'feel', '(', 'and', 'have', 'professors', '/', 'mentor', '##s', 'that', 'agree', ')', 'that', 'i', 'am', 'certainly', 'above', 'average', 'compared', 'to', 'my', 'peers', 'in', 'areas', 'i', 'can', 'loosely', 'describe', 'as', '\"', 'street', 'smart', '##s', ',', '\"', '\"', '(', 'engineering', ')', 'common', 'sense', ',', '\"', 'etc', '.', 'bad', 'at', 'the', '\"', 'book', 'smart', '##s', '\"', 'stuff', '.', 'very', 'intelligent', ',', 'but', 'struggle', 'greatly', 'with', 'exams', '.', 'basically', 'looking', 'for', 'advice', 'on', 'how', 'to', 'deal', 'with', 'this', 'sensitive', 'issue', '.', 'i', 'know', 'that', 'my', 'resume', 'is', 'automatically', 'getting', 'thrown', 'out', 'by', 'a', 'lot', 'of', 'people', 'just', 'because', 'of', 'that', 'number', '.', 'i', 'really', 'feel', 'like', 'getting', 'to', 'the', 'interview', 'is', 'the', 'hardest', 'part', ';', 'once', 'i', \"'\", 'm', 'in', 'an', 'interview', ',', 'i', 'can', 'really', 'shine', '.', 'just', 'what', 'happens', 'when', 'they', 'ask', 'about', 'the', 'grades', '?', 'is', 'it', '\"', 'ko', '##sher', '\"', 'to', 'tell', 'them', '(', 'i', 'don', \"'\", 't', 'want', 'to', 'sound', 'like', 'i', \"'\", 'm', 'trying', 'to', 'abuse', 'something', 'like', 'americans', 'with', 'disabilities', 'act', ',', 'or', 'dealing', 'with', 'a', 'stigma', ')', '.', 'but', 'how', 'else', 'am', 'i', 'supposed', 'to', 'justify', 'this', 'low', 'number', '?']\n",
      "INFO:__main__:Number of tokens: 230\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['job', 'interviews', ',', 'explaining', 'poor', 'grades', '?', 'graduating', 'engineer', 'here', '.', 'looking', 'for', 'advice', 'on', 'how', 'to', 'explain', '/', 'how', 'do', 'deal', 'with', 'the', 'subject', 'of', 'poor', 'grades', '.', 'i', \"'\", 'm', 'graduating', 'with', 'a', '2', '.', '3', '/', '4', '.', '0', 'gp', '##a', '.', 'i', 'personally', 'feel', '(', 'and', 'have', 'professors', '/', 'mentor', '##s', 'that', 'agree', ')', 'that', 'i', 'am', 'certainly', 'above', 'average', 'compared', 'to', 'my', 'peers', 'in', 'areas', 'i', 'can', 'loosely', 'describe', 'as', '\"', 'street', 'smart', '##s', ',', '\"', '\"', '(', 'engineering', ')', 'common', 'sense', ',', '\"', 'etc', '.', 'bad', 'at', 'the', '\"', 'book', 'smart', '##s', '\"', 'stuff', '.', 'very', 'intelligent', ',', 'but', 'struggle', 'greatly', 'with', 'exams', '.', 'basically', 'looking', 'for', 'advice', 'on', 'how', 'to', 'deal', 'with', 'this', 'sensitive', 'issue', '.', 'i', 'know', 'that', 'my', 'resume', 'is', 'automatically', 'getting', 'thrown', 'out', 'by', 'a', 'lot', 'of', 'people', 'just', 'because', 'of', 'that', 'number', '.', 'i', 'really', 'feel', 'like', 'getting', 'to', 'the', 'interview', 'is', 'the', 'hardest', 'part', ';', 'once', 'i', \"'\", 'm', 'in', 'an', 'interview', ',', 'i', 'can', 'really', 'shine', '.', 'just', 'what', 'happens', 'when', 'they', 'ask', 'about', 'the', 'grades', '?', 'is', 'it', '\"', 'ko', '##sher', '\"', 'to', 'tell', 'them', '(', 'i', 'don', \"'\", 't', 'want', 'to', 'sound', 'like', 'i', \"'\", 'm', 'trying', 'to', 'abuse', 'something', 'like', 'americans', 'with', 'disabilities', 'act', ',', 'or', 'dealing', 'with', 'a', 'stigma', ')', '.', 'but', 'how', 'else', 'am', 'i', 'supposed', 'to', 'justify', 'this', 'low', 'number', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['26', '/', 'f', 'sounding', 'off', ':', 'are', 'these', 'problems', 'and', 'frustration', '##s', 'possibly', 'signs', 'i', 'have', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['26', '/', 'f', 'sounding', 'off', ':', 'are', 'these', 'problems', 'and', 'frustration', '##s', 'possibly', 'signs', 'i', 'have', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'it', 'possible', 'to', 'have', 'anxiety', 'and', 'ad', '##hd', 'or', 'is', 'it', 'only', 'one', 'or', 'the', 'other', '?', 'i', 'know', 'i', 'should', 'talk', 'to', 'a', 'ps', '##ych', 'first', 'which', 'i', 'did', '.', 'i', 'had', 'my', 'interview', 'done', 'and', 'will', 'be', 'with', 'another', 'university', 'ps', '##ych', 'next', 'week', '.', 'but', 'when', 'i', 'see', 'her', 'how', 'can', 'i', 'make', 'sure', 'i', 'get', 'diagnosed', 'properly', '.', 'can', 'i', 'have', 'both', 'ad', '##hd', 'and', 'anxiety', '?', 'here', \"'\", 's', 'what', 'i', 'think', 'happens', 'to', 'me', 'with', 'ad', '##hd', '-', 'i', 'get', 'bored', 'of', 'things', 'easily', 'and', 'take', 'a', 'while', 'to', 'finish', 'them', 'if', 'i', 'have', 'too', '.', '-', 'i', 'get', 'pro', '##cr', '##ast', '##inate', 'a', 'lot', '.', 'like', 'finishing', 'a', 'paper', 'during', 'the', 'night', 'before', 'it', \"'\", 's', 'due', '.', '-', 'i', 'forget', 'a', 'lot', 'of', 'details', 'and', 'make', 'a', 'lot', 'of', 'careless', 'mistakes', ',', 'for', 'example', 'reading', 'a', 'sentence', 'wrong', '.', '-', 'i', 'feel', 'totally', 'uncomfortable', 'in', 'the', 'university', 'lecture', 'rooms', ',', 'like', 'i', 'just', 'want', 'to', 'get', 'up', 'and', 'not', 'be', 'there', 'or', 'something', '.', '-', 'i', 'was', 'diagnosed', 'with', 'depression', 'before', 'but', 'when', 'i', 'took', 'the', 'medication', 'i', 'just', 'feel', 'even', 'less', 'motivated', 'than', 'before', '.', '-', 'i', 'have', 'little', 'tendencies', 'like', 'fearing', 'people', 'drive', 'too', 'close', 'too', 'me', '.', '-', 'i', 'sometimes', 'can', 'talk', 'to', 'people', 'for', '5', 'min', 'and', 'have', 'good', 'enthusiasm', 'and', 'then', 'just', 'stop', 'caring', 'and', 'not', 'having', 'anything', 'to', 'say', '.', '-', 'i', 'have', 'been', 'having', 'this', 'feeling', 'lately', 'like', 'i', 'don', \"'\", 't', 'want', 'to', 'do', 'anything', 'and', 'just', 'play', 'video', 'games', 'or', 'just', 'sleep', 'and', 'lie', 'in', 'my', 'bed', '.', '-', 'i', 'looked', 'up', 'restless', 'leg', 'syndrome', 'and', 'noticed', 'i', 'might', 'have', 'it', ',', 'some', 'students', 'in', 'high', 'school', 'were', 'bothered', 'by', 'it', 'but', 'i', 'never', 'really', 'noticed', 'it', '.', '-', 'i', 'also', 'periodically', 'feel', 'extremely', 'tired', 'or', 'sleepy', 'like', 'in', 'class', 'or', 'just', 'at', 'home', 'on', 'the', 'computer', 'and', 'i', 'have', 'to', 'sleep', 'for', 'a', 'while', ',', 'it', \"'\", 's', 'not', 'if', 'it', \"'\", 's', 'i', 'have', 'to', 'sleep', 'for', '10', '-', '15', '##min', '.', 'i', 'feel', 'better', 'later', '.', '-', 'when', 'things', 'get', 'piled', 'up', 'or', 'there', \"'\", 's', 'too', 'much', 'to', 'clean', 'or', 'organize', 'i', 'just', 'won', \"'\", 't', 'do', 'anything', 'and', 'leave', 'it', 'be', '.', 'here', \"'\", 's', 'what', 'i', 'have', 'for', 'anxiety', 'i', 'think', ':', '-', 'afraid', 'of', 'new', 'things', 'like', 'going', 'to', 'the', 'interview', '##er', 'for', 'the', 'first', 'time', '.', 'i', 'try', 'to', 'be', 'extra', 'careful', 'the', 'first', 'time', 'i', 'am', 'doing', 'something', '.', 'but', 'in', 'the', 'end', 'i', 'was', 'extra', 'nervous', 'moving', 'and', 'un', '##con', '##ent', '##rating', '.', '-', 'my', 'grades', 'have', 'dramatically', 'decreased', 'from', 'sophomore', 'year', 'to', 'freshman', 'in', 'college', '(', 'i', 'am', 'in', 'college', 'now', ')', 'with', 'low', 'gp', '##a', '.', '-', 'i', 'sometimes', 'sweat', 'so', 'much', 'for', 'no', 'reason', '.', '-', 'i', 'find', 'i', 'concentrate', 'better', 'when', 'i', \"'\", 'm', 'not', 'thinking', ',', 'but', 'just', 'doing', 'anyone', 'else', 'feel', 'this', 'way', '?', '-', 'i', 'cannot', 'think', 'when', 'i', \"'\", 'm', 'anxious', 'it', 'is', 'so', 'difficult', '.', '-', 'when', 'i', 'play', 'star', '##craft', 'ii', ',', 'the', 'games', 'are', 'intense', 'and', 'when', 'things', 'get', 'heavy', 'i', 'get', 'so', 'anxious', ',', 'sweaty', ',', 'unable', 'to', 'make', 'commands', 'or', 'moves', '.', 'no', 'one', 'else', 'i', 'know', 'who', 'plays', 'this', 'game', 'feels', 'this', 'way', 'during', 'a', 'match', '.', 'i', 'feel', 'this', 'way', 'all', 'the', 'time', '.', '-', 'i', 'am', 'usually', 'more', 'confident', 'with', 'people', 'around', 'me', 'that', 'i', 'know', 'and', 'tend', 'to', 'tense', 'up', 'and', 'freeze', 'around', 'new', 'people', '.', 'so', 'i', 'just', 'want', 'to', 'know', 'if', 'it', \"'\", 's', 'ad', '##hd', 'or', 'anxiety', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'anxiety', 'it', 'is', 'exactly', '.', 'but', 'is', 'it', 'one', 'or', 'the', 'other', 'or', 'both', '?', 'if', 'it', \"'\", 's', 'both', 'how', 'can', 'i', 'tell', 'my', 'ps', '##ych', ',', 'and', 'what', 'should', 'i', 'go', 'for', 'med', '##s', 'or', 'counseling', '?', 'i', 'feel', 'i', 'can', 'control', 'my', 'anxiety', 'as', 'it', 'only', 'happens', 'under', 'certain', 'circumstances', '.', 'also', 'sorry', 'for', 'my', 'bad', 'grammar', ',', 'didn', \"'\", 't', 'proof', 'read', 'as', 'it', \"'\", 's', 'late', 'right', 'now', '.', 'thanks', 'red', '##dit', '!']\n",
      "INFO:__main__:Number of tokens: 680\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['is', 'it', 'possible', 'to', 'have', 'anxiety', 'and', 'ad', '##hd', 'or', 'is', 'it', 'only', 'one', 'or', 'the', 'other', '?', 'i', 'know', 'i', 'should', 'talk', 'to', 'a', 'ps', '##ych', 'first', 'which', 'i', 'did', '.', 'i', 'had', 'my', 'interview', 'done', 'and', 'will', 'be', 'with', 'another', 'university', 'ps', '##ych', 'next', 'week', '.', 'but', 'when', 'i', 'see', 'her', 'how', 'can', 'i', 'make', 'sure', 'i', 'get', 'diagnosed', 'properly', '.', 'can', 'i', 'have', 'both', 'ad', '##hd', 'and', 'anxiety', '?', 'here', \"'\", 's', 'what', 'i', 'think', 'happens', 'to', 'me', 'with', 'ad', '##hd', '-', 'i', 'get', 'bored', 'of', 'things', 'easily', 'and', 'take', 'a', 'while', 'to', 'finish', 'them', 'if', 'i', 'have', 'too', '.', '-', 'i', 'get', 'pro', '##cr', '##ast', '##inate', 'a', 'lot', '.', 'like', 'finishing', 'a', 'paper', 'during', 'the', 'night', 'before', 'it', \"'\", 's', 'due', '.', '-', 'i', 'forget', 'a', 'lot', 'of', 'details', 'and', 'make', 'a', 'lot', 'of', 'careless', 'mistakes', ',', 'for', 'example', 'reading', 'a', 'sentence', 'wrong', '.', '-', 'i', 'feel', 'totally', 'uncomfortable', 'in', 'the', 'university', 'lecture', 'rooms', ',', 'like', 'i', 'just', 'want', 'to', 'get', 'up', 'and', 'not', 'be', 'there', 'or', 'something', '.', '-', 'i', 'was', 'diagnosed', 'with', 'depression', 'before', 'but', 'when', 'i', 'took', 'the', 'medication', 'i', 'just', 'feel', 'even', 'less', 'motivated', 'than', 'before', '.', '-', 'i', 'have', 'little', 'tendencies', 'like', 'fearing', 'people', 'drive', 'too', 'close', 'too', 'me', '.', '-', 'i', 'sometimes', 'can', 'talk', 'to', 'people', 'for', '5', 'min', 'and', 'have', 'good', 'enthusiasm', 'and', 'then', 'just', 'stop', 'caring', 'and', 'not', 'having', 'anything', 'to', 'say', '.', '-', 'i', 'have', 'been', 'having', 'this', 'feeling', 'lately', 'like', 'i', 'don', \"'\", 't', 'want', 'to', 'do', 'anything', 'and', 'just', 'play', 'video', 'games', 'or', 'just', 'sleep', 'and', 'lie', 'in', 'my', 'bed', '.', '-', 'i', 'looked', 'up', 'restless', 'leg', 'syndrome', 'and', 'noticed', 'i', 'might', 'have', 'it', ',', 'some', 'students', 'in', 'high', 'school', 'were', 'bothered', 'by', 'it', 'but', 'i', 'never', 'really', 'noticed', 'it', '.', '-', 'i', 'also', 'periodically', 'feel', 'extremely', 'tired', 'or', 'sleepy', 'like', 'in', 'class', 'or', 'just', 'at', 'home', 'on', 'the', 'computer', 'and', 'i', 'have', 'to', 'sleep', 'for', 'a', 'while', ',', 'it', \"'\", 's', 'not', 'if', 'it', \"'\", 's', 'i', 'have', 'to', 'sleep', 'for', '10', '-', '15', '##min', '.', 'i', 'feel', 'better', 'later', '.', '-', 'when', 'things', 'get', 'piled', 'up', 'or', 'there', \"'\", 's', 'too', 'much', 'to', 'clean', 'or', 'organize', 'i', 'just', 'won', \"'\", 't', 'do', 'anything', 'and', 'leave', 'it', 'be', '.', 'here', \"'\", 's', 'what', 'i', 'have', 'for', 'anxiety', 'i', 'think', ':', '-', 'afraid', 'of', 'new', 'things', 'like', 'going', 'to', 'the', 'interview', '##er', 'for', 'the', 'first', 'time', '.', 'i', 'try', 'to', 'be', 'extra', 'careful', 'the', 'first', 'time', 'i', 'am', 'doing', 'something', '.', 'but', 'in', 'the', 'end', 'i', 'was', 'extra', 'nervous', 'moving', 'and', 'un', '##con', '##ent', '##rating', '.', '-', 'my', 'grades', 'have', 'dramatically', 'decreased', 'from', 'sophomore', 'year', 'to', 'freshman', 'in', 'college', '(', 'i', 'am', 'in', 'college', 'now', ')', 'with', 'low', 'gp', '##a', '.', '-', 'i', 'sometimes', 'sweat', 'so', 'much', 'for', 'no', 'reason', '.', '-', 'i', 'find', 'i', 'concentrate', 'better', 'when', 'i', \"'\", 'm', 'not', 'thinking', ',', 'but', 'just', 'doing', 'anyone', 'else', 'feel', 'this', 'way', '?', '-', 'i', 'cannot', 'think', 'when', 'i', \"'\", 'm', 'anxious', 'it', 'is', 'so', 'difficult', '.', '-', 'when', 'i', 'play', 'star', '##craft', 'ii', ',', 'the', 'games'], ['are', 'intense', 'and', 'when', 'things', 'get', 'heavy', 'i', 'get', 'so', 'anxious', ',', 'sweaty', ',', 'unable', 'to', 'make', 'commands', 'or', 'moves', '.', 'no', 'one', 'else', 'i', 'know', 'who', 'plays', 'this', 'game', 'feels', 'this', 'way', 'during', 'a', 'match', '.', 'i', 'feel', 'this', 'way', 'all', 'the', 'time', '.', '-', 'i', 'am', 'usually', 'more', 'confident', 'with', 'people', 'around', 'me', 'that', 'i', 'know', 'and', 'tend', 'to', 'tense', 'up', 'and', 'freeze', 'around', 'new', 'people', '.', 'so', 'i', 'just', 'want', 'to', 'know', 'if', 'it', \"'\", 's', 'ad', '##hd', 'or', 'anxiety', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'anxiety', 'it', 'is', 'exactly', '.', 'but', 'is', 'it', 'one', 'or', 'the', 'other', 'or', 'both', '?', 'if', 'it', \"'\", 's', 'both', 'how', 'can', 'i', 'tell', 'my', 'ps', '##ych', ',', 'and', 'what', 'should', 'i', 'go', 'for', 'med', '##s', 'or', 'counseling', '?', 'i', 'feel', 'i', 'can', 'control', 'my', 'anxiety', 'as', 'it', 'only', 'happens', 'under', 'certain', 'circumstances', '.', 'also', 'sorry', 'for', 'my', 'bad', 'grammar', ',', 'didn', \"'\", 't', 'proof', 'read', 'as', 'it', \"'\", 's', 'late', 'right', 'now', '.', 'thanks', 'red', '##dit', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'have', 'experience', 'with', 'concert', '##a', 'resulting', 'in', 'ins', '##om', '##nia', '?', 'i', 'just', 'started', 'taking', 'concert', '##a', '36', '##mg', ',', 'and', 'i', 'noticed', 'the', 'benefits', 'very', 'quickly', '.', 'i', 'felt', 'relaxed', 'as', 'well', 'as', 'alert', ',', 'had', 'improvements', 'with', 'social', 'interaction', 'and', 'started', 'getting', 'things', 'done', '.', 'it', 'has', 'been', 'great', 'so', 'far', '.', 'the', 'only', 'side', 'effects', 'i', \"'\", 've', 'noticed', 'are', 'loss', 'of', 'appetite', 'and', 'ins', '##om', '##nia', '.', 'i', 'haven', \"'\", 't', 'been', 'sleeping', 'very', 'well', 'for', 'the', 'past', 'few', 'months', 'before', 'i', 'started', 'the', 'medication', ',', 'but', 'due', 'to', 'the', 'fact', 'that', 'i', \"'\", 'm', 'starting', 'to', 'get', 'things', 'done', 'and', 'i', 'feel', 'a', 'little', 'bit', 'more', 'in', 'control', 'i', 'was', 'hoping', 'that', 'i', 'would', 'start', 'sleeping', 'better', '.', 'i', 'know', 'ins', '##om', '##nia', 'is', 'one', 'of', 'the', 'side', 'effects', ',', 'but', 'was', 'wondering', 'if', 'anyone', 'had', 'experience', 'with', 'the', 'ins', '##om', '##nia', 'wan', '##ing', 'over', 'time', '.', 'it', \"'\", 's', '5', 'am', 'now', 'and', 'my', 'body', 'feels', 'tired', ',', 'but', 'when', 'i', 'try', 'to', 'fall', 'asleep', 'it', 'feels', 'fruit', '##less', '.', 'any', 'responses', 'would', 'be', 'greatly', 'appreciated', '.']\n",
      "INFO:__main__:Number of tokens: 184\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'have', 'experience', 'with', 'concert', '##a', 'resulting', 'in', 'ins', '##om', '##nia', '?', 'i', 'just', 'started', 'taking', 'concert', '##a', '36', '##mg', ',', 'and', 'i', 'noticed', 'the', 'benefits', 'very', 'quickly', '.', 'i', 'felt', 'relaxed', 'as', 'well', 'as', 'alert', ',', 'had', 'improvements', 'with', 'social', 'interaction', 'and', 'started', 'getting', 'things', 'done', '.', 'it', 'has', 'been', 'great', 'so', 'far', '.', 'the', 'only', 'side', 'effects', 'i', \"'\", 've', 'noticed', 'are', 'loss', 'of', 'appetite', 'and', 'ins', '##om', '##nia', '.', 'i', 'haven', \"'\", 't', 'been', 'sleeping', 'very', 'well', 'for', 'the', 'past', 'few', 'months', 'before', 'i', 'started', 'the', 'medication', ',', 'but', 'due', 'to', 'the', 'fact', 'that', 'i', \"'\", 'm', 'starting', 'to', 'get', 'things', 'done', 'and', 'i', 'feel', 'a', 'little', 'bit', 'more', 'in', 'control', 'i', 'was', 'hoping', 'that', 'i', 'would', 'start', 'sleeping', 'better', '.', 'i', 'know', 'ins', '##om', '##nia', 'is', 'one', 'of', 'the', 'side', 'effects', ',', 'but', 'was', 'wondering', 'if', 'anyone', 'had', 'experience', 'with', 'the', 'ins', '##om', '##nia', 'wan', '##ing', 'over', 'time', '.', 'it', \"'\", 's', '5', 'am', 'now', 'and', 'my', 'body', 'feels', 'tired', ',', 'but', 'when', 'i', 'try', 'to', 'fall', 'asleep', 'it', 'feels', 'fruit', '##less', '.', 'any', 'responses', 'would', 'be', 'greatly', 'appreciated', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['70', 'mg', 'of', 'v', '##y', '##van', '##se', 'and', 'can', 'tell', 'i', \"'\", 'm', 'getting', 'too', 'dependent', '.', '.', '.', 'i', \"'\", 've', 'been', 'taking', '70', 'mg', 'of', 'v', '##y', '##van', '##se', 'for', 'about', '2', 'years', 'now', '.', 'it', 'works', 'wonders', 'when', 'i', 'need', 'to', 'get', 'shit', 'done', '.', '.', '.', 'especially', 'in', 'college', 'with', 'the', 'heavy', 'course', 'load', 'i', 'tend', 'to', 'take', '.', 'i', \"'\", 'm', 'noticing', ',', 'however', ',', 'that', 'i', \"'\", 'm', 'starting', 'to', 'rely', 'on', 'it', 'too', 'much', 'because', 'i', 'hate', 'the', 'ha', '##zine', '##ss', 'that', 'i', 'feel', 'when', 'i', 'don', \"'\", 't', 'have', 'the', 'med', '##s', '.', 'i', 'become', 'an', 'emotional', 'basket', 'case', 'and', 'can', \"'\", 't', 'seem', 'to', 'sort', 'anything', 'out', 'without', 'the', 'help', 'of', 'a', 'friend', '.', 'now', 'i', \"'\", 've', 'started', 'taking', 'it', 'once', 'in', 'the', 'morning', '(', 'around', '6', ':', '00', 'a', '.', 'm', '.', ')', 'to', 'focus', 'through', 'classes', 'and', 'then', 'again', 'at', '8', ':', '00', 'p', '.', 'm', '.', 'so', 'i', 'can', 'stay', 'up', 'all', 'night', 'and', 'get', 'work', 'done', '.', 'this', 'is', 'only', 'during', 'like', 'finals', 'week', 'and', 'busy', 'times', 'during', 'the', 'quarter', 'but', 'i', 'know', 'i', 'can', \"'\", 't', 'keep', 'doing', 'this', '.', '.', '.', 'any', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 200\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['70', 'mg', 'of', 'v', '##y', '##van', '##se', 'and', 'can', 'tell', 'i', \"'\", 'm', 'getting', 'too', 'dependent', '.', '.', '.', 'i', \"'\", 've', 'been', 'taking', '70', 'mg', 'of', 'v', '##y', '##van', '##se', 'for', 'about', '2', 'years', 'now', '.', 'it', 'works', 'wonders', 'when', 'i', 'need', 'to', 'get', 'shit', 'done', '.', '.', '.', 'especially', 'in', 'college', 'with', 'the', 'heavy', 'course', 'load', 'i', 'tend', 'to', 'take', '.', 'i', \"'\", 'm', 'noticing', ',', 'however', ',', 'that', 'i', \"'\", 'm', 'starting', 'to', 'rely', 'on', 'it', 'too', 'much', 'because', 'i', 'hate', 'the', 'ha', '##zine', '##ss', 'that', 'i', 'feel', 'when', 'i', 'don', \"'\", 't', 'have', 'the', 'med', '##s', '.', 'i', 'become', 'an', 'emotional', 'basket', 'case', 'and', 'can', \"'\", 't', 'seem', 'to', 'sort', 'anything', 'out', 'without', 'the', 'help', 'of', 'a', 'friend', '.', 'now', 'i', \"'\", 've', 'started', 'taking', 'it', 'once', 'in', 'the', 'morning', '(', 'around', '6', ':', '00', 'a', '.', 'm', '.', ')', 'to', 'focus', 'through', 'classes', 'and', 'then', 'again', 'at', '8', ':', '00', 'p', '.', 'm', '.', 'so', 'i', 'can', 'stay', 'up', 'all', 'night', 'and', 'get', 'work', 'done', '.', 'this', 'is', 'only', 'during', 'like', 'finals', 'week', 'and', 'busy', 'times', 'during', 'the', 'quarter', 'but', 'i', 'know', 'i', 'can', \"'\", 't', 'keep', 'doing', 'this', '.', '.', '.', 'any', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'ho', '##mies', ',', 'i', 'know', 'you', \"'\", 've', 'been', 'working', 'hard', 'on', 'monitoring', 'and', 'improving', 'yourselves', '.', 'let', \"'\", 's', 'share', 'some', 'successes', 'we', \"'\", 've', 'had', 'recently', '!', 'i', \"'\", 've', 'been', 'doing', 'pretty', 'well', 'at', 'my', 'current', 'te', '##mp', 'assignment', 'with', 'focus', '##sing', 'on', 'my', 'project', 'and', 'also', 'being', 'able', 'to', 'handle', 'minor', 'tasks', 'that', 'get', 'thrown', 'my', 'way', 'every', 'day', '.', 'i', 'never', 'knew', 'i', 'could', 'be', 'good', 'at', 'jug', '##gling', '!', 'what', \"'\", 's', 'been', 'self', '-', 'af', '##firm', '##ing', 'for', 'you', 'lately', '?']\n",
      "INFO:__main__:Number of tokens: 90\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'ho', '##mies', ',', 'i', 'know', 'you', \"'\", 've', 'been', 'working', 'hard', 'on', 'monitoring', 'and', 'improving', 'yourselves', '.', 'let', \"'\", 's', 'share', 'some', 'successes', 'we', \"'\", 've', 'had', 'recently', '!', 'i', \"'\", 've', 'been', 'doing', 'pretty', 'well', 'at', 'my', 'current', 'te', '##mp', 'assignment', 'with', 'focus', '##sing', 'on', 'my', 'project', 'and', 'also', 'being', 'able', 'to', 'handle', 'minor', 'tasks', 'that', 'get', 'thrown', 'my', 'way', 'every', 'day', '.', 'i', 'never', 'knew', 'i', 'could', 'be', 'good', 'at', 'jug', '##gling', '!', 'what', \"'\", 's', 'been', 'self', '-', 'af', '##firm', '##ing', 'for', 'you', 'lately', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', ',', 'add', '##eral', '##l', ',', 'pain', 'and', 'tattoo', '##ing', 'as', 'the', 'title', '(', 'may', ')', 'suggest', 'i', 'have', 'ad', '##hd', 'and', 'i', 'am', 'taking', 'add', '##eral', '##l', '.', 'i', 'also', 'have', '(', 'i', 'think', ')', 'extensive', 'tattoo', '##ing', 'that', 'was', 'done', 'prior', 'to', 'my', 'diagnosis', 'and', 'med', '##s', '.', 'strange', 'question', ':', 'does', 'anyone', 'have', 'experience', 'with', 'being', 'tattooed', '-', 'with', 'ad', '##hd', '-', 'med', '##icated', 'vs', 'not', 'med', '##ic', '##ted', '?', 'i', 'have', 'an', 'appointment', 'booked', 'next', 'week', 'and', 'i', \"'\", 'm', 'trying', 'to', 'weigh', 'the', 'options', '-', 'i', 'need', 'decide', 'which', 'is', 'the', 'best', 'route', '.', 'my', 'pain', 'tolerance', 'is', 'pretty', 'high', 'but', 'at', 'all', 'previous', 'tattoo', 'sessions', 'my', 'attention', 'span', 'rival', '##led', 'a', 'chocolate', 'chip', '.', 'i', \"'\", 'm', 'scared', 'the', 'med', '##s', 'will', 'cause', 'me', 'to', 'focus', 'on', 'the', 'pain', 'instead', 'of', 'just', 'scattering', 'out', 'like', 'i', 'did', 'before', '.', '.', '.', '?', 'any', 'insights', 'would', 'be', 'appreciated', '.', 'potentially', 'helpful', 'demographic', 'info', ':', '-', 'female', '-', '31', '-', 'daily', 'add', '##eral', '##l', 'dose', '=', 'about', '10', '-', '12', '##mg', '/', 'day', 't', '##l', ';', 'dr', ':', 'ninja', '##s', '!', '.', '.', '.', 'wait', ',', 'wu', '##t', '?', 't', '##l', ';', 'dr', 'part', '2', ':', 'does', 'anyone', 'have', 'experience', 'with', 'being', 'tattooed', '-', 'with', 'ad', '##hd', '-', 'med', '##icated', 'vs', 'not', 'med', '##ic', '##ted', '?']\n",
      "INFO:__main__:Number of tokens: 221\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', ',', 'add', '##eral', '##l', ',', 'pain', 'and', 'tattoo', '##ing', 'as', 'the', 'title', '(', 'may', ')', 'suggest', 'i', 'have', 'ad', '##hd', 'and', 'i', 'am', 'taking', 'add', '##eral', '##l', '.', 'i', 'also', 'have', '(', 'i', 'think', ')', 'extensive', 'tattoo', '##ing', 'that', 'was', 'done', 'prior', 'to', 'my', 'diagnosis', 'and', 'med', '##s', '.', 'strange', 'question', ':', 'does', 'anyone', 'have', 'experience', 'with', 'being', 'tattooed', '-', 'with', 'ad', '##hd', '-', 'med', '##icated', 'vs', 'not', 'med', '##ic', '##ted', '?', 'i', 'have', 'an', 'appointment', 'booked', 'next', 'week', 'and', 'i', \"'\", 'm', 'trying', 'to', 'weigh', 'the', 'options', '-', 'i', 'need', 'decide', 'which', 'is', 'the', 'best', 'route', '.', 'my', 'pain', 'tolerance', 'is', 'pretty', 'high', 'but', 'at', 'all', 'previous', 'tattoo', 'sessions', 'my', 'attention', 'span', 'rival', '##led', 'a', 'chocolate', 'chip', '.', 'i', \"'\", 'm', 'scared', 'the', 'med', '##s', 'will', 'cause', 'me', 'to', 'focus', 'on', 'the', 'pain', 'instead', 'of', 'just', 'scattering', 'out', 'like', 'i', 'did', 'before', '.', '.', '.', '?', 'any', 'insights', 'would', 'be', 'appreciated', '.', 'potentially', 'helpful', 'demographic', 'info', ':', '-', 'female', '-', '31', '-', 'daily', 'add', '##eral', '##l', 'dose', '=', 'about', '10', '-', '12', '##mg', '/', 'day', 't', '##l', ';', 'dr', ':', 'ninja', '##s', '!', '.', '.', '.', 'wait', ',', 'wu', '##t', '?', 't', '##l', ';', 'dr', 'part', '2', ':', 'does', 'anyone', 'have', 'experience', 'with', 'being', 'tattooed', '-', 'with', 'ad', '##hd', '-', 'med', '##icated', 'vs', 'not', 'med', '##ic', '##ted', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['starting', 'concert', '##a', 'today', '.', 'anyone', 'have', 'any', 'experience', 'with', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['starting', 'concert', '##a', 'today', '.', 'anyone', 'have', 'any', 'experience', 'with', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['question', 'on', 'study', 'habits', 'with', 'ad', '##hd', 'first', 'off', ',', 'this', 'is', 'about', 'my', 'girlfriend', 'who', 'may', 'or', 'may', 'not', 'have', 'ad', '##hd', '(', 'pi', 'as', 'described', 'in', 'the', 'fa', '##q', 'here', ')', '.', 'if', 'she', 'does', 'it', 'seems', 'quite', 'mild', 'and', 'she', \"'\", 's', 'never', 'been', 'diagnosed', ',', 'but', 'she', 'has', 'a', 'few', 'siblings', 'with', 'diagnosed', 'ad', '##hd', 'and', 'she', 'shows', 'some', 'symptoms', '.', 'i', \"'\", 'm', 'looking', 'for', 'some', 'advice', 'in', 'helping', 'her', 'with', 'her', 'situation', '.', 'she', \"'\", 's', 'in', 'her', 'first', 'year', 'of', 'graduate', 'studies', 'and', 'really', 'struggling', '.', 'i', 'know', 'the', 'program', 'is', 'actually', 'quite', 'difficult', 'and', 'di', '##sor', '##gan', '##ized', 'as', 'i', 'took', 'some', 'classes', 'from', 'that', 'department', 'this', 'year', 'as', 'an', 'undergraduate', 'and', 'saw', 'first', '##hand', 'everything', 'she', \"'\", 's', 'been', 'complaining', 'about', '.', 'i', \"'\", 've', 'also', 'talked', 'to', 'a', 'couple', 'of', 'her', 'classmates', 'who', 'say', 'the', 'same', 'thing', '.', 'however', ',', 'her', 'study', 'habits', 'seem', 'to', 'me', 'at', '##ro', '##cious', ',', 'and', 'i', 'suspect', 'that', 'she', 'can', 'do', 'a', 'lot', 'better', 'and', 'be', 'a', 'much', 'less', 'stressed', '(', 'it', \"'\", 's', 'affecting', 'her', 'physical', 'health', 'now', ')', 'if', 'she', 'improves', 'them', '.', 'my', 'primary', 'concern', 'is', 'that', 'she', 'watches', 'television', 'in', 'the', 'background', 'while', 'she', 'reads', 'articles', 'and', 'writes', 'papers', '(', 'and', 'pretty', 'much', 'every', 'other', 'form', 'of', 'studying', ',', 'in', 'a', 'non', '-', 'native', 'language', 'nonetheless', ')', '.', 'most', 'of', 'the', 'time', 'it', \"'\", 's', 'hu', '##lu', 'or', 'netflix', 'running', 'in', 'a', 'background', 'tab', 'so', 'she', \"'\", 's', 'just', 'hearing', 'the', 'audio', 'but', 'sometimes', 'it', \"'\", 's', 'visible', 'on', '##screen', 'as', 'well', '.', 'i', \"'\", 've', 'suggested', 'a', 'few', 'times', 'that', 'she', 'stop', 'it', ',', 'but', 'she', 'says', 'that', 'she', 'gets', 'distracted', 'whether', 'or', 'not', 'shows', 'are', 'playing', 'and', 'having', 'the', 'shows', 'lets', 'her', 'at', 'least', 'keep', 'her', 'attention', 'on', 'the', 'computer', 'instead', 'of', 'drifting', 'to', 'something', 'elsewhere', 'in', 'the', 'room', 'or', 'outside', '.', 'in', 'my', 'experience', 'it', \"'\", 's', 'next', 'to', 'impossible', 'to', 'maintain', 'focus', 'like', 'that', 'but', 'i', 'realize', 'it', 'may', 'differ', 'from', 'person', 'to', 'person', '.', 'so', 'finally', 'my', 'question', ':', 'if', 'she', 'has', 'ad', '##hd', '-', 'pi', ',', 'could', 'it', 'be', 'the', 'case', 'that', 'the', 'strategic', 'distraction', 'helps', '?', 'or', 'would', 'she', 'likely', 'be', 'better', 'off', 'if', 'she', 'can', 'adjust', 'to', 'silence', 'or', 'non', '-', 'lyrical', 'classical', 'music', 'or', 'something', 'like', 'that', '?', 'i', \"'\", 'd', 'be', 'happy', 'to', 'answer', 'more', 'questions', 'if', 'i', \"'\", 've', 'left', 'out', 'potentially', 'useful', 'information', '.', 'thanks', 'guys', '!', '*', '*', 't', '##l', ';', 'dr', ':', '*', '*', 'can', 'listening', 'to', 'television', 'but', 'not', 'visually', 'watching', 'help', 'prevent', 'someone', 'with', 'ad', '##hd', '-', 'pi', 'from', 'getting', 'too', 'distracted', 'from', 'studying', '/', 'working', 'by', 'other', 'things', '?']\n",
      "INFO:__main__:Number of tokens: 446\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['question', 'on', 'study', 'habits', 'with', 'ad', '##hd', 'first', 'off', ',', 'this', 'is', 'about', 'my', 'girlfriend', 'who', 'may', 'or', 'may', 'not', 'have', 'ad', '##hd', '(', 'pi', 'as', 'described', 'in', 'the', 'fa', '##q', 'here', ')', '.', 'if', 'she', 'does', 'it', 'seems', 'quite', 'mild', 'and', 'she', \"'\", 's', 'never', 'been', 'diagnosed', ',', 'but', 'she', 'has', 'a', 'few', 'siblings', 'with', 'diagnosed', 'ad', '##hd', 'and', 'she', 'shows', 'some', 'symptoms', '.', 'i', \"'\", 'm', 'looking', 'for', 'some', 'advice', 'in', 'helping', 'her', 'with', 'her', 'situation', '.', 'she', \"'\", 's', 'in', 'her', 'first', 'year', 'of', 'graduate', 'studies', 'and', 'really', 'struggling', '.', 'i', 'know', 'the', 'program', 'is', 'actually', 'quite', 'difficult', 'and', 'di', '##sor', '##gan', '##ized', 'as', 'i', 'took', 'some', 'classes', 'from', 'that', 'department', 'this', 'year', 'as', 'an', 'undergraduate', 'and', 'saw', 'first', '##hand', 'everything', 'she', \"'\", 's', 'been', 'complaining', 'about', '.', 'i', \"'\", 've', 'also', 'talked', 'to', 'a', 'couple', 'of', 'her', 'classmates', 'who', 'say', 'the', 'same', 'thing', '.', 'however', ',', 'her', 'study', 'habits', 'seem', 'to', 'me', 'at', '##ro', '##cious', ',', 'and', 'i', 'suspect', 'that', 'she', 'can', 'do', 'a', 'lot', 'better', 'and', 'be', 'a', 'much', 'less', 'stressed', '(', 'it', \"'\", 's', 'affecting', 'her', 'physical', 'health', 'now', ')', 'if', 'she', 'improves', 'them', '.', 'my', 'primary', 'concern', 'is', 'that', 'she', 'watches', 'television', 'in', 'the', 'background', 'while', 'she', 'reads', 'articles', 'and', 'writes', 'papers', '(', 'and', 'pretty', 'much', 'every', 'other', 'form', 'of', 'studying', ',', 'in', 'a', 'non', '-', 'native', 'language', 'nonetheless', ')', '.', 'most', 'of', 'the', 'time', 'it', \"'\", 's', 'hu', '##lu', 'or', 'netflix', 'running', 'in', 'a', 'background', 'tab', 'so', 'she', \"'\", 's', 'just', 'hearing', 'the', 'audio', 'but', 'sometimes', 'it', \"'\", 's', 'visible', 'on', '##screen', 'as', 'well', '.', 'i', \"'\", 've', 'suggested', 'a', 'few', 'times', 'that', 'she', 'stop', 'it', ',', 'but', 'she', 'says', 'that', 'she', 'gets', 'distracted', 'whether', 'or', 'not', 'shows', 'are', 'playing', 'and', 'having', 'the', 'shows', 'lets', 'her', 'at', 'least', 'keep', 'her', 'attention', 'on', 'the', 'computer', 'instead', 'of', 'drifting', 'to', 'something', 'elsewhere', 'in', 'the', 'room', 'or', 'outside', '.', 'in', 'my', 'experience', 'it', \"'\", 's', 'next', 'to', 'impossible', 'to', 'maintain', 'focus', 'like', 'that', 'but', 'i', 'realize', 'it', 'may', 'differ', 'from', 'person', 'to', 'person', '.', 'so', 'finally', 'my', 'question', ':', 'if', 'she', 'has', 'ad', '##hd', '-', 'pi', ',', 'could', 'it', 'be', 'the', 'case', 'that', 'the', 'strategic', 'distraction', 'helps', '?', 'or', 'would', 'she', 'likely', 'be', 'better', 'off', 'if', 'she', 'can', 'adjust', 'to', 'silence', 'or', 'non', '-', 'lyrical', 'classical', 'music', 'or', 'something', 'like', 'that', '?', 'i', \"'\", 'd', 'be', 'happy', 'to', 'answer', 'more', 'questions', 'if', 'i', \"'\", 've', 'left', 'out', 'potentially', 'useful', 'information', '.', 'thanks', 'guys', '!', '*', '*', 't', '##l', ';', 'dr', ':', '*', '*', 'can', 'listening', 'to', 'television', 'but', 'not', 'visually', 'watching', 'help', 'prevent', 'someone', 'with', 'ad', '##hd', '-', 'pi', 'from', 'getting', 'too', 'distracted', 'from', 'studying', '/', 'working', 'by', 'other', 'things', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'anyone', 'familiar', 'the', 'accommodation', 'request', 'process', 'for', 'the', 'gr', '##e', '?']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'anyone', 'familiar', 'the', 'accommodation', 'request', 'process', 'for', 'the', 'gr', '##e', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'am', '38', 'and', 'just', 'started', 'taking', 'dex', '##amp', '##het', '##amine', '##s', 'hello', ',', 'everyone', '.', 'i', 'had', 'been', 'lurking', 'here', 'previously', 'picking', 'up', 'on', 'your', 'stories', 'and', 'thoughts', '.', 'the', 'community', 'here', 'seems', 'very', 'supportive', '.', 'this', 'post', 'is', 'really', 'just', 'me', 'writing', 'down', 'some', 'thoughts', 'and', 'getting', 'some', 'support', '.', 'i', 'was', 'diagnosed', 'when', 'i', 'was', '25', 'and', 'refused', 'to', 'take', 'med', '##s', '.', 'since', 'then', 'i', 'have', 'mainly', 'worked', 'in', 'situations', 'where', 'my', 'work', 'was', 'organised', 'and', 'led', 'by', 'others', '.', 'i', 'am', 'now', 'working', 'in', 'a', 'director', 'role', 'where', 'i', 'need', 'to', 'organise', 'others', 'and', 'myself', '.', 'as', 'all', 'of', 'you', 'can', 'understand', 'it', 'isn', \"'\", 't', 'easy', '.', 'i', 'was', 'prescribed', 'some', 'dex', 'yesterday', 'and', 'took', 'my', 'first', '5', '##mg', 'at', '8', '##am', '.', 'it', \"'\", 's', 'now', '11', '##am', 'here', 'in', 'australia', 'and', 'things', 'are', 'going', 'well', '.', 'i', 'had', 'a', 'meeting', 'at', '9', '##am', 'and', 'actually', 'heard', 'everything', 'that', 'was', 'said', '.', 'besides', 'stopping', 'to', 'read', 'this', 'post', ',', 'i', \"'\", 've', 'gotten', 'a', 'lot', 'done', '.', 'context', '-', 'switching', 'is', 'also', 'a', 'bit', 'easier', '.', 'there', 'are', 'some', 'things', 'that', 'i', 'need', 'to', 'start', 'that', 'haven', \"'\", 't', 'quite', 'happened', 'yet', '.', '.', '.', 'hopefully', ',', 'i', \"'\", 'll', 'get', 'there', '.', 'thanks', 'for', 'reading', '!', '!']\n",
      "INFO:__main__:Number of tokens: 215\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'am', '38', 'and', 'just', 'started', 'taking', 'dex', '##amp', '##het', '##amine', '##s', 'hello', ',', 'everyone', '.', 'i', 'had', 'been', 'lurking', 'here', 'previously', 'picking', 'up', 'on', 'your', 'stories', 'and', 'thoughts', '.', 'the', 'community', 'here', 'seems', 'very', 'supportive', '.', 'this', 'post', 'is', 'really', 'just', 'me', 'writing', 'down', 'some', 'thoughts', 'and', 'getting', 'some', 'support', '.', 'i', 'was', 'diagnosed', 'when', 'i', 'was', '25', 'and', 'refused', 'to', 'take', 'med', '##s', '.', 'since', 'then', 'i', 'have', 'mainly', 'worked', 'in', 'situations', 'where', 'my', 'work', 'was', 'organised', 'and', 'led', 'by', 'others', '.', 'i', 'am', 'now', 'working', 'in', 'a', 'director', 'role', 'where', 'i', 'need', 'to', 'organise', 'others', 'and', 'myself', '.', 'as', 'all', 'of', 'you', 'can', 'understand', 'it', 'isn', \"'\", 't', 'easy', '.', 'i', 'was', 'prescribed', 'some', 'dex', 'yesterday', 'and', 'took', 'my', 'first', '5', '##mg', 'at', '8', '##am', '.', 'it', \"'\", 's', 'now', '11', '##am', 'here', 'in', 'australia', 'and', 'things', 'are', 'going', 'well', '.', 'i', 'had', 'a', 'meeting', 'at', '9', '##am', 'and', 'actually', 'heard', 'everything', 'that', 'was', 'said', '.', 'besides', 'stopping', 'to', 'read', 'this', 'post', ',', 'i', \"'\", 've', 'gotten', 'a', 'lot', 'done', '.', 'context', '-', 'switching', 'is', 'also', 'a', 'bit', 'easier', '.', 'there', 'are', 'some', 'things', 'that', 'i', 'need', 'to', 'start', 'that', 'haven', \"'\", 't', 'quite', 'happened', 'yet', '.', '.', '.', 'hopefully', ',', 'i', \"'\", 'll', 'get', 'there', '.', 'thanks', 'for', 'reading', '!', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', 'guys', 'struggle', 'with', 'motivation', '?', 'i', \"'\", 've', 'actually', 'yet', 'to', 'to', 'be', 'diagnosed', 'with', 'ad', '##hd', 'so', 'i', 'may', 'not', 'even', 'have', 'it', '.', 'but', 'this', 'is', 'the', 'sub', '##red', '##dit', 'where', 'i', 'feel', 'the', 'most', 'comfortable', ',', 'and', 'i', \"'\", 'll', 'be', 'undergoing', 'an', 'eva', '##l', 'next', 'week', 'to', 'see', 'if', 'i', 'actually', 'have', 'it', '.', 'anyway', ',', 'how', 'are', 'you', 'guys', 'with', 'motivation', '?', 'internally', ',', 'i', 'have', 'this', 'fire', 'burning', 'inside', 'of', 'me', 'to', 'really', 'do', 'things', 'but', 'i', \"'\", 've', 'gotten', 'to', 'a', 'point', 'where', 'i', 'will', 'literally', 'lay', 'around', 'all', 'day', 'and', 'not', 'do', 'a', 'thing', '.', 'i', \"'\", 'm', 'a', 'medical', 'student', 'and', 'i', 'have', 'a', 'neuroscience', 'test', 'tomorrow', 'that', 'i', 'haven', \"'\", 't', 'looked', 'at', 'a', 'single', 'page', 'of', 'information', 'for', '.', 'i', 'think', 'i', \"'\", 've', 'kinda', 'given', 'up', 'after', 'struggling', 'to', 'get', 'my', 'life', 'together', 'for', 'so', 'long', 'without', 'being', 'able', 'to', 'turn', 'it', 'around', '.', 'at', 'this', 'point', 'i', \"'\", 'm', 'actually', 'hoping', 'that', 'i', 'have', 'ad', '##hd', 'because', 'at', 'least', 'then', 'i', 'will', 'feel', 'like', 'there', 'is', 'a', 'light', 'at', 'the', 'end', 'of', 'this', 'tunnel', 'if', 'i', 'get', 'some', 'treatment', ',', 'because', 'right', 'now', 'there', 'isn', \"'\", 't', 'one', '.']\n",
      "INFO:__main__:Number of tokens: 204\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', 'guys', 'struggle', 'with', 'motivation', '?', 'i', \"'\", 've', 'actually', 'yet', 'to', 'to', 'be', 'diagnosed', 'with', 'ad', '##hd', 'so', 'i', 'may', 'not', 'even', 'have', 'it', '.', 'but', 'this', 'is', 'the', 'sub', '##red', '##dit', 'where', 'i', 'feel', 'the', 'most', 'comfortable', ',', 'and', 'i', \"'\", 'll', 'be', 'undergoing', 'an', 'eva', '##l', 'next', 'week', 'to', 'see', 'if', 'i', 'actually', 'have', 'it', '.', 'anyway', ',', 'how', 'are', 'you', 'guys', 'with', 'motivation', '?', 'internally', ',', 'i', 'have', 'this', 'fire', 'burning', 'inside', 'of', 'me', 'to', 'really', 'do', 'things', 'but', 'i', \"'\", 've', 'gotten', 'to', 'a', 'point', 'where', 'i', 'will', 'literally', 'lay', 'around', 'all', 'day', 'and', 'not', 'do', 'a', 'thing', '.', 'i', \"'\", 'm', 'a', 'medical', 'student', 'and', 'i', 'have', 'a', 'neuroscience', 'test', 'tomorrow', 'that', 'i', 'haven', \"'\", 't', 'looked', 'at', 'a', 'single', 'page', 'of', 'information', 'for', '.', 'i', 'think', 'i', \"'\", 've', 'kinda', 'given', 'up', 'after', 'struggling', 'to', 'get', 'my', 'life', 'together', 'for', 'so', 'long', 'without', 'being', 'able', 'to', 'turn', 'it', 'around', '.', 'at', 'this', 'point', 'i', \"'\", 'm', 'actually', 'hoping', 'that', 'i', 'have', 'ad', '##hd', 'because', 'at', 'least', 'then', 'i', 'will', 'feel', 'like', 'there', 'is', 'a', 'light', 'at', 'the', 'end', 'of', 'this', 'tunnel', 'if', 'i', 'get', 'some', 'treatment', ',', 'because', 'right', 'now', 'there', 'isn', \"'\", 't', 'one', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'long', 'does', 'it', 'usually', 'take', 'to', 'find', 'the', 'right', 'med', 'and', 'dos', '##age', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'long', 'does', 'it', 'usually', 'take', 'to', 'find', 'the', 'right', 'med', 'and', 'dos', '##age', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['after', 'two', 'failed', 'med', '##s', ',', 'i', 'just', 'got', 'put', 'on', 'add', '##eral', '##l', 'xl']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['after', 'two', 'failed', 'med', '##s', ',', 'i', 'just', 'got', 'put', 'on', 'add', '##eral', '##l', 'xl']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'know', 'that', 'your', 'ad', '##hd', 'treatment', 'is', 'really', 'working', 'for', 'you', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'know', 'that', 'your', 'ad', '##hd', 'treatment', 'is', 'really', 'working', 'for', 'you', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 'evaluated', 'for', 'add', '/', 'ad', '##hd', 'today', '.', 'what', 'should', 'i', 'expect', '?', 'any', 'tips', '/', 'suggestions', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 'evaluated', 'for', 'add', '/', 'ad', '##hd', 'today', '.', 'what', 'should', 'i', 'expect', '?', 'any', 'tips', '/', 'suggestions', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 're', '-', 'prescribed', 'ad', '##hd', 'medicine', '.', 'in', 'third', 'grade', 'i', 'was', 'diagnosed', 'ad', '##hd', 'and', 'prescribed', 'concert', '##a', '.', 'i', 'took', 'this', 'medication', 'until', '7th', 'grade', 'at', 'which', 'point', 'the', 'crash', 'it', 'gave', 'me', 'became', 'unbearable', '.', 'i', 'am', 'now', 'a', 'senior', 'in', 'high', 'school', 'about', 'to', 'go', 'to', 'college', 'in', 'the', 'fall', 'and', 'i', \"'\", 'm', 'very', 'interested', 'in', 'getting', 'prescribed', 'add', '##eral', '##l', 'or', 'dex', 'because', 'my', 'lack', 'of', 'motivation', 'has', 'become', 'ridiculous', 'and', 'i', 'want', 'to', 'make', 'sure', 'it', 'clears', 'up', 'for', 'my', 'first', 'year', 'of', 'college', '.', 'my', 'main', 'question', 'is', 'will', 'i', 'need', 'a', 're', '-', 'diagnosis', '?', 'how', 'do', 'i', 'go', 'about', 'getting', 'switched', 'to', 'add', '##eral', '##l', 'or', 'dex', 'rather', 'than', 'concert', '##a', '?', 'tips', 'or', 'rec', '##com', '##end', '##ations', '?']\n",
      "INFO:__main__:Number of tokens: 130\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 're', '-', 'prescribed', 'ad', '##hd', 'medicine', '.', 'in', 'third', 'grade', 'i', 'was', 'diagnosed', 'ad', '##hd', 'and', 'prescribed', 'concert', '##a', '.', 'i', 'took', 'this', 'medication', 'until', '7th', 'grade', 'at', 'which', 'point', 'the', 'crash', 'it', 'gave', 'me', 'became', 'unbearable', '.', 'i', 'am', 'now', 'a', 'senior', 'in', 'high', 'school', 'about', 'to', 'go', 'to', 'college', 'in', 'the', 'fall', 'and', 'i', \"'\", 'm', 'very', 'interested', 'in', 'getting', 'prescribed', 'add', '##eral', '##l', 'or', 'dex', 'because', 'my', 'lack', 'of', 'motivation', 'has', 'become', 'ridiculous', 'and', 'i', 'want', 'to', 'make', 'sure', 'it', 'clears', 'up', 'for', 'my', 'first', 'year', 'of', 'college', '.', 'my', 'main', 'question', 'is', 'will', 'i', 'need', 'a', 're', '-', 'diagnosis', '?', 'how', 'do', 'i', 'go', 'about', 'getting', 'switched', 'to', 'add', '##eral', '##l', 'or', 'dex', 'rather', 'than', 'concert', '##a', '?', 'tips', 'or', 'rec', '##com', '##end', '##ations', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'vs', 'just', 'being', 'an', 'idiot', 'so', 'i', \"'\", 've', 'always', 'joking', '##ly', 'considered', 'myself', 'having', 'ad', '##hd', '.', 'but', 'after', 'doing', 'some', 'more', 'research', 'on', 'the', 'matter', ',', 'i', \"'\", 'm', 'starting', 'to', 'really', 'convince', 'myself', 'that', 'i', 'do', 'have', 'it', '.', 'for', 'instance', ',', 'i', \"'\", 've', 'always', 'been', 'an', 'un', '##con', '##tro', '##lla', '##bly', 'hyper', '##active', 'child', ',', 'i', 'can', \"'\", 't', 'seem', 'to', 'control', 'myself', 'when', 'i', 'get', 'slightly', 'excited', 'or', 'am', 'in', 'a', 'comfortable', 'environment', '(', 'talk', 'too', 'loudly', 'and', 'too', 'fast', ')', ',', 'i', 'always', 'mis', '##pl', '##ace', 'things', ',', 'frequently', 'forget', 'about', 'meetings', '/', 'class', ',', 'and', 'lately', 'i', \"'\", 've', 'been', 'having', 'trouble', 'paying', 'attention', 'to', 'what', 'somebody', 'is', 'saying', 'when', 'they', 'are', 'talking', 'to', 'me', '(', 'my', 'thoughts', 'wonder', 'elsewhere', ')', '.', 'but', 'my', 'question', 'is', ',', 'where', 'do', 'you', 'draw', 'the', 'line', 'between', 'having', 'ad', '##hd', 'and', 'just', 'being', 'an', 'inc', '##omp', '##ete', '##nt', 'human', 'being', '?']\n",
      "INFO:__main__:Number of tokens: 158\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'vs', 'just', 'being', 'an', 'idiot', 'so', 'i', \"'\", 've', 'always', 'joking', '##ly', 'considered', 'myself', 'having', 'ad', '##hd', '.', 'but', 'after', 'doing', 'some', 'more', 'research', 'on', 'the', 'matter', ',', 'i', \"'\", 'm', 'starting', 'to', 'really', 'convince', 'myself', 'that', 'i', 'do', 'have', 'it', '.', 'for', 'instance', ',', 'i', \"'\", 've', 'always', 'been', 'an', 'un', '##con', '##tro', '##lla', '##bly', 'hyper', '##active', 'child', ',', 'i', 'can', \"'\", 't', 'seem', 'to', 'control', 'myself', 'when', 'i', 'get', 'slightly', 'excited', 'or', 'am', 'in', 'a', 'comfortable', 'environment', '(', 'talk', 'too', 'loudly', 'and', 'too', 'fast', ')', ',', 'i', 'always', 'mis', '##pl', '##ace', 'things', ',', 'frequently', 'forget', 'about', 'meetings', '/', 'class', ',', 'and', 'lately', 'i', \"'\", 've', 'been', 'having', 'trouble', 'paying', 'attention', 'to', 'what', 'somebody', 'is', 'saying', 'when', 'they', 'are', 'talking', 'to', 'me', '(', 'my', 'thoughts', 'wonder', 'elsewhere', ')', '.', 'but', 'my', 'question', 'is', ',', 'where', 'do', 'you', 'draw', 'the', 'line', 'between', 'having', 'ad', '##hd', 'and', 'just', 'being', 'an', 'inc', '##omp', '##ete', '##nt', 'human', 'being', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['attention', ':', 'vitamin', '##water', '’', 's', 'new', 'gateway', 'to', 'add', '##eral', '##l', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['attention', ':', 'vitamin', '##water', '’', 's', 'new', 'gateway', 'to', 'add', '##eral', '##l', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['follow', 'up', ':', 'just', 'had', 'my', 'appointment', '.', 'i', 'was', 'diagnosed', 'with', 'add', 'and', 'given', 'a', 'prescription', '.', 'question', 'though', '.']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['follow', 'up', ':', 'just', 'had', 'my', 'appointment', '.', 'i', 'was', 'diagnosed', 'with', 'add', 'and', 'given', 'a', 'prescription', '.', 'question', 'though', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['as', 'we', 'near', '2000', 'subscription', '##s', '.', '.', '.', 'how', 'has', '/', 'r', '/', 'ad', '##hd', 'helped', 'you', '?', 'how', 'could', 'we', 'make', 'it', 'even', 'better', '?', '(', 'stats', 'inside', ')', 'since', 'the', 'merger', 'with', '/', 'r', '/', 'add', ',', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'growth', 'has', 'sky', '##rock', '##ete', '##d', '.', 'we', 'have', '*', '*', 'added', 'over', '1', '##k', 'members', '*', '*', '(', '3', 'months', ')', 'and', 'the', 'threads', 'almost', 'always', 'have', 'a', 'few', 'comments', '.', 'i', 'am', 'interested', 'to', 'hear', 'from', 'the', 'community', 'on', 'what', 'they', 'have', 'learned', 'about', 'themselves', 'and', 'ad', '##hd', '.', '.', '.', 'along', 'with', 'what', 'they', 'would', 'like', 'to', 'see', 'happen', 'in', 'the', 'future', '.', 'with', 'the', 'increase', 'we', 'have', 'seen', 'a', 'lot', 'of', '*', '*', '\"', 'da', '##e', 'feel', 'x', 'while', 'on', 'add', '##eral', '##l', '\"', ',', \"'\", 'here', 'is', 'my', \"'\", 'unique', \"'\", 'life', '-', 'story', ',', 'do', 'i', 'have', 'add', '?', \"'\", ',', 'and', '\"', 'my', 'life', 'with', 'ad', '##hd', 'is', 'crazy', '.', 'how', 'can', 'i', 'fix', 'it', '?', '\"', '.', '*', '*', 'our', 'ad', '##hd', 'attention', 'is', 'precious', 'and', 'this', 'cl', '##utter', 'has', 'kept', 'me', '(', 'and', 'possibly', 'others', ')', 'away', 'due', 'to', 'over', '##w', '##helm', '.', 'many', 'threads', 'could', 'be', 'eliminated', 'with', 'a', 'quick', 'goo', '##gling', 'before', 'posting', '.', 'to', 'fix', 'this', '*', '*', 'we', 'created', 'a', 'fa', '##q', '*', '*', '(', 'which', 'you', 'can', 'contribute', 'to', '.', 'linked', 'below', '.', ')', '.', 'next', 'step', 'might', 'be', 'to', 'put', 'some', '*', '*', 'warnings', '/', 'information', 'on', 'the', 'link', 'submission', 'page', '*', '*', '.', 'i', 'have', 'also', 'thought', 'about', 'starting', 'some', '*', '*', 'categories', '*', '*', 'which', 'are', 'denoted', 'by', 'text', 'tags', '[', 'da', '##e', ']', 'and', 'converted', 'into', 'different', 'colors', 'or', 'have', 'a', 'small', 'picture', 'at', 'the', 'beginning', '/', 'end', 'to', 'provide', 'a', '*', '*', 'visual', 'distinction', '*', '*', 'between', 'posts', 'about', 'medication', 'and', 'those', 'about', 'positive', 'tips', '.', 'enough', 'talking', 'for', 'now', '.', 'onto', 'the', 'changes', 'and', 'stats', '*', '*', '*', '*', '*', 'recent', 'changes', 'by', 'mod', '##s', '*', '*', ':', '*', 'we', 'have', '*', '*', 'updated', 'the', 'side', '##bar', '.', '*', '*', '*', '*', '*', 'fa', '##q', '*', '*', '-', 'we', 'posted', 'our', '*', 'alpha', '*', '[', 'ad', '##hd', 'fa', '##q', ']', '(', 'http', ':', '/', '/', 'code', '.', 'red', '##dit', '.', 'com', '/', 'wi', '##ki', '/', 'help', '/', 'fa', '##q', '##s', '/', 'ad', '##hd', ')', 'to', 'hopefully', 'cut', 'back', 'on', 'common', 'repeating', 'threads', '.', '*', '*', '*', 'added', 'flair', '!', '*', '*', 'on', 'the', 'right', 'side', 'under', 'the', 'sub', '##scribe', 'link', 'you', 'can', 'click', 'edit', '(', 'next', 'to', 'your', 'user', '##name', ')', 'to', 'select', 'your', 'ad', '##hd', 'type', '*', 'ad', '##hd', '-', 'pi', '(', 'primarily', 'ina', '##tten', '##tive', ')', ',', 'ad', '##hd', '-', 'c', '(', 'combined', ')', ',', 'ad', '##hd', '-', 'ph', '(', 'hyper', '##active', ')', ',', 'parent', 'or', 'ad', '##hd', 'and', 'parent', '*', '.', '.', 'we', 'may', 'also', 'add', 'more', 'flair', 'if', 'wanted', '.', 'i', 'like', 'custom', 'flair', 'because', 'it', 'lets', 'me', 'quickly', 'identify', 'certain', 'users', 'in', 'a', 'thread', '(', 'like', 'res', 'tag', '##ging', ')', '.', '*', 'the', '*', '*', 'cs', '##s', '*', '*', 'has', 'been', 'slightly', 'modified', 'to', 'give', 'more', 'spa', '##cing', '.', '*', '*', 'if', 'any', 'of', 'you', 'have', 'cs', '##s', 'experience', 'and', 'want', 'to', 'help', 'out', '.', '.', '.', 'please', 'volunteer', '!', '*', '*', '*', '*', '*', 'ad', '##hd', 'chat', '*', '*', '-', 'we', 'have', 'an', '[', 'ir', '##c', 'channel', ']', '(', 'http', ':', '/', '/', 'web', '##cha', '##t', '.', 'free', '##no', '##de', '.', 'net', '/', '?', 'channels', '=', '/', 'r', '/', 'ad', '##hd', ')', 'to', 'chat', 'about', 'ad', '##hd', '*', '*', '*', 'message', 'mod', '##s', '*', '*', 'if', 'you', 'ever', 'have', 'any', 'questions', ',', 'a', 'post', 'gets', '*', '*', 'stuck', 'in', 'the', 'spa', '##m', 'filter', '*', '*', '(', 'doesn', \"'\", 't', 'appear', 'immediately', ')', ',', 'or', 'just', 'want', 'to', 'give', 'us', 'a', '*', 'internet', 'high', '-', 'five', '*', 'you', 'can', 'click', '*', '*', '[', 'message', 'the', 'moderator', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '?', 'to', '=', '%', '2', '##fr', '%', '2', '##fa', '##dh', '##d', ')', '*', '*', 'below', 'the', 'ad', 'to', 'the', 'right', '.', '*', '*', '*', '*', '*', 'stats', ':', '*', '*', '*', '*', '*', '800', 'members', '*', '*', 'have', 'joined', 'in', 'the', 'past', '2', 'months', '*', 'we', 'are', 'getting', '*', '*', '~', '15', 'subscription', '##s', 'a', 'day', '*', '*', '*', '*', '*', '~', '500', 'unique', 'visitors', '*', '*', 'daily', '(', 'our', 'peak', 'is', '95', '##2', ')', 'making', '*', '*', '2000', 'impressions', '*', '*', '(', 'page', '-', 'views', ')', '*', 'there', 'are', 'no', 'easy', 'stats', 'about', 'comments', 'and', 'posts', 'but', 'those', 'have', 'increased', 'tremendous', '##ly', '.', '*', '*', 'keep', 'the', 'discussion', 'flowing', '!', '*', '*', '*', '*', '*', 'my', '2', 'cents', '(', 'i', 'do', 'not', 'speak', 'for', 'all', 'moderator', '##s', ')', ':', '*', '*', '*', 'always', 'keep', 'things', 'positive', '!', '*', '*', 'i', 'have', 'had', 'angry', 'replies', 'to', 'comments', 'because', 'my', 'opinion', 'was', 'different', '.', 'even', 'if', 'someone', 'makes', 'an', 'outrageous', 'claim', '(', '*', '*', '*', 'all', 'add', '##eral', '##l', 'users', 'are', 'drug', 'addict', '##s', '!', '*', '*', '*', ')', ',', 'please', 'respond', 'in', 'a', 'civil', 'manner', '.', '*', '*', '*', 'if', 'they', 'were', 'standing', 'next', 'to', 'you', 'would', 'you', 'say', 'that', '?', '*', '*', '*', '*', '*', '*', 'up', '##vot', '##e', '!', '*', '*', 'the', 'more', 'you', 'up', '##vot', '##e', ',', 'the', 'more', 'people', 'see', 'our', 'popular', 'threads', 'on', 'their', 'front', 'page', '(', 'and', 'it', 'will', 'stick', 'up', 'higher', 'longer', 'on', '/', 'r', '/', 'ad', '##hd', ')', '*', '*', '*', 'report', 'malicious', 'comments', 'or', 'spa', '##m', '.', '*', '*', '-', 'if', 'someone', 'is', 'being', 'a', 'dick', 'let', 'us', 'know', '.', '*', '*', '*', 'ad', '##hd', '-', 'friendly', 'text', '*', '*', '-', 'use', 'the', 'limited', 'text', 'options', '(', 'bullets', ',', '*', '*', 'bold', '*', '*', ',', '*', '*', '*', 'it', '##alic', '*', '*', '*', ')', 'to', 'make', 'your', 'posts', '/', 'comments', '*', '*', 'ad', '##hd', 'friendly', '*', '*', '.', 'i', '*', '*', 'strongly', '*', '*', 'suggest', '[', 'red', '##dit', 'enhancement', 'suite', ']', '(', 'http', ':', '/', '/', 'red', '##dit', '##en', '##han', '##ce', '##ments', '##uit', '##e', '.', 'com', ')', 'to', 'give', 'a', 'live', 'preview', 'and', 'make', 'commenting', 'easier', '.', 'my', 'guess', 'is', 'people', 'with', 'ad', '##hd', 'find', 'it', 'hard', 'to', 'memo', '##rize', 'all', 'these', 'rules', '.', '*', 'include', 'a', '*', '*', 't', '##ld', '##r', '*', '*', 'if', 'your', 'post', 'is', 'a', 'long', 'one', '.', '(', 'for', 'obvious', 'reasons', ')', 'that', \"'\", 's', 'it', '!', 'thank', 'you', 'for', 'all', 'your', 'effort', '!', 'i', 'know', 'how', 'hard', 'it', 'can', 'be', 'to', 'respond', 'to', 'someone', 'in', 'need', 'at', '8', '##pm', 'while', 'un', '##med', '##icated', 'in', 'a', 'thread', '.', 'hope', 'to', 'hear', 'from', 'you', 'all', 'about', 'your', 'experience', 'with', '/', 'r', '/', 'ad', '##hd', '!', '*', '*', '*', '*', '*', 't', '##ld', '##r', ':', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'is', 'bigger', '.', 'mod', '##s', 'have', 'been', 'working', 'to', 'improve', 'your', 'experience', '.', 'read', 'the', 'side', '##bar', 'and', '[', 'fa', '##q', ']', '(', 'http', ':', '/', '/', 'code', '.', 'red', '##dit', '.', 'com', '/', 'wi', '##ki', '/', 'help', '/', 'fa', '##q', '##s', '/', 'ad', '##hd', ')', '.', 'don', \"'\", 't', 'be', 'a', 'dick', '.', 'tell', 'us', 'how', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'has', 'helped', 'you', 'and', 'how', 'we', 'can', 'improve', 'it', '!', '*', '*', '*', '*', '*', '*', '*', 'edit', ':', 'i', 'have', 'been', 'thinking', 'what', 'to', 'call', 'the', 'members', 'of', 'this', 'group', '.', '[', '/', 'r', '/', 'trees', ']', '(', '/', 'r', '/', 'trees', ')', 'has', 'en', '##ts', '.', 'what', 'could', 'be', 'a', 'name', 'for', 'our', 'community', 'here', '?', 'i', 'want', 'to', 'change', 'the', '1917', '*', 'readers', '*', 'on', 'the', 'side', '##bar', 'to', 'something', 'else', '.', '.', '.', 'any', 'creative', 'ideas', '?', '*', '*', 'edit', '2', ':', 'i', 'hear', 'the', 'call', 'for', 'more', 'scientific', 'articles', '.', 'i', 'will', 'dig', 'some', 'up', 'from', 'my', 'ever', '##note', 'collection', 'and', 'post', '.']\n",
      "INFO:__main__:Number of tokens: 1322\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['as', 'we', 'near', '2000', 'subscription', '##s', '.', '.', '.', 'how', 'has', '/', 'r', '/', 'ad', '##hd', 'helped', 'you', '?', 'how', 'could', 'we', 'make', 'it', 'even', 'better', '?', '(', 'stats', 'inside', ')', 'since', 'the', 'merger', 'with', '/', 'r', '/', 'add', ',', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'growth', 'has', 'sky', '##rock', '##ete', '##d', '.', 'we', 'have', '*', '*', 'added', 'over', '1', '##k', 'members', '*', '*', '(', '3', 'months', ')', 'and', 'the', 'threads', 'almost', 'always', 'have', 'a', 'few', 'comments', '.', 'i', 'am', 'interested', 'to', 'hear', 'from', 'the', 'community', 'on', 'what', 'they', 'have', 'learned', 'about', 'themselves', 'and', 'ad', '##hd', '.', '.', '.', 'along', 'with', 'what', 'they', 'would', 'like', 'to', 'see', 'happen', 'in', 'the', 'future', '.', 'with', 'the', 'increase', 'we', 'have', 'seen', 'a', 'lot', 'of', '*', '*', '\"', 'da', '##e', 'feel', 'x', 'while', 'on', 'add', '##eral', '##l', '\"', ',', \"'\", 'here', 'is', 'my', \"'\", 'unique', \"'\", 'life', '-', 'story', ',', 'do', 'i', 'have', 'add', '?', \"'\", ',', 'and', '\"', 'my', 'life', 'with', 'ad', '##hd', 'is', 'crazy', '.', 'how', 'can', 'i', 'fix', 'it', '?', '\"', '.', '*', '*', 'our', 'ad', '##hd', 'attention', 'is', 'precious', 'and', 'this', 'cl', '##utter', 'has', 'kept', 'me', '(', 'and', 'possibly', 'others', ')', 'away', 'due', 'to', 'over', '##w', '##helm', '.', 'many', 'threads', 'could', 'be', 'eliminated', 'with', 'a', 'quick', 'goo', '##gling', 'before', 'posting', '.', 'to', 'fix', 'this', '*', '*', 'we', 'created', 'a', 'fa', '##q', '*', '*', '(', 'which', 'you', 'can', 'contribute', 'to', '.', 'linked', 'below', '.', ')', '.', 'next', 'step', 'might', 'be', 'to', 'put', 'some', '*', '*', 'warnings', '/', 'information', 'on', 'the', 'link', 'submission', 'page', '*', '*', '.', 'i', 'have', 'also', 'thought', 'about', 'starting', 'some', '*', '*', 'categories', '*', '*', 'which', 'are', 'denoted', 'by', 'text', 'tags', '[', 'da', '##e', ']', 'and', 'converted', 'into', 'different', 'colors', 'or', 'have', 'a', 'small', 'picture', 'at', 'the', 'beginning', '/', 'end', 'to', 'provide', 'a', '*', '*', 'visual', 'distinction', '*', '*', 'between', 'posts', 'about', 'medication', 'and', 'those', 'about', 'positive', 'tips', '.', 'enough', 'talking', 'for', 'now', '.', 'onto', 'the', 'changes', 'and', 'stats', '*', '*', '*', '*', '*', 'recent', 'changes', 'by', 'mod', '##s', '*', '*', ':', '*', 'we', 'have', '*', '*', 'updated', 'the', 'side', '##bar', '.', '*', '*', '*', '*', '*', 'fa', '##q', '*', '*', '-', 'we', 'posted', 'our', '*', 'alpha', '*', '[', 'ad', '##hd', 'fa', '##q', ']', '(', 'http', ':', '/', '/', 'code', '.', 'red', '##dit', '.', 'com', '/', 'wi', '##ki', '/', 'help', '/', 'fa', '##q', '##s', '/', 'ad', '##hd', ')', 'to', 'hopefully', 'cut', 'back', 'on', 'common', 'repeating', 'threads', '.', '*', '*', '*', 'added', 'flair', '!', '*', '*', 'on', 'the', 'right', 'side', 'under', 'the', 'sub', '##scribe', 'link', 'you', 'can', 'click', 'edit', '(', 'next', 'to', 'your', 'user', '##name', ')', 'to', 'select', 'your', 'ad', '##hd', 'type', '*', 'ad', '##hd', '-', 'pi', '(', 'primarily', 'ina', '##tten', '##tive', ')', ',', 'ad', '##hd', '-', 'c', '(', 'combined', ')', ',', 'ad', '##hd', '-', 'ph', '(', 'hyper', '##active', ')', ',', 'parent', 'or', 'ad', '##hd', 'and', 'parent', '*', '.', '.', 'we', 'may', 'also', 'add', 'more', 'flair', 'if', 'wanted', '.', 'i', 'like', 'custom', 'flair', 'because', 'it', 'lets', 'me', 'quickly', 'identify', 'certain', 'users', 'in', 'a', 'thread', '(', 'like', 'res', 'tag', '##ging', ')', '.', '*', 'the'], ['*', '*', 'cs', '##s', '*', '*', 'has', 'been', 'slightly', 'modified', 'to', 'give', 'more', 'spa', '##cing', '.', '*', '*', 'if', 'any', 'of', 'you', 'have', 'cs', '##s', 'experience', 'and', 'want', 'to', 'help', 'out', '.', '.', '.', 'please', 'volunteer', '!', '*', '*', '*', '*', '*', 'ad', '##hd', 'chat', '*', '*', '-', 'we', 'have', 'an', '[', 'ir', '##c', 'channel', ']', '(', 'http', ':', '/', '/', 'web', '##cha', '##t', '.', 'free', '##no', '##de', '.', 'net', '/', '?', 'channels', '=', '/', 'r', '/', 'ad', '##hd', ')', 'to', 'chat', 'about', 'ad', '##hd', '*', '*', '*', 'message', 'mod', '##s', '*', '*', 'if', 'you', 'ever', 'have', 'any', 'questions', ',', 'a', 'post', 'gets', '*', '*', 'stuck', 'in', 'the', 'spa', '##m', 'filter', '*', '*', '(', 'doesn', \"'\", 't', 'appear', 'immediately', ')', ',', 'or', 'just', 'want', 'to', 'give', 'us', 'a', '*', 'internet', 'high', '-', 'five', '*', 'you', 'can', 'click', '*', '*', '[', 'message', 'the', 'moderator', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '?', 'to', '=', '%', '2', '##fr', '%', '2', '##fa', '##dh', '##d', ')', '*', '*', 'below', 'the', 'ad', 'to', 'the', 'right', '.', '*', '*', '*', '*', '*', 'stats', ':', '*', '*', '*', '*', '*', '800', 'members', '*', '*', 'have', 'joined', 'in', 'the', 'past', '2', 'months', '*', 'we', 'are', 'getting', '*', '*', '~', '15', 'subscription', '##s', 'a', 'day', '*', '*', '*', '*', '*', '~', '500', 'unique', 'visitors', '*', '*', 'daily', '(', 'our', 'peak', 'is', '95', '##2', ')', 'making', '*', '*', '2000', 'impressions', '*', '*', '(', 'page', '-', 'views', ')', '*', 'there', 'are', 'no', 'easy', 'stats', 'about', 'comments', 'and', 'posts', 'but', 'those', 'have', 'increased', 'tremendous', '##ly', '.', '*', '*', 'keep', 'the', 'discussion', 'flowing', '!', '*', '*', '*', '*', '*', 'my', '2', 'cents', '(', 'i', 'do', 'not', 'speak', 'for', 'all', 'moderator', '##s', ')', ':', '*', '*', '*', 'always', 'keep', 'things', 'positive', '!', '*', '*', 'i', 'have', 'had', 'angry', 'replies', 'to', 'comments', 'because', 'my', 'opinion', 'was', 'different', '.', 'even', 'if', 'someone', 'makes', 'an', 'outrageous', 'claim', '(', '*', '*', '*', 'all', 'add', '##eral', '##l', 'users', 'are', 'drug', 'addict', '##s', '!', '*', '*', '*', ')', ',', 'please', 'respond', 'in', 'a', 'civil', 'manner', '.', '*', '*', '*', 'if', 'they', 'were', 'standing', 'next', 'to', 'you', 'would', 'you', 'say', 'that', '?', '*', '*', '*', '*', '*', '*', 'up', '##vot', '##e', '!', '*', '*', 'the', 'more', 'you', 'up', '##vot', '##e', ',', 'the', 'more', 'people', 'see', 'our', 'popular', 'threads', 'on', 'their', 'front', 'page', '(', 'and', 'it', 'will', 'stick', 'up', 'higher', 'longer', 'on', '/', 'r', '/', 'ad', '##hd', ')', '*', '*', '*', 'report', 'malicious', 'comments', 'or', 'spa', '##m', '.', '*', '*', '-', 'if', 'someone', 'is', 'being', 'a', 'dick', 'let', 'us', 'know', '.', '*', '*', '*', 'ad', '##hd', '-', 'friendly', 'text', '*', '*', '-', 'use', 'the', 'limited', 'text', 'options', '(', 'bullets', ',', '*', '*', 'bold', '*', '*', ',', '*', '*', '*', 'it', '##alic', '*', '*', '*', ')', 'to', 'make', 'your', 'posts', '/', 'comments', '*', '*', 'ad', '##hd', 'friendly', '*', '*', '.', 'i', '*', '*', 'strongly', '*', '*', 'suggest', '[', 'red', '##dit', 'enhancement', 'suite', ']', '(', 'http', ':', '/', '/', 'red', '##dit', '##en', '##han', '##ce', '##ments', '##uit', '##e', '.', 'com', ')', 'to', 'give', 'a', 'live', 'preview', 'and'], ['make', 'commenting', 'easier', '.', 'my', 'guess', 'is', 'people', 'with', 'ad', '##hd', 'find', 'it', 'hard', 'to', 'memo', '##rize', 'all', 'these', 'rules', '.', '*', 'include', 'a', '*', '*', 't', '##ld', '##r', '*', '*', 'if', 'your', 'post', 'is', 'a', 'long', 'one', '.', '(', 'for', 'obvious', 'reasons', ')', 'that', \"'\", 's', 'it', '!', 'thank', 'you', 'for', 'all', 'your', 'effort', '!', 'i', 'know', 'how', 'hard', 'it', 'can', 'be', 'to', 'respond', 'to', 'someone', 'in', 'need', 'at', '8', '##pm', 'while', 'un', '##med', '##icated', 'in', 'a', 'thread', '.', 'hope', 'to', 'hear', 'from', 'you', 'all', 'about', 'your', 'experience', 'with', '/', 'r', '/', 'ad', '##hd', '!', '*', '*', '*', '*', '*', 't', '##ld', '##r', ':', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'is', 'bigger', '.', 'mod', '##s', 'have', 'been', 'working', 'to', 'improve', 'your', 'experience', '.', 'read', 'the', 'side', '##bar', 'and', '[', 'fa', '##q', ']', '(', 'http', ':', '/', '/', 'code', '.', 'red', '##dit', '.', 'com', '/', 'wi', '##ki', '/', 'help', '/', 'fa', '##q', '##s', '/', 'ad', '##hd', ')', '.', 'don', \"'\", 't', 'be', 'a', 'dick', '.', 'tell', 'us', 'how', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'has', 'helped', 'you', 'and', 'how', 'we', 'can', 'improve', 'it', '!', '*', '*', '*', '*', '*', '*', '*', 'edit', ':', 'i', 'have', 'been', 'thinking', 'what', 'to', 'call', 'the', 'members', 'of', 'this', 'group', '.', '[', '/', 'r', '/', 'trees', ']', '(', '/', 'r', '/', 'trees', ')', 'has', 'en', '##ts', '.', 'what', 'could', 'be', 'a', 'name', 'for', 'our', 'community', 'here', '?', 'i', 'want', 'to', 'change', 'the', '1917', '*', 'readers', '*', 'on', 'the', 'side', '##bar', 'to', 'something', 'else', '.', '.', '.', 'any', 'creative', 'ideas', '?', '*', '*', 'edit', '2', ':', 'i', 'hear', 'the', 'call', 'for', 'more', 'scientific', 'articles', '.', 'i', 'will', 'dig', 'some', 'up', 'from', 'my', 'ever', '##note', 'collection', 'and', 'post', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['avoiding', 'things', '/', 'life', 'i', 'find', 'myself', 'avoiding', 'stuff', 'all', 'of', 'the', 'time', '(', 'especially', 'emails', 'about', 'things', 'i', 'don', \"'\", 't', 'want', 'to', 'deal', 'with', ')', ',', 'and', 'the', 'best', 'way', 'to', 'deal', 'with', 'it', 'is', 'have', 'my', 'best', 'friend', '(', 'who', ',', 'right', 'now', ',', 'lives', 'next', 'door', 'to', 'me', ')', 'sit', 'down', 'and', 'help', 'me', 'get', 'started', 'handling', 'it', 'all', 'after', 'a', 'few', 'days', 'of', 'hiding', 'from', 'it', '.', 'she', \"'\", 's', 'not', 'always', 'going', 'to', 'be', 'there', ',', 'as', 'we', 'graduate', 'next', 'year', '.', 'what', 'do', 'you', 'guys', 'do', 'when', 'you', 'start', 'to', 'feel', 'the', 'things', 'you', 'push', 'away', 'and', 'pro', '##cr', '##ast', '##inate', 'on', 'creeping', 'towards', 'you', '?', 'i', 'struggle', 'most', 'with', 'things', 'that', 'are', 'large', ',', 'da', '##unt', '##ing', ',', 'and', 'don', \"'\", 't', 'have', 'an', 'immediate', 'deadline', '.', 'would', 'love', 'to', 'hear', 'what', 'you', 'guys', 'do', 'to', 'deal', 'with', 'this', '!', 'i', \"'\", 'd', 'say', 'this', 'is', 'pro', '##cr', '##ast', '##ination', ',', 'but', 'somehow', 'it', 'feels', 'different', '.', '.', '.', 'maybe', 'it', \"'\", 's', 'the', 'same', '.', 'just', 'as', 'a', 'heads', 'up', ',', 'dealing', 'with', 'things', 'as', 'they', 'come', 'to', 'you', 'probably', 'wouldn', \"'\", 't', 'work', 'for', 'me', '.', 'i', 'understand', 'it', \"'\", 's', 'effective', 'for', 'quick', 'tasks', ',', 'but', 'i', 'don', \"'\", 't', 'think', 'it', \"'\", 's', 'wise', 'because', 'i', 'simply', 'can', \"'\", 't', 'drop', 'what', 'i', \"'\", 'm', 'doing', 'to', 'fulfill', 'the', 'request', 'of', 'an', 'email', '(', 'if', 'it', \"'\", 's', 'a', 'large', 'task', ')', 'most', 'of', 'the', 'time', '.', 't', '##l', ';', 'dr', ':', 'how', 'do', 'you', 'stop', 'yourself', 'from', 'avoiding', 'the', 'things', 'you', 'don', \"'\", 't', 'want', 'to', 'deal', 'with', 'in', 'life', 'and', '/', 'or', 'how', 'do', 'you', 'prevent', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 282\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['avoiding', 'things', '/', 'life', 'i', 'find', 'myself', 'avoiding', 'stuff', 'all', 'of', 'the', 'time', '(', 'especially', 'emails', 'about', 'things', 'i', 'don', \"'\", 't', 'want', 'to', 'deal', 'with', ')', ',', 'and', 'the', 'best', 'way', 'to', 'deal', 'with', 'it', 'is', 'have', 'my', 'best', 'friend', '(', 'who', ',', 'right', 'now', ',', 'lives', 'next', 'door', 'to', 'me', ')', 'sit', 'down', 'and', 'help', 'me', 'get', 'started', 'handling', 'it', 'all', 'after', 'a', 'few', 'days', 'of', 'hiding', 'from', 'it', '.', 'she', \"'\", 's', 'not', 'always', 'going', 'to', 'be', 'there', ',', 'as', 'we', 'graduate', 'next', 'year', '.', 'what', 'do', 'you', 'guys', 'do', 'when', 'you', 'start', 'to', 'feel', 'the', 'things', 'you', 'push', 'away', 'and', 'pro', '##cr', '##ast', '##inate', 'on', 'creeping', 'towards', 'you', '?', 'i', 'struggle', 'most', 'with', 'things', 'that', 'are', 'large', ',', 'da', '##unt', '##ing', ',', 'and', 'don', \"'\", 't', 'have', 'an', 'immediate', 'deadline', '.', 'would', 'love', 'to', 'hear', 'what', 'you', 'guys', 'do', 'to', 'deal', 'with', 'this', '!', 'i', \"'\", 'd', 'say', 'this', 'is', 'pro', '##cr', '##ast', '##ination', ',', 'but', 'somehow', 'it', 'feels', 'different', '.', '.', '.', 'maybe', 'it', \"'\", 's', 'the', 'same', '.', 'just', 'as', 'a', 'heads', 'up', ',', 'dealing', 'with', 'things', 'as', 'they', 'come', 'to', 'you', 'probably', 'wouldn', \"'\", 't', 'work', 'for', 'me', '.', 'i', 'understand', 'it', \"'\", 's', 'effective', 'for', 'quick', 'tasks', ',', 'but', 'i', 'don', \"'\", 't', 'think', 'it', \"'\", 's', 'wise', 'because', 'i', 'simply', 'can', \"'\", 't', 'drop', 'what', 'i', \"'\", 'm', 'doing', 'to', 'fulfill', 'the', 'request', 'of', 'an', 'email', '(', 'if', 'it', \"'\", 's', 'a', 'large', 'task', ')', 'most', 'of', 'the', 'time', '.', 't', '##l', ';', 'dr', ':', 'how', 'do', 'you', 'stop', 'yourself', 'from', 'avoiding', 'the', 'things', 'you', 'don', \"'\", 't', 'want', 'to', 'deal', 'with', 'in', 'life', 'and', '/', 'or', 'how', 'do', 'you', 'prevent', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', 'question', 'regarding', 'the', 'difference', 'between', 'a', 'person', 'with', 'add', 'and', 'person', 'without', 'add', 'i', 'was', 'wondering', 'if', 'there', 'is', 'any', 'information', 'about', 'the', 'advantages', 'and', 'disadvantage', '##s', 'of', 'people', 'with', 'add', 'compared', 'to', 'people', 'without', 'it', '.', 'specifically', ',', 'if', 'a', 'person', 'with', 'add', 'is', 'on', 'medication', '(', 'for', 'example', ',', 'i', 'take', 'concert', '##a', ')', 'do', 'they', 'have', 'a', 'learning', 'advantage', 'or', 'otherwise', 'over', 'a', 'person', 'without', 'add', '?']\n",
      "INFO:__main__:Number of tokens: 71\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', 'question', 'regarding', 'the', 'difference', 'between', 'a', 'person', 'with', 'add', 'and', 'person', 'without', 'add', 'i', 'was', 'wondering', 'if', 'there', 'is', 'any', 'information', 'about', 'the', 'advantages', 'and', 'disadvantage', '##s', 'of', 'people', 'with', 'add', 'compared', 'to', 'people', 'without', 'it', '.', 'specifically', ',', 'if', 'a', 'person', 'with', 'add', 'is', 'on', 'medication', '(', 'for', 'example', ',', 'i', 'take', 'concert', '##a', ')', 'do', 'they', 'have', 'a', 'learning', 'advantage', 'or', 'otherwise', 'over', 'a', 'person', 'without', 'add', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'curious', ',', 'does', 'anyone', 'here', 'have', 'something', 'besides', 'their', 'ad', '##hd', '?', 'i', \"'\", 've', 'heard', 'many', 'people', 'have', 'multiple', 'disorders', ',', 'and', 'i', \"'\", 'm', 'not', 'exception', 'to', 'that', '.', 'first', 'of', 'all', ',', 'i', 'have', 'ad', '##hd', '-', 'pi', ',', 'and', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'it', 'as', 'long', 'as', 'i', 'can', 'remember', '.', 'you', \"'\", 've', 'probably', 'figured', 'that', 'out', 'since', 'this', 'is', 'the', 'ad', '##hd', 'sub', '##red', '##dit', '.', 'now', ',', 'i', \"'\", 've', 'also', 'been', 'diagnosed', 'with', 'auditory', 'processing', 'disorder', '(', 'ap', '##d', ')', 'and', 'd', '##ys', '##graph', '##ia', '.', 'for', 'those', 'who', 'don', \"'\", 't', 'know', 'ap', '##d', 'is', 'pretty', 'similar', 'to', 'ad', '##hd', 'in', 'some', 'ways', '.', 'i', \"'\", 'm', 'not', 'completely', 'sure', 'about', 'it', ',', 'but', 'my', 'ad', '##hd', 'may', 'in', 'fact', 'be', 'the', 'reason', 'i', 'have', 'it', '.', 'for', 'those', 'who', 'want', 'more', 'info', 'on', 'it', ',', 'just', 'look', '[', 'here', '.', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'auditory', '_', 'processing', '_', 'disorder', ')', 'as', 'for', 'd', '##ys', '##graph', '##ia', ',', 'it', \"'\", 's', 'basically', 'means', 'i', 'can', \"'\", 't', 'draw', 'a', 'straight', 'line', 'if', 'my', 'life', 'depended', 'on', 'it', '.', 'those', 'who', 'want', 'to', 'read', 'about', 'it', 'can', 'do', 'so', '[', 'here', '.', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'd', '##ys', '##graph', '##ia', ')', 'when', 'i', 'was', 'younger', ',', 'i', 'also', 'had', 'a', 'speech', 'delay', '.', 'basically', ',', 'i', 'wasn', \"'\", 't', 'speaking', 'by', 'the', 'time', 'i', 'should', 'have', 'been', '(', 'i', 'believe', 'i', 'was', '2', 'and', 'still', 'couldn', \"'\", 't', 'really', 'speak', ')', ',', 'so', 'my', 'parents', 'got', 'speech', 'therapy', 'for', 'me', '.', 'it', 'did', 'work', ',', 'but', 'i', 'still', 'have', 'some', 'speech', 'problems', ',', 'like', 'an', 'unusual', 'voice', '(', 'if', 'you', 'want', 'to', 'hear', 'me', 'speak', 'i', 'guess', 'i', 'could', 'up', '##load', 'a', 'clip', ')', ',', 'and', 'occasional', 'stu', '##ttering', 'and', 'what', '##not', '.', 'also', ',', 'i', \"'\", 'm', 'not', 'sure', 'if', 'it', \"'\", 's', 'because', 'of', 'the', 'ad', '##hd', ',', 'the', 'speech', 'delay', ',', 'or', 'just', 'something', 'else', '(', 'or', 'a', 'combo', 'of', 'th', '##ses', ')', ',', 'but', 'my', 'brain', 'and', 'mouth', 'don', \"'\", 't', 'have', 'the', 'best', 'connection', '.', 'i', 'often', 'have', 'problems', 'putting', 'my', 'thoughts', 'into', 'words', ',', 'and', 'when', 'i', 'speak', 'i', 'sometimes', 'mess', 'up', 'what', 'i', 'want', 'to', 'say', '.', 'this', 'doesn', \"'\", 't', 'stop', 'with', 'speaking', ',', 'as', 'this', 'problems', 'crosses', 'over', 'into', 'writing', 'as', 'well', '.', 'basically', ',', 'this', 'all', 'means', 'i', \"'\", 'm', 'not', 'good', 'with', 'words', 'so', 'i', \"'\", 'm', 'not', 'good', 'as', 'expressing', 'my', 'self', 'or', 'giving', 'advice', 'or', 'anything', '.', 'but', 'wait', '!', 'there', \"'\", 's', 'more', '!', 'due', 'to', 'my', 'disorders', ',', 'i', 'do', 'suffer', 'from', 'self', '-', 'esteem', 'problems', '.', 'now', ',', 'i', \"'\", 'm', 'not', 'suicidal', 'or', 'anything', '(', 'not', 'even', 'close', ')', 'but', 'lately', ',', 'i', 'haven', \"'\", 't', 'been', 'doing', 'that', 'great', 'in', 'college', ',', 'and', 'i', 'do', 'have', 'some', 'social', 'problems', 'thanks', 'to', 'my', 'awkward', '##ness', '.', 'now', ',', 'last', 'but', 'not', 'least', 'is', 'a', 'physical', 'problems', '.', 'i', 'was', 'born', 'with', 'two', 'heart', 'defects', ',', 'one', 'which', 'was', 'a', 'hole', 'in', 'the', 'wall', 'that', 'separates', 'the', 'two', 'sides', 'of', 'my', 'heart', ',', 'and', 'the', 'other', 'problem', 'was', 'that', 'one', 'of', 'the', 'valves', 'wasn', \"'\", 't', 'working', 'correctly', '.', 'i', 'had', 'open', 'heart', 'surgery', 'when', 'i', 'was', '1', 'to', 'correct', 'these', ',', 'but', 'i', 'often', 'get', 'pv', '##cs', 'and', 'there', 'is', 'mild', 'blood', 'flow', 'reg', '##urg', '##itation', 'at', 'the', 'valve', '.', 'i', 'don', \"'\", 't', 'need', 'any', 'more', 'surgery', 'or', 'anything', 'now', ',', 'i', 'just', 'sometimes', 'wonder', 'if', 'poor', 'blood', 'flow', 'is', 'the', 'source', 'for', 'at', 'least', 'some', 'of', 'my', 'disorders', '.', '.', '.', 'well', ',', 'that', 'was', 'longer', 'than', 'i', 'expected', '.', 'i', 'guess', 'i', 'wanted', 'to', 'talk', 'a', 'bit', '.', ':', 'p', 'now', ',', 'what', 'about', 'you', 'guys', ',', 'hmm', '?']\n",
      "INFO:__main__:Number of tokens: 652\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['just', 'curious', ',', 'does', 'anyone', 'here', 'have', 'something', 'besides', 'their', 'ad', '##hd', '?', 'i', \"'\", 've', 'heard', 'many', 'people', 'have', 'multiple', 'disorders', ',', 'and', 'i', \"'\", 'm', 'not', 'exception', 'to', 'that', '.', 'first', 'of', 'all', ',', 'i', 'have', 'ad', '##hd', '-', 'pi', ',', 'and', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'it', 'as', 'long', 'as', 'i', 'can', 'remember', '.', 'you', \"'\", 've', 'probably', 'figured', 'that', 'out', 'since', 'this', 'is', 'the', 'ad', '##hd', 'sub', '##red', '##dit', '.', 'now', ',', 'i', \"'\", 've', 'also', 'been', 'diagnosed', 'with', 'auditory', 'processing', 'disorder', '(', 'ap', '##d', ')', 'and', 'd', '##ys', '##graph', '##ia', '.', 'for', 'those', 'who', 'don', \"'\", 't', 'know', 'ap', '##d', 'is', 'pretty', 'similar', 'to', 'ad', '##hd', 'in', 'some', 'ways', '.', 'i', \"'\", 'm', 'not', 'completely', 'sure', 'about', 'it', ',', 'but', 'my', 'ad', '##hd', 'may', 'in', 'fact', 'be', 'the', 'reason', 'i', 'have', 'it', '.', 'for', 'those', 'who', 'want', 'more', 'info', 'on', 'it', ',', 'just', 'look', '[', 'here', '.', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'auditory', '_', 'processing', '_', 'disorder', ')', 'as', 'for', 'd', '##ys', '##graph', '##ia', ',', 'it', \"'\", 's', 'basically', 'means', 'i', 'can', \"'\", 't', 'draw', 'a', 'straight', 'line', 'if', 'my', 'life', 'depended', 'on', 'it', '.', 'those', 'who', 'want', 'to', 'read', 'about', 'it', 'can', 'do', 'so', '[', 'here', '.', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'd', '##ys', '##graph', '##ia', ')', 'when', 'i', 'was', 'younger', ',', 'i', 'also', 'had', 'a', 'speech', 'delay', '.', 'basically', ',', 'i', 'wasn', \"'\", 't', 'speaking', 'by', 'the', 'time', 'i', 'should', 'have', 'been', '(', 'i', 'believe', 'i', 'was', '2', 'and', 'still', 'couldn', \"'\", 't', 'really', 'speak', ')', ',', 'so', 'my', 'parents', 'got', 'speech', 'therapy', 'for', 'me', '.', 'it', 'did', 'work', ',', 'but', 'i', 'still', 'have', 'some', 'speech', 'problems', ',', 'like', 'an', 'unusual', 'voice', '(', 'if', 'you', 'want', 'to', 'hear', 'me', 'speak', 'i', 'guess', 'i', 'could', 'up', '##load', 'a', 'clip', ')', ',', 'and', 'occasional', 'stu', '##ttering', 'and', 'what', '##not', '.', 'also', ',', 'i', \"'\", 'm', 'not', 'sure', 'if', 'it', \"'\", 's', 'because', 'of', 'the', 'ad', '##hd', ',', 'the', 'speech', 'delay', ',', 'or', 'just', 'something', 'else', '(', 'or', 'a', 'combo', 'of', 'th', '##ses', ')', ',', 'but', 'my', 'brain', 'and', 'mouth', 'don', \"'\", 't', 'have', 'the', 'best', 'connection', '.', 'i', 'often', 'have', 'problems', 'putting', 'my', 'thoughts', 'into', 'words', ',', 'and', 'when', 'i', 'speak', 'i', 'sometimes', 'mess', 'up', 'what', 'i', 'want', 'to', 'say', '.', 'this', 'doesn', \"'\", 't', 'stop', 'with', 'speaking', ',', 'as', 'this', 'problems', 'crosses', 'over', 'into', 'writing', 'as', 'well', '.', 'basically', ',', 'this', 'all', 'means', 'i', \"'\", 'm', 'not', 'good', 'with', 'words', 'so', 'i', \"'\", 'm', 'not', 'good', 'as', 'expressing', 'my', 'self', 'or', 'giving', 'advice', 'or', 'anything', '.', 'but', 'wait', '!', 'there', \"'\", 's', 'more', '!', 'due', 'to', 'my', 'disorders', ',', 'i', 'do', 'suffer', 'from', 'self', '-', 'esteem', 'problems', '.', 'now', ',', 'i', \"'\", 'm', 'not', 'suicidal', 'or', 'anything', '(', 'not', 'even', 'close', ')', 'but', 'lately', ',', 'i', 'haven', \"'\", 't', 'been', 'doing', 'that', 'great', 'in', 'college', ',', 'and', 'i', 'do', 'have', 'some', 'social', 'problems', 'thanks', 'to', 'my', 'awkward', '##ness', '.', 'now', ',', 'last', 'but', 'not', 'least'], ['is', 'a', 'physical', 'problems', '.', 'i', 'was', 'born', 'with', 'two', 'heart', 'defects', ',', 'one', 'which', 'was', 'a', 'hole', 'in', 'the', 'wall', 'that', 'separates', 'the', 'two', 'sides', 'of', 'my', 'heart', ',', 'and', 'the', 'other', 'problem', 'was', 'that', 'one', 'of', 'the', 'valves', 'wasn', \"'\", 't', 'working', 'correctly', '.', 'i', 'had', 'open', 'heart', 'surgery', 'when', 'i', 'was', '1', 'to', 'correct', 'these', ',', 'but', 'i', 'often', 'get', 'pv', '##cs', 'and', 'there', 'is', 'mild', 'blood', 'flow', 'reg', '##urg', '##itation', 'at', 'the', 'valve', '.', 'i', 'don', \"'\", 't', 'need', 'any', 'more', 'surgery', 'or', 'anything', 'now', ',', 'i', 'just', 'sometimes', 'wonder', 'if', 'poor', 'blood', 'flow', 'is', 'the', 'source', 'for', 'at', 'least', 'some', 'of', 'my', 'disorders', '.', '.', '.', 'well', ',', 'that', 'was', 'longer', 'than', 'i', 'expected', '.', 'i', 'guess', 'i', 'wanted', 'to', 'talk', 'a', 'bit', '.', ':', 'p', 'now', ',', 'what', 'about', 'you', 'guys', ',', 'hmm', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['distracted', 'kids', \"'\", 'have', 'sharpe', '##r', 'brains', \"'\"]\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['distracted', 'kids', \"'\", 'have', 'sharpe', '##r', 'brains', \"'\"]]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['amp', '##het', '##amine', 'and', 'behaviour', '##al', 'change', '?', 'help', 'please', '.', 'after', 'med', '##ica', '##ting', 'with', 'dex', '##amp', '##het', '##amine', '##s', 'for', 'almost', 'two', 'years', 'now', 'i', 'am', 'ready', 'to', 'drop', 'it', '.', 'not', 'because', 'i', 'don', \"'\", 't', 'need', 'it', 'or', 'because', 'i', 'can', 'handle', 'the', 'vic', '##iss', '##itude', '##s', 'of', 'ad', '##hd', 'life', 'without', 'it', 'but', 'because', 'i', 'feel', 'like', 'it', 'is', 'actually', 'changing', 'who', 'i', 'am', 'on', 'a', 'day', 'to', 'day', 'basis', '.', 'obviously', 'some', 'of', 'this', 'is', 'straight', 'down', 'to', 'being', 'more', 'focused', ',', 'organised', 'and', 'purpose', 'or', 'goal', '-', 'driven', 'but', 'i', 'feel', 'there', 'is', 'more', 'to', 'it', 'than', 'that', '.', 'i', 'actually', 'want', 'to', 'drop', 'it', 'all', 'together', 'but', 'i', \"'\", 've', 'just', 'enrolled', 'to', 'study', 'engineering', 'and', 'for', 'obvious', 'reasons', 'i', \"'\", 'm', 'reluctant', '.', 'i', 'feel', 'ok', 'taking', 'it', ',', 'the', 'eu', '##ph', '##oric', 'feeling', 'dim', '##s', 'after', 'a', 'couple', 'of', 'days', 'and', 'i', 'feel', '\"', 'in', 'the', 'zone', '\"', 'but', 'no', 'matter', 'the', 'dose', 'i', 'feel', 'robotic', ',', 'like', 'my', 'mind', 'is', 'amplified', 'yet', 'dead', '##ened', 'at', 'the', 'same', 'time', 'and', 'i', 'don', \"'\", 't', 'really', 'know', 'how', 'to', 'art', '##iculate', 'it', 'exactly', 'but', 'its', 'like', 'my', 'emotions', 'almost', 'disappear', '.', 'if', 'you', 'could', 'imagine', 'the', 'world', 'as', 'the', 'spectrum', 'of', 'your', 'emotion', ',', 'happiness', 'being', 'the', 'north', 'pole', 'and', 'sadness', 'being', 'the', 'south', ',', 'the', 'dex', '##es', 'limit', 'my', 'flu', '##ct', '##uation', 'to', 'a', 'band', 'of', 'a', 'few', 'degrees', 'either', 'side', 'of', 'the', 'equator', 'or', 'neutral', ',', 'fear', 'and', 'desire', 'are', 'still', 'rampant', 'as', 'ever', ',', 'maybe', 'even', 'more', 'so', 'but', 'the', 'true', 'emotions', 'are', 'muted', ',', 'dull', '.', 'i', 'look', 'back', 'at', 'the', 'past', 'year', '-', '-', 'an', 'effort', 'in', 'itself', '-', '-', 'and', 'feel', 'like', 'while', 'i', '\"', 'achieved', '\"', 'i', 'wasn', \"'\", 't', 'really', 'there', 'enjoying', 'life', '.', 'it', 'is', 'as', 'if', 'my', 'emotions', 'attribute', 'how', 'intense', 'a', 'memory', 'is', 'and', 'for', 'the', 'duration', 'of', 'this', 'time', '.', '.', '.', 'there', 'isn', \"'\", 't', 'really', 'anything', 'there', '.', 'even', 'now', 'off', 'it', 'for', 'the', 'past', 'two', 'days', 'and', 'yes', 'work', 'is', 'harder', 'but', 'i', 'am', 'almost', 'instantly', 'happier', ',', 'like', 'an', 'immense', 'weight', 'that', 'was', 'su', '##ff', '##ocating', 'me', 'has', 'been', 'lifted', '.', 'i', 'already', 'feel', 'more', 'emotional', ',', 'like', 'actually', 'able', 'to', 'laugh', 'from', 'the', 'belly', ',', 'and', 'able', 'to', 'feel', 'sadness', 'at', 'being', 'away', 'from', 'my', 'daughter', '.', 'the', 'problem', 'is', '.', 'awaiting', 'me', 'is', 'a', 'life', 'of', 'chaos', 'and', 'fuck', '-', 'ups', 'i', 'i', 'go', 'off', 'it', 'and', 'i', 'fear', 'rev', '##ert', '##ing', 'to', 'the', 'state', 'that', 'had', 'me', 'seek', 'out', 'medication', 'in', 'the', 'first', 'place', '.', 'if', 'i', 'was', 'single', 'i', \"'\", 'd', 'motor', 'on', 'for', 'a', 'few', 'years', 'and', 'just', 'get', 'my', 'degree', 'done', 'and', 'dust', '##ed', 'but', 'i', 'have', 'a', 'partner', 'with', 'her', 'own', 'issues', '(', 'pts', '##d', '+', 'depression', ')', 'and', 'we', 'have', 'a', 'one', 'year', 'old', 'daughter', 'who', 'i', 'have', 'to', 'try', 'and', 'be', 'a', 'good', '(', 'emotionally', 'true', ')', 'father', 'to', '.', 'i', \"'\", 'm', 'fucked', 'either', 'way', 'or', 'what', '?', 'wat', 'do', '?', 't', '##ld', '##r', ':', 'amp', '##het', '##amine', '##s', 'give', 'me', 'a', 'neutral', '\"', 'de', '##pressive', '\"', 'state', 'and', 'i', \"'\", 'm', 'stump', '##ed', '.']\n",
      "INFO:__main__:Number of tokens: 530\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['amp', '##het', '##amine', 'and', 'behaviour', '##al', 'change', '?', 'help', 'please', '.', 'after', 'med', '##ica', '##ting', 'with', 'dex', '##amp', '##het', '##amine', '##s', 'for', 'almost', 'two', 'years', 'now', 'i', 'am', 'ready', 'to', 'drop', 'it', '.', 'not', 'because', 'i', 'don', \"'\", 't', 'need', 'it', 'or', 'because', 'i', 'can', 'handle', 'the', 'vic', '##iss', '##itude', '##s', 'of', 'ad', '##hd', 'life', 'without', 'it', 'but', 'because', 'i', 'feel', 'like', 'it', 'is', 'actually', 'changing', 'who', 'i', 'am', 'on', 'a', 'day', 'to', 'day', 'basis', '.', 'obviously', 'some', 'of', 'this', 'is', 'straight', 'down', 'to', 'being', 'more', 'focused', ',', 'organised', 'and', 'purpose', 'or', 'goal', '-', 'driven', 'but', 'i', 'feel', 'there', 'is', 'more', 'to', 'it', 'than', 'that', '.', 'i', 'actually', 'want', 'to', 'drop', 'it', 'all', 'together', 'but', 'i', \"'\", 've', 'just', 'enrolled', 'to', 'study', 'engineering', 'and', 'for', 'obvious', 'reasons', 'i', \"'\", 'm', 'reluctant', '.', 'i', 'feel', 'ok', 'taking', 'it', ',', 'the', 'eu', '##ph', '##oric', 'feeling', 'dim', '##s', 'after', 'a', 'couple', 'of', 'days', 'and', 'i', 'feel', '\"', 'in', 'the', 'zone', '\"', 'but', 'no', 'matter', 'the', 'dose', 'i', 'feel', 'robotic', ',', 'like', 'my', 'mind', 'is', 'amplified', 'yet', 'dead', '##ened', 'at', 'the', 'same', 'time', 'and', 'i', 'don', \"'\", 't', 'really', 'know', 'how', 'to', 'art', '##iculate', 'it', 'exactly', 'but', 'its', 'like', 'my', 'emotions', 'almost', 'disappear', '.', 'if', 'you', 'could', 'imagine', 'the', 'world', 'as', 'the', 'spectrum', 'of', 'your', 'emotion', ',', 'happiness', 'being', 'the', 'north', 'pole', 'and', 'sadness', 'being', 'the', 'south', ',', 'the', 'dex', '##es', 'limit', 'my', 'flu', '##ct', '##uation', 'to', 'a', 'band', 'of', 'a', 'few', 'degrees', 'either', 'side', 'of', 'the', 'equator', 'or', 'neutral', ',', 'fear', 'and', 'desire', 'are', 'still', 'rampant', 'as', 'ever', ',', 'maybe', 'even', 'more', 'so', 'but', 'the', 'true', 'emotions', 'are', 'muted', ',', 'dull', '.', 'i', 'look', 'back', 'at', 'the', 'past', 'year', '-', '-', 'an', 'effort', 'in', 'itself', '-', '-', 'and', 'feel', 'like', 'while', 'i', '\"', 'achieved', '\"', 'i', 'wasn', \"'\", 't', 'really', 'there', 'enjoying', 'life', '.', 'it', 'is', 'as', 'if', 'my', 'emotions', 'attribute', 'how', 'intense', 'a', 'memory', 'is', 'and', 'for', 'the', 'duration', 'of', 'this', 'time', '.', '.', '.', 'there', 'isn', \"'\", 't', 'really', 'anything', 'there', '.', 'even', 'now', 'off', 'it', 'for', 'the', 'past', 'two', 'days', 'and', 'yes', 'work', 'is', 'harder', 'but', 'i', 'am', 'almost', 'instantly', 'happier', ',', 'like', 'an', 'immense', 'weight', 'that', 'was', 'su', '##ff', '##ocating', 'me', 'has', 'been', 'lifted', '.', 'i', 'already', 'feel', 'more', 'emotional', ',', 'like', 'actually', 'able', 'to', 'laugh', 'from', 'the', 'belly', ',', 'and', 'able', 'to', 'feel', 'sadness', 'at', 'being', 'away', 'from', 'my', 'daughter', '.', 'the', 'problem', 'is', '.', 'awaiting', 'me', 'is', 'a', 'life', 'of', 'chaos', 'and', 'fuck', '-', 'ups', 'i', 'i', 'go', 'off', 'it', 'and', 'i', 'fear', 'rev', '##ert', '##ing', 'to', 'the', 'state', 'that', 'had', 'me', 'seek', 'out', 'medication', 'in', 'the', 'first', 'place', '.', 'if', 'i', 'was', 'single', 'i', \"'\", 'd', 'motor', 'on', 'for', 'a', 'few', 'years', 'and', 'just', 'get', 'my', 'degree', 'done', 'and', 'dust', '##ed', 'but', 'i', 'have', 'a', 'partner', 'with', 'her', 'own', 'issues', '(', 'pts', '##d', '+', 'depression', ')', 'and', 'we', 'have', 'a', 'one', 'year', 'old', 'daughter', 'who', 'i', 'have', 'to', 'try', 'and', 'be', 'a', 'good', '(', 'emotionally', 'true', ')', 'father', 'to', '.', 'i', \"'\", 'm', 'fucked', 'either', 'way', 'or', 'what', '?', 'wat', 'do', '?', 't', '##ld', '##r', ':', 'amp', '##het'], ['##amine', '##s', 'give', 'me', 'a', 'neutral', '\"', 'de', '##pressive', '\"', 'state', 'and', 'i', \"'\", 'm', 'stump', '##ed', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['you', 'must', 'watch', 'this', 'ad', '##hd', 'lecture', 'by', 'dr', '.', 'russell', 'bark', '##ley', '.', 'this', 'is', 'what', 'science', 'knows', 'about', 'ad', '##hd', '.']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['you', 'must', 'watch', 'this', 'ad', '##hd', 'lecture', 'by', 'dr', '.', 'russell', 'bark', '##ley', '.', 'this', 'is', 'what', 'science', 'knows', 'about', 'ad', '##hd', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'have', 'a', 'lot', 'of', 'flashbacks', 'of', 'little', 'stupid', 'things', 'you', 'did', '?', 'hey', 'guys', 'i', \"'\", 'm', 'just', 'trying', 'to', 'figure', 'out', 'if', 'it', \"'\", 's', 'me', 'or', 'normal', '?', 'all', 'the', 'time', 'i', 'get', 'flashbacks', 'of', 'times', 'when', 'i', 'maybe', 'said', 'something', 'without', 'thinking', 'or', 'even', 'doing', 'something', '.', 'usually', 'they', 'pop', 'up', 'due', 'to', 'something', 'that', 'reminds', 'me', 'of', 'them', '.', 'most', 'of', 'these', 'are', 'really', 'embarrassing', 'for', 'me', 'and', 'i', 'just', 'can', \"'\", 't', 'get', 'them', 'out', 'of', 'my', 'head', '.', 'is', 'this', 'a', 'possible', 'effect', 'to', 'ad', '##hd', ',', 'among', 'other', 'things', '.', 'this', 'is', 'just', 'one', 'of', 'those', 'prevalent', '.', 'i', 'am', 'seeing', 'a', 'ps', '##ych', 'soon', 'hopefully', 'to', 'great', 'anxiety', 'or', 'ad', '##hd', '.', 'but', 'this', 'thing', 'i', 'have', 'what', 'is', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 131\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'have', 'a', 'lot', 'of', 'flashbacks', 'of', 'little', 'stupid', 'things', 'you', 'did', '?', 'hey', 'guys', 'i', \"'\", 'm', 'just', 'trying', 'to', 'figure', 'out', 'if', 'it', \"'\", 's', 'me', 'or', 'normal', '?', 'all', 'the', 'time', 'i', 'get', 'flashbacks', 'of', 'times', 'when', 'i', 'maybe', 'said', 'something', 'without', 'thinking', 'or', 'even', 'doing', 'something', '.', 'usually', 'they', 'pop', 'up', 'due', 'to', 'something', 'that', 'reminds', 'me', 'of', 'them', '.', 'most', 'of', 'these', 'are', 'really', 'embarrassing', 'for', 'me', 'and', 'i', 'just', 'can', \"'\", 't', 'get', 'them', 'out', 'of', 'my', 'head', '.', 'is', 'this', 'a', 'possible', 'effect', 'to', 'ad', '##hd', ',', 'among', 'other', 'things', '.', 'this', 'is', 'just', 'one', 'of', 'those', 'prevalent', '.', 'i', 'am', 'seeing', 'a', 'ps', '##ych', 'soon', 'hopefully', 'to', 'great', 'anxiety', 'or', 'ad', '##hd', '.', 'but', 'this', 'thing', 'i', 'have', 'what', 'is', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'encounter', 'a', 'psychologist', 'reluctant', 'to', 'med', '##icate', '?', 'so', 'i', 'have', 'seen', 'my', 'clinical', 'psychologist', '(', 'ed', '##d', ')', '(', 'she', 'also', 'specializes', 'in', 'add', '/', 'anxiety', ')', '4', 'times', 'now', 'over', 'the', 'last', 'month', 'and', 'the', 'idea', 'of', 'trying', 'a', 'medication', 'hasn', \"'\", 't', 'even', 'come', 'up', 'yet', '.', 'im', '##o', 'i', \"'\", 'am', 'ad', '##hd', '-', 'pi', 'to', 'a', 't', ',', 'although', 'this', 'is', 'a', 'conclusion', 'i', 'haven', \"'\", 't', 'heard', 'my', 'ps', '##ych', 'come', 'to', '.', 'the', 'cb', '##t', 'is', 'very', 'helpful', 'and', 'i', \"'\", 'm', 'glad', 'i', 'didn', \"'\", 't', 'walk', 'out', 'of', 'our', 'first', 'session', 'with', 'a', 'diagnosis', 'and', 'a', 'script', 'in', 'hand', ',', 'i', 'really', 'am', 'thankful', 'she', 'listen', '##s', 'to', 'me', 'and', 'is', 'taking', 'the', 'time', 'to', 'understand', 'my', 'issues', '.', 'i', 'just', 'fear', 'that', 'cb', '##t', 'is', 'all', 'she', 'is', 'going', 'to', 'be', 'willing', 'to', 'pursue', 'and', 'i', 'know', 'that', 'i', \"'\", 'm', 'going', 'to', 'need', 'that', 'initial', 'push', 'from', 'medication', 'to', 'create', 'new', 'habits', 'and', 'gather', 'some', 'motivation', 'to', 'make', 'some', 'necessary', 'changes', '.', 'and', 'i', 'don', \"'\", 't', 'think', 'cb', '##t', 'alone', 'can', 'help', 'with', 'your', 'focus', '/', 'brain', 'chatter', '/', 'distract', '##ibility', '/', 'brain', 'fog', 'etc', '.', 'but', 'i', 'could', 'be', 'wrong', '.', 'so', 'i', 'guess', 'my', 'question', 'is', 'whether', 'anyone', 'else', 'here', 'has', 'had', 'a', 'similar', 'experience', 'with', 'a', 'psychologist', 'who', 'was', 'against', 'medication', 'in', 'general', '.', 'and', 'how', 'should', 'i', 'bring', 'up', 'the', 'idea', 'that', 'i', 'want', 'to', 'try', 'a', 'new', 'medication', '?', '(', 'my', 'gp', 'put', 'me', 'on', 'concert', '##a', 'last', 'year', 'with', 'no', 'effect', ')', '.', 'i', \"'\", 'm', 'just', 'worried', 'she', 'is', 'against', 'medication', 'in', 'general', 'and', 'i', \"'\", 've', 'wasted', 'all', 'this', 'time', 'when', 'i', 'should', 'have', 'seen', 'a', 'psychiatrist', '.', 'edit', '-', 'i', \"'\", 'am', 'aware', 'psychologists', 'cannot', 'pre', '##scribe', 'medication', '.', 'but', 'they', 'can', 'write', 'suggestions', 'on', 'treatment', 'plans', 'that', 'you', 'then', 'take', 'to', 'your', 'doctor', '.']\n",
      "INFO:__main__:Number of tokens: 318\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'encounter', 'a', 'psychologist', 'reluctant', 'to', 'med', '##icate', '?', 'so', 'i', 'have', 'seen', 'my', 'clinical', 'psychologist', '(', 'ed', '##d', ')', '(', 'she', 'also', 'specializes', 'in', 'add', '/', 'anxiety', ')', '4', 'times', 'now', 'over', 'the', 'last', 'month', 'and', 'the', 'idea', 'of', 'trying', 'a', 'medication', 'hasn', \"'\", 't', 'even', 'come', 'up', 'yet', '.', 'im', '##o', 'i', \"'\", 'am', 'ad', '##hd', '-', 'pi', 'to', 'a', 't', ',', 'although', 'this', 'is', 'a', 'conclusion', 'i', 'haven', \"'\", 't', 'heard', 'my', 'ps', '##ych', 'come', 'to', '.', 'the', 'cb', '##t', 'is', 'very', 'helpful', 'and', 'i', \"'\", 'm', 'glad', 'i', 'didn', \"'\", 't', 'walk', 'out', 'of', 'our', 'first', 'session', 'with', 'a', 'diagnosis', 'and', 'a', 'script', 'in', 'hand', ',', 'i', 'really', 'am', 'thankful', 'she', 'listen', '##s', 'to', 'me', 'and', 'is', 'taking', 'the', 'time', 'to', 'understand', 'my', 'issues', '.', 'i', 'just', 'fear', 'that', 'cb', '##t', 'is', 'all', 'she', 'is', 'going', 'to', 'be', 'willing', 'to', 'pursue', 'and', 'i', 'know', 'that', 'i', \"'\", 'm', 'going', 'to', 'need', 'that', 'initial', 'push', 'from', 'medication', 'to', 'create', 'new', 'habits', 'and', 'gather', 'some', 'motivation', 'to', 'make', 'some', 'necessary', 'changes', '.', 'and', 'i', 'don', \"'\", 't', 'think', 'cb', '##t', 'alone', 'can', 'help', 'with', 'your', 'focus', '/', 'brain', 'chatter', '/', 'distract', '##ibility', '/', 'brain', 'fog', 'etc', '.', 'but', 'i', 'could', 'be', 'wrong', '.', 'so', 'i', 'guess', 'my', 'question', 'is', 'whether', 'anyone', 'else', 'here', 'has', 'had', 'a', 'similar', 'experience', 'with', 'a', 'psychologist', 'who', 'was', 'against', 'medication', 'in', 'general', '.', 'and', 'how', 'should', 'i', 'bring', 'up', 'the', 'idea', 'that', 'i', 'want', 'to', 'try', 'a', 'new', 'medication', '?', '(', 'my', 'gp', 'put', 'me', 'on', 'concert', '##a', 'last', 'year', 'with', 'no', 'effect', ')', '.', 'i', \"'\", 'm', 'just', 'worried', 'she', 'is', 'against', 'medication', 'in', 'general', 'and', 'i', \"'\", 've', 'wasted', 'all', 'this', 'time', 'when', 'i', 'should', 'have', 'seen', 'a', 'psychiatrist', '.', 'edit', '-', 'i', \"'\", 'am', 'aware', 'psychologists', 'cannot', 'pre', '##scribe', 'medication', '.', 'but', 'they', 'can', 'write', 'suggestions', 'on', 'treatment', 'plans', 'that', 'you', 'then', 'take', 'to', 'your', 'doctor', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'and', 'ant', '##ac', '##ids']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'and', 'ant', '##ac', '##ids']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['v', '##y', '##van', '##se', 'dos', '##age', '(', 'li', '##sd', '##ex', '##am', '##fe', '##tam', '##ine', ')', 'ok', ',', 'so', 'i', 'have', 'been', 'diagnosed', 'since', 'i', 'was', '10', '(', '2000', ')', '.', 'was', 'on', 'concert', '##a', 'until', '17', '(', 'i', 'went', 'off', 'them', 'for', 'my', 'last', 'year', 'of', 'high', 'school', ')', '.', 'went', 'back', 'on', 'them', 'for', 'a', 'month', 'that', 'august', '.', 'they', 'stopped', 'working', 'so', 'i', 'figured', 'i', 'would', 'try', 'new', 'med', '##s', '.', 'day', '##tra', '##na', '(', 'awful', ')', 'didn', \"'\", 't', 'work', '.', 'i', 'found', 'v', '##y', '##van', '##se', '.', 'i', 'now', 'have', 'decent', 'grades', '.', 'however', ',', 'i', 'am', 'noticing', 'how', 'much', 'i', 'change', 'on', 'them', '.', 'i', 'become', 'withdrawn', 'from', 'society', 'and', 'loaf', 'around', 'my', 'dorm', '(', 'i', 'don', \"'\", 't', 'leave', 'except', 'for', 'class', ')', '.', 'is', 'the', 'dose', 'too', 'high', '?', 'i', 'have', 'been', 'on', 'it', 'for', 'two', 'years', '.']\n",
      "INFO:__main__:Number of tokens: 145\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['v', '##y', '##van', '##se', 'dos', '##age', '(', 'li', '##sd', '##ex', '##am', '##fe', '##tam', '##ine', ')', 'ok', ',', 'so', 'i', 'have', 'been', 'diagnosed', 'since', 'i', 'was', '10', '(', '2000', ')', '.', 'was', 'on', 'concert', '##a', 'until', '17', '(', 'i', 'went', 'off', 'them', 'for', 'my', 'last', 'year', 'of', 'high', 'school', ')', '.', 'went', 'back', 'on', 'them', 'for', 'a', 'month', 'that', 'august', '.', 'they', 'stopped', 'working', 'so', 'i', 'figured', 'i', 'would', 'try', 'new', 'med', '##s', '.', 'day', '##tra', '##na', '(', 'awful', ')', 'didn', \"'\", 't', 'work', '.', 'i', 'found', 'v', '##y', '##van', '##se', '.', 'i', 'now', 'have', 'decent', 'grades', '.', 'however', ',', 'i', 'am', 'noticing', 'how', 'much', 'i', 'change', 'on', 'them', '.', 'i', 'become', 'withdrawn', 'from', 'society', 'and', 'loaf', 'around', 'my', 'dorm', '(', 'i', 'don', \"'\", 't', 'leave', 'except', 'for', 'class', ')', '.', 'is', 'the', 'dose', 'too', 'high', '?', 'i', 'have', 'been', 'on', 'it', 'for', 'two', 'years', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['exploring', 'prescription', '##s', ';', 'from', 'concert', 'to', 'v', '##y', '##van', '##se', 'i', \"'\", 'm', 'a', 'first', 'time', 'poster', 'to', 'r', '/', 'ad', '##hd', ',', 'but', 'have', 'learned', 'so', 'much', 'from', 'everyone', 'sharing', 'their', 'experiences', '.', 'thank', 'you', 'all', '!', '27', '##yo', '/', 'm', 'recently', 'diagnosed', 'with', 'adult', 'ad', '##hd', 'with', 'a', 'como', '##rb', '##idi', '##ty', 'of', 'anxiety', '.', 'the', 'doctor', 'initially', 'prescribed', 'me', 'concert', '##a', '27', '##mg', 'and', '36', '##mg', 'to', 'try', '.', '36', '##mg', 'was', 'way', 'too', 'strong', 'and', 'left', 'me', 'anxious', 'with', 'that', 'bugs', 'under', 'the', 'skin', 'feeling', '.', 'i', 'liked', 'the', '27', '##mg', 'dose', 'but', 'found', 'that', 'anxiety', 'was', 'still', 'an', 'issue', '.', 'other', 'con', '##s', 'were', ':', 'continued', 'lack', 'of', 'motivation', ',', 'mood', '##iness', ',', 'and', 'ir', '##rita', '##bility', 'in', 'the', 'evening', 'when', 'it', 'wore', 'off', '.', 'i', 'enjoyed', 'that', 'i', 'could', 'concentrate', 'better', 'and', 'my', 'sleeping', 'habits', 'became', 'like', 'clock', '##work', '.', 'in', 'bed', 'by', '9', ':', '30', '##pm', 'and', 'up', 'at', '6', ':', '30', 'every', 'day', ';', 'it', 'was', 'wonderful', '.', 'the', 'extra', 'energy', 'was', 'great', '.', 'now', 'i', \"'\", 'm', 'trying', 'v', '##y', '##van', '##se', '40', '##mg', '.', 'i', 'enjoy', 'this', 'one', 'much', 'more', '.', 'far', 'more', 'energy', 'and', 'my', 'sleep', 'pattern', 'is', 'still', 'fairly', 'regular', ',', 'though', 'it', \"'\", 's', 'hard', 'to', 'fall', 'asleep', '.', 'the', 'added', 'improvements', 'over', 'concert', '##a', 'were', ':', 'low', 'to', 'no', 'anxiety', ',', 'high', 'increase', 'in', 'motivation', ',', 'even', 'better', 'concentration', ',', 'and', 'best', 'of', 'all', ',', 'i', 'haven', \"'\", 't', 'felt', 'this', 'happy', 'and', 'even', '##ed', 'out', 'in', 'years', '.', 'i', \"'\", 'm', 'even', 'running', 'again', '.', 'con', '##s', 'are', ':', 'long', 'effect', 'time', '(', 'take', 'it', 'at', '6', ':', '45', '##am', 'and', 'it', 'doesn', \"'\", 't', 'wear', 'off', 'until', 'midnight', ')', ',', 'i', 'can', 'get', 'a', 'little', 'too', 'hyper', 'if', 'i', \"'\", 'm', 'not', 'mind', '##ful', 'of', 'taking', 'a', 'breath', 'and', 'keeping', 'a', 'steady', 'pace', '.', 'to', 'make', 'sure', 'i', 'get', 'to', 'sleep', 'at', 'a', 'decent', 'time', 'i', \"'\", 've', 'started', 'drinking', 'cha', '##mo', '##mi', '##le', 'and', 'vale', '##rian', 'tea', 'before', 'bed', 'and', 'i', 'have', 'cl', '##ona', '##ze', '##pa', '##m', 'in', 'case', 'of', 'emergencies', '.', 'overall', ',', 'i', \"'\", 'd', 'say', 'v', '##y', '##van', '##se', 'is', 'the', 'winner', ',', 'though', 'i', 'might', 'try', '30', '##mg', 'to', 'see', 'if', 'that', 'takes', 'the', '\"', 'edge', '\"', 'off', '.', 'my', 'only', 'fear', 'now', 'is', 'the', 'drug', 'losing', 'it', \"'\", 's', 'effectiveness', '.', 'please', 'comment', '!', 'i', 'would', 'love', 'to', 'hear', 'about', 'other', 'people', \"'\", 's', 'experiences', '.', 'edit', ';', 'mental', 'chatter', '/', 'noise', '-', 'concert', '##a', 'got', 'rid', 'of', 'my', 'mental', 'chatter', 'which', 'is', 'a', 'plus', 'over', 'v', '##y', '##van', '##se', '.', 'with', 'v', '##y', '##van', '##se', 'the', 'chatter', 'is', 'still', 'there', ',', 'but', 'now', 'i', 'can', 'control', '/', 'guide', 'it', 'towards', 'being', 'productive', '.']\n",
      "INFO:__main__:Number of tokens: 457\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['exploring', 'prescription', '##s', ';', 'from', 'concert', 'to', 'v', '##y', '##van', '##se', 'i', \"'\", 'm', 'a', 'first', 'time', 'poster', 'to', 'r', '/', 'ad', '##hd', ',', 'but', 'have', 'learned', 'so', 'much', 'from', 'everyone', 'sharing', 'their', 'experiences', '.', 'thank', 'you', 'all', '!', '27', '##yo', '/', 'm', 'recently', 'diagnosed', 'with', 'adult', 'ad', '##hd', 'with', 'a', 'como', '##rb', '##idi', '##ty', 'of', 'anxiety', '.', 'the', 'doctor', 'initially', 'prescribed', 'me', 'concert', '##a', '27', '##mg', 'and', '36', '##mg', 'to', 'try', '.', '36', '##mg', 'was', 'way', 'too', 'strong', 'and', 'left', 'me', 'anxious', 'with', 'that', 'bugs', 'under', 'the', 'skin', 'feeling', '.', 'i', 'liked', 'the', '27', '##mg', 'dose', 'but', 'found', 'that', 'anxiety', 'was', 'still', 'an', 'issue', '.', 'other', 'con', '##s', 'were', ':', 'continued', 'lack', 'of', 'motivation', ',', 'mood', '##iness', ',', 'and', 'ir', '##rita', '##bility', 'in', 'the', 'evening', 'when', 'it', 'wore', 'off', '.', 'i', 'enjoyed', 'that', 'i', 'could', 'concentrate', 'better', 'and', 'my', 'sleeping', 'habits', 'became', 'like', 'clock', '##work', '.', 'in', 'bed', 'by', '9', ':', '30', '##pm', 'and', 'up', 'at', '6', ':', '30', 'every', 'day', ';', 'it', 'was', 'wonderful', '.', 'the', 'extra', 'energy', 'was', 'great', '.', 'now', 'i', \"'\", 'm', 'trying', 'v', '##y', '##van', '##se', '40', '##mg', '.', 'i', 'enjoy', 'this', 'one', 'much', 'more', '.', 'far', 'more', 'energy', 'and', 'my', 'sleep', 'pattern', 'is', 'still', 'fairly', 'regular', ',', 'though', 'it', \"'\", 's', 'hard', 'to', 'fall', 'asleep', '.', 'the', 'added', 'improvements', 'over', 'concert', '##a', 'were', ':', 'low', 'to', 'no', 'anxiety', ',', 'high', 'increase', 'in', 'motivation', ',', 'even', 'better', 'concentration', ',', 'and', 'best', 'of', 'all', ',', 'i', 'haven', \"'\", 't', 'felt', 'this', 'happy', 'and', 'even', '##ed', 'out', 'in', 'years', '.', 'i', \"'\", 'm', 'even', 'running', 'again', '.', 'con', '##s', 'are', ':', 'long', 'effect', 'time', '(', 'take', 'it', 'at', '6', ':', '45', '##am', 'and', 'it', 'doesn', \"'\", 't', 'wear', 'off', 'until', 'midnight', ')', ',', 'i', 'can', 'get', 'a', 'little', 'too', 'hyper', 'if', 'i', \"'\", 'm', 'not', 'mind', '##ful', 'of', 'taking', 'a', 'breath', 'and', 'keeping', 'a', 'steady', 'pace', '.', 'to', 'make', 'sure', 'i', 'get', 'to', 'sleep', 'at', 'a', 'decent', 'time', 'i', \"'\", 've', 'started', 'drinking', 'cha', '##mo', '##mi', '##le', 'and', 'vale', '##rian', 'tea', 'before', 'bed', 'and', 'i', 'have', 'cl', '##ona', '##ze', '##pa', '##m', 'in', 'case', 'of', 'emergencies', '.', 'overall', ',', 'i', \"'\", 'd', 'say', 'v', '##y', '##van', '##se', 'is', 'the', 'winner', ',', 'though', 'i', 'might', 'try', '30', '##mg', 'to', 'see', 'if', 'that', 'takes', 'the', '\"', 'edge', '\"', 'off', '.', 'my', 'only', 'fear', 'now', 'is', 'the', 'drug', 'losing', 'it', \"'\", 's', 'effectiveness', '.', 'please', 'comment', '!', 'i', 'would', 'love', 'to', 'hear', 'about', 'other', 'people', \"'\", 's', 'experiences', '.', 'edit', ';', 'mental', 'chatter', '/', 'noise', '-', 'concert', '##a', 'got', 'rid', 'of', 'my', 'mental', 'chatter', 'which', 'is', 'a', 'plus', 'over', 'v', '##y', '##van', '##se', '.', 'with', 'v', '##y', '##van', '##se', 'the', 'chatter', 'is', 'still', 'there', ',', 'but', 'now', 'i', 'can', 'control', '/', 'guide', 'it', 'towards', 'being', 'productive', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['studying', 'technique', 'using', 'medication', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '12', 'years', 'ago', ',', 'but', 'i', 'just', 'started', 'using', 'medication', 'to', 'treat', 'it', 'recently', '.', 'i', 'was', 'wondering', 'how', 'you', 'all', 'use', 'the', 'medication', 'to', 'help', 'you', 'study', '.', 'tips', ',', 'tricks', ',', 'duration', 'etc', '.', 'i', 'rather', 'take', 'the', 'med', '##s', 'when', 'i', 'need', 'them', ',', 'so', 'that', 'i', 'can', 'slow', 'the', 'tolerance', '.']\n",
      "INFO:__main__:Number of tokens: 65\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['studying', 'technique', 'using', 'medication', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '12', 'years', 'ago', ',', 'but', 'i', 'just', 'started', 'using', 'medication', 'to', 'treat', 'it', 'recently', '.', 'i', 'was', 'wondering', 'how', 'you', 'all', 'use', 'the', 'medication', 'to', 'help', 'you', 'study', '.', 'tips', ',', 'tricks', ',', 'duration', 'etc', '.', 'i', 'rather', 'take', 'the', 'med', '##s', 'when', 'i', 'need', 'them', ',', 'so', 'that', 'i', 'can', 'slow', 'the', 'tolerance', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ever', 'have', 'troubles', 'turning', 'off', 'your', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ever', 'have', 'troubles', 'turning', 'off', 'your', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['as', 'an', 'elementary', 'school', 'teacher', ',', 'we', 'are', 'encouraged', 'to', 'try', 'to', 'understand', 'different', 'disorders', 'through', 'simulation', 'activities', 'such', 'as', 'these', ':']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['as', 'an', 'elementary', 'school', 'teacher', ',', 'we', 'are', 'encouraged', 'to', 'try', 'to', 'understand', 'different', 'disorders', 'through', 'simulation', 'activities', 'such', 'as', 'these', ':']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'can', 'you', 'tell', 'when', 'your', 'medication', 'is', 'working', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'can', 'you', 'tell', 'when', 'your', 'medication', 'is', 'working', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['on', 'medication', 'timing', 'and', 'college', 'study', 'habits', 'every', 'morning', 'i', 'take', 'a', 'v', '##y', '##van', '##se', '60', '##mg', 'at', '8', ':', '00', '##am', 'or', 'so', '.', 'this', 'keeps', 'me', 'ticking', 'through', 'most', 'of', 'my', 'day', ',', 'but', 'seems', 'to', 'have', 'worn', 'off', 'completely', 'by', '3', '##pm', '.', 'if', 'i', 'were', 'at', 'a', 'job', ',', 'i', 'could', 'stretch', 'myself', 'to', 'make', 'that', 'window', 'fit', 'until', 'i', 'got', 'off', '.', 'however', ',', 'it', 'seems', 'to', 'be', 'expected', 'of', 'college', 'students', 'to', 'attend', 'lectures', 'in', 'the', 'daylight', 'hours', 'and', 'study', 'through', 'the', 'evenings', '.', 'the', 'work', '##load', '##s', 'are', 'cal', '##ib', '##rated', 'accordingly', '.', 'if', 'i', 'take', 'my', 'med', '##s', 'later', 'in', 'the', 'day', ',', 'i', 'may', 'as', 'well', 'give', 'up', 'sleep', 'that', 'night', '.', 'how', 'do', 'i', 'work', '18', 'hours', 'of', 'productivity', 'into', '6', '?']\n",
      "INFO:__main__:Number of tokens: 133\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['on', 'medication', 'timing', 'and', 'college', 'study', 'habits', 'every', 'morning', 'i', 'take', 'a', 'v', '##y', '##van', '##se', '60', '##mg', 'at', '8', ':', '00', '##am', 'or', 'so', '.', 'this', 'keeps', 'me', 'ticking', 'through', 'most', 'of', 'my', 'day', ',', 'but', 'seems', 'to', 'have', 'worn', 'off', 'completely', 'by', '3', '##pm', '.', 'if', 'i', 'were', 'at', 'a', 'job', ',', 'i', 'could', 'stretch', 'myself', 'to', 'make', 'that', 'window', 'fit', 'until', 'i', 'got', 'off', '.', 'however', ',', 'it', 'seems', 'to', 'be', 'expected', 'of', 'college', 'students', 'to', 'attend', 'lectures', 'in', 'the', 'daylight', 'hours', 'and', 'study', 'through', 'the', 'evenings', '.', 'the', 'work', '##load', '##s', 'are', 'cal', '##ib', '##rated', 'accordingly', '.', 'if', 'i', 'take', 'my', 'med', '##s', 'later', 'in', 'the', 'day', ',', 'i', 'may', 'as', 'well', 'give', 'up', 'sleep', 'that', 'night', '.', 'how', 'do', 'i', 'work', '18', 'hours', 'of', 'productivity', 'into', '6', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'this', 'unusual', '?']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'this', 'unusual', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'diagnosed', 'what', 'should', 'i', 'expect', '?']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'diagnosed', 'what', 'should', 'i', 'expect', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ne', '##uro', '##psy', '##ch', 'testing', '?', 'hi', 'so', 'im', 'going', 'to', 'get', 'tested', 'for', 'ad', '##hd', ',', 'im', 'in', 'college', 'and', 'i', 'am', 'wondering', 'what', 'the', 'testing', 'is', 'like', 'if', 'others', 'have', 'done', 'it', '?', 'also', 'is', 'this', 'the', 'only', 'way', 'to', 'get', 'tested', '.', '.', '.', '?', 'i', 'enjoy', 'standardized', 'testing', 'so', 'would', 'my', 'ad', '##hd', 'if', 'i', 'have', 'it', 'even', 'show', 'up', '?', 'standardized', 'testing', 'is', 'one', 'of', 'the', 'few', 'things', 'that', 'i', 'can', 'concentrate', 'on', 'cause', 'its', 'almost', 'like', 'a', 'way', 'for', 'me', 'to', 'med', '##itate', 'and', 'forget', 'everything', 'else', '.', '.', '.', '.', 'but', 'i', 'have', 'almost', 'all', 'the', 'symptoms', 'of', 'the', 'ina', '##tten', '##tive', 'ad', '##hd', 'and', 'my', 'teachers', 'in', 'the', 'past', 'have', 'even', 'called', 'me', 'a', '\"', 'space', 'cadet', '\"', 'cause', 'i', 'can', 'never', 'focus', 'and', 'always', 'day', 'dream', 'and', 'my', 'parents', 'would', 'always', 'call', 'me', 'the', '\"', 'absent', 'minded', 'professor', '\"', '.', '.', '.', '.', '.', 'any', 'knowledge', 'on', 'these', 'kinds', 'of', 'tests', 'would', 'be', 'helpful', 'thanks']\n",
      "INFO:__main__:Number of tokens: 164\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ne', '##uro', '##psy', '##ch', 'testing', '?', 'hi', 'so', 'im', 'going', 'to', 'get', 'tested', 'for', 'ad', '##hd', ',', 'im', 'in', 'college', 'and', 'i', 'am', 'wondering', 'what', 'the', 'testing', 'is', 'like', 'if', 'others', 'have', 'done', 'it', '?', 'also', 'is', 'this', 'the', 'only', 'way', 'to', 'get', 'tested', '.', '.', '.', '?', 'i', 'enjoy', 'standardized', 'testing', 'so', 'would', 'my', 'ad', '##hd', 'if', 'i', 'have', 'it', 'even', 'show', 'up', '?', 'standardized', 'testing', 'is', 'one', 'of', 'the', 'few', 'things', 'that', 'i', 'can', 'concentrate', 'on', 'cause', 'its', 'almost', 'like', 'a', 'way', 'for', 'me', 'to', 'med', '##itate', 'and', 'forget', 'everything', 'else', '.', '.', '.', '.', 'but', 'i', 'have', 'almost', 'all', 'the', 'symptoms', 'of', 'the', 'ina', '##tten', '##tive', 'ad', '##hd', 'and', 'my', 'teachers', 'in', 'the', 'past', 'have', 'even', 'called', 'me', 'a', '\"', 'space', 'cadet', '\"', 'cause', 'i', 'can', 'never', 'focus', 'and', 'always', 'day', 'dream', 'and', 'my', 'parents', 'would', 'always', 'call', 'me', 'the', '\"', 'absent', 'minded', 'professor', '\"', '.', '.', '.', '.', '.', 'any', 'knowledge', 'on', 'these', 'kinds', 'of', 'tests', 'would', 'be', 'helpful', 'thanks']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['lets', 'talk', 'diet', '!', 'so', 'i', \"'\", 've', 'been', 'doing', 'a', 'little', 'research', 'on', 'the', 'side', 'trying', 'to', 'figure', 'out', '\"', 'the', 'ideal', 'ad', '##hd', 'diet', '\"', '.', 'basically', 'what', 'i', \"'\", 've', 'landed', 'on', 'is', 'that', 'there', 'isn', \"'\", 't', 'one', '.', 'the', 'science', 'is', 'new', 'and', 'basically', 'inc', '##on', '##clusive', '.', 'however', 'i', 'did', 'find', '*', 'some', '*', 'relative', 'consist', '##encies', 'perhaps', 'worth', 'some', 'discussion', '.', 'this', 'is', 'the', 'basis', 'of', 'what', 'i', 'have', 'found', '(', 'mostly', 'from', 'the', 'web', '##md', 'article', ')', ':', '*', 'eat', 'a', 'high', '-', 'protein', 'diet', ',', 'including', 'beans', ',', 'cheese', ',', 'eggs', ',', 'meat', ',', 'and', 'nuts', '.', 'add', 'protein', 'foods', 'in', 'the', 'morning', 'and', 'for', 'after', '-', 'school', 'snacks', ',', 'to', 'improve', 'concentration', 'and', 'possibly', 'increase', 'the', 'time', 'ad', '##hd', 'medications', 'work', '.', '*', 'eat', 'fewer', 'simple', 'car', '##bo', '##hy', '##dra', '##tes', ',', 'such', 'as', 'candy', ',', 'corn', 'syrup', ',', 'honey', ',', 'sugar', ',', 'products', 'made', 'from', 'white', 'flour', ',', 'white', 'rice', ',', 'and', 'potatoes', 'without', 'the', 'skins', '.', '*', 'eat', 'more', 'complex', 'car', '##bo', '##hy', '##dra', '##tes', ',', 'such', 'as', 'vegetables', 'and', 'some', 'fruits', '(', 'including', 'orange', '##s', ',', 'tang', '##erine', '##s', ',', 'pear', '##s', ',', 'grape', '##fr', '##uit', ',', 'apples', ',', 'and', 'ki', '##wi', ')', '.', 'eating', 'complex', 'car', '##bs', 'at', 'night', 'may', 'aid', 'sleep', '.', '*', 'eat', 'more', 'omega', '-', '3', 'fatty', 'acids', ',', 'such', 'as', 'those', 'found', 'in', 'tuna', ',', 'salmon', ',', 'other', 'cold', '-', 'water', 'white', 'fish', ',', 'walnut', '##s', ',', 'brazil', 'nuts', ',', 'and', 'olive', 'and', 'can', '##ola', 'oil', '.', 'omega', '-', '3', 'fatty', 'acids', 'are', 'also', 'available', 'in', 'supplement', 'form', '.', '*', 'am', '##en', 'and', 'so', '##gn', 'suggest', 'that', 'all', 'people', 'with', 'ad', '##hd', 'should', 'take', 'a', '100', '%', 'vitamin', 'and', 'mineral', 'supplement', 'each', 'day', '.', '*', 'some', 'children', 'do', 'become', 'hyper', '##active', 'after', 'eating', 'candy', 'or', 'other', 'sugar', '##y', 'foods', '.', 'no', 'evidence', 'indicates', ',', 'however', ',', 'that', 'this', 'is', 'a', 'cause', 'of', 'ad', '##hd', '.', 'for', 'best', 'overall', 'nutrition', ',', 'sugar', '##y', 'foods', 'should', 'be', 'a', 'small', 'part', 'of', 'anyone', \"'\", 's', 'diet', ',', 'though', 'there', 'is', 'probably', 'not', 'much', 'harm', 'for', 'a', 'child', 'or', 'adult', 'with', 'ad', '##hd', 'to', 'try', 'eliminating', 'sugar', '##y', 'foods', 'to', 'see', 'if', 'symptoms', 'improve', '.', 'i', 'think', 'if', 'adequate', 'exercise', 'and', 'breathing', 'exercises', '/', 'meditation', 'were', 'added', 'here', 'it', 'would', 'be', 'hard', 'to', 'go', 'wrong', '.', 'the', 'only', 'thing', 'i', 'can', 'personally', 'at', '##test', 'to', 'is', 'my', 'omega', '-', '3', 'supplement', 'which', 'drastically', 'helps', 'my', 'brain', 'fog', 'and', 'fatigue', ';', 'as', 'well', 'as', 'limiting', 'the', 'sugar', 'intake', 'to', 'help', 'keep', 'me', '\"', 'level', '\"', '.', 'from', 'what', 'i', \"'\", 've', 'read', 'it', 'seems', 'the', '[', 'pale', '##o', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'pale', '##o', '/', ')', 'and', '[', 'ke', '##to', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ke', '##to', '/', ')', 'diet', '##s', 'are', 'popular', 'among', 'ra', '##dd', '##iter', '##s', 'but', 'there', 'hasn', \"'\", 't', 'been', 'a', 'whole', 'lot', 'of', 'discussion', 'on', 'the', 'subject', '.', 'so', 'how', 'about', 'any', 'of', 'you', '?', 'have', 'you', 'been', 'able', 'to', 'pin', '##point', 'a', 'specific', 'food', 'in', 'your', 'diet', 'that', 'triggers', 'any', 'symptoms', 'or', 'found', 'a', 'supplement', '/', 'diet', 'plan', 'that', 'works', 'especially', 'well', 'for', 'you', '?', '[', 'web', '##md', 'article', ']', '(', 'http', ':', '/', '/', 'www', '.', 'web', '##md', '.', 'com', '/', 'add', '-', 'ad', '##hd', '/', 'guide', '/', 'ad', '##hd', '-', 'diet', '##s', ')', '[', 'the', 'fein', '##gold', 'diet', 'program', ']', '(', 'http', ':', '/', '/', 'www', '.', 'fein', '##gold', '.', 'org', '/', ')', '[', 'a', 'short', 'mayo', 'clinic', 'blur', '##b', 'about', 'additive', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'mayo', '##cl', '##ini', '##c', '.', 'com', '/', 'health', '/', 'ad', '##hd', '/', 'an', '##01', '##7', '##21', ')']\n",
      "INFO:__main__:Number of tokens: 630\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['lets', 'talk', 'diet', '!', 'so', 'i', \"'\", 've', 'been', 'doing', 'a', 'little', 'research', 'on', 'the', 'side', 'trying', 'to', 'figure', 'out', '\"', 'the', 'ideal', 'ad', '##hd', 'diet', '\"', '.', 'basically', 'what', 'i', \"'\", 've', 'landed', 'on', 'is', 'that', 'there', 'isn', \"'\", 't', 'one', '.', 'the', 'science', 'is', 'new', 'and', 'basically', 'inc', '##on', '##clusive', '.', 'however', 'i', 'did', 'find', '*', 'some', '*', 'relative', 'consist', '##encies', 'perhaps', 'worth', 'some', 'discussion', '.', 'this', 'is', 'the', 'basis', 'of', 'what', 'i', 'have', 'found', '(', 'mostly', 'from', 'the', 'web', '##md', 'article', ')', ':', '*', 'eat', 'a', 'high', '-', 'protein', 'diet', ',', 'including', 'beans', ',', 'cheese', ',', 'eggs', ',', 'meat', ',', 'and', 'nuts', '.', 'add', 'protein', 'foods', 'in', 'the', 'morning', 'and', 'for', 'after', '-', 'school', 'snacks', ',', 'to', 'improve', 'concentration', 'and', 'possibly', 'increase', 'the', 'time', 'ad', '##hd', 'medications', 'work', '.', '*', 'eat', 'fewer', 'simple', 'car', '##bo', '##hy', '##dra', '##tes', ',', 'such', 'as', 'candy', ',', 'corn', 'syrup', ',', 'honey', ',', 'sugar', ',', 'products', 'made', 'from', 'white', 'flour', ',', 'white', 'rice', ',', 'and', 'potatoes', 'without', 'the', 'skins', '.', '*', 'eat', 'more', 'complex', 'car', '##bo', '##hy', '##dra', '##tes', ',', 'such', 'as', 'vegetables', 'and', 'some', 'fruits', '(', 'including', 'orange', '##s', ',', 'tang', '##erine', '##s', ',', 'pear', '##s', ',', 'grape', '##fr', '##uit', ',', 'apples', ',', 'and', 'ki', '##wi', ')', '.', 'eating', 'complex', 'car', '##bs', 'at', 'night', 'may', 'aid', 'sleep', '.', '*', 'eat', 'more', 'omega', '-', '3', 'fatty', 'acids', ',', 'such', 'as', 'those', 'found', 'in', 'tuna', ',', 'salmon', ',', 'other', 'cold', '-', 'water', 'white', 'fish', ',', 'walnut', '##s', ',', 'brazil', 'nuts', ',', 'and', 'olive', 'and', 'can', '##ola', 'oil', '.', 'omega', '-', '3', 'fatty', 'acids', 'are', 'also', 'available', 'in', 'supplement', 'form', '.', '*', 'am', '##en', 'and', 'so', '##gn', 'suggest', 'that', 'all', 'people', 'with', 'ad', '##hd', 'should', 'take', 'a', '100', '%', 'vitamin', 'and', 'mineral', 'supplement', 'each', 'day', '.', '*', 'some', 'children', 'do', 'become', 'hyper', '##active', 'after', 'eating', 'candy', 'or', 'other', 'sugar', '##y', 'foods', '.', 'no', 'evidence', 'indicates', ',', 'however', ',', 'that', 'this', 'is', 'a', 'cause', 'of', 'ad', '##hd', '.', 'for', 'best', 'overall', 'nutrition', ',', 'sugar', '##y', 'foods', 'should', 'be', 'a', 'small', 'part', 'of', 'anyone', \"'\", 's', 'diet', ',', 'though', 'there', 'is', 'probably', 'not', 'much', 'harm', 'for', 'a', 'child', 'or', 'adult', 'with', 'ad', '##hd', 'to', 'try', 'eliminating', 'sugar', '##y', 'foods', 'to', 'see', 'if', 'symptoms', 'improve', '.', 'i', 'think', 'if', 'adequate', 'exercise', 'and', 'breathing', 'exercises', '/', 'meditation', 'were', 'added', 'here', 'it', 'would', 'be', 'hard', 'to', 'go', 'wrong', '.', 'the', 'only', 'thing', 'i', 'can', 'personally', 'at', '##test', 'to', 'is', 'my', 'omega', '-', '3', 'supplement', 'which', 'drastically', 'helps', 'my', 'brain', 'fog', 'and', 'fatigue', ';', 'as', 'well', 'as', 'limiting', 'the', 'sugar', 'intake', 'to', 'help', 'keep', 'me', '\"', 'level', '\"', '.', 'from', 'what', 'i', \"'\", 've', 'read', 'it', 'seems', 'the', '[', 'pale', '##o', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'pale', '##o', '/', ')', 'and', '[', 'ke', '##to', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ke', '##to', '/', ')', 'diet', '##s', 'are', 'popular', 'among', 'ra', '##dd', '##iter', '##s', 'but', 'there', 'hasn', \"'\", 't', 'been', 'a', 'whole', 'lot', 'of', 'discussion', 'on', 'the', 'subject', '.', 'so', 'how', 'about', 'any', 'of'], ['you', '?', 'have', 'you', 'been', 'able', 'to', 'pin', '##point', 'a', 'specific', 'food', 'in', 'your', 'diet', 'that', 'triggers', 'any', 'symptoms', 'or', 'found', 'a', 'supplement', '/', 'diet', 'plan', 'that', 'works', 'especially', 'well', 'for', 'you', '?', '[', 'web', '##md', 'article', ']', '(', 'http', ':', '/', '/', 'www', '.', 'web', '##md', '.', 'com', '/', 'add', '-', 'ad', '##hd', '/', 'guide', '/', 'ad', '##hd', '-', 'diet', '##s', ')', '[', 'the', 'fein', '##gold', 'diet', 'program', ']', '(', 'http', ':', '/', '/', 'www', '.', 'fein', '##gold', '.', 'org', '/', ')', '[', 'a', 'short', 'mayo', 'clinic', 'blur', '##b', 'about', 'additive', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'mayo', '##cl', '##ini', '##c', '.', 'com', '/', 'health', '/', 'ad', '##hd', '/', 'an', '##01', '##7', '##21', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['are', 'any', 'you', 'quiet', '?', 'hey', '!', 'i', 'was', 'diagnosed', 'this', 'year', 'and', 'have', 'been', 'on', 'medication', ',', 'but', 'prior', 'to', 'this', ',', 'i', 'was', 'constantly', 'looking', 'up', 'information', 'about', 'ad', '##hd', '.', 'one', 'of', 'the', 'most', 'common', 'signs', 'is', 'that', 'people', 'with', 'it', 'talk', 'a', 'lot', 'naturally', '.', 'i', 'always', 'see', 'things', 'saying', 'they', 'will', 'say', 'things', 'without', 'thinking', 'and', 'such', ',', 'however', ',', 'i', \"'\", 'm', 'completely', 'opposite', '.', 'i', \"'\", 'm', 'a', 'naturally', 'quiet', 'person', '.', 'i', 'have', 'a', 'million', 'things', 'going', 'on', 'in', 'my', 'mind', ',', 'but', 'it', \"'\", 's', 'like', 'i', 'just', 'never', 'let', 'them', 'out', '.', 'not', 'a', 'psychologist', ',', 'but', 'i', \"'\", 'm', 'pretty', 'sure', 'that', 'this', 'sp', '##routed', 'around', 'late', 'elementary', 'and', 'early', 'middle', 'school', 'with', 'my', 'sister', '.', 'she', \"'\", 's', 'two', 'years', 'older', 'than', 'me', ',', 'so', 'around', 'the', 'time', 'she', 'was', 'in', '7th', 'to', '8th', 'grade', '(', '5th', 'and', '6th', 'for', 'me', ')', ',', 'whenever', 'i', 'would', 'start', 'bug', '##ging', 'her', 'and', 'her', 'friends', ',', 'i', 'would', 'get', 'the', 'generic', '\"', 'go', 'away', ',', '\"', '\"', 'no', 'one', 'cares', ',', '\"', '\"', 'leave', 'us', 'alone', '\"', 'responses', '(', 'not', 'all', 'the', 'time', ',', 'but', 'sometimes', ')', 'and', 'i', 'guess', 'i', 'sort', 'of', 'took', 'it', 'to', 'heart', 'and', 'started', 'to', 'cl', '##amp', 'up', ',', 'but', 'i', 'dig', '##ress', '.', 'that', 'little', '^', 'sp', '##iel', '^', 'isn', \"'\", 't', 'really', 'important', ',', 'but', 'i', 'just', 'wanted', 'to', 'know', 'does', 'anyone', 'else', 'do', 'this', '?', 'instead', 'of', 'simply', 'letting', 'all', 'the', 'thoughts', 'out', ',', 'you', 'stay', 'quiet', 'and', 'none', 'of', 'them', 'come', 'out', '.', 'edit', ':', 'well', 'it', \"'\", 's', 'nice', 'to', 'know', 'i', \"'\", 'm', 'not', 'the', 'only', 'one', '.', 'it', 'seems', 'like', 'it', \"'\", 's', 'not', 'uncommon', 'for', 'people', 'with', 'ad', '##hd', 'to', 'be', 'quiet', 'in', 'new', 'situations', '.', 'i', 'wonder', 'if', 'it', 'has', 'to', 'do', 'with', 'maturity', 'as', 'we', 'grow', 'older', 'and', 'forming', 'into', 'society', '?', 'my', 'assumption', 'is', 'that', 'it', \"'\", 's', 'similar', 'to', 'a', 'heart', 'attack', '.', 'as', 'we', 'get', 'older', ',', 'we', 'learn', 'to', 'filter', 'our', 'thoughts', '.', 'but', 'when', 'the', 'time', 'comes', 'to', 'speak', 'and', 'we', 'attempt', 'to', 'open', 'a', 'little', ',', 'we', 'have', 'so', 'many', 'different', 'things', 'trying', 'to', 'escape', 'that', 'they', 'cl', '##og', 'the', 'passage', 'and', 'nothing', 'gets', 'through', '.', 'however', ',', 'when', 'we', \"'\", 're', 'with', 'close', 'friends', ',', 'we', 'simply', 'open', 'up', 'the', 'flood', 'gates', '(', 'and', 'now', 'the', 'flood', 'gates', 'will', 'open', '!', '!', '!', '<', '-', '-', '[', 'enter', 'shi', '##kari', 'reference', ']', ')', 'and', 'everything', 'pour', '##s', 'out', '.']\n",
      "INFO:__main__:Number of tokens: 421\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['are', 'any', 'you', 'quiet', '?', 'hey', '!', 'i', 'was', 'diagnosed', 'this', 'year', 'and', 'have', 'been', 'on', 'medication', ',', 'but', 'prior', 'to', 'this', ',', 'i', 'was', 'constantly', 'looking', 'up', 'information', 'about', 'ad', '##hd', '.', 'one', 'of', 'the', 'most', 'common', 'signs', 'is', 'that', 'people', 'with', 'it', 'talk', 'a', 'lot', 'naturally', '.', 'i', 'always', 'see', 'things', 'saying', 'they', 'will', 'say', 'things', 'without', 'thinking', 'and', 'such', ',', 'however', ',', 'i', \"'\", 'm', 'completely', 'opposite', '.', 'i', \"'\", 'm', 'a', 'naturally', 'quiet', 'person', '.', 'i', 'have', 'a', 'million', 'things', 'going', 'on', 'in', 'my', 'mind', ',', 'but', 'it', \"'\", 's', 'like', 'i', 'just', 'never', 'let', 'them', 'out', '.', 'not', 'a', 'psychologist', ',', 'but', 'i', \"'\", 'm', 'pretty', 'sure', 'that', 'this', 'sp', '##routed', 'around', 'late', 'elementary', 'and', 'early', 'middle', 'school', 'with', 'my', 'sister', '.', 'she', \"'\", 's', 'two', 'years', 'older', 'than', 'me', ',', 'so', 'around', 'the', 'time', 'she', 'was', 'in', '7th', 'to', '8th', 'grade', '(', '5th', 'and', '6th', 'for', 'me', ')', ',', 'whenever', 'i', 'would', 'start', 'bug', '##ging', 'her', 'and', 'her', 'friends', ',', 'i', 'would', 'get', 'the', 'generic', '\"', 'go', 'away', ',', '\"', '\"', 'no', 'one', 'cares', ',', '\"', '\"', 'leave', 'us', 'alone', '\"', 'responses', '(', 'not', 'all', 'the', 'time', ',', 'but', 'sometimes', ')', 'and', 'i', 'guess', 'i', 'sort', 'of', 'took', 'it', 'to', 'heart', 'and', 'started', 'to', 'cl', '##amp', 'up', ',', 'but', 'i', 'dig', '##ress', '.', 'that', 'little', '^', 'sp', '##iel', '^', 'isn', \"'\", 't', 'really', 'important', ',', 'but', 'i', 'just', 'wanted', 'to', 'know', 'does', 'anyone', 'else', 'do', 'this', '?', 'instead', 'of', 'simply', 'letting', 'all', 'the', 'thoughts', 'out', ',', 'you', 'stay', 'quiet', 'and', 'none', 'of', 'them', 'come', 'out', '.', 'edit', ':', 'well', 'it', \"'\", 's', 'nice', 'to', 'know', 'i', \"'\", 'm', 'not', 'the', 'only', 'one', '.', 'it', 'seems', 'like', 'it', \"'\", 's', 'not', 'uncommon', 'for', 'people', 'with', 'ad', '##hd', 'to', 'be', 'quiet', 'in', 'new', 'situations', '.', 'i', 'wonder', 'if', 'it', 'has', 'to', 'do', 'with', 'maturity', 'as', 'we', 'grow', 'older', 'and', 'forming', 'into', 'society', '?', 'my', 'assumption', 'is', 'that', 'it', \"'\", 's', 'similar', 'to', 'a', 'heart', 'attack', '.', 'as', 'we', 'get', 'older', ',', 'we', 'learn', 'to', 'filter', 'our', 'thoughts', '.', 'but', 'when', 'the', 'time', 'comes', 'to', 'speak', 'and', 'we', 'attempt', 'to', 'open', 'a', 'little', ',', 'we', 'have', 'so', 'many', 'different', 'things', 'trying', 'to', 'escape', 'that', 'they', 'cl', '##og', 'the', 'passage', 'and', 'nothing', 'gets', 'through', '.', 'however', ',', 'when', 'we', \"'\", 're', 'with', 'close', 'friends', ',', 'we', 'simply', 'open', 'up', 'the', 'flood', 'gates', '(', 'and', 'now', 'the', 'flood', 'gates', 'will', 'open', '!', '!', '!', '<', '-', '-', '[', 'enter', 'shi', '##kari', 'reference', ']', ')', 'and', 'everything', 'pour', '##s', 'out', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'an', '11', '##mm', 'ara', '##ch', '##no', '##id', 'cy', '##st', 'in', 'my', 'brain', '[', 'proof', 'inside', ']', '.']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'an', '11', '##mm', 'ara', '##ch', '##no', '##id', 'cy', '##st', 'in', 'my', 'brain', '[', 'proof', 'inside', ']', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'ad', '##hd', '?', '(', 'a', 'photo', 'mont', '##age', ')', 'http', ':', '/', '/', 'www', '.', 'julian', '##nan', '##wil', '##son', '.', 'com', '/', '2012', '/', '02', '/', 'what', '-', 'is', '-', 'ad', '##hd', '-', 'photo', '-', 'mont', '##age', '.', 'html']\n",
      "INFO:__main__:Number of tokens: 41\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'ad', '##hd', '?', '(', 'a', 'photo', 'mont', '##age', ')', 'http', ':', '/', '/', 'www', '.', 'julian', '##nan', '##wil', '##son', '.', 'com', '/', '2012', '/', '02', '/', 'what', '-', 'is', '-', 'ad', '##hd', '-', 'photo', '-', 'mont', '##age', '.', 'html']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['v', '##y', '##van', '##se', 'sweaty', 'hands', 'and', 'numb', '##ness', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['v', '##y', '##van', '##se', 'sweaty', 'hands', 'and', 'numb', '##ness', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'tolerance', 'i', 'was', 'wondering', 'if', 'any', 'of', 'you', 'could', 'answer', 'a', 'question', 'regarding', 'add', '##eral', '##l', 'tolerance', '.', 'does', 'the', 'tolerance', 'ever', 'end', '?', 'is', 'there', 'a', 'dose', 'that', 'i', 'will', 'be', 'able', 'to', 'take', 'that', 'is', 'continuously', 'effective', '?', 'for', 'example', ',', 'i', 'am', 'on', '20', 'mg', 'right', 'now', 'and', 'i', 'feel', 'that', 'it', 'isn', \"'\", 't', 'working', 'anymore', '.', 'if', 'i', 'go', 'to', '30', '.', '.', 'what', 'about', '40', '?', 'what', 'happens', 'if', 'i', 'go', 'passed', 'that', 'as', 'well', 'just', 'to', 'get', 'an', 'effect', '?', 'how', 'does', 'this', 'work', '?', 'if', 'anyone', 'knows', 'it', 'would', 'be', 'much', 'appreciated', '.', 'thanks', '.']\n",
      "INFO:__main__:Number of tokens: 106\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'tolerance', 'i', 'was', 'wondering', 'if', 'any', 'of', 'you', 'could', 'answer', 'a', 'question', 'regarding', 'add', '##eral', '##l', 'tolerance', '.', 'does', 'the', 'tolerance', 'ever', 'end', '?', 'is', 'there', 'a', 'dose', 'that', 'i', 'will', 'be', 'able', 'to', 'take', 'that', 'is', 'continuously', 'effective', '?', 'for', 'example', ',', 'i', 'am', 'on', '20', 'mg', 'right', 'now', 'and', 'i', 'feel', 'that', 'it', 'isn', \"'\", 't', 'working', 'anymore', '.', 'if', 'i', 'go', 'to', '30', '.', '.', 'what', 'about', '40', '?', 'what', 'happens', 'if', 'i', 'go', 'passed', 'that', 'as', 'well', 'just', 'to', 'get', 'an', 'effect', '?', 'how', 'does', 'this', 'work', '?', 'if', 'anyone', 'knows', 'it', 'would', 'be', 'much', 'appreciated', '.', 'thanks', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['don', \"'\", 't', 'eat', 'on', 'my', 'med', '##s', 'i', 'literally', 'have', 'to', 'force', 'myself', 'to', 'eat', 'something', 'when', 'i', 'take', 'my', 'med', '##s', '.', 'i', 'am', '5', '`', '1', 'and', '100', '##lb', '.', 'off', 'them', 'i', 'gain', '5', '##lb', 'per', 'week', '.', 'i', 'am', 'just', 'repulsed', 'by', 'food', 'on', 'the', 'med', '##s', '.', 'even', 'though', 'i', 'might', 'order', 'something', '(', 'i', 'am', 'at', 'school', ')', ',', 'i', 'will', 'only', 'eat', 'half', '.', 'i', 'get', 'stares', 'then', 'about', 'wasting', 'food', '.', 'how', 'can', 'i', 'force', 'myself', 'to', 'eat', '?']\n",
      "INFO:__main__:Number of tokens: 88\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['don', \"'\", 't', 'eat', 'on', 'my', 'med', '##s', 'i', 'literally', 'have', 'to', 'force', 'myself', 'to', 'eat', 'something', 'when', 'i', 'take', 'my', 'med', '##s', '.', 'i', 'am', '5', '`', '1', 'and', '100', '##lb', '.', 'off', 'them', 'i', 'gain', '5', '##lb', 'per', 'week', '.', 'i', 'am', 'just', 'repulsed', 'by', 'food', 'on', 'the', 'med', '##s', '.', 'even', 'though', 'i', 'might', 'order', 'something', '(', 'i', 'am', 'at', 'school', ')', ',', 'i', 'will', 'only', 'eat', 'half', '.', 'i', 'get', 'stares', 'then', 'about', 'wasting', 'food', '.', 'how', 'can', 'i', 'force', 'myself', 'to', 'eat', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', ',', 'cigarettes', ',', 'and', 'drugs', '.', 'since', 'i', \"'\", 'm', 'sure', 'most', 'here', 'by', 'now', 'have', 'read', 'an', 'article', 'or', 'heard', 'someone', 'mention', 'that', 'some', 'research', 'has', 'shown', 'those', 'with', 'ad', '##hd', 'have', 'a', 'higher', 'likely', 'hood', 'of', '\"', 'substance', 'abuse', '\"', 'including', ',', 'but', 'not', 'limited', 'to', ':', 'cigarettes', ',', 'alcohol', ',', 'illicit', 'drugs', ',', 'etc', '.', '.', '.', '.', '*', '*', 'to', 'be', 'very', 'clear', 'i', \"'\", 'm', 'not', 'seeking', 'to', 'discuss', 'recreational', 'use', 'of', 'drugs', 'as', 'it', 'is', 'clearly', 'against', 'the', 'rules', 'in', 'the', 'side', '##bar', '*', '*', 'i', \"'\", 'm', 'more', 'curious', 'about', 'how', 'many', 'here', 'match', 'this', 'type', 'of', 'description', 'of', 'ad', '##hd', 'people', '.', '*', 'if', 'you', 'fit', 'this', 'type', 'of', 'ad', '##hd', 'personality', 'what', 'is', 'it', 'you', 'do', '/', 'did', '/', 'have', 'done', '?', '*', 'what', 'was', 'your', 'experience', '?', '*', 'what', 'made', 'you', 'start', '?', '*', 'have', 'you', 'stopped', '?', '*', 'has', 'being', 'diagnosed', 'or', 'getting', 'medication', 'changed', 'these', 'habits', '?', '*', 'why', 'do', 'you', 'think', 'you', 'are', 'more', 'prone', 'to', 'such', 'behavior', '?', '*', '*', 'here', \"'\", 's', 'my', 'experience', ':', '*', '*', '*', 'i', 'smoke', 'cigarettes', ',', 'i', 'occasionally', 'use', 'cannabis', ',', 'have', 'done', 'l', '##sd', '.', '*', 'i', 'started', 'smoking', 'second', 'semester', 'of', 'college', 'when', 'everything', 'was', 'going', 'done', 'hill', 'and', 'i', 'was', 'having', 'a', 'lot', 'of', 'emotional', 'and', 'stress', 'problems', '.', 'i', 'still', 'smoke', 'almost', '3', 'years', 'later', '.', 'i', 'then', 'tried', 'cannabis', 'with', 'some', 'friends', 'who', 'smoked', 'in', 'highs', '##cho', '##ol', 'and', 'yes', 'it', 'caused', 'me', 'many', 'problems', ',', 'it', 'made', 'my', 'already', 'horrible', 'pro', '##cr', '##ast', '##ination', 'and', 'time', 'management', 'issues', 'worse', 'and', 'created', 'a', 'lot', 'of', 'wasted', 'time', 'and', 'money', '.', '*', 'i', 'started', 'due', 'to', 'the', 'promise', 'of', 'escape', '.', 'both', 'cigarettes', 'and', 'cannabis', 'released', 'me', 'from', 'my', 'problems', ',', 'cannabis', 'more', 'so', '.', '*', 'i', 'still', 'smoke', 'cigarettes', 'like', 'a', 'train', 'but', 'i', 'have', 'stopped', 'cannabis', ',', 'this', 'semester', 'at', 'least', '.', '*', 'i', 'stopped', 'smoking', 'when', 'i', 'first', 'got', 'diagnosed', 'and', 'put', 'on', 'medication', '.', 'i', 'was', 'thinking', 'it', 'might', 'help', 'me', 'call', 'it', 'quit', '##s', 'if', 'the', 'medication', 'is', 'supplying', 'the', 'stimulation', 'my', 'brain', 'wants', '.', 'started', 'back', 'up', 'very', 'quickly', 'and', 'it', 'almost', 'makes', 'me', 'want', 'to', 'smoke', 'more', '.', 'for', 'everything', 'else', 'i', 'do', 'not', 'want', 'to', 'mix', 'medication', 'with', 'any', 'other', 'substances', '.', '*', 'i', 'don', \"'\", 't', 'think', 'i', 'could', 'pin', '##point', 'a', 'reason', 'i', 'think', 'i', \"'\", 'm', 'more', 'prone', 'than', 'others', 'due', 'to', 'the', 'ad', '##hd', 'aspect', 'in', 'my', 'life', '.', 'i', 'have', 'a', 'lot', 'of', 'other', 'issues', 'that', 'contribute', 'to', 'why', 'i', 'do', 'what', 'i', 'do', ',', 'depression', 'being', 'a', 'big', 'one', '.', 'which', 'i', 'think', 'is', 'a', 'sign', 'of', 'the', 'pressure', 'this', 'societal', 'structure', 'puts', 'on', 'ad', '##hd', 'brains', 'and', 'the', 'damage', 'it', 'causes', 'the', 'person', ',', 'just', 'thoughts', 'i', 'really', 'have', 'no', 'idea', '.', '*', '*', 'conclusion', '*', '*', 'so', 'that', \"'\", 's', 'pretty', 'much', 'it', ',', 'just', 'interested', 'in', 'other', 'ad', '##hd', 'people', \"'\", 's', 'experiences', '.', '.', 'if', 'any', 'of', 'you', 'are', 'cigarette', 'smoke', '##rs', 'you', 'should', 'check', 'out', '[', '/', 'r', '/', 'cigarettes', ']', '(', '/', 'r', '/', 'cigarettes', ')', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', 'some', 'research', 'suggest', 'people', 'with', 'ad', '##hd', 'are', 'more', 'prone', 'to', 'substance', 'abuse', '.', 'what', 'are', 'your', 'experiences', 'with', 'substance', 'use', 'and', '/', 'or', 'abuse', '?', 'i', 'smoke', 'a', 'lot', 'of', 'cigarettes', '.']\n",
      "INFO:__main__:Number of tokens: 565\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['ad', '##hd', ',', 'cigarettes', ',', 'and', 'drugs', '.', 'since', 'i', \"'\", 'm', 'sure', 'most', 'here', 'by', 'now', 'have', 'read', 'an', 'article', 'or', 'heard', 'someone', 'mention', 'that', 'some', 'research', 'has', 'shown', 'those', 'with', 'ad', '##hd', 'have', 'a', 'higher', 'likely', 'hood', 'of', '\"', 'substance', 'abuse', '\"', 'including', ',', 'but', 'not', 'limited', 'to', ':', 'cigarettes', ',', 'alcohol', ',', 'illicit', 'drugs', ',', 'etc', '.', '.', '.', '.', '*', '*', 'to', 'be', 'very', 'clear', 'i', \"'\", 'm', 'not', 'seeking', 'to', 'discuss', 'recreational', 'use', 'of', 'drugs', 'as', 'it', 'is', 'clearly', 'against', 'the', 'rules', 'in', 'the', 'side', '##bar', '*', '*', 'i', \"'\", 'm', 'more', 'curious', 'about', 'how', 'many', 'here', 'match', 'this', 'type', 'of', 'description', 'of', 'ad', '##hd', 'people', '.', '*', 'if', 'you', 'fit', 'this', 'type', 'of', 'ad', '##hd', 'personality', 'what', 'is', 'it', 'you', 'do', '/', 'did', '/', 'have', 'done', '?', '*', 'what', 'was', 'your', 'experience', '?', '*', 'what', 'made', 'you', 'start', '?', '*', 'have', 'you', 'stopped', '?', '*', 'has', 'being', 'diagnosed', 'or', 'getting', 'medication', 'changed', 'these', 'habits', '?', '*', 'why', 'do', 'you', 'think', 'you', 'are', 'more', 'prone', 'to', 'such', 'behavior', '?', '*', '*', 'here', \"'\", 's', 'my', 'experience', ':', '*', '*', '*', 'i', 'smoke', 'cigarettes', ',', 'i', 'occasionally', 'use', 'cannabis', ',', 'have', 'done', 'l', '##sd', '.', '*', 'i', 'started', 'smoking', 'second', 'semester', 'of', 'college', 'when', 'everything', 'was', 'going', 'done', 'hill', 'and', 'i', 'was', 'having', 'a', 'lot', 'of', 'emotional', 'and', 'stress', 'problems', '.', 'i', 'still', 'smoke', 'almost', '3', 'years', 'later', '.', 'i', 'then', 'tried', 'cannabis', 'with', 'some', 'friends', 'who', 'smoked', 'in', 'highs', '##cho', '##ol', 'and', 'yes', 'it', 'caused', 'me', 'many', 'problems', ',', 'it', 'made', 'my', 'already', 'horrible', 'pro', '##cr', '##ast', '##ination', 'and', 'time', 'management', 'issues', 'worse', 'and', 'created', 'a', 'lot', 'of', 'wasted', 'time', 'and', 'money', '.', '*', 'i', 'started', 'due', 'to', 'the', 'promise', 'of', 'escape', '.', 'both', 'cigarettes', 'and', 'cannabis', 'released', 'me', 'from', 'my', 'problems', ',', 'cannabis', 'more', 'so', '.', '*', 'i', 'still', 'smoke', 'cigarettes', 'like', 'a', 'train', 'but', 'i', 'have', 'stopped', 'cannabis', ',', 'this', 'semester', 'at', 'least', '.', '*', 'i', 'stopped', 'smoking', 'when', 'i', 'first', 'got', 'diagnosed', 'and', 'put', 'on', 'medication', '.', 'i', 'was', 'thinking', 'it', 'might', 'help', 'me', 'call', 'it', 'quit', '##s', 'if', 'the', 'medication', 'is', 'supplying', 'the', 'stimulation', 'my', 'brain', 'wants', '.', 'started', 'back', 'up', 'very', 'quickly', 'and', 'it', 'almost', 'makes', 'me', 'want', 'to', 'smoke', 'more', '.', 'for', 'everything', 'else', 'i', 'do', 'not', 'want', 'to', 'mix', 'medication', 'with', 'any', 'other', 'substances', '.', '*', 'i', 'don', \"'\", 't', 'think', 'i', 'could', 'pin', '##point', 'a', 'reason', 'i', 'think', 'i', \"'\", 'm', 'more', 'prone', 'than', 'others', 'due', 'to', 'the', 'ad', '##hd', 'aspect', 'in', 'my', 'life', '.', 'i', 'have', 'a', 'lot', 'of', 'other', 'issues', 'that', 'contribute', 'to', 'why', 'i', 'do', 'what', 'i', 'do', ',', 'depression', 'being', 'a', 'big', 'one', '.', 'which', 'i', 'think', 'is', 'a', 'sign', 'of', 'the', 'pressure', 'this', 'societal', 'structure', 'puts', 'on', 'ad', '##hd', 'brains', 'and', 'the', 'damage', 'it', 'causes', 'the', 'person', ',', 'just', 'thoughts', 'i', 'really', 'have', 'no', 'idea', '.', '*', '*', 'conclusion', '*', '*', 'so', 'that', \"'\", 's', 'pretty', 'much', 'it', ',', 'just', 'interested', 'in', 'other', 'ad', '##hd', 'people', \"'\", 's', 'experiences', '.', '.', 'if', 'any', 'of', 'you', 'are', 'cigarette', 'smoke', '##rs', 'you', 'should', 'check', 'out', '['], ['/', 'r', '/', 'cigarettes', ']', '(', '/', 'r', '/', 'cigarettes', ')', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', 'some', 'research', 'suggest', 'people', 'with', 'ad', '##hd', 'are', 'more', 'prone', 'to', 'substance', 'abuse', '.', 'what', 'are', 'your', 'experiences', 'with', 'substance', 'use', 'and', '/', 'or', 'abuse', '?', 'i', 'smoke', 'a', 'lot', 'of', 'cigarettes', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['generic', 'concert', '##a', 'is', 'anyone', 'taking', 'generic', 'concert', '##a', '?', 'how', 'much', 'do', 'you', 'pay', 'for', 'it', 'per', 'month', 'either', 'out', 'of', 'pocket', 'or', 'w', '/', 'insurance', '?', 'i', \"'\", 've', 'just', 'got', 'a', 'script', 'for', 'concert', '##a', '36', '##mg', '.', 'my', 'psychiatrist', 'told', 'me', 'it', 'had', 'recently', 'gone', 'generic', ',', 'but', 'i', \"'\", 've', 'called', 'around', '15', 'ph', '##arm', '##acies', 'and', 'their', 'quote', 'ranges', 'from', '$', '160', '~', '$', '226', '/', 'month', '.', 'that', \"'\", 's', 'almost', 'as', 'much', 'as', 'the', 'brand', 'name', '!', 'i', 'live', 'in', 'o', '##c', 'by', 'the', 'way', ',', 'and', 'i', 'do', 'have', 'insurance', '.', 'it', 'only', 'covers', '50', '%', 'of', 'my', 'medication', 'cost', 'though', '.']\n",
      "INFO:__main__:Number of tokens: 111\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['generic', 'concert', '##a', 'is', 'anyone', 'taking', 'generic', 'concert', '##a', '?', 'how', 'much', 'do', 'you', 'pay', 'for', 'it', 'per', 'month', 'either', 'out', 'of', 'pocket', 'or', 'w', '/', 'insurance', '?', 'i', \"'\", 've', 'just', 'got', 'a', 'script', 'for', 'concert', '##a', '36', '##mg', '.', 'my', 'psychiatrist', 'told', 'me', 'it', 'had', 'recently', 'gone', 'generic', ',', 'but', 'i', \"'\", 've', 'called', 'around', '15', 'ph', '##arm', '##acies', 'and', 'their', 'quote', 'ranges', 'from', '$', '160', '~', '$', '226', '/', 'month', '.', 'that', \"'\", 's', 'almost', 'as', 'much', 'as', 'the', 'brand', 'name', '!', 'i', 'live', 'in', 'o', '##c', 'by', 'the', 'way', ',', 'and', 'i', 'do', 'have', 'insurance', '.', 'it', 'only', 'covers', '50', '%', 'of', 'my', 'medication', 'cost', 'though', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['had', 'a', 'panic', 'attack', '.', 'vomit', '##ed', 'up', 'my', 'v', '##y', '##van', '##se', '.', 'this', 'is', 'the', 'first', 'day', 'i', 'haven', \"'\", 't', 'had', 'my', 'medication', 'since', 'starting', 'treatment', '.']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['had', 'a', 'panic', 'attack', '.', 'vomit', '##ed', 'up', 'my', 'v', '##y', '##van', '##se', '.', 'this', 'is', 'the', 'first', 'day', 'i', 'haven', \"'\", 't', 'had', 'my', 'medication', 'since', 'starting', 'treatment', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'tried', 'the', 'ps', '##ych', '##med', '##op', '##ti', '##mi', '##zer', '-', 'add', 'app', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'tried', 'the', 'ps', '##ych', '##med', '##op', '##ti', '##mi', '##zer', '-', 'add', 'app', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'know', 'any', 'doctors', 'in', 'massachusetts', 'that', 'i', 'could', 'use', 'to', 'get', 'tested', '?', 'my', 'doctor', 'was', 'going', 'to', 'have', 'my', 'see', 'a', 'certain', 'ne', '##uro', '##logist', ',', 'but', 'he', 'has', 'no', 'open', 'appointments', 'until', 'middle', 'of', 'may', ':', '/', '.', 'any', 'doctors', 'people', 'could', 'recommend', 'in', 'mass', '?', 'also', 'does', 'it', 'have', 'to', 'be', 'a', 'ne', '##uro', '##logist', 'to', 'dia', '##gno', '##se', 'you', 'or', 'could', 'i', 'use', 'a', 'psychiatrist', 'or', 'some', 'other', 'type', 'of', 'doctor', '?', 'thanks', 'for', 'any', 'help', '!']\n",
      "INFO:__main__:Number of tokens: 82\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'know', 'any', 'doctors', 'in', 'massachusetts', 'that', 'i', 'could', 'use', 'to', 'get', 'tested', '?', 'my', 'doctor', 'was', 'going', 'to', 'have', 'my', 'see', 'a', 'certain', 'ne', '##uro', '##logist', ',', 'but', 'he', 'has', 'no', 'open', 'appointments', 'until', 'middle', 'of', 'may', ':', '/', '.', 'any', 'doctors', 'people', 'could', 'recommend', 'in', 'mass', '?', 'also', 'does', 'it', 'have', 'to', 'be', 'a', 'ne', '##uro', '##logist', 'to', 'dia', '##gno', '##se', 'you', 'or', 'could', 'i', 'use', 'a', 'psychiatrist', 'or', 'some', 'other', 'type', 'of', 'doctor', '?', 'thanks', 'for', 'any', 'help', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'introduction', ',', 'background', ',', 'and', 'questions', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'introduction', ',', 'background', ',', 'and', 'questions', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'know', 'when', 'your', 'med', '##s', 'stop', 'working', '?', 'how', 'do', 'you', 'know', 'when', 'your', 'medication', 'isn', \"'\", 't', 'effective', 'anymore', '?', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '-', 'pi', 'a', 'little', 'over', 'a', 'year', 'ago', '.', 'i', \"'\", 've', 'been', 'on', '50', '##mg', 'of', 'v', '##y', '##van', '##se', 'for', 'about', '10', 'months', 'now', '.', 'i', 'don', \"'\", 't', 'take', 'it', 'on', 'the', 'weekends', 'for', 'the', 'most', 'part', '.', 'i', 'feel', 'like', 'i', \"'\", 've', 'hit', 'a', 'point', 'where', 'my', 'v', '##y', '##van', '##se', 'just', 'isn', \"'\", 't', 'doing', 'it', 'for', 'me', 'anymore', '.', 'i', 'was', 'completely', 'over', '##loaded', 'with', 'school', '##work', 'in', 'january', 'and', 'february', '.', 'i', 'started', 'the', 'horrible', 'cycle', 'of', 'staying', 'up', 'too', 'late', 'working', 'and', 'not', 'getting', 'enough', 'sleep', ',', 'so', 'the', 'med', '##s', 'weren', \"'\", 't', 'effective', 'and', 'then', 'because', 'the', 'med', '##s', 'weren', \"'\", 't', 'effective', ',', 'i', 'was', 'staying', 'up', 'later', 'doing', 'work', 'and', 'not', 'getting', 'enough', 'sleep', 'causing', 'the', 'med', '##s', 'to', 'be', 'less', 'effective', '.', 'i', 'got', 'on', 'a', 'much', 'better', 'sleep', 'schedule', 'at', 'the', 'start', 'of', 'this', 'month', 'but', 'i', \"'\", 'm', 'still', 'not', 'feeling', 'like', 'me', 'med', 'are', 'really', 'working', '.', 'at', 'this', 'point', ',', 'it', 'only', 'really', 'feels', 'like', 'they', 'give', 'me', 'energy', 'and', 'a', 'bit', 'of', 'clarity', 'of', 'thought', ',', 'but', 'no', 'help', 'focusing', '.', 'when', 'i', 'don', \"'\", 't', 'take', 'them', 'on', 'the', 'weekends', ',', 'i', 'don', \"'\", 't', 'feel', 'really', 'different', ',', 'just', 'more', 'tired', '.', 'i', \"'\", 've', 'started', 'getting', 'a', 'coffee', '(', 'double', 'shot', 'la', '##tte', ')', 'every', 'morning', ',', 'something', 'that', 'i', 'haven', \"'\", 't', 'done', 'since', 'starting', 'medication', '.', 'mornings', 'are', 'all', 'right', 'for', 'me', 'now', 'that', 'i', 'have', 'the', 'coffee', ',', 'but', 'my', 'mind', 'goes', 'everywhere', 'after', 'lunch', '##time', '.', 'starting', 'my', 'homework', 'has', 'become', 'really', 'hard', 'again', '.', 'because', 'i', 'can', \"'\", 't', 'focus', ',', 'i', \"'\", 'm', 'pro', '##cr', '##ast', '##inating', 'a', 'lot', 'more', 'and', 'feel', 'like', 'i', \"'\", 've', 'lost', 'my', 'motivation', 'to', 'work', '.', 'i', \"'\", 'm', 'not', 'really', 'sure', 'what', 'to', 'do', 'now', '.', 'i', \"'\", 'm', 'kind', 'of', 'afraid', 'of', 'trying', 'new', 'medication', 'since', 'v', '##y', '##van', '##se', 'was', 'only', 'the', 'second', 'one', 'i', 'tried', ',', 'and', 'i', 'had', 'a', 'terrible', 'experience', 'on', 'the', 'first', 'medication', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', '-', 'i', 'feel', 'like', 'my', 'med', '##s', 'aren', \"'\", 't', 'effective', 'anymore', 'and', 'don', \"'\", 't', 'know', 'why', 'or', 'what', 'to', 'do', '.']\n",
      "INFO:__main__:Number of tokens: 406\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'know', 'when', 'your', 'med', '##s', 'stop', 'working', '?', 'how', 'do', 'you', 'know', 'when', 'your', 'medication', 'isn', \"'\", 't', 'effective', 'anymore', '?', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '-', 'pi', 'a', 'little', 'over', 'a', 'year', 'ago', '.', 'i', \"'\", 've', 'been', 'on', '50', '##mg', 'of', 'v', '##y', '##van', '##se', 'for', 'about', '10', 'months', 'now', '.', 'i', 'don', \"'\", 't', 'take', 'it', 'on', 'the', 'weekends', 'for', 'the', 'most', 'part', '.', 'i', 'feel', 'like', 'i', \"'\", 've', 'hit', 'a', 'point', 'where', 'my', 'v', '##y', '##van', '##se', 'just', 'isn', \"'\", 't', 'doing', 'it', 'for', 'me', 'anymore', '.', 'i', 'was', 'completely', 'over', '##loaded', 'with', 'school', '##work', 'in', 'january', 'and', 'february', '.', 'i', 'started', 'the', 'horrible', 'cycle', 'of', 'staying', 'up', 'too', 'late', 'working', 'and', 'not', 'getting', 'enough', 'sleep', ',', 'so', 'the', 'med', '##s', 'weren', \"'\", 't', 'effective', 'and', 'then', 'because', 'the', 'med', '##s', 'weren', \"'\", 't', 'effective', ',', 'i', 'was', 'staying', 'up', 'later', 'doing', 'work', 'and', 'not', 'getting', 'enough', 'sleep', 'causing', 'the', 'med', '##s', 'to', 'be', 'less', 'effective', '.', 'i', 'got', 'on', 'a', 'much', 'better', 'sleep', 'schedule', 'at', 'the', 'start', 'of', 'this', 'month', 'but', 'i', \"'\", 'm', 'still', 'not', 'feeling', 'like', 'me', 'med', 'are', 'really', 'working', '.', 'at', 'this', 'point', ',', 'it', 'only', 'really', 'feels', 'like', 'they', 'give', 'me', 'energy', 'and', 'a', 'bit', 'of', 'clarity', 'of', 'thought', ',', 'but', 'no', 'help', 'focusing', '.', 'when', 'i', 'don', \"'\", 't', 'take', 'them', 'on', 'the', 'weekends', ',', 'i', 'don', \"'\", 't', 'feel', 'really', 'different', ',', 'just', 'more', 'tired', '.', 'i', \"'\", 've', 'started', 'getting', 'a', 'coffee', '(', 'double', 'shot', 'la', '##tte', ')', 'every', 'morning', ',', 'something', 'that', 'i', 'haven', \"'\", 't', 'done', 'since', 'starting', 'medication', '.', 'mornings', 'are', 'all', 'right', 'for', 'me', 'now', 'that', 'i', 'have', 'the', 'coffee', ',', 'but', 'my', 'mind', 'goes', 'everywhere', 'after', 'lunch', '##time', '.', 'starting', 'my', 'homework', 'has', 'become', 'really', 'hard', 'again', '.', 'because', 'i', 'can', \"'\", 't', 'focus', ',', 'i', \"'\", 'm', 'pro', '##cr', '##ast', '##inating', 'a', 'lot', 'more', 'and', 'feel', 'like', 'i', \"'\", 've', 'lost', 'my', 'motivation', 'to', 'work', '.', 'i', \"'\", 'm', 'not', 'really', 'sure', 'what', 'to', 'do', 'now', '.', 'i', \"'\", 'm', 'kind', 'of', 'afraid', 'of', 'trying', 'new', 'medication', 'since', 'v', '##y', '##van', '##se', 'was', 'only', 'the', 'second', 'one', 'i', 'tried', ',', 'and', 'i', 'had', 'a', 'terrible', 'experience', 'on', 'the', 'first', 'medication', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', '-', 'i', 'feel', 'like', 'my', 'med', '##s', 'aren', \"'\", 't', 'effective', 'anymore', 'and', 'don', \"'\", 't', 'know', 'why', 'or', 'what', 'to', 'do', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['roy', 'g', '.', 'bi', '##v', 'lives', 'in', 'my', 'closet', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['roy', 'g', '.', 'bi', '##v', 'lives', 'in', 'my', 'closet', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['stop', 'taking', 'rita', '##lin', '=', 'depression', '?']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['stop', 'taking', 'rita', '##lin', '=', 'depression', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['this', 'guy', 'might', 'make', 'you', 'happy', '.', 'i', \"'\", 've', 'been', 'struggling', 'lately', '.', 'i', 'won', \"'\", 't', 'go', 'into', 'detail', 'but', 'i', 'have', 'found', 'life', 'to', 'be', 'overwhelming', '.', 'incident', '##ally', ',', 'an', 'accountant', 'from', 'sri', 'lanka', 'came', 'up', 'to', 'my', 'workplace', '(', 'mines', '##ite', ')', 'this', 'week', '.', 'we', 'gave', 'him', 'a', 'show', 'of', 'what', 'we', 'were', 'about', 'and', 'i', 'got', 'to', 'know', 'him', 'a', 'little', '.', 'anyway', 'he', 'sensed', 'my', 'un', '##ha', '##pp', '##iness', 'although', 'i', 'though', 'i', 'was', 'hiding', 'it', 'well', 'and', 'had', 'a', 'good', 'talk', 'to', 'me', ',', 'and', 'when', 'he', 'left', ',', 'gave', 'me', 'some', 'talks', 'by', 'this', 'guy', '.', 'i', 'gave', 'them', 'a', 'go', ',', 'having', 'been', 'interested', 'by', 'buddhist', 'teachings', 'but', 'never', 'really', 'clicking', 'with', 'it', 'but', 'these', ',', 'this', 'monk', '.', '.', '.', 'he', 'just', 'capt', '##ivated', 'me', '.', 'so', 'easy', 'going', ',', 'so', 'open', '.', 'so', ',', 'i', 'feel', 'calm', '##er', 'and', 'happier', 'than', 'i', 'have', 'in', 'years', 'and', 'being', 'at', 'a', 'point', 'of', 'giving', 'up', 'medication', 'yet', 'again', 'only', 'days', 'ago', ',', 'i', 'feel', 'light', 'and', 'just', '.', '.', '.', 'good', '!', 'i', 'know', 'i', 'have', 'sc', '##t', '/', 'ad', '##hd', '-', 'pi', 'and', 'i', 'am', 'sure', 'that', 'the', 'flu', '##ct', '##uating', 'anxiety', '/', 'self', 'inc', '##rim', '##ination', '/', 'border', '##line', 'and', 'other', 'issues', 'that', 'i', 'deal', 'with', 'are', 'likely', 'mirrored', 'in', 'others', 'here', 'and', 'so', 'i', 'feel', 'that', 'it', 'is', 'right', 'to', 'share', 'this', '.', 'http', ':', '/', '/', 'www', '.', 'dh', '##am', '##mal', '##oka', '.', 'org', '.', 'au', '/', 'downloads', '/', 'item', '##list', '/', 'category', '/', '18', '-', 'aj', '##ah', '##n', '-', 'bra', '##hma', '##va', '##ms', '##o', '.', 'html', 'a', 'collection', 'of', 'the', 'first', '20', 'talks', 'and', 'some', 'guided', 'meditation', ':', 'https', ':', '/', '/', 'www', '.', 'rapids', '##har', '##e', '.', 'com', '/', '#', '!', 'download', '|', '47', '##2', '|', '381', '##0', '##9', '##6', '##8', '##9', '##59', '|', 'ah', '##jan', '_', 'bra', '##hma', '##va', '##ms', '##o', '.', 'zip', '|', '127', '##33', '##46']\n",
      "INFO:__main__:Number of tokens: 325\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['this', 'guy', 'might', 'make', 'you', 'happy', '.', 'i', \"'\", 've', 'been', 'struggling', 'lately', '.', 'i', 'won', \"'\", 't', 'go', 'into', 'detail', 'but', 'i', 'have', 'found', 'life', 'to', 'be', 'overwhelming', '.', 'incident', '##ally', ',', 'an', 'accountant', 'from', 'sri', 'lanka', 'came', 'up', 'to', 'my', 'workplace', '(', 'mines', '##ite', ')', 'this', 'week', '.', 'we', 'gave', 'him', 'a', 'show', 'of', 'what', 'we', 'were', 'about', 'and', 'i', 'got', 'to', 'know', 'him', 'a', 'little', '.', 'anyway', 'he', 'sensed', 'my', 'un', '##ha', '##pp', '##iness', 'although', 'i', 'though', 'i', 'was', 'hiding', 'it', 'well', 'and', 'had', 'a', 'good', 'talk', 'to', 'me', ',', 'and', 'when', 'he', 'left', ',', 'gave', 'me', 'some', 'talks', 'by', 'this', 'guy', '.', 'i', 'gave', 'them', 'a', 'go', ',', 'having', 'been', 'interested', 'by', 'buddhist', 'teachings', 'but', 'never', 'really', 'clicking', 'with', 'it', 'but', 'these', ',', 'this', 'monk', '.', '.', '.', 'he', 'just', 'capt', '##ivated', 'me', '.', 'so', 'easy', 'going', ',', 'so', 'open', '.', 'so', ',', 'i', 'feel', 'calm', '##er', 'and', 'happier', 'than', 'i', 'have', 'in', 'years', 'and', 'being', 'at', 'a', 'point', 'of', 'giving', 'up', 'medication', 'yet', 'again', 'only', 'days', 'ago', ',', 'i', 'feel', 'light', 'and', 'just', '.', '.', '.', 'good', '!', 'i', 'know', 'i', 'have', 'sc', '##t', '/', 'ad', '##hd', '-', 'pi', 'and', 'i', 'am', 'sure', 'that', 'the', 'flu', '##ct', '##uating', 'anxiety', '/', 'self', 'inc', '##rim', '##ination', '/', 'border', '##line', 'and', 'other', 'issues', 'that', 'i', 'deal', 'with', 'are', 'likely', 'mirrored', 'in', 'others', 'here', 'and', 'so', 'i', 'feel', 'that', 'it', 'is', 'right', 'to', 'share', 'this', '.', 'http', ':', '/', '/', 'www', '.', 'dh', '##am', '##mal', '##oka', '.', 'org', '.', 'au', '/', 'downloads', '/', 'item', '##list', '/', 'category', '/', '18', '-', 'aj', '##ah', '##n', '-', 'bra', '##hma', '##va', '##ms', '##o', '.', 'html', 'a', 'collection', 'of', 'the', 'first', '20', 'talks', 'and', 'some', 'guided', 'meditation', ':', 'https', ':', '/', '/', 'www', '.', 'rapids', '##har', '##e', '.', 'com', '/', '#', '!', 'download', '|', '47', '##2', '|', '381', '##0', '##9', '##6', '##8', '##9', '##59', '|', 'ah', '##jan', '_', 'bra', '##hma', '##va', '##ms', '##o', '.', 'zip', '|', '127', '##33', '##46']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['nico', '##tine', ':', 'worth', 'experimenting', '?', 'no', 'i', \"'\", 'm', 'not', 'about', 'to', 'start', 'smoking', '.', 'i', 'was', 'just', 'reading', 'over', 'the', 'psycho', '##active', 'effects', 'of', 'various', 'recreational', 'drugs', '(', 'bored', ',', 'internet', ',', 'you', 'know', 'how', 'it', 'is', ')', 'and', 'was', 'struck', 'by', 'the', 'similarities', 'between', 'nico', '##tine', 'and', 'amp', '##het', '##amine', '##s', '.', 'i', 'already', 'take', 'add', '##eral', '##l', 'but', 'will', 'be', 'stopping', 'shortly', 'for', 'my', 'usual', '\"', 'rest', 'period', '\"', '(', 'i', 'work', 'in', 'two', 'week', 'on', '/', 'two', 'week', 'off', 'rotating', 'shifts', 'and', 'very', 'rarely', 'take', 'medication', 'while', 'on', 'break', ')', '.', 'seems', 'to', 'me', 'like', 'a', 'prime', 'opportunity', 'to', 'do', 'some', 'experimentation', '!', 'so', '!', 'here', \"'\", 's', 'what', 'i', \"'\", 'd', 'like', 'to', 'know', 'before', 'i', 'go', 'out', 'and', 'buy', 'a', 'pack', 'of', 'nico', '##tine', 'gum', ':', '1', '.', 'has', 'anyone', 'else', 'out', 'there', 'tried', 'using', 'pure', 'nico', '##tine', '(', 'as', 'in', 'gum', 'or', 'patches', ',', 'not', 'cigarettes', ')', 'as', 'a', 'replacement', 'and', '/', 'or', 'adjunct', 'to', 'traditional', 'st', '##im', '##ula', '##nts', '?', 'if', 'so', ',', 'how', 'did', 'it', 'work', 'out', '?', 'were', 'the', 'side', 'effects', 'worse', 'than', 'regular', 'st', '##im', '##ula', '##nts', 'or', 'about', 'the', 'same', '?', 'were', 'the', 'st', '##im', '##ula', '##nt', 'effects', 'strong', 'enough', 'to', 'affect', 'the', 'ad', '##hd', 'or', 'was', 'it', 'more', 'like', 'caf', '##fi', '##ene', '(', 'all', 'buzz', ',', 'no', 'focus', ')', '?', '2', '.', 'how', 'addict', '##ive', 'is', 'non', '-', 'cigarette', 'nico', '##tine', '?', 'am', 'i', 'going', 'to', 'develop', 'some', 'sort', 'of', 'horrific', 'gum', 'chewing', 'habit', 'or', 'is', 'it', 'safe', 'to', 'experiment', 'with', '?', '3', '.', 'what', \"'\", 's', 'the', 'best', 'way', 'to', 'get', 'a', 'controlled', 'dose', '?', 'i', \"'\", 'm', 'planning', 'on', 'buying', 'gum', 'since', 'it', 'seems', 'easiest', 'but', 'i', 'don', \"'\", 't', 'want', 'to', 'accidentally', 'give', 'myself', 'a', 'heart', 'attack', 'by', 'chewing', 'too', 'long', '.', 'i', 'have', 'the', 'body', 'type', 'of', 'your', 'average', 'scare', '##crow', 'and', 'the', 'metabolism', 'of', 'a', 'rode', '##nt', 'so', 'over', '##dos', '##ing', 'is', 'always', 'a', 'concern', '.', '4', '.', 'how', 'long', 'does', 'the', '\"', 'high', '\"', 'last', 'and', 'is', 'there', 'a', 'significant', 'crash', 'afterwards', '?', 'should', 'i', 'eat', 'something', 'beforehand', '?', 'are', 'there', 'any', 'particular', 'foods', 'that', 'affect', 'absorption', '(', 'i', '.', 'e', '.', 'grape', '##fr', '##uit', 'juice', ')', '?', 'massive', 'thanks', 'to', 'anyone', 'who', 'responds', '!', 'and', 'sorry', 'for', 'sounding', 'a', 'bit', 'like', 'a', 'crazy', 'person', '.', 'i', 'promise', 'i', \"'\", 'm', 'not', 'an', 'addict', ',', 'just', 'enthusiastic', 'about', 'exploring', 'all', 'my', 'options', '.', '*', '*', 'edit', '*', '*', 'alright', 'so', 'i', 'bought', 'a', 'case', 'of', 'nico', '##tine', 'gum', 'yesterday', 'after', 'getting', 'off', 'work', '.', 'i', 'did', 'not', 'try', 'them', 'out', 'immediately', 'since', 'i', 'was', 'still', 'under', 'the', 'effects', 'of', 'add', '##eral', '##l', '.', 'i', 'spent', 'the', 'next', '.', '.', '.', 'jesus', 'i', 'guess', '24', 'hours', '?', 'asleep', '(', 'crashing', 'off', 'the', 'add', '##eral', '##l', ')', 'then', 'woke', 'up', 'and', 'spent', 'awhile', '\"', 'confirming', '\"', 'that', 'i', 'was', 'indeed', 'no', 'longer', 'under', 'the', 'influence', 'of', 'st', '##im', '##ula', '##nts', '.', 'this', 'consisted', 'of', 'having', 'my', 'partner', 'monitor', 'my', 'behavior', ',', 'which', 'he', 'described', 'as', '\"', 'absolutely', 'irritating', ',', 'now', 'decide', 'what', 'fucking', 'channel', 'you', 'want', 'to', 'watch', 'already', 'and', 'stop', 'bothering', 'the', 'cats', '.', '\"', 'so', 'i', 'read', 'the', 'instructions', 'on', 'the', 'gum', '(', '2', '##mg', ',', 'nico', '##ret', '##te', ')', 'and', 'chewed', 'a', 'piece', '.', 'first', 'impressions', ':', 'dizzy', ',', 'quite', 'calm', ',', 'maybe', 'a', 'little', 'sleepy', '.', 'after', 'awhile', 'i', 'began', 'to', 'feel', 'slightly', 'more', 'awake', ',', 'but', 'didn', \"'\", 't', 'feel', 'the', 'usual', 'drive', 'to', 'do', 'something', 'productive', 'with', 'my', 'time', 'like', 'i', 'do', 'while', 'on', 'add', '##eral', '##l', '.', 'managed', 'to', 'read', 'a', 'few', 'short', 'stories', 'with', 'minimal', 'distraction', ',', 'checked', 'the', 'internet', 'without', 'getting', 'lost', '.', 'it', \"'\", 's', 'a', 'very', 'different', 'high', 'to', 'add', '##eral', '##l', '.', 'initial', 'impressions', 'suggest', 'that', 'it', 'would', 'be', 'good', 'during', 'times', 'when', 'i', 'don', \"'\", 't', 'need', 'to', 'be', 'productive', ',', 'but', 'would', 'like', 'to', 'avoid', 'destroying', 'my', 'apartment', 'in', 'a', 'fit', 'of', 'boredom', '.', 'it', 'seems', 'to', 'keep', 'me', 'focused', 'enough', 'to', 'stay', 'on', 'one', 'task', 'but', 'doesn', \"'\", 't', 'give', 'the', 'kind', 'of', 'motivation', 'to', 'do', 'anything', 'worth', '##while', 'like', 'add', '##eral', '##l', 'does', '.', 'in', 'fact', 'it', 'feels', 'a', 'lot', 'like', 'rita', '##lin', '-', 'a', 'sort', 'of', 'quiet', ',', 'calm', 'high', 'that', 'keeps', 'me', 'content', 'but', 'not', 'active', '.', 'definitely', 'not', 'a', 'replacement', 'for', 'my', 'usual', 'medication', 'but', 'so', 'far', 'is', 'showing', 'promise', 'as', 'an', 'alternative', 'for', 'use', 'on', 'my', 'days', 'off', '.']\n",
      "INFO:__main__:Number of tokens: 737\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['nico', '##tine', ':', 'worth', 'experimenting', '?', 'no', 'i', \"'\", 'm', 'not', 'about', 'to', 'start', 'smoking', '.', 'i', 'was', 'just', 'reading', 'over', 'the', 'psycho', '##active', 'effects', 'of', 'various', 'recreational', 'drugs', '(', 'bored', ',', 'internet', ',', 'you', 'know', 'how', 'it', 'is', ')', 'and', 'was', 'struck', 'by', 'the', 'similarities', 'between', 'nico', '##tine', 'and', 'amp', '##het', '##amine', '##s', '.', 'i', 'already', 'take', 'add', '##eral', '##l', 'but', 'will', 'be', 'stopping', 'shortly', 'for', 'my', 'usual', '\"', 'rest', 'period', '\"', '(', 'i', 'work', 'in', 'two', 'week', 'on', '/', 'two', 'week', 'off', 'rotating', 'shifts', 'and', 'very', 'rarely', 'take', 'medication', 'while', 'on', 'break', ')', '.', 'seems', 'to', 'me', 'like', 'a', 'prime', 'opportunity', 'to', 'do', 'some', 'experimentation', '!', 'so', '!', 'here', \"'\", 's', 'what', 'i', \"'\", 'd', 'like', 'to', 'know', 'before', 'i', 'go', 'out', 'and', 'buy', 'a', 'pack', 'of', 'nico', '##tine', 'gum', ':', '1', '.', 'has', 'anyone', 'else', 'out', 'there', 'tried', 'using', 'pure', 'nico', '##tine', '(', 'as', 'in', 'gum', 'or', 'patches', ',', 'not', 'cigarettes', ')', 'as', 'a', 'replacement', 'and', '/', 'or', 'adjunct', 'to', 'traditional', 'st', '##im', '##ula', '##nts', '?', 'if', 'so', ',', 'how', 'did', 'it', 'work', 'out', '?', 'were', 'the', 'side', 'effects', 'worse', 'than', 'regular', 'st', '##im', '##ula', '##nts', 'or', 'about', 'the', 'same', '?', 'were', 'the', 'st', '##im', '##ula', '##nt', 'effects', 'strong', 'enough', 'to', 'affect', 'the', 'ad', '##hd', 'or', 'was', 'it', 'more', 'like', 'caf', '##fi', '##ene', '(', 'all', 'buzz', ',', 'no', 'focus', ')', '?', '2', '.', 'how', 'addict', '##ive', 'is', 'non', '-', 'cigarette', 'nico', '##tine', '?', 'am', 'i', 'going', 'to', 'develop', 'some', 'sort', 'of', 'horrific', 'gum', 'chewing', 'habit', 'or', 'is', 'it', 'safe', 'to', 'experiment', 'with', '?', '3', '.', 'what', \"'\", 's', 'the', 'best', 'way', 'to', 'get', 'a', 'controlled', 'dose', '?', 'i', \"'\", 'm', 'planning', 'on', 'buying', 'gum', 'since', 'it', 'seems', 'easiest', 'but', 'i', 'don', \"'\", 't', 'want', 'to', 'accidentally', 'give', 'myself', 'a', 'heart', 'attack', 'by', 'chewing', 'too', 'long', '.', 'i', 'have', 'the', 'body', 'type', 'of', 'your', 'average', 'scare', '##crow', 'and', 'the', 'metabolism', 'of', 'a', 'rode', '##nt', 'so', 'over', '##dos', '##ing', 'is', 'always', 'a', 'concern', '.', '4', '.', 'how', 'long', 'does', 'the', '\"', 'high', '\"', 'last', 'and', 'is', 'there', 'a', 'significant', 'crash', 'afterwards', '?', 'should', 'i', 'eat', 'something', 'beforehand', '?', 'are', 'there', 'any', 'particular', 'foods', 'that', 'affect', 'absorption', '(', 'i', '.', 'e', '.', 'grape', '##fr', '##uit', 'juice', ')', '?', 'massive', 'thanks', 'to', 'anyone', 'who', 'responds', '!', 'and', 'sorry', 'for', 'sounding', 'a', 'bit', 'like', 'a', 'crazy', 'person', '.', 'i', 'promise', 'i', \"'\", 'm', 'not', 'an', 'addict', ',', 'just', 'enthusiastic', 'about', 'exploring', 'all', 'my', 'options', '.', '*', '*', 'edit', '*', '*', 'alright', 'so', 'i', 'bought', 'a', 'case', 'of', 'nico', '##tine', 'gum', 'yesterday', 'after', 'getting', 'off', 'work', '.', 'i', 'did', 'not', 'try', 'them', 'out', 'immediately', 'since', 'i', 'was', 'still', 'under', 'the', 'effects', 'of', 'add', '##eral', '##l', '.', 'i', 'spent', 'the', 'next', '.', '.', '.', 'jesus', 'i', 'guess', '24', 'hours', '?', 'asleep', '(', 'crashing', 'off', 'the', 'add', '##eral', '##l', ')', 'then', 'woke', 'up', 'and', 'spent', 'awhile', '\"', 'confirming', '\"', 'that', 'i', 'was', 'indeed', 'no', 'longer', 'under', 'the', 'influence', 'of', 'st', '##im', '##ula', '##nts', '.', 'this', 'consisted', 'of', 'having', 'my', 'partner', 'monitor', 'my', 'behavior', ',', 'which', 'he', 'described', 'as', '\"', 'absolutely', 'irritating', ',', 'now', 'decide', 'what', 'fucking', 'channel'], ['you', 'want', 'to', 'watch', 'already', 'and', 'stop', 'bothering', 'the', 'cats', '.', '\"', 'so', 'i', 'read', 'the', 'instructions', 'on', 'the', 'gum', '(', '2', '##mg', ',', 'nico', '##ret', '##te', ')', 'and', 'chewed', 'a', 'piece', '.', 'first', 'impressions', ':', 'dizzy', ',', 'quite', 'calm', ',', 'maybe', 'a', 'little', 'sleepy', '.', 'after', 'awhile', 'i', 'began', 'to', 'feel', 'slightly', 'more', 'awake', ',', 'but', 'didn', \"'\", 't', 'feel', 'the', 'usual', 'drive', 'to', 'do', 'something', 'productive', 'with', 'my', 'time', 'like', 'i', 'do', 'while', 'on', 'add', '##eral', '##l', '.', 'managed', 'to', 'read', 'a', 'few', 'short', 'stories', 'with', 'minimal', 'distraction', ',', 'checked', 'the', 'internet', 'without', 'getting', 'lost', '.', 'it', \"'\", 's', 'a', 'very', 'different', 'high', 'to', 'add', '##eral', '##l', '.', 'initial', 'impressions', 'suggest', 'that', 'it', 'would', 'be', 'good', 'during', 'times', 'when', 'i', 'don', \"'\", 't', 'need', 'to', 'be', 'productive', ',', 'but', 'would', 'like', 'to', 'avoid', 'destroying', 'my', 'apartment', 'in', 'a', 'fit', 'of', 'boredom', '.', 'it', 'seems', 'to', 'keep', 'me', 'focused', 'enough', 'to', 'stay', 'on', 'one', 'task', 'but', 'doesn', \"'\", 't', 'give', 'the', 'kind', 'of', 'motivation', 'to', 'do', 'anything', 'worth', '##while', 'like', 'add', '##eral', '##l', 'does', '.', 'in', 'fact', 'it', 'feels', 'a', 'lot', 'like', 'rita', '##lin', '-', 'a', 'sort', 'of', 'quiet', ',', 'calm', 'high', 'that', 'keeps', 'me', 'content', 'but', 'not', 'active', '.', 'definitely', 'not', 'a', 'replacement', 'for', 'my', 'usual', 'medication', 'but', 'so', 'far', 'is', 'showing', 'promise', 'as', 'an', 'alternative', 'for', 'use', 'on', 'my', 'days', 'off', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ps', '##ych', 'just', 'prescribed', 'me', 'z', '##yp', '##re', '##xa', '(', 'an', 'anti', '-', 'psychotic', ')', '?', 'is', 'he', 'stall', '##ing', 'in', 'order', 'to', 'humour', 'me', '?', 'i', 'was', 'on', 'lex', '##ap', '##ro', '(', '10', '##mg', ')', 'for', 'a', 'month', 'beforehand', 'as', 'per', 'his', 'recommendation', ',', 'and', 'he', \"'\", 's', 'now', 'prescribed', 'z', '##yp', '##re', '##xa', '(', '2', '.', '5', '##mg', ')', ',', 'a', 'type', 'of', 'medication', 'that', \"'\", 's', 'generally', 'given', 'to', 'people', 'with', 'bipolar', '.', 'he', 'prescribed', 'me', 'this', 'because', 'i', 'told', 'him', 'i', 'was', 'having', 'trouble', 'sleeping', '(', 'a', 'side', 'effect', 'of', 'lex', '##ap', '##ro', ',', 'which', 'i', 'also', 'pointed', 'out', 'to', 'him', ')', ',', 'so', 'he', 'said', 'that', 'i', 'should', 'take', 'z', '##yp', '##re', '##xa', 'for', 'a', 'month', 'and', 'see', 'how', 'i', 'go', '.', 'i', 'took', 'it', 'for', 'a', 'day', 'before', 'i', 'decided', 'i', \"'\", 'm', 'not', 'taking', 'it', 'any', 'longer', ',', 'as', 'it', 'made', 'me', 'feel', 'like', 'a', 'zombie', 'the', 'day', 'after', 'and', 'actually', 'made', 'my', 'sleep', 'far', 'worse', '.', 'i', 'can', \"'\", 't', 'have', 'this', 'happening', 'to', 'me', 'now', 'as', 'i', \"'\", 'm', 'on', 'my', 'last', 'attempt', 'at', 'university', '(', 'which', 'has', 'already', 'been', 'hind', '##ered', 'with', 'a', 'chest', 'infection', 'in', 'the', 'first', 'week', ')', ',', 'and', 'the', 'effects', 'of', 'my', 'condition', 'are', 'just', ',', 'well', ',', 'fucking', 'with', 'me', '.', 'badly', '.', 'is', 'my', 'ps', '##ych', 'just', 'doing', 'this', 'because', 'he', 'thinks', 'i', \"'\", 'm', 'another', 'un', '##i', 'student', 'fa', '##king', 'the', 'symptoms', 'to', 'get', 'a', 'prescription', '?', 'he', 'warned', 'me', 'right', 'off', 'the', 'bat', 'in', 'our', 'first', 'session', 'that', 'doctors', 'don', \"'\", 't', 'want', 'to', 'pre', '##scribe', 'this', 'stuff', 'due', 'to', 'how', 'much', 'the', 'govt', 'is', 'press', '##uring', 'doctors', 'on', 'this', 'stuff', '(', 'i', \"'\", 'm', 'in', 'australia', ')', '.', 'i', 'wanna', 'stick', 'with', 'him', 'cause', 'he', \"'\", 's', 'really', 'helped', 'my', 'family', 'before', ',', 'but', 'i', 'think', 'he', \"'\", 's', 'just', 'fucking', 'with', 'me', 'at', 'this', 'stage', '.', 'has', 'anyone', 'else', 'taken', 'z', '##yp', '##re', '##xa', 'or', 'any', 'other', 'anti', '-', 'psychotic', '##s', 'for', 'ad', '##hd', 'and', 'found', 'it', \"'\", 's', 'helped', 'them', '?', 't', '##l', ';', 'dr', 'psychiatrist', 'prescribed', 'anti', '-', 'psychotic', '##s', ';', 'i', 'think', 'he', \"'\", 's', 'stall', '##ing', 'because', 'he', 'doesn', \"'\", 't', 'think', 'i', 'have', 'a', 'problem', '.', 'apologies', 'for', 'the', 'ran', '##t', ',', 'good', 'luck', 'getting', 'through', 'it', 'fellow', 'focus', '-', 'lacking', 'lack', '##ey', '##s', '.']\n",
      "INFO:__main__:Number of tokens: 390\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ps', '##ych', 'just', 'prescribed', 'me', 'z', '##yp', '##re', '##xa', '(', 'an', 'anti', '-', 'psychotic', ')', '?', 'is', 'he', 'stall', '##ing', 'in', 'order', 'to', 'humour', 'me', '?', 'i', 'was', 'on', 'lex', '##ap', '##ro', '(', '10', '##mg', ')', 'for', 'a', 'month', 'beforehand', 'as', 'per', 'his', 'recommendation', ',', 'and', 'he', \"'\", 's', 'now', 'prescribed', 'z', '##yp', '##re', '##xa', '(', '2', '.', '5', '##mg', ')', ',', 'a', 'type', 'of', 'medication', 'that', \"'\", 's', 'generally', 'given', 'to', 'people', 'with', 'bipolar', '.', 'he', 'prescribed', 'me', 'this', 'because', 'i', 'told', 'him', 'i', 'was', 'having', 'trouble', 'sleeping', '(', 'a', 'side', 'effect', 'of', 'lex', '##ap', '##ro', ',', 'which', 'i', 'also', 'pointed', 'out', 'to', 'him', ')', ',', 'so', 'he', 'said', 'that', 'i', 'should', 'take', 'z', '##yp', '##re', '##xa', 'for', 'a', 'month', 'and', 'see', 'how', 'i', 'go', '.', 'i', 'took', 'it', 'for', 'a', 'day', 'before', 'i', 'decided', 'i', \"'\", 'm', 'not', 'taking', 'it', 'any', 'longer', ',', 'as', 'it', 'made', 'me', 'feel', 'like', 'a', 'zombie', 'the', 'day', 'after', 'and', 'actually', 'made', 'my', 'sleep', 'far', 'worse', '.', 'i', 'can', \"'\", 't', 'have', 'this', 'happening', 'to', 'me', 'now', 'as', 'i', \"'\", 'm', 'on', 'my', 'last', 'attempt', 'at', 'university', '(', 'which', 'has', 'already', 'been', 'hind', '##ered', 'with', 'a', 'chest', 'infection', 'in', 'the', 'first', 'week', ')', ',', 'and', 'the', 'effects', 'of', 'my', 'condition', 'are', 'just', ',', 'well', ',', 'fucking', 'with', 'me', '.', 'badly', '.', 'is', 'my', 'ps', '##ych', 'just', 'doing', 'this', 'because', 'he', 'thinks', 'i', \"'\", 'm', 'another', 'un', '##i', 'student', 'fa', '##king', 'the', 'symptoms', 'to', 'get', 'a', 'prescription', '?', 'he', 'warned', 'me', 'right', 'off', 'the', 'bat', 'in', 'our', 'first', 'session', 'that', 'doctors', 'don', \"'\", 't', 'want', 'to', 'pre', '##scribe', 'this', 'stuff', 'due', 'to', 'how', 'much', 'the', 'govt', 'is', 'press', '##uring', 'doctors', 'on', 'this', 'stuff', '(', 'i', \"'\", 'm', 'in', 'australia', ')', '.', 'i', 'wanna', 'stick', 'with', 'him', 'cause', 'he', \"'\", 's', 'really', 'helped', 'my', 'family', 'before', ',', 'but', 'i', 'think', 'he', \"'\", 's', 'just', 'fucking', 'with', 'me', 'at', 'this', 'stage', '.', 'has', 'anyone', 'else', 'taken', 'z', '##yp', '##re', '##xa', 'or', 'any', 'other', 'anti', '-', 'psychotic', '##s', 'for', 'ad', '##hd', 'and', 'found', 'it', \"'\", 's', 'helped', 'them', '?', 't', '##l', ';', 'dr', 'psychiatrist', 'prescribed', 'anti', '-', 'psychotic', '##s', ';', 'i', 'think', 'he', \"'\", 's', 'stall', '##ing', 'because', 'he', 'doesn', \"'\", 't', 'think', 'i', 'have', 'a', 'problem', '.', 'apologies', 'for', 'the', 'ran', '##t', ',', 'good', 'luck', 'getting', 'through', 'it', 'fellow', 'focus', '-', 'lacking', 'lack', '##ey', '##s', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'is', 'a', 'gift', '!', 'take', 'it', 'or', 'leave', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'is', 'a', 'gift', '!', 'take', 'it', 'or', 'leave', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'sleep', 'problems', ':', 'causes', 'and', 'tips', 'to', 'rest', 'better', 'tonight', '!', 'they', 'sleep', 'through', 'two', 'or', 'three', 'alarms', ',', 'as', 'well', 'as', 'the', 'attempts', 'of', 'family', 'members', 'to', 'get', 'them', 'out', 'of', 'bed', '.', 'ad', '##hd', 'sleeper', '##s', 'are', 'commonly', 'ir', '##rita', '##ble', ',', 'even', 'combat', '##ive', ',', 'when', 'rouse', '##d', 'before', 'they', 'are', 'ready', '.', 'many', 'of', 'them', 'say', 'they', 'are', 'not', 'fully', 'alert', 'until', 'noon', '.', 'as', 'long', 'as', 'persons', 'with', 'ad', '##hd', 'were', 'interested', 'in', 'or', 'challenged', 'by', 'what', 'they', 'were', 'doing', ',', 'they', 'did', 'not', 'demonstrate', 'symptoms', 'of', 'the', 'disorder', '.', '(', 'this', 'phenomenon', 'is', 'called', 'hyper', '##fo', '##cus', 'by', 'some', ',', 'and', 'is', 'often', 'considered', 'to', 'be', 'an', 'ad', '##hd', 'pattern', '.', ')', 'if', ',', 'on', 'the', 'other', 'hand', ',', 'an', 'individual', 'with', 'ad', '##hd', 'loses', 'interest', 'in', 'an', 'activity', ',', 'his', 'nervous', 'system', 'di', '##sen', '##ga', '##ges', ',', 'in', 'search', 'of', 'something', 'more', 'interesting', '.', 'sometimes', 'this', 'di', '##sen', '##ga', '##gement', 'is', 'so', 'abrupt', 'as', 'to', 'induce', 'sudden', 'extreme', 'dr', '##ows', '##iness', ',', 'even', 'to', 'the', 'point', 'of', 'falling', 'asleep', '.', 'http', ':', '/', '/', 'www', '.', 'add', '##itude', '##ma', '##g', '.', 'com', '/', 'ad', '##hd', '/', 'article', '/', '75', '##7', '.', 'html']\n",
      "INFO:__main__:Number of tokens: 201\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'sleep', 'problems', ':', 'causes', 'and', 'tips', 'to', 'rest', 'better', 'tonight', '!', 'they', 'sleep', 'through', 'two', 'or', 'three', 'alarms', ',', 'as', 'well', 'as', 'the', 'attempts', 'of', 'family', 'members', 'to', 'get', 'them', 'out', 'of', 'bed', '.', 'ad', '##hd', 'sleeper', '##s', 'are', 'commonly', 'ir', '##rita', '##ble', ',', 'even', 'combat', '##ive', ',', 'when', 'rouse', '##d', 'before', 'they', 'are', 'ready', '.', 'many', 'of', 'them', 'say', 'they', 'are', 'not', 'fully', 'alert', 'until', 'noon', '.', 'as', 'long', 'as', 'persons', 'with', 'ad', '##hd', 'were', 'interested', 'in', 'or', 'challenged', 'by', 'what', 'they', 'were', 'doing', ',', 'they', 'did', 'not', 'demonstrate', 'symptoms', 'of', 'the', 'disorder', '.', '(', 'this', 'phenomenon', 'is', 'called', 'hyper', '##fo', '##cus', 'by', 'some', ',', 'and', 'is', 'often', 'considered', 'to', 'be', 'an', 'ad', '##hd', 'pattern', '.', ')', 'if', ',', 'on', 'the', 'other', 'hand', ',', 'an', 'individual', 'with', 'ad', '##hd', 'loses', 'interest', 'in', 'an', 'activity', ',', 'his', 'nervous', 'system', 'di', '##sen', '##ga', '##ges', ',', 'in', 'search', 'of', 'something', 'more', 'interesting', '.', 'sometimes', 'this', 'di', '##sen', '##ga', '##gement', 'is', 'so', 'abrupt', 'as', 'to', 'induce', 'sudden', 'extreme', 'dr', '##ows', '##iness', ',', 'even', 'to', 'the', 'point', 'of', 'falling', 'asleep', '.', 'http', ':', '/', '/', 'www', '.', 'add', '##itude', '##ma', '##g', '.', 'com', '/', 'ad', '##hd', '/', 'article', '/', '75', '##7', '.', 'html']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', 'dex', '##ed', '##rine', 'lose', 'it', \"'\", 's', 'effectiveness', 'in', '1', 'week', '?', 'or', 'am', 'i', 'just', 'over', '-', 'thinking', 'this', 'way', 'too', 'much', '?', 'i', 'started', 'taking', '20', '##mg', 'ir', '/', '2', '##x', 'a', 'day', 'a', 'week', 'ago', '.', 'i', \"'\", 'm', 'a', 'bigger', 'guy', ',', '6', \"'\", '1', '\"', '230', '##lb', '##s', 'with', 'high', 'functioning', 'kidney', '##s', '.', 'last', 'week', 'was', 'fantastic', '.', 'i', 'was', 'in', 'the', 'zone', 'and', 'focused', 'for', 'like', '8', '-', '10', 'hours', '.', 'now', '.', '.', '.', '6', 'tops', '.', 'i', \"'\", 'm', '26', 'and', 'very', 'very', 'new', 'to', 'all', 'this', ',', 'any', 'help', '/', 'advice', '/', 'what', 'to', 'ask', 'my', 'doctor', '(', 'who', 'i', 'see', 'on', 'thursday', ')', 'would', 'be', 'helpful', '.', 'thanks', 'so', 'much', '!']\n",
      "INFO:__main__:Number of tokens: 123\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', 'dex', '##ed', '##rine', 'lose', 'it', \"'\", 's', 'effectiveness', 'in', '1', 'week', '?', 'or', 'am', 'i', 'just', 'over', '-', 'thinking', 'this', 'way', 'too', 'much', '?', 'i', 'started', 'taking', '20', '##mg', 'ir', '/', '2', '##x', 'a', 'day', 'a', 'week', 'ago', '.', 'i', \"'\", 'm', 'a', 'bigger', 'guy', ',', '6', \"'\", '1', '\"', '230', '##lb', '##s', 'with', 'high', 'functioning', 'kidney', '##s', '.', 'last', 'week', 'was', 'fantastic', '.', 'i', 'was', 'in', 'the', 'zone', 'and', 'focused', 'for', 'like', '8', '-', '10', 'hours', '.', 'now', '.', '.', '.', '6', 'tops', '.', 'i', \"'\", 'm', '26', 'and', 'very', 'very', 'new', 'to', 'all', 'this', ',', 'any', 'help', '/', 'advice', '/', 'what', 'to', 'ask', 'my', 'doctor', '(', 'who', 'i', 'see', 'on', 'thursday', ')', 'would', 'be', 'helpful', '.', 'thanks', 'so', 'much', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['self', '-', 'lo', '##athing', '(', 'long', ')']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['self', '-', 'lo', '##athing', '(', 'long', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['test', 'if', 'they', 'are', 'working']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['test', 'if', 'they', 'are', 'working']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'best', 'natural', 'way', 'to', 'treat', 'ad', '##hd', '/', 'add']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'best', 'natural', 'way', 'to', 'treat', 'ad', '##hd', '/', 'add']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['going', 'to', 'my', 'doc', 'tomorrow', ',', 'what', 'are', 'some', 'med', '##s', 'i', 'should', 'discuss', 'with', 'him', '?']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['going', 'to', 'my', 'doc', 'tomorrow', ',', 'what', 'are', 'some', 'med', '##s', 'i', 'should', 'discuss', 'with', 'him', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '-', 'pi', 'or', 'sc', '##t', '?', 'article', 'based', 'on', 'dr', '.', 'bark', '##ley', \"'\", 's', 'research', '.']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '-', 'pi', 'or', 'sc', '##t', '?', 'article', 'based', 'on', 'dr', '.', 'bark', '##ley', \"'\", 's', 'research', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['u', '##gh', '##hh', ',', 'how', 'do', 'i', 'get', 'myself', 'to', 'even', 'start', 'something', '##s', '?', 'this', 'has', 'been', 'a', 'problem', 'all', 'of', 'my', 'life', ',', 'and', 'i', \"'\", 'm', 'sure', 'for', 'many', 'out', 'there', '.', 'how', 'do', 'you', 'get', 'yourself', 'to', 'do', 'things', '?', 'i', \"'\", 've', 'tried', 'the', 'whole', 'reward', '##ing', 'myself', 'method', ',', 'but', 'in', 'the', 'moment', ',', 'it', \"'\", 's', 'much', 'more', 'reward', '##ing', 'to', 'just', 'lay', 'in', 'bed', '.', '.', '.', 'so', 'that', 'didn', \"'\", 't', 'work', '.', 'anxiety', 'plays', 'a', 'role', 'in', 'this', ',', 'some', 'days', 'i', 'just', 'feel', 'so', 'much', 'anxiety', 'about', 'getting', 'up', '/', 'going', 'out', 'i', 'just', 'give', 'up', '(', 'i', 'haven', \"'\", 't', 'done', 'this', 'in', 'at', 'least', 'a', 'month', '.', 'in', 'comparison', 'to', '3', 'times', 'a', 'week', ',', 'i', \"'\", 'm', 'pretty', 'proud', 'of', 'myself', ')', '.', 'i', 'guess', 'i', \"'\", 'd', 'just', 'like', 'some', 'advice', 'on', 'how', 'to', 'persuade', 'my', 'ad', '##hd', 'brain', 'that', 'cleaning', 'my', 'room', 'will', 'be', 'fun', ',', 'or', 'studying', ',', 'or', 'running', 'er', '##rand', '##s', '.', 'how', 'did', 'you', 'do', 'it', '?', 'edit', ':', 'thanks', 'for', 'all', 'of', 'the', 'response', 'so', 'far', ',', 'i', 'want', 'to', 'mention', 'something', '##s', 'i', 'left', 'out', '.', 'it', \"'\", 's', 'really', 'not', 'the', 'doing', 'it', 'part', ',', 'it', \"'\", 's', 'the', 'getting', 'up', 'part', '/', 'getting', 'started', 'part', '.', 'i', 'was', 'attempting', 'to', 'clean', 'my', 'room', 'today', 'and', 'it', 'took', 'me', 'almost', 'three', 'hours', 'to', 'get', 'started', '(', 'thus', 'this', 'post', ')', '.', 'however', ',', 'once', 'i', 'get', 'started', 'it', \"'\", 's', 'not', 'as', 'bad', 'to', 'do', 'things', '.', 'i', \"'\", 'll', 'do', '13', 'things', 'at', 'the', 'same', 'time', ',', 'but', 'they', \"'\", 're', 'getting', 'done', '.', 'i', 'can', 'get', 'up', 'and', 'run', '5', 'miles', 'the', 'problem', 'is', 'getting', 'myself', 'to', 'start', '.', 'it', \"'\", 's', 'not', 'like', 'i', 'don', \"'\", 't', 'enjoy', 'running', ',', 'i', 'do', '!', 'but', 'unless', 'i', 'have', 'a', 'sincere', 'desire', 'to', 'do', 'something', ',', 'getting', 'started', 'is', 'such', 'a', 'difficult', 'task', '.', 'the', 'visual', '##ization', 'does', 'help', ',', 'i', \"'\", 've', 'recently', 'learned', 'that', ',', 'but', 'i', 'haven', \"'\", 't', 'started', 'using', 'it', 'for', 'short', 'term', 'goals', '.', 'i', \"'\", 'll', 'definitely', 'try', 'to', 'ins', '##ti', '##l', 'that', '.', 'i', 'really', 'don', \"'\", 't', 'know', 'how', 'you', 'guys', 'can', 'study', 'in', 'coffee', 'shops', ',', 'i', 'have', 'to', 'be', 'in', 'absolute', 'silence', '.', 'i', 'can', 'use', 'the', 'school', 'library', 'during', 'the', 'day', 'when', 'natural', 'light', 'comes', 'in', ',', 'but', 'at', 'night', 'the', 'white', '(', 'flickering', ')', 'fluorescent', 'lights', 'make', 'me', 'want', 'to', 'throw', 'up', '.', 'i', \"'\", 've', 'gotten', 'much', 'better', 'at', 'getting', 'myself', 'to', 'study', 'at', 'home', 'since', 'putting', 'in', 'some', 'new', 'practices', '.', 'it', \"'\", 's', 'just', 'that', 'some', 'days', 'i', 'wake', 'up', 'and', 'feel', 'like', 'the', 'importance', 'of', 'sleep', 'greatly', 'out', '##weig', '##hs', 'anything', 'else', '.', 'those', 'are', 'the', 'hard', 'days', '.', 'i', 'just', 'can', \"'\", 't', 'coa', '##x', 'myself', 'to', 'move', '.']\n",
      "INFO:__main__:Number of tokens: 481\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['u', '##gh', '##hh', ',', 'how', 'do', 'i', 'get', 'myself', 'to', 'even', 'start', 'something', '##s', '?', 'this', 'has', 'been', 'a', 'problem', 'all', 'of', 'my', 'life', ',', 'and', 'i', \"'\", 'm', 'sure', 'for', 'many', 'out', 'there', '.', 'how', 'do', 'you', 'get', 'yourself', 'to', 'do', 'things', '?', 'i', \"'\", 've', 'tried', 'the', 'whole', 'reward', '##ing', 'myself', 'method', ',', 'but', 'in', 'the', 'moment', ',', 'it', \"'\", 's', 'much', 'more', 'reward', '##ing', 'to', 'just', 'lay', 'in', 'bed', '.', '.', '.', 'so', 'that', 'didn', \"'\", 't', 'work', '.', 'anxiety', 'plays', 'a', 'role', 'in', 'this', ',', 'some', 'days', 'i', 'just', 'feel', 'so', 'much', 'anxiety', 'about', 'getting', 'up', '/', 'going', 'out', 'i', 'just', 'give', 'up', '(', 'i', 'haven', \"'\", 't', 'done', 'this', 'in', 'at', 'least', 'a', 'month', '.', 'in', 'comparison', 'to', '3', 'times', 'a', 'week', ',', 'i', \"'\", 'm', 'pretty', 'proud', 'of', 'myself', ')', '.', 'i', 'guess', 'i', \"'\", 'd', 'just', 'like', 'some', 'advice', 'on', 'how', 'to', 'persuade', 'my', 'ad', '##hd', 'brain', 'that', 'cleaning', 'my', 'room', 'will', 'be', 'fun', ',', 'or', 'studying', ',', 'or', 'running', 'er', '##rand', '##s', '.', 'how', 'did', 'you', 'do', 'it', '?', 'edit', ':', 'thanks', 'for', 'all', 'of', 'the', 'response', 'so', 'far', ',', 'i', 'want', 'to', 'mention', 'something', '##s', 'i', 'left', 'out', '.', 'it', \"'\", 's', 'really', 'not', 'the', 'doing', 'it', 'part', ',', 'it', \"'\", 's', 'the', 'getting', 'up', 'part', '/', 'getting', 'started', 'part', '.', 'i', 'was', 'attempting', 'to', 'clean', 'my', 'room', 'today', 'and', 'it', 'took', 'me', 'almost', 'three', 'hours', 'to', 'get', 'started', '(', 'thus', 'this', 'post', ')', '.', 'however', ',', 'once', 'i', 'get', 'started', 'it', \"'\", 's', 'not', 'as', 'bad', 'to', 'do', 'things', '.', 'i', \"'\", 'll', 'do', '13', 'things', 'at', 'the', 'same', 'time', ',', 'but', 'they', \"'\", 're', 'getting', 'done', '.', 'i', 'can', 'get', 'up', 'and', 'run', '5', 'miles', 'the', 'problem', 'is', 'getting', 'myself', 'to', 'start', '.', 'it', \"'\", 's', 'not', 'like', 'i', 'don', \"'\", 't', 'enjoy', 'running', ',', 'i', 'do', '!', 'but', 'unless', 'i', 'have', 'a', 'sincere', 'desire', 'to', 'do', 'something', ',', 'getting', 'started', 'is', 'such', 'a', 'difficult', 'task', '.', 'the', 'visual', '##ization', 'does', 'help', ',', 'i', \"'\", 've', 'recently', 'learned', 'that', ',', 'but', 'i', 'haven', \"'\", 't', 'started', 'using', 'it', 'for', 'short', 'term', 'goals', '.', 'i', \"'\", 'll', 'definitely', 'try', 'to', 'ins', '##ti', '##l', 'that', '.', 'i', 'really', 'don', \"'\", 't', 'know', 'how', 'you', 'guys', 'can', 'study', 'in', 'coffee', 'shops', ',', 'i', 'have', 'to', 'be', 'in', 'absolute', 'silence', '.', 'i', 'can', 'use', 'the', 'school', 'library', 'during', 'the', 'day', 'when', 'natural', 'light', 'comes', 'in', ',', 'but', 'at', 'night', 'the', 'white', '(', 'flickering', ')', 'fluorescent', 'lights', 'make', 'me', 'want', 'to', 'throw', 'up', '.', 'i', \"'\", 've', 'gotten', 'much', 'better', 'at', 'getting', 'myself', 'to', 'study', 'at', 'home', 'since', 'putting', 'in', 'some', 'new', 'practices', '.', 'it', \"'\", 's', 'just', 'that', 'some', 'days', 'i', 'wake', 'up', 'and', 'feel', 'like', 'the', 'importance', 'of', 'sleep', 'greatly', 'out', '##weig', '##hs', 'anything', 'else', '.', 'those', 'are', 'the', 'hard', 'days', '.', 'i', 'just', 'can', \"'\", 't', 'coa', '##x', 'myself', 'to', 'move', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'caution', ']', 'med', '##ica', '##ting', 'to', 'your', 'schedule', 'or', 'scheduling', 'to', 'your', 'medication', '?', '*', '*', 'in', 'case', 'the', 'title', 'is', 'slightly', 'confusing', ':', '*', '*', '*', '*', '*', 'what', 'i', \"'\", 'm', 'trying', 'to', 'say', 'is', ':', 'are', 'you', 'creating', 'a', 'life', '/', 'sleep', 'schedule', 'and', 'taking', 'medication', 'to', 'aid', 'life', 'within', 'that', 'schedule', 'or', 'are', 'you', 'using', 'medication', 'to', 'schedule', 'our', 'life', '?', 'are', 'you', 'using', 'medication', 'to', 'modify', 'your', 'time', '?', '*', '*', 'my', 'experience', ':', '*', '*', '*', '*', '*', 'recently', 'i', 'had', 'the', 'realization', 'that', 'i', 'was', 'starting', 'to', 'use', 'my', 'medication', 'in', 'order', 'to', 'bend', 'and', 'twist', 'my', 'schedule', 'to', 'my', 'det', '##rim', '##ent', ',', 'something', 'i', \"'\", 'm', 'not', 'proud', 'of', '.', 'what', 'i', 'mean', 'by', 'this', 'is', 'that', 'i', 'was', 'using', 'my', 'medication', 'as', 'a', 'way', 'to', 'manipulate', 'how', 'my', 'day', '/', 'night', 'is', 'going', 'to', 'happen', ',', 'here', 'is', 'an', 'example', ':', '*', 'stayed', 'up', 'till', '6', '##am', 'goo', '##fin', '##g', 'around', 'playing', 'hack', '##y', 'sack', 'with', 'friends', '.', 'i', 'have', 'class', 'at', '9', '##am', '.', '*', 'popped', '70', '##mg', 'v', '##y', '##van', '##se', 'and', 'went', 'to', 'bed', '.', '*', 'v', '##y', '##van', '##se', 'starts', 'kicking', 'in', ',', 'get', 'up', 'around', '8', '##am', '.', '*', 'add', '##eral', '##l', '20', '##mg', 'booster', 'at', 'midday', '.', '*', 'work', '3', '##pm', '-', 'close', '(', '10', ':', '30', '##pm', ')', '*', 'half', 'an', 'add', '##eral', '##l', 'tab', ',', '10', '##mg', ',', 'around', '5', '##pm', 'to', 'help', 'finish', 'work', 'and', 'be', 'able', 'to', 'go', 'home', 'and', 'try', 'to', 'get', 'homework', 'done', '.', '(', 'this', 'half', 'tab', 'is', 'usually', 'taken', 'in', 'the', 'mornings', 'as', 'a', 'way', 'to', 'help', 'wake', 'me', 'up', 'faster', ',', 'as', 'advised', 'by', 'my', 'doctor', ')', '.', 'this', 'destroyed', 'my', 'sleep', 'schedule', 'causing', 'me', 'to', 'stay', 'up', 'very', 'late', 'and', 'then', 'completely', 'rely', 'on', 'my', 'medication', 'to', 'wake', 'me', 'up', 'while', 'running', 'on', 'very', 'little', 'sleep', '.', 'i', 'would', 'also', 'eat', 'a', 'lot', 'less', '.', 'little', 'sleep', 'and', 'little', 'food', 'brought', 'down', 'my', 'cognitive', 'abilities', '.', 'this', 'would', 'lead', 'to', 'days', 'filled', 'with', 'nothing', 'getting', 'done', 'because', 'it', 'completely', 'shot', 'my', 'motivation', 'towards', 'anything', 'important', '.', 'being', 'mentally', 'fog', '##gy', 'with', 'complete', 'lack', 'of', 'motivation', 'and', 'on', 'medication', 'lead', 'to', 'a', 'ridiculous', 'amount', 'of', 'time', 'on', 'the', 'internet', 'doing', 'essentially', 'nothing', '.', '*', '*', 'discussion', 'and', 'purpose', '*', '*', '*', '*', '*', 'this', 'is', 'sort', 'of', 'a', 'sharing', 'my', 'experience', 'post', 'but', 'also', 'a', 'warning', 'to', 'be', 'aware', 'of', 'how', 'you', 'are', 'using', 'your', 'medication', 'what', 'are', 'your', 'experiences', '?', 'anyone', 'else', 'found', 'them', 'se', '##lves', 'doing', 'this', '?', 'thoughts', 'and', 'opinions', 'on', 'the', 'situation', '?']\n",
      "INFO:__main__:Number of tokens: 433\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'caution', ']', 'med', '##ica', '##ting', 'to', 'your', 'schedule', 'or', 'scheduling', 'to', 'your', 'medication', '?', '*', '*', 'in', 'case', 'the', 'title', 'is', 'slightly', 'confusing', ':', '*', '*', '*', '*', '*', 'what', 'i', \"'\", 'm', 'trying', 'to', 'say', 'is', ':', 'are', 'you', 'creating', 'a', 'life', '/', 'sleep', 'schedule', 'and', 'taking', 'medication', 'to', 'aid', 'life', 'within', 'that', 'schedule', 'or', 'are', 'you', 'using', 'medication', 'to', 'schedule', 'our', 'life', '?', 'are', 'you', 'using', 'medication', 'to', 'modify', 'your', 'time', '?', '*', '*', 'my', 'experience', ':', '*', '*', '*', '*', '*', 'recently', 'i', 'had', 'the', 'realization', 'that', 'i', 'was', 'starting', 'to', 'use', 'my', 'medication', 'in', 'order', 'to', 'bend', 'and', 'twist', 'my', 'schedule', 'to', 'my', 'det', '##rim', '##ent', ',', 'something', 'i', \"'\", 'm', 'not', 'proud', 'of', '.', 'what', 'i', 'mean', 'by', 'this', 'is', 'that', 'i', 'was', 'using', 'my', 'medication', 'as', 'a', 'way', 'to', 'manipulate', 'how', 'my', 'day', '/', 'night', 'is', 'going', 'to', 'happen', ',', 'here', 'is', 'an', 'example', ':', '*', 'stayed', 'up', 'till', '6', '##am', 'goo', '##fin', '##g', 'around', 'playing', 'hack', '##y', 'sack', 'with', 'friends', '.', 'i', 'have', 'class', 'at', '9', '##am', '.', '*', 'popped', '70', '##mg', 'v', '##y', '##van', '##se', 'and', 'went', 'to', 'bed', '.', '*', 'v', '##y', '##van', '##se', 'starts', 'kicking', 'in', ',', 'get', 'up', 'around', '8', '##am', '.', '*', 'add', '##eral', '##l', '20', '##mg', 'booster', 'at', 'midday', '.', '*', 'work', '3', '##pm', '-', 'close', '(', '10', ':', '30', '##pm', ')', '*', 'half', 'an', 'add', '##eral', '##l', 'tab', ',', '10', '##mg', ',', 'around', '5', '##pm', 'to', 'help', 'finish', 'work', 'and', 'be', 'able', 'to', 'go', 'home', 'and', 'try', 'to', 'get', 'homework', 'done', '.', '(', 'this', 'half', 'tab', 'is', 'usually', 'taken', 'in', 'the', 'mornings', 'as', 'a', 'way', 'to', 'help', 'wake', 'me', 'up', 'faster', ',', 'as', 'advised', 'by', 'my', 'doctor', ')', '.', 'this', 'destroyed', 'my', 'sleep', 'schedule', 'causing', 'me', 'to', 'stay', 'up', 'very', 'late', 'and', 'then', 'completely', 'rely', 'on', 'my', 'medication', 'to', 'wake', 'me', 'up', 'while', 'running', 'on', 'very', 'little', 'sleep', '.', 'i', 'would', 'also', 'eat', 'a', 'lot', 'less', '.', 'little', 'sleep', 'and', 'little', 'food', 'brought', 'down', 'my', 'cognitive', 'abilities', '.', 'this', 'would', 'lead', 'to', 'days', 'filled', 'with', 'nothing', 'getting', 'done', 'because', 'it', 'completely', 'shot', 'my', 'motivation', 'towards', 'anything', 'important', '.', 'being', 'mentally', 'fog', '##gy', 'with', 'complete', 'lack', 'of', 'motivation', 'and', 'on', 'medication', 'lead', 'to', 'a', 'ridiculous', 'amount', 'of', 'time', 'on', 'the', 'internet', 'doing', 'essentially', 'nothing', '.', '*', '*', 'discussion', 'and', 'purpose', '*', '*', '*', '*', '*', 'this', 'is', 'sort', 'of', 'a', 'sharing', 'my', 'experience', 'post', 'but', 'also', 'a', 'warning', 'to', 'be', 'aware', 'of', 'how', 'you', 'are', 'using', 'your', 'medication', 'what', 'are', 'your', 'experiences', '?', 'anyone', 'else', 'found', 'them', 'se', '##lves', 'doing', 'this', '?', 'thoughts', 'and', 'opinions', 'on', 'the', 'situation', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'suggestion', ']', 'idea', 'for', 'flair']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'suggestion', ']', 'idea', 'for', 'flair']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['maybe', 'this', 'is', 'crazy', ',', 'but', ':', 'inappropriate', 'love', '.', 'so', 'here', \"'\", 's', 'the', 'deal', '.', 'i', \"'\", 've', 'put', 'emotional', ',', 'personal', 'stuff', 'on', 'here', 'before', 'and', 'it', \"'\", 's', 'always', 'received', 'well', '.', '*', '*', 'this', 'i', \"'\", 'm', 'not', 'sure', 'about', '.', 'i', 'just', 'have', 'a', 'hu', '##nch', '.', '*', '*', 'might', 'be', 'crazy', ',', 'but', 'here', 'goes', ':', 'so', 'how', 'many', 'other', 'ad', '##hd', '##ers', 'fall', 'in', 'love', 'with', 'the', 'people', 'they', 'can', 'never', 'be', 'with', '?', 'this', 'something', 'i', \"'\", 've', 'experienced', 'over', 'and', 'over', '.', 'i', 'fall', 'in', 'love', '(', 'really', 'hard', ')', 'with', 'someone', ',', 'and', 'they', \"'\", 're', 'either', 'out', 'of', 'my', 'age', 'range', 'or', 'their', 'relationship', 'with', 'me', 'means', 'it', 'can', 'never', 'happen', '(', 'hypothetical', 'examples', ':', 'professor', ',', 'parent', \"'\", 's', 'friend', ',', 'boss', ',', 'etc', '.', ')', '.', 'my', 'theory', 'is', 'that', 'it', \"'\", 's', 'ad', '##hd', '.', 'because', 'we', 'can', \"'\", 't', 'keep', 'things', 'constantly', 'in', 'mind', '.', 'that', \"'\", 's', 'why', 'we', 'often', 'do', 'things', 'that', 'hurt', 'our', 'long', '-', 'term', 'goals', ',', 'because', 'we', 'don', \"'\", 't', 'have', 'them', 'sitting', 'there', 'filtering', 'our', 'actions', 'like', 'normal', 'people', 'do', '.', 'and', 'when', 'it', 'comes', 'to', 'love', ',', 'maybe', 'we', 'can', \"'\", 't', 'remember', 'all', 'the', 'time', 'why', 'we', '*', 'can', \"'\", 't', '*', 'be', 'in', 'a', 'relationship', 'with', 'that', 'person', '.', 'we', 'live', 'in', 'the', 'moment', ',', 'and', 'so', 'all', 'we', 'can', 'see', 'when', 'around', 'them', 'is', 'all', 'the', 'things', 'we', 'love', 'about', 'them', '.', 'our', 'minds', 'fan', '##tas', '##ize', 'because', 'you', 'don', \"'\", 't', 'remember', 'that', 'it', \"'\", 's', 'only', 'going', 'to', 'make', 'things', 'worse', '.', 'and', 'sometimes', 'you', 'feel', 'per', '##verted', ',', 'or', 'ashamed', ',', 'or', 'just', 'like', 'a', 'freak', ',', 'but', 'you', 'forget', 'about', 'it', 'all', 'when', 'you', \"'\", 're', 'around', 'them', 'because', 'you', \"'\", 're', 'enjoying', 'them', 'so', 'much', '.', 'what', 'do', 'you', 'think', ',', 'r', '/', 'ad', '##hd', '?', 'is', 'this', 'theory', 'completely', 'off', '?', 'does', 'this', 'happen', 'to', 'you', '?']\n",
      "INFO:__main__:Number of tokens: 330\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['maybe', 'this', 'is', 'crazy', ',', 'but', ':', 'inappropriate', 'love', '.', 'so', 'here', \"'\", 's', 'the', 'deal', '.', 'i', \"'\", 've', 'put', 'emotional', ',', 'personal', 'stuff', 'on', 'here', 'before', 'and', 'it', \"'\", 's', 'always', 'received', 'well', '.', '*', '*', 'this', 'i', \"'\", 'm', 'not', 'sure', 'about', '.', 'i', 'just', 'have', 'a', 'hu', '##nch', '.', '*', '*', 'might', 'be', 'crazy', ',', 'but', 'here', 'goes', ':', 'so', 'how', 'many', 'other', 'ad', '##hd', '##ers', 'fall', 'in', 'love', 'with', 'the', 'people', 'they', 'can', 'never', 'be', 'with', '?', 'this', 'something', 'i', \"'\", 've', 'experienced', 'over', 'and', 'over', '.', 'i', 'fall', 'in', 'love', '(', 'really', 'hard', ')', 'with', 'someone', ',', 'and', 'they', \"'\", 're', 'either', 'out', 'of', 'my', 'age', 'range', 'or', 'their', 'relationship', 'with', 'me', 'means', 'it', 'can', 'never', 'happen', '(', 'hypothetical', 'examples', ':', 'professor', ',', 'parent', \"'\", 's', 'friend', ',', 'boss', ',', 'etc', '.', ')', '.', 'my', 'theory', 'is', 'that', 'it', \"'\", 's', 'ad', '##hd', '.', 'because', 'we', 'can', \"'\", 't', 'keep', 'things', 'constantly', 'in', 'mind', '.', 'that', \"'\", 's', 'why', 'we', 'often', 'do', 'things', 'that', 'hurt', 'our', 'long', '-', 'term', 'goals', ',', 'because', 'we', 'don', \"'\", 't', 'have', 'them', 'sitting', 'there', 'filtering', 'our', 'actions', 'like', 'normal', 'people', 'do', '.', 'and', 'when', 'it', 'comes', 'to', 'love', ',', 'maybe', 'we', 'can', \"'\", 't', 'remember', 'all', 'the', 'time', 'why', 'we', '*', 'can', \"'\", 't', '*', 'be', 'in', 'a', 'relationship', 'with', 'that', 'person', '.', 'we', 'live', 'in', 'the', 'moment', ',', 'and', 'so', 'all', 'we', 'can', 'see', 'when', 'around', 'them', 'is', 'all', 'the', 'things', 'we', 'love', 'about', 'them', '.', 'our', 'minds', 'fan', '##tas', '##ize', 'because', 'you', 'don', \"'\", 't', 'remember', 'that', 'it', \"'\", 's', 'only', 'going', 'to', 'make', 'things', 'worse', '.', 'and', 'sometimes', 'you', 'feel', 'per', '##verted', ',', 'or', 'ashamed', ',', 'or', 'just', 'like', 'a', 'freak', ',', 'but', 'you', 'forget', 'about', 'it', 'all', 'when', 'you', \"'\", 're', 'around', 'them', 'because', 'you', \"'\", 're', 'enjoying', 'them', 'so', 'much', '.', 'what', 'do', 'you', 'think', ',', 'r', '/', 'ad', '##hd', '?', 'is', 'this', 'theory', 'completely', 'off', '?', 'does', 'this', 'happen', 'to', 'you', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['med', '##s', 'on', 'the', 'weekends', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['med', '##s', 'on', 'the', 'weekends', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'win', 'wednesday', ']', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'let', \"'\", 's', 'celebrate', 'together', '!', 'hey', 'everyone', '!', 'my', 'name', 'is', 'brandon', '.', '.', '.', '.', 'i', \"'\", 'm', '28', ',', 'an', 'ad', '##hd', 'coach', ',', 'and', 'an', 'ad', '##hd', 'support', 'group', 'fa', '##ci', '##lita', '##tor', '.', 'at', 'the', 'beginning', 'of', 'each', 'support', 'group', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', '*', '*', 'what', 'surprised', 'me', 'was', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '!', '*', '*', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', 'i', 'would', 'like', 'to', 'make', 'this', 'a', 'weekly', 'thread', '.', 'i', 'will', 'start', 'posting', 'it', 'every', 'wednesday', 'morning', '(', 'unless', 'there', 'is', 'an', 'out', '##cr', '##y', 'from', 'the', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'masses', ')', '.', '*', '*', '*', 'wins', 'can', 'be', 'big', 'or', 'small', '.', '*', 'got', 'into', 'the', 'college', 'of', 'your', 'choice', '*', 'went', 'to', 'bed', 'before', '3a', '##m', 'all', 'week', '*', 'picked', 'the', 'trash', 'up', 'from', 'your', 'room', '*', 'started', 'watching', 'only', '7', 'hours', 'of', 'tv', 'a', 'day', '(', 'instead', 'of', '9', ')', '*', '*', '*', 'we', 'all', 'are', 'at', 'different', 'points', 'in', 'our', 'lives', 'but', '*', '*', 'we', 'all', 'have', 'positive', 'things', 'happening', 'every', 'day', '*', '*', '.', 'people', 'with', 'ad', '##hd', 'often', 'lack', 'the', 'awareness', 'to', 'notice', '(', 'and', 'recall', ')', 'these', 'awesome', 'moments', '.', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', 'i', 'understand', 'it', 'might', 'be', 'hard', '(', 'initially', ')', 'to', 'think', 'of', 'something', '.', '*', '*', 'if', 'you', 'cannot', 'think', 'of', 'a', '\"', 'win', '\"', 'then', 'i', 'invite', 'you', 'to', 'name', '1', '(', 'or', 'more', ')', 'things', 'you', 'are', 'grateful', 'for', '.', '*', '*', '*', '*', '*', 'i', \"'\", 'll', 'start', '.', '.', '.', '*', '*', '*', 'i', 'finally', 'called', 'com', '##cast', 'to', 'reduce', 'my', 'cable', 'bill', '*', '*', 'after', 'pro', '##cr', '##ast', '##inating', 'on', 'that', 'for', 'months', 'and', 'months', '.', 'it', 'wasn', \"'\", 't', 'that', 'hard', 'at', 'all', '(', 'but', 'i', 'built', 'it', 'up', 'to', 'be', 'in', 'my', 'mind', ')', '.', 'i', 'now', 'save', '$', '25', 'each', 'month', '.']\n",
      "INFO:__main__:Number of tokens: 436\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'win', 'wednesday', ']', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'let', \"'\", 's', 'celebrate', 'together', '!', 'hey', 'everyone', '!', 'my', 'name', 'is', 'brandon', '.', '.', '.', '.', 'i', \"'\", 'm', '28', ',', 'an', 'ad', '##hd', 'coach', ',', 'and', 'an', 'ad', '##hd', 'support', 'group', 'fa', '##ci', '##lita', '##tor', '.', 'at', 'the', 'beginning', 'of', 'each', 'support', 'group', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', '*', '*', 'what', 'surprised', 'me', 'was', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '!', '*', '*', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', 'i', 'would', 'like', 'to', 'make', 'this', 'a', 'weekly', 'thread', '.', 'i', 'will', 'start', 'posting', 'it', 'every', 'wednesday', 'morning', '(', 'unless', 'there', 'is', 'an', 'out', '##cr', '##y', 'from', 'the', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'masses', ')', '.', '*', '*', '*', 'wins', 'can', 'be', 'big', 'or', 'small', '.', '*', 'got', 'into', 'the', 'college', 'of', 'your', 'choice', '*', 'went', 'to', 'bed', 'before', '3a', '##m', 'all', 'week', '*', 'picked', 'the', 'trash', 'up', 'from', 'your', 'room', '*', 'started', 'watching', 'only', '7', 'hours', 'of', 'tv', 'a', 'day', '(', 'instead', 'of', '9', ')', '*', '*', '*', 'we', 'all', 'are', 'at', 'different', 'points', 'in', 'our', 'lives', 'but', '*', '*', 'we', 'all', 'have', 'positive', 'things', 'happening', 'every', 'day', '*', '*', '.', 'people', 'with', 'ad', '##hd', 'often', 'lack', 'the', 'awareness', 'to', 'notice', '(', 'and', 'recall', ')', 'these', 'awesome', 'moments', '.', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', 'i', 'understand', 'it', 'might', 'be', 'hard', '(', 'initially', ')', 'to', 'think', 'of', 'something', '.', '*', '*', 'if', 'you', 'cannot', 'think', 'of', 'a', '\"', 'win', '\"', 'then', 'i', 'invite', 'you', 'to', 'name', '1', '(', 'or', 'more', ')', 'things', 'you', 'are', 'grateful', 'for', '.', '*', '*', '*', '*', '*', 'i', \"'\", 'll', 'start', '.', '.', '.', '*', '*', '*', 'i', 'finally', 'called', 'com', '##cast', 'to', 'reduce', 'my', 'cable', 'bill', '*', '*', 'after', 'pro', '##cr', '##ast', '##inating', 'on', 'that', 'for', 'months', 'and', 'months', '.', 'it', 'wasn', \"'\", 't', 'that', 'hard', 'at', 'all', '(', 'but', 'i', 'built', 'it', 'up', 'to', 'be', 'in', 'my', 'mind', ')', '.', 'i', 'now', 'save', '$', '25', 'each', 'month', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'your', 'anti', '-', 'ad', '##hd', 'mantra', '##s', '?', 'the', 'thing', 'is', 'due', 'tomorrow', '.', 'you', \"'\", 've', 'cleared', 'the', 'day', ',', 'have', 'your', 'work', 'environment', 'set', 'up', 'just', 'how', 'it', 'needs', 'to', 'be', ',', 'and', 'there', 'is', 'nothing', 'in', 'your', 'way', '.', 'except', 'red', '##dit', '.', 'or', 'laundry', '.', 'or', 'the', 'cat', '.', 'or', 'any', 'external', 'stimulus', 'that', 'temper', '##s', 'the', 'monkey', 'scratching', 'at', 'the', 'walls', 'of', 'your', 'skull', '.', 'there', 'is', 'something', 'you', 'say', 'to', 'yourself', ',', 'some', 'words', 'or', 'thoughts', 'or', 'imagery', 'or', 'state', 'of', 'mind', 'that', 'makes', 'it', 'possible', 'to', 'live', 'in', 'a', 'quiet', 'world', 'long', 'enough', 'to', 'finish', 'your', 'task', '.', 'what', 'is', 'that', 'thing', '?', '(', 'ps', ':', 'mine', 'is', 'to', 'imagine', 'my', 'self', 'as', 'a', 'machine', ',', 'and', 'i', 'route', 'around', 'damaged', 'systems', '.', ')']\n",
      "INFO:__main__:Number of tokens: 133\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'your', 'anti', '-', 'ad', '##hd', 'mantra', '##s', '?', 'the', 'thing', 'is', 'due', 'tomorrow', '.', 'you', \"'\", 've', 'cleared', 'the', 'day', ',', 'have', 'your', 'work', 'environment', 'set', 'up', 'just', 'how', 'it', 'needs', 'to', 'be', ',', 'and', 'there', 'is', 'nothing', 'in', 'your', 'way', '.', 'except', 'red', '##dit', '.', 'or', 'laundry', '.', 'or', 'the', 'cat', '.', 'or', 'any', 'external', 'stimulus', 'that', 'temper', '##s', 'the', 'monkey', 'scratching', 'at', 'the', 'walls', 'of', 'your', 'skull', '.', 'there', 'is', 'something', 'you', 'say', 'to', 'yourself', ',', 'some', 'words', 'or', 'thoughts', 'or', 'imagery', 'or', 'state', 'of', 'mind', 'that', 'makes', 'it', 'possible', 'to', 'live', 'in', 'a', 'quiet', 'world', 'long', 'enough', 'to', 'finish', 'your', 'task', '.', 'what', 'is', 'that', 'thing', '?', '(', 'ps', ':', 'mine', 'is', 'to', 'imagine', 'my', 'self', 'as', 'a', 'machine', ',', 'and', 'i', 'route', 'around', 'damaged', 'systems', '.', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'week', 'of', 'add', '##eral', '##l', ':', 'i', 'hope', 'this', 'inspire', '##s', 'some', 'of', 'you', 'guys', 'hey', 'r', '/', 'ad', '##hd', '.', 'long', '-', 'time', 'listener', ',', 'first', 'time', 'caller', ':', ')', 'i', \"'\", 've', 'struggled', 'with', 'add', '(', 'ad', '##hd', '-', 'pi', 'specifically', ')', 'my', 'entire', 'life', '.', 'i', 'had', 'the', 'typical', 'story', 'where', 'my', 'teachers', 'all', 'told', 'my', 'parents', 'how', 'bright', 'i', 'was', 'but', 'that', 'i', 'don', \"'\", 't', 'focus', 'in', 'class', '.', 'eventually', 'everyone', 'thought', 'i', 'was', 'just', 'lazy', 'and', 'didn', \"'\", 't', 'care', ',', 'and', 'i', 'started', 'to', 'believe', 'it', 'too', ',', 'which', 'greatly', 'affected', 'my', 'self', 'esteem', '.', 'once', 'i', 'started', 'to', 'suspect', 'it', 'might', 'be', 'add', ',', 'my', 'dad', 'was', 'not', 'rec', '##eptive', 'to', 'testing', 'at', 'all', '.', 'he', 'was', 'a', 're', '##media', '##l', 'english', 'teacher', 'at', 'my', 'local', 'high', 'school', ',', 'and', 'he', 'saw', 'add', '/', 'ad', '##hd', 'dia', '##gno', '##ses', 'wi', '##eld', '##ed', 'like', 'weapons', 'against', 'any', 'behavioral', 'or', 'scholastic', 'complaints', 'brought', 'to', 'parents', \"'\", 'attention', '.', 'once', 'i', 'finally', 'saw', 'a', 'psychologist', 'and', 'was', 'formally', 'diagnosed', ',', 'he', 'became', 'much', 'more', 'rec', '##eptive', 'and', 'even', 'recognized', 'problems', 'in', 'his', 'own', 'behavior', 'that', 'lead', 'him', 'to', 'think', 'he', 'may', 'have', 'it', 'too', '.', 'i', \"'\", 've', 'tried', 'several', 'medications', 'over', 'the', 'years', 'since', 'then', ',', 'but', 'nothing', 'seemed', 'to', 'work', '.', 'the', 'last', 'one', 'i', 'was', 'on', ',', 'st', '##rat', '##tera', ',', 'didn', \"'\", 't', 'appear', 'to', 'do', 'anything', 'at', 'all', ',', 'but', 'since', 'it', 'was', 'slow', '-', 'acting', 'i', 'was', 'told', 'to', 'give', 'it', '4', '-', '6', 'weeks', 'before', 'i', 'looked', 'for', 'any', 'results', '.', 'at', '$', '50', 'per', 'ref', '##ill', ',', 'that', 'just', 'wasn', \"'\", 't', 'feasible', '.', 'i', 'took', 'my', 'diagnosis', 'to', 'a', 'different', 'doctor', ',', 'who', 'echoed', 'my', 'frustration', '##s', 'with', 'the', 'types', 'of', 'medicines', 'i', 'had', 'been', 'prescribed', '.', 'when', 'it', 'takes', '4', '-', '6', 'weeks', 'to', 'see', 'if', 'a', 'medication', 'is', 'even', 'working', ',', 'i', 'can', 'imagine', 'that', 'being', 'frustrating', 'for', 'a', 'doctor', '.', 'he', 'prescribed', 'what', 'he', 'called', 'a', 'more', '\"', 'active', '\"', 'medication', ':', 'add', '##eral', '##l', '.', 'i', \"'\", 'd', 'heard', 'of', 'it', 'before', ',', 'usually', 'mentioned', 'in', 'conjunction', 'with', 'rita', '##lin', ',', 'and', 'was', 'aware', 'of', 'the', 'stigma', 'associated', 'with', 'taking', 'said', 'medications', '.', 'we', \"'\", 've', '*', 'all', '*', 'seen', 'the', 'south', 'park', 'episode', ',', 'i', \"'\", 'm', 'sure', '.', 'today', 'marks', 'the', 'end', 'of', 'the', 'first', 'week', 'i', 'have', 'taken', 'the', 'new', 'prescription', 'for', 'add', '##eral', '##l', '.', 'i', \"'\", 'm', 'completely', 'sold', '.', 'i', \"'\", 've', 'at', '*', 'least', '*', 'doubled', 'my', 'productivity', 'at', 'work', '.', 'things', 'that', 'used', 'to', 'seem', 'like', 'a', 'giant', 'has', '##sle', 'are', 'now', 'easily', 'conquer', '##able', '.', 'it', \"'\", 's', 'like', 'your', 'goal', 'is', 'at', 'the', 'end', 'of', 'a', 'racetrack', 'and', 'the', 'hurdles', 'looked', 'giant', 'and', 'imposing', ',', 'but', 'now', 'you', \"'\", 're', 'able', 'to', 'see', 'that', 'they', \"'\", 're', 'really', 'only', 'an', 'inch', 'tall', '(', 'if', 'they', \"'\", 're', 'even', 'there', 'at', 'all', ')', '.', 'it', 'has', 'also', 'affected', 'my', 'confidence', 'and', ',', 'by', 'extension', ',', 'my', 'willingness', 'to', 'engage', 'in', 'social', 'activities', '.', 'this', 'is', 'a', 'true', 'testament', 'to', 'the', 'positive', 'effects', 'the', '*', 'right', '*', 'medication', 'can', 'have', 'on', 'a', 'person', '.', 'i', 'feel', 'like', 'things', 'are', 'so', 'much', 'more', 'possible', 'than', 'they', 'were', 'before', ',', 'and', 'am', 'eager', 'to', 'start', 'all', 'of', 'it', '.', 'if', 'you', 'are', 'or', 'think', 'you', 'may', 'be', 'struggling', 'with', 'add', 'or', 'ad', '##hd', ',', 'please', 'please', 'please', 'try', 'and', 'do', 'whatever', 'you', 'can', 'to', 'get', 'tested', '.', 'the', 'right', 'medication', 'can', 'make', 'all', 'the', 'difference', 'in', 'the', 'world', '.']\n",
      "INFO:__main__:Number of tokens: 594\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['first', 'week', 'of', 'add', '##eral', '##l', ':', 'i', 'hope', 'this', 'inspire', '##s', 'some', 'of', 'you', 'guys', 'hey', 'r', '/', 'ad', '##hd', '.', 'long', '-', 'time', 'listener', ',', 'first', 'time', 'caller', ':', ')', 'i', \"'\", 've', 'struggled', 'with', 'add', '(', 'ad', '##hd', '-', 'pi', 'specifically', ')', 'my', 'entire', 'life', '.', 'i', 'had', 'the', 'typical', 'story', 'where', 'my', 'teachers', 'all', 'told', 'my', 'parents', 'how', 'bright', 'i', 'was', 'but', 'that', 'i', 'don', \"'\", 't', 'focus', 'in', 'class', '.', 'eventually', 'everyone', 'thought', 'i', 'was', 'just', 'lazy', 'and', 'didn', \"'\", 't', 'care', ',', 'and', 'i', 'started', 'to', 'believe', 'it', 'too', ',', 'which', 'greatly', 'affected', 'my', 'self', 'esteem', '.', 'once', 'i', 'started', 'to', 'suspect', 'it', 'might', 'be', 'add', ',', 'my', 'dad', 'was', 'not', 'rec', '##eptive', 'to', 'testing', 'at', 'all', '.', 'he', 'was', 'a', 're', '##media', '##l', 'english', 'teacher', 'at', 'my', 'local', 'high', 'school', ',', 'and', 'he', 'saw', 'add', '/', 'ad', '##hd', 'dia', '##gno', '##ses', 'wi', '##eld', '##ed', 'like', 'weapons', 'against', 'any', 'behavioral', 'or', 'scholastic', 'complaints', 'brought', 'to', 'parents', \"'\", 'attention', '.', 'once', 'i', 'finally', 'saw', 'a', 'psychologist', 'and', 'was', 'formally', 'diagnosed', ',', 'he', 'became', 'much', 'more', 'rec', '##eptive', 'and', 'even', 'recognized', 'problems', 'in', 'his', 'own', 'behavior', 'that', 'lead', 'him', 'to', 'think', 'he', 'may', 'have', 'it', 'too', '.', 'i', \"'\", 've', 'tried', 'several', 'medications', 'over', 'the', 'years', 'since', 'then', ',', 'but', 'nothing', 'seemed', 'to', 'work', '.', 'the', 'last', 'one', 'i', 'was', 'on', ',', 'st', '##rat', '##tera', ',', 'didn', \"'\", 't', 'appear', 'to', 'do', 'anything', 'at', 'all', ',', 'but', 'since', 'it', 'was', 'slow', '-', 'acting', 'i', 'was', 'told', 'to', 'give', 'it', '4', '-', '6', 'weeks', 'before', 'i', 'looked', 'for', 'any', 'results', '.', 'at', '$', '50', 'per', 'ref', '##ill', ',', 'that', 'just', 'wasn', \"'\", 't', 'feasible', '.', 'i', 'took', 'my', 'diagnosis', 'to', 'a', 'different', 'doctor', ',', 'who', 'echoed', 'my', 'frustration', '##s', 'with', 'the', 'types', 'of', 'medicines', 'i', 'had', 'been', 'prescribed', '.', 'when', 'it', 'takes', '4', '-', '6', 'weeks', 'to', 'see', 'if', 'a', 'medication', 'is', 'even', 'working', ',', 'i', 'can', 'imagine', 'that', 'being', 'frustrating', 'for', 'a', 'doctor', '.', 'he', 'prescribed', 'what', 'he', 'called', 'a', 'more', '\"', 'active', '\"', 'medication', ':', 'add', '##eral', '##l', '.', 'i', \"'\", 'd', 'heard', 'of', 'it', 'before', ',', 'usually', 'mentioned', 'in', 'conjunction', 'with', 'rita', '##lin', ',', 'and', 'was', 'aware', 'of', 'the', 'stigma', 'associated', 'with', 'taking', 'said', 'medications', '.', 'we', \"'\", 've', '*', 'all', '*', 'seen', 'the', 'south', 'park', 'episode', ',', 'i', \"'\", 'm', 'sure', '.', 'today', 'marks', 'the', 'end', 'of', 'the', 'first', 'week', 'i', 'have', 'taken', 'the', 'new', 'prescription', 'for', 'add', '##eral', '##l', '.', 'i', \"'\", 'm', 'completely', 'sold', '.', 'i', \"'\", 've', 'at', '*', 'least', '*', 'doubled', 'my', 'productivity', 'at', 'work', '.', 'things', 'that', 'used', 'to', 'seem', 'like', 'a', 'giant', 'has', '##sle', 'are', 'now', 'easily', 'conquer', '##able', '.', 'it', \"'\", 's', 'like', 'your', 'goal', 'is', 'at', 'the', 'end', 'of', 'a', 'racetrack', 'and', 'the', 'hurdles', 'looked', 'giant', 'and', 'imposing', ',', 'but', 'now', 'you', \"'\", 're', 'able', 'to', 'see', 'that', 'they', \"'\", 're', 'really', 'only', 'an', 'inch', 'tall', '(', 'if', 'they', \"'\", 're', 'even', 'there', 'at', 'all', ')', '.', 'it', 'has', 'also', 'affected', 'my', 'confidence', 'and', ',', 'by', 'extension', ',', 'my', 'willingness', 'to', 'engage', 'in', 'social', 'activities', '.', 'this'], ['is', 'a', 'true', 'testament', 'to', 'the', 'positive', 'effects', 'the', '*', 'right', '*', 'medication', 'can', 'have', 'on', 'a', 'person', '.', 'i', 'feel', 'like', 'things', 'are', 'so', 'much', 'more', 'possible', 'than', 'they', 'were', 'before', ',', 'and', 'am', 'eager', 'to', 'start', 'all', 'of', 'it', '.', 'if', 'you', 'are', 'or', 'think', 'you', 'may', 'be', 'struggling', 'with', 'add', 'or', 'ad', '##hd', ',', 'please', 'please', 'please', 'try', 'and', 'do', 'whatever', 'you', 'can', 'to', 'get', 'tested', '.', 'the', 'right', 'medication', 'can', 'make', 'all', 'the', 'difference', 'in', 'the', 'world', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['te', '##va', 'methyl', '##ph', '##eni', '##date', 'er', '(', 'generic', 'concert', '##a', ')', 'vs', 'concert', '##a', 'for', 'the', 'first', '-', 'time', 'i', 'was', 'given', 'te', '##va', 'methyl', '##ph', '##eni', '##date', 'er', 'the', 'generic', 'form', 'of', 'concert', '##a', ',', 'because', 'my', 'insurance', 'company', 'will', 'not', 'cover', 'me', 'if', 'i', 'can', 'get', 'the', 'cheaper', 'generic', 'concert', '##a', '.', 'tomorrow', 'will', 'be', 'the', 'first', 'day', 'i', 'take', 'the', 'generic', 'form', ',', 'what', 'am', 'i', 'to', 'expect', '?', 'will', 'it', 'be', 'effective', '?']\n",
      "INFO:__main__:Number of tokens: 78\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['te', '##va', 'methyl', '##ph', '##eni', '##date', 'er', '(', 'generic', 'concert', '##a', ')', 'vs', 'concert', '##a', 'for', 'the', 'first', '-', 'time', 'i', 'was', 'given', 'te', '##va', 'methyl', '##ph', '##eni', '##date', 'er', 'the', 'generic', 'form', 'of', 'concert', '##a', ',', 'because', 'my', 'insurance', 'company', 'will', 'not', 'cover', 'me', 'if', 'i', 'can', 'get', 'the', 'cheaper', 'generic', 'concert', '##a', '.', 'tomorrow', 'will', 'be', 'the', 'first', 'day', 'i', 'take', 'the', 'generic', 'form', ',', 'what', 'am', 'i', 'to', 'expect', '?', 'will', 'it', 'be', 'effective', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', 'find', 'red', '##dit', 'to', 'be', 'absolutely', 'horrible', 'for', 'your', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', 'find', 'red', '##dit', 'to', 'be', 'absolutely', 'horrible', 'for', 'your', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['academia', 'hates', 'us', ',', 'and', 'it', 'kind', 'of', 'piss', '##es', 'me', 'off', '.', 'i', 'can', 'only', 'speak', 'for', 'american', '(', 'specifically', 'washington', 'state', ')', 'schools', ',', 'but', 'i', 'am', 'so', 'fed', 'up', 'with', 'this', '.', 'when', 'i', 'was', 'in', 'kindergarten', 'i', 'was', 'selected', 'be', 'enrolled', 'in', 'a', 'highly', '-', 'capable', 'program', '.', 'of', 'those', 'offered', 'to', 'take', 'the', 'test', ',', 'i', 'was', 'in', 'the', 'top', '2', '%', 'of', 'test', '##ers', '.', 'i', 'am', 'now', 'on', 'my', 'last', 'semester', 'of', 'high', 'school', ',', 'with', 'a', 'cumulative', '(', 'un', '##weight', '##ed', ')', 'gp', '##a', 'of', '2', '.', '98', 'and', 'an', 'sat', 'score', 'of', '2000', '.', 'i', 'cannot', 'go', 'to', 'the', 'college', 'of', 'my', 'choice', '.', 'when', 'i', 'tell', 'people', 'i', 'have', 'a', '3', '.', '0', 'they', 'just', 'gap', '##e', '.', 'they', 'protest', ',', 'saying', ',', '\"', 'but', 'you', \"'\", 're', 'so', 'smart', '!', 'that', 'can', \"'\", 't', 'be', 'true', '\"', 'i', 'feel', 'that', 'i', 'have', 'struggled', 'for', 'that', '2', '.', '98', 'oh', 'so', 'much', 'more', 'than', 'any', 'of', 'the', 'honor', 'society', 'kids', 'realize', '.', 'this', 'school', 'system', 'is', 'stacked', 'against', 'us', '.', 'we', 'are', 'working', 'in', 'a', 'machine', 'that', 'is', 'not', 'geared', 'to', 'handle', 'us', '.', 'we', 'are', 'playing', 'academia', 'on', 'hard', 'mode', '.', 'we', 'are', 'the', 'true', 'heroes', '.', 't', '##l', ';', 'dr', ':', 'i', 'am', 'so', 'frustrated', 'that', 'we', 'have', 'to', 'perform', 'in', 'the', 'academic', 'world', 'with', 'the', 'decks', 'stacked', 'against', 'us', '.']\n",
      "INFO:__main__:Number of tokens: 233\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['academia', 'hates', 'us', ',', 'and', 'it', 'kind', 'of', 'piss', '##es', 'me', 'off', '.', 'i', 'can', 'only', 'speak', 'for', 'american', '(', 'specifically', 'washington', 'state', ')', 'schools', ',', 'but', 'i', 'am', 'so', 'fed', 'up', 'with', 'this', '.', 'when', 'i', 'was', 'in', 'kindergarten', 'i', 'was', 'selected', 'be', 'enrolled', 'in', 'a', 'highly', '-', 'capable', 'program', '.', 'of', 'those', 'offered', 'to', 'take', 'the', 'test', ',', 'i', 'was', 'in', 'the', 'top', '2', '%', 'of', 'test', '##ers', '.', 'i', 'am', 'now', 'on', 'my', 'last', 'semester', 'of', 'high', 'school', ',', 'with', 'a', 'cumulative', '(', 'un', '##weight', '##ed', ')', 'gp', '##a', 'of', '2', '.', '98', 'and', 'an', 'sat', 'score', 'of', '2000', '.', 'i', 'cannot', 'go', 'to', 'the', 'college', 'of', 'my', 'choice', '.', 'when', 'i', 'tell', 'people', 'i', 'have', 'a', '3', '.', '0', 'they', 'just', 'gap', '##e', '.', 'they', 'protest', ',', 'saying', ',', '\"', 'but', 'you', \"'\", 're', 'so', 'smart', '!', 'that', 'can', \"'\", 't', 'be', 'true', '\"', 'i', 'feel', 'that', 'i', 'have', 'struggled', 'for', 'that', '2', '.', '98', 'oh', 'so', 'much', 'more', 'than', 'any', 'of', 'the', 'honor', 'society', 'kids', 'realize', '.', 'this', 'school', 'system', 'is', 'stacked', 'against', 'us', '.', 'we', 'are', 'working', 'in', 'a', 'machine', 'that', 'is', 'not', 'geared', 'to', 'handle', 'us', '.', 'we', 'are', 'playing', 'academia', 'on', 'hard', 'mode', '.', 'we', 'are', 'the', 'true', 'heroes', '.', 't', '##l', ';', 'dr', ':', 'i', 'am', 'so', 'frustrated', 'that', 'we', 'have', 'to', 'perform', 'in', 'the', 'academic', 'world', 'with', 'the', 'decks', 'stacked', 'against', 'us', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['solutions', 'for', 'dry', 'mouth', 'on', 'add', '##eral', '##l', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['solutions', 'for', 'dry', 'mouth', 'on', 'add', '##eral', '##l', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['pi', '##race', '##tam', '?']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['pi', '##race', '##tam', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hey', 'add', '##eral', '##l', 'pro', '##s', ',', 'i', 'want', 'to', 'hear', 'from', 'you', '!', 'i', 'was', 'recently', 'prescribed', '10', 'mg', 'add', '##eral', '##l', '2', '-', '3', '##x', 'a', 'day', 'for', 'add', '.', 'i', 'had', 'never', 'taken', 'add', '##eral', '##l', 'before', ',', 'or', 'any', 'other', 'add', 'med', 'for', 'that', 'matter', '.', 'i', 'haven', \"'\", 't', 'noticed', 'much', 'of', 'a', 'difference', 'except', 'my', 'ins', '##omi', '##a', '(', 'which', 'was', 'already', 'pretty', 'bad', ')', 'is', 'now', 'much', ',', 'much', 'worse', '.', 'i', 'was', 'up', 'for', 'two', 'days', 'straight', 'while', 'working', '10', '-', 'hour', 'shifts', 'as', 'a', 'nurse', '.', 'it', 'pretty', 'much', 'killed', 'me', ',', 'but', 'i', 'just', 'couldn', \"'\", 't', 'sleep', '.', 'i', 'am', 'still', 'wanting', 'to', 'give', 'add', '##eral', '##l', 'a', 'chance', ',', 'so', 'i', \"'\", 'm', 'wondering', 'how', 'i', 'should', 'take', 'it', '.', 'maybe', 'if', 'i', 'changed', 'the', 'dose', 'or', 'schedule', 'for', 'taking', 'it', ',', 'i', \"'\", 'd', 'have', 'better', 'results', '?', 'right', 'now', 'i', 'take', 'one', 'at', '5', ':', '40', '##am', ',', 'then', 'another', 'at', 'around', '11', 'and', 'sometimes', 'another', 'at', '4', '##ish', ',', 'although', 'that', 'one', 'might', 'be', 'the', 'biggest', 'contributor', 'to', 'the', 'ins', '##om', '##nia', '.', 'please', 'help', '!', 't', '##l', ';', 'dr', ':', 'if', 'you', 'are', 'having', 'success', 'with', 'add', '##eral', '##l', '(', 'regular', ',', 'not', 'x', '##r', ')', ',', 'what', 'is', 'your', 'dose', 'and', 'how', 'often', '/', 'what', 'times', 'do', 'you', 'take', 'it', '?', 'i', 'realize', 'everyone', \"'\", 's', 'body', 'and', 'situation', 'is', 'different', ',', 'and', 'that', 'what', 'works', 'for', 'you', 'might', 'not', 'work', 'for', 'me', '.', 'i', \"'\", 'm', 'just', 'curious', 'to', 'hear', 'from', 'those', 'who', 'were', 'really', 'helped', 'by', 'this', 'drug', '.', 'thank', 'you', '!']\n",
      "INFO:__main__:Number of tokens: 271\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hey', 'add', '##eral', '##l', 'pro', '##s', ',', 'i', 'want', 'to', 'hear', 'from', 'you', '!', 'i', 'was', 'recently', 'prescribed', '10', 'mg', 'add', '##eral', '##l', '2', '-', '3', '##x', 'a', 'day', 'for', 'add', '.', 'i', 'had', 'never', 'taken', 'add', '##eral', '##l', 'before', ',', 'or', 'any', 'other', 'add', 'med', 'for', 'that', 'matter', '.', 'i', 'haven', \"'\", 't', 'noticed', 'much', 'of', 'a', 'difference', 'except', 'my', 'ins', '##omi', '##a', '(', 'which', 'was', 'already', 'pretty', 'bad', ')', 'is', 'now', 'much', ',', 'much', 'worse', '.', 'i', 'was', 'up', 'for', 'two', 'days', 'straight', 'while', 'working', '10', '-', 'hour', 'shifts', 'as', 'a', 'nurse', '.', 'it', 'pretty', 'much', 'killed', 'me', ',', 'but', 'i', 'just', 'couldn', \"'\", 't', 'sleep', '.', 'i', 'am', 'still', 'wanting', 'to', 'give', 'add', '##eral', '##l', 'a', 'chance', ',', 'so', 'i', \"'\", 'm', 'wondering', 'how', 'i', 'should', 'take', 'it', '.', 'maybe', 'if', 'i', 'changed', 'the', 'dose', 'or', 'schedule', 'for', 'taking', 'it', ',', 'i', \"'\", 'd', 'have', 'better', 'results', '?', 'right', 'now', 'i', 'take', 'one', 'at', '5', ':', '40', '##am', ',', 'then', 'another', 'at', 'around', '11', 'and', 'sometimes', 'another', 'at', '4', '##ish', ',', 'although', 'that', 'one', 'might', 'be', 'the', 'biggest', 'contributor', 'to', 'the', 'ins', '##om', '##nia', '.', 'please', 'help', '!', 't', '##l', ';', 'dr', ':', 'if', 'you', 'are', 'having', 'success', 'with', 'add', '##eral', '##l', '(', 'regular', ',', 'not', 'x', '##r', ')', ',', 'what', 'is', 'your', 'dose', 'and', 'how', 'often', '/', 'what', 'times', 'do', 'you', 'take', 'it', '?', 'i', 'realize', 'everyone', \"'\", 's', 'body', 'and', 'situation', 'is', 'different', ',', 'and', 'that', 'what', 'works', 'for', 'you', 'might', 'not', 'work', 'for', 'me', '.', 'i', \"'\", 'm', 'just', 'curious', 'to', 'hear', 'from', 'those', 'who', 'were', 'really', 'helped', 'by', 'this', 'drug', '.', 'thank', 'you', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['rita', '##lin', 'dos', '##age', 'and', 'administration']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['rita', '##lin', 'dos', '##age', 'and', 'administration']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'ad', '##hd', 'to', 'others', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'ad', '##hd', 'to', 'others', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['advice', 'wanted', ';', 'balance', ',', 'sport', 'and', 'recreation', 'and', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['advice', 'wanted', ';', 'balance', ',', 'sport', 'and', 'recreation', 'and', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['organizational', 'tips', 'for', 'managing', 'ad', '##hd', ':', 'plan', ',', 'visual', '##ize', ',', '&', 'reward', '.']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['organizational', 'tips', 'for', 'managing', 'ad', '##hd', ':', 'plan', ',', 'visual', '##ize', ',', '&', 'reward', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['recommendation', 'for', 'dr', 'in', 'atlanta', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['recommendation', 'for', 'dr', 'in', 'atlanta', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'yesterday', '33', '##yo', 'male', 'here', '.', 'i', 'suspected', 'that', 'i', 'had', 'ad', '##hd', 'for', 'years', ',', 'but', 'several', 'things', 'happening', 'in', 'my', 'professional', 'life', 'finally', 'made', 'me', 'to', 'seek', 'council', 'from', 'a', 'psychologist', ',', 'and', 'they', 'quickly', 'affirmed', 'that', 'i', 'had', 'ad', '##hd', '.', 'in', 'may', 'ways', ',', 'it', \"'\", 's', 'a', 'relief', 'since', 'i', 'now', 'know', 'what', \"'\", 's', 'going', 'on', 'in', 'my', 'brain', '.', 'before', ',', 'it', 'was', 'like', 'there', 'was', 'a', 'puzzle', 'piece', 'missing', 'from', 'my', 'brain', 'or', 'something', '.', 'the', 'effects', 'of', 'ad', '##hd', 'have', 'frustrated', 'me', 'for', 'a', 'long', 'time', ',', 'and', 'i', \"'\", 'm', 'glad', 'that', 'i', 'now', 'have', 'the', 'knowledge', 'to', 'start', 'tack', '##ling', 'this', '.', 'this', 'is', 'all', 'very', 'new', 'to', 'me', ',', 'so', 'i', 'have', 'some', 'questions', 'that', 'i', \"'\", 'd', 'like', 'to', 'get', 'feedback', 'on', ':', '-', 'the', 'psychologist', 'mentioned', '5', 'medication', 'options', '-', 'i', 'remember', 'rita', '##lin', 'and', 'add', '##eral', '##l', ',', 'but', 'forgot', 'the', 'others', '(', 'all', 'were', 'st', '##im', '##ula', '##nts', ',', 'ii', '##rc', ')', '.', 'are', 'they', 'all', 'basically', 'the', 'same', ',', 'or', 'should', 'i', 'ask', 'for', 'one', 'over', 'the', 'others', '?', 'my', 'first', 'cousin', 'said', 'add', '##eral', '##l', 'changed', 'his', 'life', 'for', 'the', 'better', ',', 'so', 'i', 'was', 'going', 'to', 'bring', 'that', 'up', '.', '-', 'despite', 'being', 'relieved', 'that', 'i', 'finally', 'know', 'what', \"'\", 's', 'going', 'on', 'in', 'my', 'head', ',', 'it', \"'\", 's', 'a', 'bit', 'de', '##pressing', 'that', 'i', 'have', 'to', 'take', 'med', '##s', 'to', 'become', 'a', \"'\", 'normal', \"'\", 'person', '.', 'does', 'anyone', 'else', 'deal', 'with', 'this', '?', '-', 'are', 'their', 'any', 'benefits', 'to', 'having', 'ad', '##hd', '?', 'since', 'this', 'is', 'who', 'i', 'am', ',', 'i', 'want', 'to', 'play', 'to', 'my', 'strengths', 'as', 'much', 'as', 'possible', '.', '-', 'do', 'you', 'talk', 'about', 'ad', '##hd', 'with', 'your', 'friends', ',', 'or', 'do', 'you', 'keep', 'it', 'to', 'yourself', '?', '-', 'are', 'there', 'any', 'health', 'implications', 'with', 'med', '##s', 'that', 'i', 'should', 'know', 'about', '?', 'i', \"'\", 'm', '6', \"'\", '1', ',', '240', 'pounds', ',', 'eat', 'reasonably', 'healthy', 'and', 'ride', 'my', 'bike', 'several', 'times', 'a', 'week', '.', 'starting', 'in', 'two', 'weeks', ',', 'i', \"'\", 'm', 'going', 'to', 'be', 'working', 'out', 'at', 'a', 'cross', '##fi', '##t', 'gym', 'and', 'eating', 'the', 'pale', '##o', 'diet', 'because', 'i', 'want', 'to', 'weigh', 'less', 'than', '200', 'pounds', '.', 'anything', 'i', 'should', 'or', 'shouldn', \"'\", 't', 'do', 'while', 'on', 'med', '##s', '?', '-', 'regarding', 'weight', ',', 'is', 'it', 'really', 'true', 'that', 'you', 'lose', 'weight', 'while', 'taking', 'it', '.', 'this', 'could', 'work', 'out', 'nicely', '.', ':', ')', '-', 'if', 'you', 'could', 'put', 'yourself', 'in', 'my', 'shoes', ',', 'is', 'there', 'anything', 'that', 'i', 'need', 'to', 'know', 'right', 'now', '?', 'i', 'have', 'a', 'follow', '##up', 'visit', 'with', 'the', 'psychologist', 'tomorrow', 'and', 'would', 'love', 'to', 'know', 'if', 'there', 'is', 'anything', 'i', 'should', 'be', 'talking', 'about', 'with', 'them', '.']\n",
      "INFO:__main__:Number of tokens: 466\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'yesterday', '33', '##yo', 'male', 'here', '.', 'i', 'suspected', 'that', 'i', 'had', 'ad', '##hd', 'for', 'years', ',', 'but', 'several', 'things', 'happening', 'in', 'my', 'professional', 'life', 'finally', 'made', 'me', 'to', 'seek', 'council', 'from', 'a', 'psychologist', ',', 'and', 'they', 'quickly', 'affirmed', 'that', 'i', 'had', 'ad', '##hd', '.', 'in', 'may', 'ways', ',', 'it', \"'\", 's', 'a', 'relief', 'since', 'i', 'now', 'know', 'what', \"'\", 's', 'going', 'on', 'in', 'my', 'brain', '.', 'before', ',', 'it', 'was', 'like', 'there', 'was', 'a', 'puzzle', 'piece', 'missing', 'from', 'my', 'brain', 'or', 'something', '.', 'the', 'effects', 'of', 'ad', '##hd', 'have', 'frustrated', 'me', 'for', 'a', 'long', 'time', ',', 'and', 'i', \"'\", 'm', 'glad', 'that', 'i', 'now', 'have', 'the', 'knowledge', 'to', 'start', 'tack', '##ling', 'this', '.', 'this', 'is', 'all', 'very', 'new', 'to', 'me', ',', 'so', 'i', 'have', 'some', 'questions', 'that', 'i', \"'\", 'd', 'like', 'to', 'get', 'feedback', 'on', ':', '-', 'the', 'psychologist', 'mentioned', '5', 'medication', 'options', '-', 'i', 'remember', 'rita', '##lin', 'and', 'add', '##eral', '##l', ',', 'but', 'forgot', 'the', 'others', '(', 'all', 'were', 'st', '##im', '##ula', '##nts', ',', 'ii', '##rc', ')', '.', 'are', 'they', 'all', 'basically', 'the', 'same', ',', 'or', 'should', 'i', 'ask', 'for', 'one', 'over', 'the', 'others', '?', 'my', 'first', 'cousin', 'said', 'add', '##eral', '##l', 'changed', 'his', 'life', 'for', 'the', 'better', ',', 'so', 'i', 'was', 'going', 'to', 'bring', 'that', 'up', '.', '-', 'despite', 'being', 'relieved', 'that', 'i', 'finally', 'know', 'what', \"'\", 's', 'going', 'on', 'in', 'my', 'head', ',', 'it', \"'\", 's', 'a', 'bit', 'de', '##pressing', 'that', 'i', 'have', 'to', 'take', 'med', '##s', 'to', 'become', 'a', \"'\", 'normal', \"'\", 'person', '.', 'does', 'anyone', 'else', 'deal', 'with', 'this', '?', '-', 'are', 'their', 'any', 'benefits', 'to', 'having', 'ad', '##hd', '?', 'since', 'this', 'is', 'who', 'i', 'am', ',', 'i', 'want', 'to', 'play', 'to', 'my', 'strengths', 'as', 'much', 'as', 'possible', '.', '-', 'do', 'you', 'talk', 'about', 'ad', '##hd', 'with', 'your', 'friends', ',', 'or', 'do', 'you', 'keep', 'it', 'to', 'yourself', '?', '-', 'are', 'there', 'any', 'health', 'implications', 'with', 'med', '##s', 'that', 'i', 'should', 'know', 'about', '?', 'i', \"'\", 'm', '6', \"'\", '1', ',', '240', 'pounds', ',', 'eat', 'reasonably', 'healthy', 'and', 'ride', 'my', 'bike', 'several', 'times', 'a', 'week', '.', 'starting', 'in', 'two', 'weeks', ',', 'i', \"'\", 'm', 'going', 'to', 'be', 'working', 'out', 'at', 'a', 'cross', '##fi', '##t', 'gym', 'and', 'eating', 'the', 'pale', '##o', 'diet', 'because', 'i', 'want', 'to', 'weigh', 'less', 'than', '200', 'pounds', '.', 'anything', 'i', 'should', 'or', 'shouldn', \"'\", 't', 'do', 'while', 'on', 'med', '##s', '?', '-', 'regarding', 'weight', ',', 'is', 'it', 'really', 'true', 'that', 'you', 'lose', 'weight', 'while', 'taking', 'it', '.', 'this', 'could', 'work', 'out', 'nicely', '.', ':', ')', '-', 'if', 'you', 'could', 'put', 'yourself', 'in', 'my', 'shoes', ',', 'is', 'there', 'anything', 'that', 'i', 'need', 'to', 'know', 'right', 'now', '?', 'i', 'have', 'a', 'follow', '##up', 'visit', 'with', 'the', 'psychologist', 'tomorrow', 'and', 'would', 'love', 'to', 'know', 'if', 'there', 'is', 'anything', 'i', 'should', 'be', 'talking', 'about', 'with', 'them', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'doctor', 'is', 'taking', 'me', 'off', 'add', '##eral', '##l', 'after', '12', '+', 'years', '.']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'doctor', 'is', 'taking', 'me', 'off', 'add', '##eral', '##l', 'after', '12', '+', 'years', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['would', 'it', 'be', 'appropriate', 'to', 'classify', 'ad', '##hd', 'as', 'either', 'intro', '##verted', 'and', 'ex', '##tro', '##verted', '?', 'basically', ',', 'what', 'i', 'realized', 'was', 'that', 'there', 'may', 'be', 'two', 'types', 'of', 'people', 'with', 'ad', '##hd', ':', 'those', 'who', 'are', 'distracted', 'by', 'external', 'stimuli', 'in', 'their', 'surroundings', ',', 'and', 'those', 'who', 'are', 'distracted', 'by', 'internal', 'stimuli', 'within', 'their', 'own', 'imagination', '##s', '.', 'the', 'first', 'type', ',', 'the', 'ex', '##tro', '##verted', 'ad', '##hd', ',', 'is', 'the', 'one', 'most', 'often', 'im', '##itated', 'when', 'parody', '##ing', 'ad', '##hd', '.', 'these', 'are', 'the', '\"', 'they', 'say', 'i', \"'\", 'm', 'ad', '##hd', ',', 'but', 'they', 'don', '##t', 'under', '##hey', 'look', 'a', 'rabbit', '!', '\"', 'type', ',', 'the', 'ones', 'who', 'are', 'talking', 'with', 'friends', 'then', 'suddenly', 'walking', 'away', 'to', 'mess', 'with', 'something', 'cool', 'they', 'saw', '.', 'in', 'other', 'words', ',', 'they', 'are', 'distracted', 'ex', '##tro', '##verted', '##ly', '.', 'the', 'second', 'type', ',', 'the', 'intro', '##verted', 'ad', '##hd', ',', 'will', 'go', 'on', 'wild', 'tangent', '##s', 'during', 'conversations', 'following', 'their', 'own', 'tu', '##mu', '##lt', '##uous', 'thought', 'process', ',', 'or', 'sit', 'day', '##dre', '##ami', '##ng', 'and', 'have', 'to', 'be', 'jolted', 'from', 'their', 'rev', '##eil', '##le', 'to', 'interact', 'again', '.', 'i', 'would', 'also', 'imagine', 'that', 'this', 'is', 'what', 'gives', 'rise', 'to', 'finger', 'tap', '##pers', 'and', 'doo', '##dler', '##s', ',', 'because', 'they', 'become', 'eng', '##ross', '##ed', 'with', 'a', 'beat', 'or', 'feel', 'the', 'ins', '##ati', '##able', 'urge', 'to', 'draw', 'a', 'picture', '.', 'i', \"'\", 'm', 'not', 'really', 'sure', 'if', 'these', 'are', 'in', 'any', 'way', 'substantially', 'different', 'from', 'one', 'another', ',', 'or', 'exist', 'in', 'any', 'ad', '##hd', 'person', ',', 'but', 'i', 'would', 'like', 'to', 'get', '/', 'r', '/', 'ad', '##hd', \"'\", 's', 'opinions', 'on', 'this', '.', 'is', 'this', 'like', 'a', 'personality', 'type', 'in', 'that', 'it', 'is', 'a', 'spectrum', 'of', 'preference', '?']\n",
      "INFO:__main__:Number of tokens: 286\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['would', 'it', 'be', 'appropriate', 'to', 'classify', 'ad', '##hd', 'as', 'either', 'intro', '##verted', 'and', 'ex', '##tro', '##verted', '?', 'basically', ',', 'what', 'i', 'realized', 'was', 'that', 'there', 'may', 'be', 'two', 'types', 'of', 'people', 'with', 'ad', '##hd', ':', 'those', 'who', 'are', 'distracted', 'by', 'external', 'stimuli', 'in', 'their', 'surroundings', ',', 'and', 'those', 'who', 'are', 'distracted', 'by', 'internal', 'stimuli', 'within', 'their', 'own', 'imagination', '##s', '.', 'the', 'first', 'type', ',', 'the', 'ex', '##tro', '##verted', 'ad', '##hd', ',', 'is', 'the', 'one', 'most', 'often', 'im', '##itated', 'when', 'parody', '##ing', 'ad', '##hd', '.', 'these', 'are', 'the', '\"', 'they', 'say', 'i', \"'\", 'm', 'ad', '##hd', ',', 'but', 'they', 'don', '##t', 'under', '##hey', 'look', 'a', 'rabbit', '!', '\"', 'type', ',', 'the', 'ones', 'who', 'are', 'talking', 'with', 'friends', 'then', 'suddenly', 'walking', 'away', 'to', 'mess', 'with', 'something', 'cool', 'they', 'saw', '.', 'in', 'other', 'words', ',', 'they', 'are', 'distracted', 'ex', '##tro', '##verted', '##ly', '.', 'the', 'second', 'type', ',', 'the', 'intro', '##verted', 'ad', '##hd', ',', 'will', 'go', 'on', 'wild', 'tangent', '##s', 'during', 'conversations', 'following', 'their', 'own', 'tu', '##mu', '##lt', '##uous', 'thought', 'process', ',', 'or', 'sit', 'day', '##dre', '##ami', '##ng', 'and', 'have', 'to', 'be', 'jolted', 'from', 'their', 'rev', '##eil', '##le', 'to', 'interact', 'again', '.', 'i', 'would', 'also', 'imagine', 'that', 'this', 'is', 'what', 'gives', 'rise', 'to', 'finger', 'tap', '##pers', 'and', 'doo', '##dler', '##s', ',', 'because', 'they', 'become', 'eng', '##ross', '##ed', 'with', 'a', 'beat', 'or', 'feel', 'the', 'ins', '##ati', '##able', 'urge', 'to', 'draw', 'a', 'picture', '.', 'i', \"'\", 'm', 'not', 'really', 'sure', 'if', 'these', 'are', 'in', 'any', 'way', 'substantially', 'different', 'from', 'one', 'another', ',', 'or', 'exist', 'in', 'any', 'ad', '##hd', 'person', ',', 'but', 'i', 'would', 'like', 'to', 'get', '/', 'r', '/', 'ad', '##hd', \"'\", 's', 'opinions', 'on', 'this', '.', 'is', 'this', 'like', 'a', 'personality', 'type', 'in', 'that', 'it', 'is', 'a', 'spectrum', 'of', 'preference', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['cleaning', 'the', 'apartment', 'but', 'can', \"'\", 't', 'seem', 'to', 'make', 'a', 'dent', '-', 'advice', '?', 'r', '/', 'ad', '##hd', ',', 'i', \"'\", 've', 'lu', '##rked', 'and', 'even', 'commented', 'a', 'few', 'times', 'for', 'a', 'while', 'now', 'but', 'it', \"'\", 's', 'time', 'to', 'seek', 'your', 'help', '.', 'i', 'can', 'usually', 'manage', 'cleaning', 'up', 'after', 'myself', '/', 'putting', 'things', 'away', ',', 'but', 'it', 'seems', 'there', \"'\", 's', 'a', 'critical', 'mass', 'that', 'i', 'hit', 'where', 'i', 'just', 'can', \"'\", 't', 'deal', 'with', 'it', 'anymore', '.', 'after', 'this', 'point', ',', 'i', \"'\", 'm', 'basically', 'paralyzed', 'to', 'even', 'make', 'a', 'dent', 'in', 'the', 'di', '##sor', '##gan', '##ization', 'that', 'is', 'my', 'apartment', '.', 'up', 'until', 'now', ',', 'i', \"'\", 've', 'always', 'had', 'external', 'pressure', 'to', 'keep', 'stuff', 'fairly', 'organized', 'because', 'of', 'roommate', '##s', 'during', 'under', '##grad', ',', 'etc', ',', 'but', 'now', 'i', \"'\", 'm', 'living', 'on', 'my', 'own', 'and', 'the', 'past', 'mid', '##ter', '##m', 'cycle', 'left', 'me', 'not', 'caring', 'about', 'my', 'surroundings', 'and', 'not', 'putting', 'things', 'back', 'in', 'their', 'place', 'in', 'favor', 'of', 'forcing', 'myself', 'to', 'stay', 'on', 'task', 'to', 'study', '.', 'so', 'do', 'you', 'have', 'any', 'helpful', 'hints', 'to', 'help', 'myself', 'tackle', 'cleaning', 'up', 'the', 'apartment', '?', 'i', \"'\", 've', 'tried', 'to', 'just', '*', 'do', '*', 'it', 'but', 'i', 'end', 'up', 'wandering', 'from', 'room', 'to', 'room', ',', 'putting', 'one', 'thing', 'over', 'here', ',', 'one', 'thing', 'over', 'there', ',', 'and', 'never', 'make', 'any', 'progress', '.', 'then', 'i', 'get', 'frustrated', 'and', 'stop', '.', 'thanks', 'a', 'bunch', ',', 'you', 'guys', 'have', 'already', 'helped', 'me', 'more', 'than', 'you', 'know', 'prior', 'to', 'this', 'post', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', 'i', 'can', \"'\", 't', 'manage', 'to', 'get', 'my', 'apartment', 'clean', 'and', 'need', 'advice', '/', 'tips', '/', 'mental', 'tricks', 'to', 'make', 'it', 'less', 'of', 'a', 'challenge', '.']\n",
      "INFO:__main__:Number of tokens: 288\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['cleaning', 'the', 'apartment', 'but', 'can', \"'\", 't', 'seem', 'to', 'make', 'a', 'dent', '-', 'advice', '?', 'r', '/', 'ad', '##hd', ',', 'i', \"'\", 've', 'lu', '##rked', 'and', 'even', 'commented', 'a', 'few', 'times', 'for', 'a', 'while', 'now', 'but', 'it', \"'\", 's', 'time', 'to', 'seek', 'your', 'help', '.', 'i', 'can', 'usually', 'manage', 'cleaning', 'up', 'after', 'myself', '/', 'putting', 'things', 'away', ',', 'but', 'it', 'seems', 'there', \"'\", 's', 'a', 'critical', 'mass', 'that', 'i', 'hit', 'where', 'i', 'just', 'can', \"'\", 't', 'deal', 'with', 'it', 'anymore', '.', 'after', 'this', 'point', ',', 'i', \"'\", 'm', 'basically', 'paralyzed', 'to', 'even', 'make', 'a', 'dent', 'in', 'the', 'di', '##sor', '##gan', '##ization', 'that', 'is', 'my', 'apartment', '.', 'up', 'until', 'now', ',', 'i', \"'\", 've', 'always', 'had', 'external', 'pressure', 'to', 'keep', 'stuff', 'fairly', 'organized', 'because', 'of', 'roommate', '##s', 'during', 'under', '##grad', ',', 'etc', ',', 'but', 'now', 'i', \"'\", 'm', 'living', 'on', 'my', 'own', 'and', 'the', 'past', 'mid', '##ter', '##m', 'cycle', 'left', 'me', 'not', 'caring', 'about', 'my', 'surroundings', 'and', 'not', 'putting', 'things', 'back', 'in', 'their', 'place', 'in', 'favor', 'of', 'forcing', 'myself', 'to', 'stay', 'on', 'task', 'to', 'study', '.', 'so', 'do', 'you', 'have', 'any', 'helpful', 'hints', 'to', 'help', 'myself', 'tackle', 'cleaning', 'up', 'the', 'apartment', '?', 'i', \"'\", 've', 'tried', 'to', 'just', '*', 'do', '*', 'it', 'but', 'i', 'end', 'up', 'wandering', 'from', 'room', 'to', 'room', ',', 'putting', 'one', 'thing', 'over', 'here', ',', 'one', 'thing', 'over', 'there', ',', 'and', 'never', 'make', 'any', 'progress', '.', 'then', 'i', 'get', 'frustrated', 'and', 'stop', '.', 'thanks', 'a', 'bunch', ',', 'you', 'guys', 'have', 'already', 'helped', 'me', 'more', 'than', 'you', 'know', 'prior', 'to', 'this', 'post', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', 'i', 'can', \"'\", 't', 'manage', 'to', 'get', 'my', 'apartment', 'clean', 'and', 'need', 'advice', '/', 'tips', '/', 'mental', 'tricks', 'to', 'make', 'it', 'less', 'of', 'a', 'challenge', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'to', 'explain', 'to', 'employers', 'the', 'medication', 'you', 'are', 'taking', '?', '(', 'drug', 'testing', ')', 'i', 'am', 'graduating', 'college', 'next', 'year', ',', 'and', 'have', 'recently', 'started', 'applying', 'for', 'internship', '##s', '/', 'other', 'career', 'opportunities', '.', 'many', 'of', 'these', 'companies', 'require', 'drug', 'testing', 'before', 'being', 'offered', 'any', 'position', '.', 'now', 'i', 'understand', 'that', 'having', 'a', 'prescription', 'for', 'the', 'medication', 'makes', 'it', 'legal', 'to', 'be', 'taking', 'them', ',', 'but', 'how', 'do', 'i', 'explain', 'this', 'to', 'an', 'employer', 'after', 'a', 'drug', 'screening', 'comes', 'back', 'with', 'amp', '##het', '##amine', '##s', '?', 'pharmacy', 'receipts', ',', 'doctor', 'papers', '?', '-', 'explaining', 'to', 'a', 'friend', 'seems', 'easier', 'then', 'explaining', 'your', 'diagnosed', 'with', 'ad', '##hd', 'to', 'an', 'employer', '.']\n",
      "INFO:__main__:Number of tokens: 111\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'to', 'explain', 'to', 'employers', 'the', 'medication', 'you', 'are', 'taking', '?', '(', 'drug', 'testing', ')', 'i', 'am', 'graduating', 'college', 'next', 'year', ',', 'and', 'have', 'recently', 'started', 'applying', 'for', 'internship', '##s', '/', 'other', 'career', 'opportunities', '.', 'many', 'of', 'these', 'companies', 'require', 'drug', 'testing', 'before', 'being', 'offered', 'any', 'position', '.', 'now', 'i', 'understand', 'that', 'having', 'a', 'prescription', 'for', 'the', 'medication', 'makes', 'it', 'legal', 'to', 'be', 'taking', 'them', ',', 'but', 'how', 'do', 'i', 'explain', 'this', 'to', 'an', 'employer', 'after', 'a', 'drug', 'screening', 'comes', 'back', 'with', 'amp', '##het', '##amine', '##s', '?', 'pharmacy', 'receipts', ',', 'doctor', 'papers', '?', '-', 'explaining', 'to', 'a', 'friend', 'seems', 'easier', 'then', 'explaining', 'your', 'diagnosed', 'with', 'ad', '##hd', 'to', 'an', 'employer', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['recently', 'diagnosed', 'with', 'ad', '##hd', ',', 'not', 'sure', 'what', 'to', 'do', '.', 'when', 'i', 'was', 'in', 'second', 'grade', ',', 'i', 'was', 'a', 'problem', 'child', '.', 'during', 'a', 'parent', 'teacher', 'conference', ',', 'my', 'teacher', 'explained', 'to', 'my', 'parents', 'that', 'she', 'thought', 'i', 'might', 'have', 'ad', '##hd', ',', 'and', 'offered', 'advice', 'and', 'information', 'on', 'pursuing', 'the', 'matter', '.', 'rather', 'than', 'taking', 'her', 'advice', ',', 'my', 'parents', 'took', 'offense', 'and', 'moved', 'me', 'into', 'an', 'accelerated', 'learning', 'program', ',', 'where', 'i', 'fl', '##ound', '##ered', 'and', 'fell', 'apart', '.', 'i', \"'\", 've', 'been', 'struggling', 'with', 'social', 'anxiety', 'and', 'depression', 'for', 'probably', 'nine', 'years', '(', 'i', \"'\", 'm', '23', 'now', ')', 'or', 'so', ',', 'and', 'a', 'few', 'ago', 'i', 'decided', 'that', 'i', \"'\", 'd', 'had', 'enough', '.', 'i', 'started', 'seeing', 'a', 'professional', 'about', 'my', 'issues', '.', 'after', 'a', 'few', 'sessions', ',', 'he', 'felt', 'comfortable', 'making', 'dia', '##gno', '##ses', '.', 'he', 'confirmed', 'my', 'fears', 'of', 'ga', '##d', ',', 'social', 'anxiety', ',', 'and', 'depression', ',', 'but', 'also', 'told', 'me', 'i', 'had', 'all', 'the', 'hallmark', '##s', 'of', 'ad', '##hd', ',', 'and', 'it', 'was', 'probably', 'the', 'reason', 'my', 'life', 'has', 'never', 'really', 'gotten', 'off', 'the', 'ground', ',', 'and', 'could', 'be', 'the', 'root', 'cause', 'of', 'my', 'depression', '.', 'times', 'have', 'been', 'tough', 'financially', '(', 'i', \"'\", 'm', 'not', 'in', 'school', 'and', 'i', \"'\", 'm', 'stuck', 'in', 'a', 'dead', '-', 'end', 'minimum', 'wage', 'job', ')', ',', 'however', ',', 'so', 'i', 'had', 'to', 'stop', 'seeing', 'him', ',', 'and', 'i', 'certainly', 'can', \"'\", 't', 'afford', 'the', 'medication', 'he', 'recommended', 'to', 'me', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '.', 'i', \"'\", 've', 'always', 'felt', 'like', 'a', 'broken', ',', 'worthless', ',', 'disappointing', 'piece', 'of', 'shit', ',', 'but', 'if', 'ad', '##hd', 'really', 'is', 'the', 'cause', 'of', 'my', 'list', '##lessness', ',', 'the', 'knowledge', 'that', 'i', 'might', 'be', 'able', 'to', 'turn', 'my', 'life', 'around', 'has', 'given', 'me', 'a', 'lot', 'of', 'hope', ',', 'but', 'i', 'don', \"'\", 't', 'even', 'know', 'where', 'to', 'begin', '.', 'how', 'can', 'i', 'learn', 'to', 'live', 'with', 'this', '?', 'more', 'importantly', ',', 'how', 'can', 'i', 'use', 'the', 'knowledge', 'that', 'i', 'have', 'ad', '##hd', 'to', 'actively', 'def', '##y', 'it', 'and', 'make', 'my', 'life', 'more', 'stable', 'and', 'reward', '##ing', ',', 'without', 'having', 'to', 'pay', 'for', 'prescription', '##s', 'i', 'can', \"'\", 't', 'afford', '?', 'or', 'are', 'pills', 'the', 'only', 'answer', '?']\n",
      "INFO:__main__:Number of tokens: 375\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['recently', 'diagnosed', 'with', 'ad', '##hd', ',', 'not', 'sure', 'what', 'to', 'do', '.', 'when', 'i', 'was', 'in', 'second', 'grade', ',', 'i', 'was', 'a', 'problem', 'child', '.', 'during', 'a', 'parent', 'teacher', 'conference', ',', 'my', 'teacher', 'explained', 'to', 'my', 'parents', 'that', 'she', 'thought', 'i', 'might', 'have', 'ad', '##hd', ',', 'and', 'offered', 'advice', 'and', 'information', 'on', 'pursuing', 'the', 'matter', '.', 'rather', 'than', 'taking', 'her', 'advice', ',', 'my', 'parents', 'took', 'offense', 'and', 'moved', 'me', 'into', 'an', 'accelerated', 'learning', 'program', ',', 'where', 'i', 'fl', '##ound', '##ered', 'and', 'fell', 'apart', '.', 'i', \"'\", 've', 'been', 'struggling', 'with', 'social', 'anxiety', 'and', 'depression', 'for', 'probably', 'nine', 'years', '(', 'i', \"'\", 'm', '23', 'now', ')', 'or', 'so', ',', 'and', 'a', 'few', 'ago', 'i', 'decided', 'that', 'i', \"'\", 'd', 'had', 'enough', '.', 'i', 'started', 'seeing', 'a', 'professional', 'about', 'my', 'issues', '.', 'after', 'a', 'few', 'sessions', ',', 'he', 'felt', 'comfortable', 'making', 'dia', '##gno', '##ses', '.', 'he', 'confirmed', 'my', 'fears', 'of', 'ga', '##d', ',', 'social', 'anxiety', ',', 'and', 'depression', ',', 'but', 'also', 'told', 'me', 'i', 'had', 'all', 'the', 'hallmark', '##s', 'of', 'ad', '##hd', ',', 'and', 'it', 'was', 'probably', 'the', 'reason', 'my', 'life', 'has', 'never', 'really', 'gotten', 'off', 'the', 'ground', ',', 'and', 'could', 'be', 'the', 'root', 'cause', 'of', 'my', 'depression', '.', 'times', 'have', 'been', 'tough', 'financially', '(', 'i', \"'\", 'm', 'not', 'in', 'school', 'and', 'i', \"'\", 'm', 'stuck', 'in', 'a', 'dead', '-', 'end', 'minimum', 'wage', 'job', ')', ',', 'however', ',', 'so', 'i', 'had', 'to', 'stop', 'seeing', 'him', ',', 'and', 'i', 'certainly', 'can', \"'\", 't', 'afford', 'the', 'medication', 'he', 'recommended', 'to', 'me', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '.', 'i', \"'\", 've', 'always', 'felt', 'like', 'a', 'broken', ',', 'worthless', ',', 'disappointing', 'piece', 'of', 'shit', ',', 'but', 'if', 'ad', '##hd', 'really', 'is', 'the', 'cause', 'of', 'my', 'list', '##lessness', ',', 'the', 'knowledge', 'that', 'i', 'might', 'be', 'able', 'to', 'turn', 'my', 'life', 'around', 'has', 'given', 'me', 'a', 'lot', 'of', 'hope', ',', 'but', 'i', 'don', \"'\", 't', 'even', 'know', 'where', 'to', 'begin', '.', 'how', 'can', 'i', 'learn', 'to', 'live', 'with', 'this', '?', 'more', 'importantly', ',', 'how', 'can', 'i', 'use', 'the', 'knowledge', 'that', 'i', 'have', 'ad', '##hd', 'to', 'actively', 'def', '##y', 'it', 'and', 'make', 'my', 'life', 'more', 'stable', 'and', 'reward', '##ing', ',', 'without', 'having', 'to', 'pay', 'for', 'prescription', '##s', 'i', 'can', \"'\", 't', 'afford', '?', 'or', 'are', 'pills', 'the', 'only', 'answer', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['switching', 'from', 'add', '##eral', 'to', 'viva', '##ns', '.', '.', '.', 'curious', 'how', 'they', 'are', 'different', '.', 'hey', 'guys', '.', 'due', 'to', 'me', 'experiencing', 'just', 'about', 'every', 'side', 'effect', 'that', 'one', 'can', 'possibly', 'have', 'on', 'add', '##eral', '(', 'ed', ',', 'excessive', 'sweating', ',', 'dry', 'mouth', ',', 'feeling', 'cold', ',', 'major', 'headache', '##s', ',', 't', '##we', '##aking', '/', 'nail', 'chewing', 'etc', ')', ',', 'i', 'am', 'switching', 'to', 'viva', '##ns', '.', 'my', 'doc', 'assured', 'me', 'that', 'it', 'would', 'be', 'better', 'in', 'the', 'side', 'effect', 'department', '.', 'what', 'do', 'you', 'guys', 'think', '?', 'have', 'you', 'taken', 'it', '?', 'how', 'did', 'it', 'affect', 'you', '?', 'have', 'you', 'tried', 'add', '##eral', 'as', 'well', '?', 'let', 'me', 'know', '.']\n",
      "INFO:__main__:Number of tokens: 112\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['switching', 'from', 'add', '##eral', 'to', 'viva', '##ns', '.', '.', '.', 'curious', 'how', 'they', 'are', 'different', '.', 'hey', 'guys', '.', 'due', 'to', 'me', 'experiencing', 'just', 'about', 'every', 'side', 'effect', 'that', 'one', 'can', 'possibly', 'have', 'on', 'add', '##eral', '(', 'ed', ',', 'excessive', 'sweating', ',', 'dry', 'mouth', ',', 'feeling', 'cold', ',', 'major', 'headache', '##s', ',', 't', '##we', '##aking', '/', 'nail', 'chewing', 'etc', ')', ',', 'i', 'am', 'switching', 'to', 'viva', '##ns', '.', 'my', 'doc', 'assured', 'me', 'that', 'it', 'would', 'be', 'better', 'in', 'the', 'side', 'effect', 'department', '.', 'what', 'do', 'you', 'guys', 'think', '?', 'have', 'you', 'taken', 'it', '?', 'how', 'did', 'it', 'affect', 'you', '?', 'have', 'you', 'tried', 'add', '##eral', 'as', 'well', '?', 'let', 'me', 'know', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['size', 'of', 'breakfast', 'and', 'medication', '(', 'concert', '##a', ')', 'effectiveness', 'my', 'meals', 'vary', 'quite', 'a', 'bit', '.', 'i', 'have', 'no', 'strict', 'menu', '.', 'some', 'days', 'i', \"'\", 'll', 'have', 'a', 'big', 'breakfast', '(', 'for', 'me', 'a', 'big', 'breakfast', 'is', 'a', 'whole', 'bag', '##el', 'with', 'sm', '##ear', ',', 'and', 'maybe', 'some', 'fruit', '.', '~', '600', 'cal', '##s', ')', 'and', 'other', 'days', 'i', \"'\", 'll', 'have', 'a', 'small', 'breakfast', '(', 'slim', '##fast', 'shake', '~', '180', 'cal', '##s', '.', ')', '.', 'i', \"'\", 've', 'noticed', 'that', 'on', 'days', 'when', 'i', 'have', 'a', 'very', 'small', 'breakfast', ',', 'my', 'med', '##s', 'are', 'more', 'effective', '.', 'i', \"'\", 've', 'also', 'noticed', 'that', 'when', 'i', 'have', 'a', 'big', 'breakfast', ',', 'my', 'med', '##s', 'don', \"'\", 't', 'seem', 'to', 'work', 'at', 'all', ',', 'or', 'don', \"'\", 't', 'kick', 'in', 'until', 'late', 'afternoon', '.', 'i', 'used', 'to', 'not', 'like', 'concert', '##a', 'because', 'i', 'thought', 'it', 'was', 'too', 'random', ',', 'but', 'i', 'think', 'i', \"'\", 've', 'narrowed', 'it', 'down', 'to', 'breakfast', 'being', 'the', 'deciding', 'factor', '.', 'this', 'would', 'make', 'sense', 'though', ',', 'because', 'of', 'digest', '##ion', 'and', 'stuff', '(', 'i', \"'\", 'm', 'not', 'a', 'doctor', 'lo', '##l', ')', '.', 'thoughts', '?', 'has', 'anyone', 'else', 'noticed', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 197\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['size', 'of', 'breakfast', 'and', 'medication', '(', 'concert', '##a', ')', 'effectiveness', 'my', 'meals', 'vary', 'quite', 'a', 'bit', '.', 'i', 'have', 'no', 'strict', 'menu', '.', 'some', 'days', 'i', \"'\", 'll', 'have', 'a', 'big', 'breakfast', '(', 'for', 'me', 'a', 'big', 'breakfast', 'is', 'a', 'whole', 'bag', '##el', 'with', 'sm', '##ear', ',', 'and', 'maybe', 'some', 'fruit', '.', '~', '600', 'cal', '##s', ')', 'and', 'other', 'days', 'i', \"'\", 'll', 'have', 'a', 'small', 'breakfast', '(', 'slim', '##fast', 'shake', '~', '180', 'cal', '##s', '.', ')', '.', 'i', \"'\", 've', 'noticed', 'that', 'on', 'days', 'when', 'i', 'have', 'a', 'very', 'small', 'breakfast', ',', 'my', 'med', '##s', 'are', 'more', 'effective', '.', 'i', \"'\", 've', 'also', 'noticed', 'that', 'when', 'i', 'have', 'a', 'big', 'breakfast', ',', 'my', 'med', '##s', 'don', \"'\", 't', 'seem', 'to', 'work', 'at', 'all', ',', 'or', 'don', \"'\", 't', 'kick', 'in', 'until', 'late', 'afternoon', '.', 'i', 'used', 'to', 'not', 'like', 'concert', '##a', 'because', 'i', 'thought', 'it', 'was', 'too', 'random', ',', 'but', 'i', 'think', 'i', \"'\", 've', 'narrowed', 'it', 'down', 'to', 'breakfast', 'being', 'the', 'deciding', 'factor', '.', 'this', 'would', 'make', 'sense', 'though', ',', 'because', 'of', 'digest', '##ion', 'and', 'stuff', '(', 'i', \"'\", 'm', 'not', 'a', 'doctor', 'lo', '##l', ')', '.', 'thoughts', '?', 'has', 'anyone', 'else', 'noticed', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['interesting', 'article', 'on', 'the', 'brain', \"'\", 's', '\"', 'anti', '-', 'distraction', '\"', 'measures', 'for', 'mono', '##ton', '##ous', 'speech', '.']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['interesting', 'article', 'on', 'the', 'brain', \"'\", 's', '\"', 'anti', '-', 'distraction', '\"', 'measures', 'for', 'mono', '##ton', '##ous', 'speech', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ab', '##ili', '##fy', 'and', 'v', '##y', '##van', '##se', 'could', 'i', 'take', 'this', 'as', 'i', 'need', 'it', '?', 'or', 'will', 'it', 'mess', 'me', 'up', 'too', 'much', '?']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ab', '##ili', '##fy', 'and', 'v', '##y', '##van', '##se', 'could', 'i', 'take', 'this', 'as', 'i', 'need', 'it', '?', 'or', 'will', 'it', 'mess', 'me', 'up', 'too', 'much', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'x', '##r', 'and', 'ac', '##ne']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'x', '##r', 'and', 'ac', '##ne']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'before', 'and', 'during', 'sex', 'hey', 'r', '/', 'ad', '##hd', ',', 'i', \"'\", 've', 'lu', '##rked', 'this', 'sub', '##red', '##dit', 'for', 'a', 'while', 'now', ',', 'enough', 'to', 'gather', 'that', 'there', 'are', 'a', 'lot', 'of', 'people', 'here', 'whose', 'minds', 'work', 'very', 'simi', '##lia', '##r', 'to', 'mine', '.', 'i', \"'\", 've', 'been', 'dating', 'this', 'girl', 'for', 'about', 'four', 'months', 'now', ',', 'and', 'i', 'finally', 'realized', 'that', 'whenever', 'we', \"'\", 're', 'making', 'out', 'or', 'about', 'to', 'have', 'sex', 'that', 'my', 'mind', 'is', 'almost', 'never', 'on', 'her', '.', 'its', 'not', 'that', 'i', \"'\", 'm', 'bored', ',', 'because', 'believe', 'me', ',', 'i', \"'\", 'm', 'not', ',', 'but', 'when', 'i', \"'\", 'm', 'not', 'speaking', ',', 'my', 'mind', 'just', 'wander', '##s', '.', 'i', 'end', 'up', 'thinking', 'about', 'various', 'topics', 'that', 'i', \"'\", 'm', 'sure', 'would', 'piss', 'her', 'off', 'if', 'she', 'knew', 'i', 'was', 'thinking', 'about', 'them', '.', 'during', 'sex', 'its', 'slightly', 'better', ',', 'but', 'i', 'do', 'get', 'distracted', 'sometimes', 'and', 'rev', '##ert', '##ing', 'back', 'to', 'my', 'standard', 'ways', '.', 'does', 'this', 'mean', 'that', 'the', 'relationship', 'is', 'too', 'boring', '?', 'i', 'feel', 'like', 'everything', 'is', 'going', 'fine', 'but', 'this', 'kind', 'of', 'weird', '##s', 'me', 'out', 'so', 'i', 'was', 'wondering', 'if', 'anyone', 'else', 'has', 'noticed', 'this', '.']\n",
      "INFO:__main__:Number of tokens: 199\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'before', 'and', 'during', 'sex', 'hey', 'r', '/', 'ad', '##hd', ',', 'i', \"'\", 've', 'lu', '##rked', 'this', 'sub', '##red', '##dit', 'for', 'a', 'while', 'now', ',', 'enough', 'to', 'gather', 'that', 'there', 'are', 'a', 'lot', 'of', 'people', 'here', 'whose', 'minds', 'work', 'very', 'simi', '##lia', '##r', 'to', 'mine', '.', 'i', \"'\", 've', 'been', 'dating', 'this', 'girl', 'for', 'about', 'four', 'months', 'now', ',', 'and', 'i', 'finally', 'realized', 'that', 'whenever', 'we', \"'\", 're', 'making', 'out', 'or', 'about', 'to', 'have', 'sex', 'that', 'my', 'mind', 'is', 'almost', 'never', 'on', 'her', '.', 'its', 'not', 'that', 'i', \"'\", 'm', 'bored', ',', 'because', 'believe', 'me', ',', 'i', \"'\", 'm', 'not', ',', 'but', 'when', 'i', \"'\", 'm', 'not', 'speaking', ',', 'my', 'mind', 'just', 'wander', '##s', '.', 'i', 'end', 'up', 'thinking', 'about', 'various', 'topics', 'that', 'i', \"'\", 'm', 'sure', 'would', 'piss', 'her', 'off', 'if', 'she', 'knew', 'i', 'was', 'thinking', 'about', 'them', '.', 'during', 'sex', 'its', 'slightly', 'better', ',', 'but', 'i', 'do', 'get', 'distracted', 'sometimes', 'and', 'rev', '##ert', '##ing', 'back', 'to', 'my', 'standard', 'ways', '.', 'does', 'this', 'mean', 'that', 'the', 'relationship', 'is', 'too', 'boring', '?', 'i', 'feel', 'like', 'everything', 'is', 'going', 'fine', 'but', 'this', 'kind', 'of', 'weird', '##s', 'me', 'out', 'so', 'i', 'was', 'wondering', 'if', 'anyone', 'else', 'has', 'noticed', 'this', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'wanted', 'to', 'share', 'a', 'little', 'about', 'this', 'week', '.', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'wanted', 'to', 'share', 'a', 'little', 'about', 'this', 'week', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', '/', 'r', '/', 'ad', '##hd', \"'\", 's', 'advice', '.', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', '/', 'r', '/', 'ad', '##hd', \"'\", 's', 'advice', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'have', 'a', '\"', 'coach', '\"', '?', 'my', 'doctor', 'and', 'i', 'have', 'decided', 'that', 'add', '##eral', 'isn', \"'\", 't', 'enough', 'to', 'get', 'my', 'shit', 'together', ',', 'so', 'she', 'told', 'me', 'to', 'get', 'set', 'up', 'with', 'an', 'associate', 'of', 'hers', 'who', 'specializes', 'with', 'what', 'i', \"'\", 've', 'read', 'in', 'books', 'as', 'add', 'coaching', '.', 'funny', 'thing', 'is', 'that', 'she', 'sends', 'me', 'to', 'see', 'an', 'assistant', 'to', 'call', 'on', 'my', 'behalf', 'to', 'get', 'me', 'an', 'appointment', 'for', 'assessment', '.', 'i', 'get', 'to', 'the', 'desk', ',', 'tell', 'the', 'assistant', 'what', 'i', 'need', ',', 'and', 'she', 'say', \"'\", 's', '\"', 'oh', ',', 'well', ',', 'let', 'me', 'get', 'you', 'the', 'number', 'for', 'the', 'department', 'that', 'handles', 'that', '.', '\"', 'not', 'really', 'what', 'i', 'was', 'hoping', 'for', ',', 'but', 'i', 'should', 'have', 'spoken', 'up', ',', 'because', 'not', 'even', 'half', 'an', 'hour', 'after', 'i', 'left', 'the', 'building', 'i', 'lost', 'the', 'paper', 'with', 'the', 'number', 'on', 'it', '.', 'anyway', '##s', ',', 'is', 'this', 'really', 'all', 'that', 'helpful', ',', 'or', 'would', 'i', 'be', 'wasting', 'my', 'money', '(', 'i', \"'\", 'm', 'not', 'sure', 'how', 'much', 'my', 'insurance', 'will', 'cover', 'for', 'such', 'a', 'service', ')', '?']\n",
      "INFO:__main__:Number of tokens: 185\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'have', 'a', '\"', 'coach', '\"', '?', 'my', 'doctor', 'and', 'i', 'have', 'decided', 'that', 'add', '##eral', 'isn', \"'\", 't', 'enough', 'to', 'get', 'my', 'shit', 'together', ',', 'so', 'she', 'told', 'me', 'to', 'get', 'set', 'up', 'with', 'an', 'associate', 'of', 'hers', 'who', 'specializes', 'with', 'what', 'i', \"'\", 've', 'read', 'in', 'books', 'as', 'add', 'coaching', '.', 'funny', 'thing', 'is', 'that', 'she', 'sends', 'me', 'to', 'see', 'an', 'assistant', 'to', 'call', 'on', 'my', 'behalf', 'to', 'get', 'me', 'an', 'appointment', 'for', 'assessment', '.', 'i', 'get', 'to', 'the', 'desk', ',', 'tell', 'the', 'assistant', 'what', 'i', 'need', ',', 'and', 'she', 'say', \"'\", 's', '\"', 'oh', ',', 'well', ',', 'let', 'me', 'get', 'you', 'the', 'number', 'for', 'the', 'department', 'that', 'handles', 'that', '.', '\"', 'not', 'really', 'what', 'i', 'was', 'hoping', 'for', ',', 'but', 'i', 'should', 'have', 'spoken', 'up', ',', 'because', 'not', 'even', 'half', 'an', 'hour', 'after', 'i', 'left', 'the', 'building', 'i', 'lost', 'the', 'paper', 'with', 'the', 'number', 'on', 'it', '.', 'anyway', '##s', ',', 'is', 'this', 'really', 'all', 'that', 'helpful', ',', 'or', 'would', 'i', 'be', 'wasting', 'my', 'money', '(', 'i', \"'\", 'm', 'not', 'sure', 'how', 'much', 'my', 'insurance', 'will', 'cover', 'for', 'such', 'a', 'service', ')', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sc', '##hi', '##zo', '##id', 'personality', 'disorder']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sc', '##hi', '##zo', '##id', 'personality', 'disorder']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['don', \"'\", 't', 'have', 'add', '.', 'need', 'help', '.', 'where', 'to', 'go', '?', 'i', 'currently', 'struggle', 'a', 'lot', 'with', 'academia', 'because', 'i', 'can', \"'\", 't', 'sit', 'down', 'and', 'read', 'the', 'damn', 'book', 'or', 'do', 'the', 'assignment', '.', 'i', 'just', 'can', \"'\", 't', '.', 'however', 'i', 'have', 'no', 'trouble', 'of', 'following', 'a', 'lecture', '.', 'the', 'parallels', 'between', 'my', 'problem', 'and', 'the', 'symptoms', 'of', 'add', '/', 'ad', '##hd', 'always', 'draw', 'me', 'to', 'these', 'resources', 'but', 'are', 'of', 'courses', 'targeted', 'at', 'completely', 'different', 'conditions', '.', 'r', '/', 'pro', '##cr', '##ast', '##ination', 'is', 'a', 'complete', 'joke', '.', 'self', '-', 'improvement', 'books', 'claim', 'to', 'be', 'the', 'ultimate', 'solution', 'but', 'don', \"'\", 't', 'help', 'a', 'bit', 'i', 'just', 'don', \"'\", 't', 'know', 'where', 'to', 'go', 'with', 'my', 'problems', 'that', 'nearly', 'every', 'college', 'student', 'seems', 'to', 'face', 'and', 'i', 'really', 'really', 'don', \"'\", 't', 'want', 'to', 'drop', 'out', '.', 'any', 'pointer', '##s', 'to', 'relevant', 'sub', '##red', '##dit', '##s', 'would', 'be', 'greatly', 'appreciated', '.', 'thank', 'you', '.']\n",
      "INFO:__main__:Number of tokens: 159\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['don', \"'\", 't', 'have', 'add', '.', 'need', 'help', '.', 'where', 'to', 'go', '?', 'i', 'currently', 'struggle', 'a', 'lot', 'with', 'academia', 'because', 'i', 'can', \"'\", 't', 'sit', 'down', 'and', 'read', 'the', 'damn', 'book', 'or', 'do', 'the', 'assignment', '.', 'i', 'just', 'can', \"'\", 't', '.', 'however', 'i', 'have', 'no', 'trouble', 'of', 'following', 'a', 'lecture', '.', 'the', 'parallels', 'between', 'my', 'problem', 'and', 'the', 'symptoms', 'of', 'add', '/', 'ad', '##hd', 'always', 'draw', 'me', 'to', 'these', 'resources', 'but', 'are', 'of', 'courses', 'targeted', 'at', 'completely', 'different', 'conditions', '.', 'r', '/', 'pro', '##cr', '##ast', '##ination', 'is', 'a', 'complete', 'joke', '.', 'self', '-', 'improvement', 'books', 'claim', 'to', 'be', 'the', 'ultimate', 'solution', 'but', 'don', \"'\", 't', 'help', 'a', 'bit', 'i', 'just', 'don', \"'\", 't', 'know', 'where', 'to', 'go', 'with', 'my', 'problems', 'that', 'nearly', 'every', 'college', 'student', 'seems', 'to', 'face', 'and', 'i', 'really', 'really', 'don', \"'\", 't', 'want', 'to', 'drop', 'out', '.', 'any', 'pointer', '##s', 'to', 'relevant', 'sub', '##red', '##dit', '##s', 'would', 'be', 'greatly', 'appreciated', '.', 'thank', 'you', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'causes']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'causes']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['constant', 'fatigue', '/', 'dr', '##ows', '##iness', 'when', 'off', 'medication', '.', '.', '.', 'i', 'was', 'diagnosed', 'and', 'given', 'medication', '(', 'dex', '##tro', '##amp', '##het', '##amine', ')', 'a', 'few', 'months', 'ago', 'and', 'i', 'have', 'grown', 'tolerant', 'of', 'all', 'the', 'minor', 'side', 'effects', '(', 'ie', ':', 'appetite', 'loss', ',', 'dry', 'mouth', ',', 'sore', 'tongue', ')', 'but', 'when', 'i', 'come', 'off', 'of', 'it', 'like', 'on', 'the', 'weekends', 'when', 'i', 'don', \"'\", 't', 'necessarily', 'need', 'medication', ',', 'i', 'sleep', 'for', 'roughly', '10', '+', 'hours', 'and', 'even', 'after', 'that', 'find', 'it', 'really', 'difficult', 'to', 'even', 'get', 'out', 'of', 'my', 'bed', 'to', 'go', 'get', 'breakfast', '.', 'it', 'honestly', 'feels', 'like', 'i', 'could', 'sleep', 'the', 'whole', 'day', '.', '.', 'has', 'anyone', 'else', 'seen', 'dramatic', 'increases', 'in', 'fatigue', 'or', 'dr', '##ows', '##iness', 'throughout', 'the', 'day', 'when', 'off', 'medication', '?', 'i', 'understand', 'that', 'i', 'need', 'the', 'med', '##s', 'for', 'my', 'add', ',', 'but', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'so', 'dependent', 'on', 'them', 'that', 'it', \"'\", 's', 'the', 'only', 'thing', 'that', \"'\", 'll', 'keep', 'me', 'awake', 'during', 'the', 'day', '.']\n",
      "INFO:__main__:Number of tokens: 171\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['constant', 'fatigue', '/', 'dr', '##ows', '##iness', 'when', 'off', 'medication', '.', '.', '.', 'i', 'was', 'diagnosed', 'and', 'given', 'medication', '(', 'dex', '##tro', '##amp', '##het', '##amine', ')', 'a', 'few', 'months', 'ago', 'and', 'i', 'have', 'grown', 'tolerant', 'of', 'all', 'the', 'minor', 'side', 'effects', '(', 'ie', ':', 'appetite', 'loss', ',', 'dry', 'mouth', ',', 'sore', 'tongue', ')', 'but', 'when', 'i', 'come', 'off', 'of', 'it', 'like', 'on', 'the', 'weekends', 'when', 'i', 'don', \"'\", 't', 'necessarily', 'need', 'medication', ',', 'i', 'sleep', 'for', 'roughly', '10', '+', 'hours', 'and', 'even', 'after', 'that', 'find', 'it', 'really', 'difficult', 'to', 'even', 'get', 'out', 'of', 'my', 'bed', 'to', 'go', 'get', 'breakfast', '.', 'it', 'honestly', 'feels', 'like', 'i', 'could', 'sleep', 'the', 'whole', 'day', '.', '.', 'has', 'anyone', 'else', 'seen', 'dramatic', 'increases', 'in', 'fatigue', 'or', 'dr', '##ows', '##iness', 'throughout', 'the', 'day', 'when', 'off', 'medication', '?', 'i', 'understand', 'that', 'i', 'need', 'the', 'med', '##s', 'for', 'my', 'add', ',', 'but', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'so', 'dependent', 'on', 'them', 'that', 'it', \"'\", 's', 'the', 'only', 'thing', 'that', \"'\", 'll', 'keep', 'me', 'awake', 'during', 'the', 'day', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['meeting', 'tomorrow', 'so', 'i', 'have', 'a', 'meeting', 'with', 'my', 'adviser', 'tomorrow', 'about', 'my', 'situation', '.', 'i', 'am', 'tired', 'of', 'being', 'be', '##rated', 'by', 'her', 'and', 'bel', '##itt', '##ling', 'me', 'and', 'offering', 'no', 'help', '.', 'with', 'my', 'v', '##y', '##van', '##se', 'i', 'am', 'prone', 'to', 'have', 'a', 'short', 'temper', '.', 'how', 'do', 'i', 'not', 'fly', 'off', 'the', 'handle', 'and', 'just', 'take', 'her', 'insults', '?', 'i', 'know', 'you', 'might', 'say', 'i', 'shouldn', \"'\", 't', 'put', 'up', 'with', 'it', 'but', 'at', 'this', 'point', 'i', 'am', 'defeated', '.', 'maybe', 'she', 'is', 'right', 'in', 'telling', 'me', 'i', 'do', 'not', 'have', 'what', 'it', 'takes', 'to', 'be', 'a', 'teacher', '(', 'my', 'grades', 'are', '2', '.', '7', '##gp', '##a', ')', '.']\n",
      "INFO:__main__:Number of tokens: 113\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['meeting', 'tomorrow', 'so', 'i', 'have', 'a', 'meeting', 'with', 'my', 'adviser', 'tomorrow', 'about', 'my', 'situation', '.', 'i', 'am', 'tired', 'of', 'being', 'be', '##rated', 'by', 'her', 'and', 'bel', '##itt', '##ling', 'me', 'and', 'offering', 'no', 'help', '.', 'with', 'my', 'v', '##y', '##van', '##se', 'i', 'am', 'prone', 'to', 'have', 'a', 'short', 'temper', '.', 'how', 'do', 'i', 'not', 'fly', 'off', 'the', 'handle', 'and', 'just', 'take', 'her', 'insults', '?', 'i', 'know', 'you', 'might', 'say', 'i', 'shouldn', \"'\", 't', 'put', 'up', 'with', 'it', 'but', 'at', 'this', 'point', 'i', 'am', 'defeated', '.', 'maybe', 'she', 'is', 'right', 'in', 'telling', 'me', 'i', 'do', 'not', 'have', 'what', 'it', 'takes', 'to', 'be', 'a', 'teacher', '(', 'my', 'grades', 'are', '2', '.', '7', '##gp', '##a', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['jim', 'carr', '##ey', ',', 'i', 'fucking', 'love', 'this', 'guy', '-', '\"', 'it', \"'\", 's', 'not', 'a', 'joke', 'either', ',', 'he', 'is', 'absolutely', 'a', 'prima', 'donna', ',', 'he', 'is', 'really', 'difficult', '.', '.', '.', 'i', 'have', 'seen', 'him', 'have', 'absolute', 'total', 'tan', '##trum', '##s', 'on', 'the', 'set', '.', '\"', '-', 'courtney', 'cox']\n",
      "INFO:__main__:Number of tokens: 51\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['jim', 'carr', '##ey', ',', 'i', 'fucking', 'love', 'this', 'guy', '-', '\"', 'it', \"'\", 's', 'not', 'a', 'joke', 'either', ',', 'he', 'is', 'absolutely', 'a', 'prima', 'donna', ',', 'he', 'is', 'really', 'difficult', '.', '.', '.', 'i', 'have', 'seen', 'him', 'have', 'absolute', 'total', 'tan', '##trum', '##s', 'on', 'the', 'set', '.', '\"', '-', 'courtney', 'cox']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'notice', ']', '2', ',', '000', '+', 'subscribers', 'hey', 'everyone', ',', 'i', 'just', 'wanted', 'to', 'make', 'this', 'quick', 'post', 'to', 'say', 'how', 'great', 'everyone', 'in', 'this', 'community', 'is', 'and', 'it', \"'\", 's', 'awesome', 'we', 'have', 'recently', 'hit', '2', ',', '000', 'subscribers', '.', 'also', 'to', 'remind', 'everyone', 'of', 'a', 'few', 'things', '.', '#', '#', '#', '#', '#', 'up', '##vot', '##ing', 'posts', '*', '*', '*', 'we', 'seem', 'to', 'have', 'a', 'disco', '##nne', '##ct', 'around', 'here', 'with', 'participating', 'via', 'comments', 'and', 'participating', 'with', 'up', '##vot', '##es', '.', 'if', 'you', 'are', 'taking', 'the', 'time', 'to', 'comment', 'on', 'a', 'post', 'up', '##vot', '##e', 'it', ',', 'unless', 'the', 'reason', 'you', 'are', 'commenting', 'is', 'to', 'tell', 'them', 'why', 'it', 'shouldn', \"'\", 't', 'be', 'posted', '.', 'if', 'a', 'post', 'is', 'worth', 'you', 'making', 'discussion', 'on', 'the', 'best', 'way', 'for', 'people', 'to', 'see', 'it', 'and', 'be', 'added', 'to', 'the', 'discussion', 'is', 'with', 'up', '##vot', '##es', 'and', 'we', 'have', 'a', 'real', 'lack', 'of', 'people', 'participating', 'via', 'the', 'up', '##vot', '##e', '.', 'also', 'the', 'down', '##vot', '##e', 'is', 'not', 'for', 'opinions', 'you', 'disagree', 'with', '.', '#', '#', '#', '#', '#', 'read', 'the', 'fa', '##q', '*', '*', '*', 'if', 'you', \"'\", 're', 'new', 'or', 'even', 'old', 'and', 'have', 'yet', 'to', 'read', 'the', 'fa', '##q', 'please', 'go', 'do', 'so', ',', 'it', 'is', 'there', 'for', 'a', 'reason', 'and', 'we', 'still', 'need', 'to', 'improve', 'it', 'over', 'time', '.', 'the', 'only', 'way', 'for', 'us', 'as', 'a', 'community', 'to', 'produce', 'the', 'best', 'fa', '##q', '/', 'help', 'page', 'we', 'can', 'is', 'for', 'people', 'within', 'the', 'community', 'to', 'actually', 'read', 'it', '.', '#', '#', '#', '#', '#', 'ir', '##c', '*', '*', '*', 'on', 'the', 'side', 'bar', 'there', 'is', 'a', 'link', 'to', 'the', '[', 'ir', '##c', 'chat', 'room', ']', '(', 'http', ':', '/', '/', 'web', '##cha', '##t', '.', 'free', '##no', '##de', '.', 'net', '/', '?', 'channels', '=', '/', 'r', '/', 'ad', '##hd', ')', '.', 'it', 'is', 'a', 'great', 'place', 'to', 'hang', 'out', 'and', 'talk', 'with', 'other', 'community', 'members', '.', 'there', 'are', 'also', 'weekly', 'meet', '##ups', ',', 'times', 'are', 'posted', 'on', 'the', 'side', '##bar', '.', '#', '#', '#', '#', '#', 'attitude', '*', '*', '*', 'this', 'is', 'very', 'important', 'for', 'any', 'online', 'forum', '.', 'you', 'can', 'not', 'figure', 'a', 'person', 'out', 'nor', 'know', 'their', 'intent', 'via', 'a', 'post', 'over', 'the', 'internet', 'so', 'please', 'be', 'nice', 'and', 'respectful', 'especially', 'to', 'those', 'attempting', 'to', 'bring', 'content', 'into', 'the', 'community', '.', 'if', 'someone', 'has', 'posted', 'an', 'article', 'it', 'means', 'they', 'found', 'it', 'interesting', 'or', 'simply', 'that', 'we', 'might', 'like', 'to', 'discuss', 'it', '.', 'it', 'does', 'not', 'mean', 'the', 'person', 'posting', 'agrees', 'with', 'the', 'opinion', 'of', 'the', 'article', ',', 'if', 'you', 'disagree', 'with', 'the', 'article', 'but', 'are', 'still', 'willing', 'to', 'post', 'and', 'discuss', 'it', 'then', 'up', '##vot', '##e', 'the', 'post', '.', 'this', 'is', 'what', 'up', '##vot', '##es', 'and', 'down', '##vot', '##es', 'are', 'for', ',', 'filtering', 'out', 'irrelevant', 'content', 'not', 'to', 'show', 'your', 'support', 'or', 'disagreement', 'so', 'please', 'keep', 'that', 'in', 'mind', 'so', 'we', 'can', 'continue', 'to', 'make', 'this', 'a', 'great', 'community', 'with', 'healthy', 'discussion', '.', '#', '#', '#', '#', '#', 'format', '##ting', '*', '*', '*', 'format', '##ting', 'when', 'posting', 'a', 'wall', 'of', 'text', 'is', 'extremely', 'vital', 'around', 'here', '.', 'you', 'have', 'to', 'remember', 'most', 'around', 'here', 'have', 'ad', '##hd', ':', 'p', 'reading', 'a', 'large', 'wall', 'of', 'text', 'with', 'out', 'any', 'format', '##ting', 'to', 'indicate', 'main', 'points', 'or', 'break', 'up', 'thoughts', 'and', 'sections', 'is', 'difficult', 'and', 'many', 'will', 'simply', 'look', 'at', 'it', 'and', 'move', 'on', 'without', 'reading', '.', 'as', 'you', 'can', 'see', 'this', 'post', 'is', 'format', '##ted', 'in', 'a', 'way', 'that', 'i', 'find', 'easy', 'to', 'read', 'so', 'hopefully', 'so', 'do', 'others', '.', 'remember', 'to', 'think', 'about', 'format', '##ting', 'for', 'long', 'word', '##y', 'comments', 'as', 'well', '.', '*', '*', 'format', '##ting', 'is', 'very', 'easy', '*', '*', ':', '[', 'here', 'is', 'the', 'format', '##ting', 'help', 'page', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'help', '/', 'commenting', ')', '*', 'for', 'the', 'main', 'sections', 'i', 'used', 'h', '##5', 'which', 'is', 'a', 'header', 'style', '.', 'all', 'you', 'have', 'to', 'do', 'is', 'put', 'two', 'or', 'more', \"'\", '#', \"'\", 'at', 'the', 'beginning', 'of', 'a', 'new', 'line', 'before', 'the', 'text', 'you', 'want', 'as', 'your', 'header', ',', 'there', 'is', 'h', '##2', '-', 'h', '##6', '.', 'the', 'numbers', 'indicating', 'the', 'amount', 'of', \"'\", '#', \"'\", 'needed', '.', '*', 'the', 'horizontal', 'lines', 'are', 'done', 'by', 'using', 'at', 'least', 'three', 'as', '##ter', '##isk', '##s', \"'\", '*', \"'\", 'on', 'a', 'new', 'line', '.', '*', 'bullets', 'are', 'listed', 'above', 'the', 'comment', 'section', '.', 'it', 'is', 'done', 'by', 'going', 'down', 'to', 'a', 'new', 'line', 'and', 'starting', 'it', 'with', \"'\", '*', \"'\", 'followed', 'by', \"'\", \"'\", '(', 'a', 'blank', 'space', ')', 'then', 'your', 'bullet', 'text', '.', '*', '*', '*', '#', '#', 'thanks', 'for', 'everyone', \"'\", 's', 'time', ',', 'let', \"'\", 's', 'continue', 'to', 'grow', 'our', 'community', '.', '*', '*', '*']\n",
      "INFO:__main__:Number of tokens: 784\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['[', 'notice', ']', '2', ',', '000', '+', 'subscribers', 'hey', 'everyone', ',', 'i', 'just', 'wanted', 'to', 'make', 'this', 'quick', 'post', 'to', 'say', 'how', 'great', 'everyone', 'in', 'this', 'community', 'is', 'and', 'it', \"'\", 's', 'awesome', 'we', 'have', 'recently', 'hit', '2', ',', '000', 'subscribers', '.', 'also', 'to', 'remind', 'everyone', 'of', 'a', 'few', 'things', '.', '#', '#', '#', '#', '#', 'up', '##vot', '##ing', 'posts', '*', '*', '*', 'we', 'seem', 'to', 'have', 'a', 'disco', '##nne', '##ct', 'around', 'here', 'with', 'participating', 'via', 'comments', 'and', 'participating', 'with', 'up', '##vot', '##es', '.', 'if', 'you', 'are', 'taking', 'the', 'time', 'to', 'comment', 'on', 'a', 'post', 'up', '##vot', '##e', 'it', ',', 'unless', 'the', 'reason', 'you', 'are', 'commenting', 'is', 'to', 'tell', 'them', 'why', 'it', 'shouldn', \"'\", 't', 'be', 'posted', '.', 'if', 'a', 'post', 'is', 'worth', 'you', 'making', 'discussion', 'on', 'the', 'best', 'way', 'for', 'people', 'to', 'see', 'it', 'and', 'be', 'added', 'to', 'the', 'discussion', 'is', 'with', 'up', '##vot', '##es', 'and', 'we', 'have', 'a', 'real', 'lack', 'of', 'people', 'participating', 'via', 'the', 'up', '##vot', '##e', '.', 'also', 'the', 'down', '##vot', '##e', 'is', 'not', 'for', 'opinions', 'you', 'disagree', 'with', '.', '#', '#', '#', '#', '#', 'read', 'the', 'fa', '##q', '*', '*', '*', 'if', 'you', \"'\", 're', 'new', 'or', 'even', 'old', 'and', 'have', 'yet', 'to', 'read', 'the', 'fa', '##q', 'please', 'go', 'do', 'so', ',', 'it', 'is', 'there', 'for', 'a', 'reason', 'and', 'we', 'still', 'need', 'to', 'improve', 'it', 'over', 'time', '.', 'the', 'only', 'way', 'for', 'us', 'as', 'a', 'community', 'to', 'produce', 'the', 'best', 'fa', '##q', '/', 'help', 'page', 'we', 'can', 'is', 'for', 'people', 'within', 'the', 'community', 'to', 'actually', 'read', 'it', '.', '#', '#', '#', '#', '#', 'ir', '##c', '*', '*', '*', 'on', 'the', 'side', 'bar', 'there', 'is', 'a', 'link', 'to', 'the', '[', 'ir', '##c', 'chat', 'room', ']', '(', 'http', ':', '/', '/', 'web', '##cha', '##t', '.', 'free', '##no', '##de', '.', 'net', '/', '?', 'channels', '=', '/', 'r', '/', 'ad', '##hd', ')', '.', 'it', 'is', 'a', 'great', 'place', 'to', 'hang', 'out', 'and', 'talk', 'with', 'other', 'community', 'members', '.', 'there', 'are', 'also', 'weekly', 'meet', '##ups', ',', 'times', 'are', 'posted', 'on', 'the', 'side', '##bar', '.', '#', '#', '#', '#', '#', 'attitude', '*', '*', '*', 'this', 'is', 'very', 'important', 'for', 'any', 'online', 'forum', '.', 'you', 'can', 'not', 'figure', 'a', 'person', 'out', 'nor', 'know', 'their', 'intent', 'via', 'a', 'post', 'over', 'the', 'internet', 'so', 'please', 'be', 'nice', 'and', 'respectful', 'especially', 'to', 'those', 'attempting', 'to', 'bring', 'content', 'into', 'the', 'community', '.', 'if', 'someone', 'has', 'posted', 'an', 'article', 'it', 'means', 'they', 'found', 'it', 'interesting', 'or', 'simply', 'that', 'we', 'might', 'like', 'to', 'discuss', 'it', '.', 'it', 'does', 'not', 'mean', 'the', 'person', 'posting', 'agrees', 'with', 'the', 'opinion', 'of', 'the', 'article', ',', 'if', 'you', 'disagree', 'with', 'the', 'article', 'but', 'are', 'still', 'willing', 'to', 'post', 'and', 'discuss', 'it', 'then', 'up', '##vot', '##e', 'the', 'post', '.', 'this', 'is', 'what', 'up', '##vot', '##es', 'and', 'down', '##vot', '##es', 'are', 'for', ',', 'filtering', 'out', 'irrelevant', 'content', 'not', 'to', 'show', 'your', 'support', 'or', 'disagreement', 'so', 'please', 'keep', 'that', 'in', 'mind', 'so', 'we', 'can', 'continue', 'to', 'make', 'this', 'a', 'great', 'community', 'with', 'healthy', 'discussion', '.', '#', '#', '#', '#', '#', 'format', '##ting', '*', '*', '*', 'format', '##ting', 'when', 'posting', 'a', 'wall', 'of', 'text', 'is', 'extremely', 'vital', 'around', 'here'], ['.', 'you', 'have', 'to', 'remember', 'most', 'around', 'here', 'have', 'ad', '##hd', ':', 'p', 'reading', 'a', 'large', 'wall', 'of', 'text', 'with', 'out', 'any', 'format', '##ting', 'to', 'indicate', 'main', 'points', 'or', 'break', 'up', 'thoughts', 'and', 'sections', 'is', 'difficult', 'and', 'many', 'will', 'simply', 'look', 'at', 'it', 'and', 'move', 'on', 'without', 'reading', '.', 'as', 'you', 'can', 'see', 'this', 'post', 'is', 'format', '##ted', 'in', 'a', 'way', 'that', 'i', 'find', 'easy', 'to', 'read', 'so', 'hopefully', 'so', 'do', 'others', '.', 'remember', 'to', 'think', 'about', 'format', '##ting', 'for', 'long', 'word', '##y', 'comments', 'as', 'well', '.', '*', '*', 'format', '##ting', 'is', 'very', 'easy', '*', '*', ':', '[', 'here', 'is', 'the', 'format', '##ting', 'help', 'page', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'help', '/', 'commenting', ')', '*', 'for', 'the', 'main', 'sections', 'i', 'used', 'h', '##5', 'which', 'is', 'a', 'header', 'style', '.', 'all', 'you', 'have', 'to', 'do', 'is', 'put', 'two', 'or', 'more', \"'\", '#', \"'\", 'at', 'the', 'beginning', 'of', 'a', 'new', 'line', 'before', 'the', 'text', 'you', 'want', 'as', 'your', 'header', ',', 'there', 'is', 'h', '##2', '-', 'h', '##6', '.', 'the', 'numbers', 'indicating', 'the', 'amount', 'of', \"'\", '#', \"'\", 'needed', '.', '*', 'the', 'horizontal', 'lines', 'are', 'done', 'by', 'using', 'at', 'least', 'three', 'as', '##ter', '##isk', '##s', \"'\", '*', \"'\", 'on', 'a', 'new', 'line', '.', '*', 'bullets', 'are', 'listed', 'above', 'the', 'comment', 'section', '.', 'it', 'is', 'done', 'by', 'going', 'down', 'to', 'a', 'new', 'line', 'and', 'starting', 'it', 'with', \"'\", '*', \"'\", 'followed', 'by', \"'\", \"'\", '(', 'a', 'blank', 'space', ')', 'then', 'your', 'bullet', 'text', '.', '*', '*', '*', '#', '#', 'thanks', 'for', 'everyone', \"'\", 's', 'time', ',', 'let', \"'\", 's', 'continue', 'to', 'grow', 'our', 'community', '.', '*', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['embarrassed', 'to', 'talk', 'to', 'my', 'doctor', '.', 'has', 'anybody', 'here', 'been', 'embarrassed', 'to', 'talk', 'to', 'their', 'doctor', 'about', 'a', 'potential', 'attention', 'issue', '?', 'i', \"'\", 'm', 'just', 'really', 'confused', 'if', 'i', 'actually', 'have', 'ad', '##hd', '.', 'i', \"'\", 've', 'experimented', 'with', 'add', '##eral', '##la', 'few', 'times', 'and', 'the', 'impact', 'was', 'incredible', '.', 'i', 'participated', 'in', 'class', 'for', 'the', 'first', 'time', 'in', 'years', '.', 'i', 'actually', 'read', 'something', 'for', 'more', 'than', '5', 'minutes', 'without', 'going', 'off', 'on', 'a', 'tangent', '.', 'i', \"'\", 've', 'never', 'had', 'a', 'capacity', 'to', 'get', 'lyrics', 'for', 'songs', 'like', 'everybody', 'else', ',', 'i', 'could', 'actually', 'listen', 'to', 'music', 'and', 'pay', 'attention', 'to', 'the', 'lyrics', 'long', 'enough', 'to', 'derive', 'some', 'meaning', 'from', 'them', '!', 'i', \"'\", 'm', 'just', 'sick', 'of', 'pushing', 'myself', 'to', 'the', 'limits', 'of', 'high', 'stress', 'just', 'to', 'mo', '##tiv', '##ate', 'myself', 'to', 'get', 'things', 'done', '.', 'i', 'des', '##pis', '##e', 'whoever', 'in', '##vent', '##ef', 'tab', '##bed', 'brows', '##ing', '.', 'i', 'just', 'have', 'this', 'strange', 'feeling', 'that', 'i', \"'\", 'm', 'a', 'failure', 'if', 'i', 'accept', 'that', 'i', 'have', 'a', 'problem', '.']\n",
      "INFO:__main__:Number of tokens: 175\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['embarrassed', 'to', 'talk', 'to', 'my', 'doctor', '.', 'has', 'anybody', 'here', 'been', 'embarrassed', 'to', 'talk', 'to', 'their', 'doctor', 'about', 'a', 'potential', 'attention', 'issue', '?', 'i', \"'\", 'm', 'just', 'really', 'confused', 'if', 'i', 'actually', 'have', 'ad', '##hd', '.', 'i', \"'\", 've', 'experimented', 'with', 'add', '##eral', '##la', 'few', 'times', 'and', 'the', 'impact', 'was', 'incredible', '.', 'i', 'participated', 'in', 'class', 'for', 'the', 'first', 'time', 'in', 'years', '.', 'i', 'actually', 'read', 'something', 'for', 'more', 'than', '5', 'minutes', 'without', 'going', 'off', 'on', 'a', 'tangent', '.', 'i', \"'\", 've', 'never', 'had', 'a', 'capacity', 'to', 'get', 'lyrics', 'for', 'songs', 'like', 'everybody', 'else', ',', 'i', 'could', 'actually', 'listen', 'to', 'music', 'and', 'pay', 'attention', 'to', 'the', 'lyrics', 'long', 'enough', 'to', 'derive', 'some', 'meaning', 'from', 'them', '!', 'i', \"'\", 'm', 'just', 'sick', 'of', 'pushing', 'myself', 'to', 'the', 'limits', 'of', 'high', 'stress', 'just', 'to', 'mo', '##tiv', '##ate', 'myself', 'to', 'get', 'things', 'done', '.', 'i', 'des', '##pis', '##e', 'whoever', 'in', '##vent', '##ef', 'tab', '##bed', 'brows', '##ing', '.', 'i', 'just', 'have', 'this', 'strange', 'feeling', 'that', 'i', \"'\", 'm', 'a', 'failure', 'if', 'i', 'accept', 'that', 'i', 'have', 'a', 'problem', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 'evaluated', 'in', 'nyc', '?', 'i', \"'\", 'm', '18', 'and', 'live', 'on', 'manhattan', ',', 'and', 'i', \"'\", 'd', 'prefer', 'to', 'have', 'somewhere', 'in', 'the', 'borough', '.', 'does', 'anyone', 'have', 'recommendations', 'for', 'a', 'psychiatrist', 'who', 'can', 'evaluate', 'for', 'ad', '##hd', 'and', 'isn', \"'\", 't', 'prohibit', '##ively', 'expensive', '?', 'i', 'have', 'bc', '/', 'bs', 'insurance', 'but', 'i', 'don', \"'\", 't', 'know', 'how', 'prevalent', 'that', 'is', 'up', 'here', '.']\n",
      "INFO:__main__:Number of tokens: 66\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 'evaluated', 'in', 'nyc', '?', 'i', \"'\", 'm', '18', 'and', 'live', 'on', 'manhattan', ',', 'and', 'i', \"'\", 'd', 'prefer', 'to', 'have', 'somewhere', 'in', 'the', 'borough', '.', 'does', 'anyone', 'have', 'recommendations', 'for', 'a', 'psychiatrist', 'who', 'can', 'evaluate', 'for', 'ad', '##hd', 'and', 'isn', \"'\", 't', 'prohibit', '##ively', 'expensive', '?', 'i', 'have', 'bc', '/', 'bs', 'insurance', 'but', 'i', 'don', \"'\", 't', 'know', 'how', 'prevalent', 'that', 'is', 'up', 'here', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['partly', 'broken', ':', 'which', 'of', 'your', '\"', 'executive', 'functions', '\"', 'are', 'not', 'working', 'well', '?', 'ad', '##hd', 'is', 'a', 'disorder', 'of', 'the', 'executive', 'functions', ',', 'i', \"'\", 've', 'read', '.', 'more', 'details', 'on', 'executive', 'functions', 'can', 'be', 'had', 'from', 'here', ':', 'http', ':', '/', '/', 'www', '.', 'ld', '##on', '##line', '.', 'org', '/', 'article', '/', '291', '##22', '/', 'as', 'per', 'the', '(', 'really', 'good', 'read', ')', 'article', 'above', ',', 'the', 'executive', 'functions', 'are', ':', '1', '.', 'inhibition', '-', 'the', 'ability', 'to', 'stop', 'one', \"'\", 's', 'own', 'behavior', 'at', 'the', 'appropriate', 'time', ',', 'including', 'stopping', 'actions', 'and', 'thoughts', '.', '2', '.', 'shift', '-', 'the', 'ability', 'to', 'move', 'freely', 'from', 'one', 'situation', 'to', 'another', 'and', 'to', 'think', 'flex', '##ibly', 'in', 'order', 'to', 'respond', 'appropriately', 'to', 'the', 'situation', '.', '3', '.', 'emotional', 'control', '-', 'the', 'ability', 'to', 'mod', '##ulate', 'emotional', 'responses', 'by', 'bringing', 'rational', 'thought', 'to', 'bear', 'on', 'feelings', '.', '4', '.', 'initiation', '-', 'the', 'ability', 'to', 'begin', 'a', 'task', 'or', 'activity', 'and', 'to', 'independently', 'generate', 'ideas', ',', 'responses', ',', 'or', 'problem', '-', 'solving', 'strategies', '.', '5', '.', 'working', 'memory', '-', 'the', 'capacity', 'to', 'hold', 'information', 'in', 'mind', 'for', 'the', 'purpose', 'of', 'completing', 'a', 'task', '.', '6', '.', 'planning', '/', 'organization', '-', 'the', 'ability', 'to', 'manage', 'current', 'and', 'future', '-', 'oriented', 'task', 'demands', '.', '7', '.', 'organization', 'of', 'materials', '-', 'the', 'ability', 'to', 'impose', 'order', 'on', 'work', ',', 'play', ',', 'and', 'storage', 'spaces', '.', '8', '.', 'self', '-', 'monitoring', '-', 'the', 'ability', 'to', 'monitor', 'one', \"'\", 's', 'own', 'performance', 'and', 'to', 'measure', 'it', 'against', 'some', 'standard', 'of', 'what', 'is', 'needed', 'or', 'expected', '.', 'i', 'myself', 'feel', 'that', 'i', 'have', 'problems', 'with', '5', 'of', 'the', '8', 'above', ':', 'inhibition', ',', 'shift', ',', 'initiation', ',', 'planning', '/', 'organization', 'and', 'organization', 'of', 'materials', '.', 'i', '.', 'e', '.', 'points', '1', ',', '2', ',', '4', ',', '6', ',', '7', '.', 'what', 'about', 'you', '?', 'which', 'of', 'the', 'points', 'do', 'you', 'have', 'a', 'problem', 'with', '?', '(', 'please', 'list', 'the', 'numbers', ',', 'or', 'type', 'out', 'the', 'names', ')', 'and', 'i', 'think', 'that', 'if', 'rita', '##lin', 'or', 'add', '##eral', '##l', 'are', 'general', 'purpose', 'st', '##im', '##ula', '##nts', ',', 'then', 'they', 'would', 'be', 'boost', '##ing', 'all', 'of', 'the', 'executive', 'functions', ',', 'rather', 'than', 'only', 'those', 'which', 'need', 'the', 'boost', '.', 'any', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 373\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['partly', 'broken', ':', 'which', 'of', 'your', '\"', 'executive', 'functions', '\"', 'are', 'not', 'working', 'well', '?', 'ad', '##hd', 'is', 'a', 'disorder', 'of', 'the', 'executive', 'functions', ',', 'i', \"'\", 've', 'read', '.', 'more', 'details', 'on', 'executive', 'functions', 'can', 'be', 'had', 'from', 'here', ':', 'http', ':', '/', '/', 'www', '.', 'ld', '##on', '##line', '.', 'org', '/', 'article', '/', '291', '##22', '/', 'as', 'per', 'the', '(', 'really', 'good', 'read', ')', 'article', 'above', ',', 'the', 'executive', 'functions', 'are', ':', '1', '.', 'inhibition', '-', 'the', 'ability', 'to', 'stop', 'one', \"'\", 's', 'own', 'behavior', 'at', 'the', 'appropriate', 'time', ',', 'including', 'stopping', 'actions', 'and', 'thoughts', '.', '2', '.', 'shift', '-', 'the', 'ability', 'to', 'move', 'freely', 'from', 'one', 'situation', 'to', 'another', 'and', 'to', 'think', 'flex', '##ibly', 'in', 'order', 'to', 'respond', 'appropriately', 'to', 'the', 'situation', '.', '3', '.', 'emotional', 'control', '-', 'the', 'ability', 'to', 'mod', '##ulate', 'emotional', 'responses', 'by', 'bringing', 'rational', 'thought', 'to', 'bear', 'on', 'feelings', '.', '4', '.', 'initiation', '-', 'the', 'ability', 'to', 'begin', 'a', 'task', 'or', 'activity', 'and', 'to', 'independently', 'generate', 'ideas', ',', 'responses', ',', 'or', 'problem', '-', 'solving', 'strategies', '.', '5', '.', 'working', 'memory', '-', 'the', 'capacity', 'to', 'hold', 'information', 'in', 'mind', 'for', 'the', 'purpose', 'of', 'completing', 'a', 'task', '.', '6', '.', 'planning', '/', 'organization', '-', 'the', 'ability', 'to', 'manage', 'current', 'and', 'future', '-', 'oriented', 'task', 'demands', '.', '7', '.', 'organization', 'of', 'materials', '-', 'the', 'ability', 'to', 'impose', 'order', 'on', 'work', ',', 'play', ',', 'and', 'storage', 'spaces', '.', '8', '.', 'self', '-', 'monitoring', '-', 'the', 'ability', 'to', 'monitor', 'one', \"'\", 's', 'own', 'performance', 'and', 'to', 'measure', 'it', 'against', 'some', 'standard', 'of', 'what', 'is', 'needed', 'or', 'expected', '.', 'i', 'myself', 'feel', 'that', 'i', 'have', 'problems', 'with', '5', 'of', 'the', '8', 'above', ':', 'inhibition', ',', 'shift', ',', 'initiation', ',', 'planning', '/', 'organization', 'and', 'organization', 'of', 'materials', '.', 'i', '.', 'e', '.', 'points', '1', ',', '2', ',', '4', ',', '6', ',', '7', '.', 'what', 'about', 'you', '?', 'which', 'of', 'the', 'points', 'do', 'you', 'have', 'a', 'problem', 'with', '?', '(', 'please', 'list', 'the', 'numbers', ',', 'or', 'type', 'out', 'the', 'names', ')', 'and', 'i', 'think', 'that', 'if', 'rita', '##lin', 'or', 'add', '##eral', '##l', 'are', 'general', 'purpose', 'st', '##im', '##ula', '##nts', ',', 'then', 'they', 'would', 'be', 'boost', '##ing', 'all', 'of', 'the', 'executive', 'functions', ',', 'rather', 'than', 'only', 'those', 'which', 'need', 'the', 'boost', '.', 'any', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '=', 'faster', 'reading', '?', 'i', 'have', 'a', 'question', 'for', 'all', 'of', 'you', '.', 'i', 'can', 'read', 'approximately', '750', 'words', 'per', 'minute', ',', 'wi', '##ch', 'is', 'quite', 'fast', '.', 'can', 'some', 'of', 'you', 'do', 'that', 'too', 'and', 'is', 'it', 'somehow', 'relate', '##able', 'to', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 47\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '=', 'faster', 'reading', '?', 'i', 'have', 'a', 'question', 'for', 'all', 'of', 'you', '.', 'i', 'can', 'read', 'approximately', '750', 'words', 'per', 'minute', ',', 'wi', '##ch', 'is', 'quite', 'fast', '.', 'can', 'some', 'of', 'you', 'do', 'that', 'too', 'and', 'is', 'it', 'somehow', 'relate', '##able', 'to', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'willing', 'have', 'your', 'doctors', 'been', 'to', 'accept', 'your', 'lay', '##man', \"'\", 's', 'prescription', 'recommendations', '?']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'willing', 'have', 'your', 'doctors', 'been', 'to', 'accept', 'your', 'lay', '##man', \"'\", 's', 'prescription', 'recommendations', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tomorrow', 'i', 'am', 'going', 'in', 'for', 'adult', 'ad', '##hd', 'testing', ',', 'what', 'should', 'i', 'expect', '.']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tomorrow', 'i', 'am', 'going', 'in', 'for', 'adult', 'ad', '##hd', 'testing', ',', 'what', 'should', 'i', 'expect', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', \"'\", 'mid', '-', 'life', \"'\", 'crisis', 'for', 'the', 'ad', '##hd', '-', 'adult']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', \"'\", 'mid', '-', 'life', \"'\", 'crisis', 'for', 'the', 'ad', '##hd', '-', 'adult']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'appointment', 'tomorrow', 'i', \"'\", 'm', 'sure', 'this', 'is', 'a', 'rep', '##ost', 'of', 'a', 'rep', '##ost', ',', 'but', 'i', 'have', 'my', 'first', 'appointment', 'tomorrow', 'to', 'get', 'tested', ',', 'what', 'should', 'i', 'expect', '?']\n",
      "INFO:__main__:Number of tokens: 33\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', 'appointment', 'tomorrow', 'i', \"'\", 'm', 'sure', 'this', 'is', 'a', 'rep', '##ost', 'of', 'a', 'rep', '##ost', ',', 'but', 'i', 'have', 'my', 'first', 'appointment', 'tomorrow', 'to', 'get', 'tested', ',', 'what', 'should', 'i', 'expect', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['\"', 'the', 'power', 'of', 'determination', '\"', '-', 'this', 'video', 'serves', 'as', 'a', 'great', 'inspiration']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['\"', 'the', 'power', 'of', 'determination', '\"', '-', 'this', 'video', 'serves', 'as', 'a', 'great', 'inspiration']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['/', 'r', '/', 'ad', '##hd', 'demographics', '[', 'everyone', 'please', 'participate', ']']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['/', 'r', '/', 'ad', '##hd', 'demographics', '[', 'everyone', 'please', 'participate', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['questions', 'about', 'switching', 'medications', 'and', 'prescription', 'prices', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['questions', 'about', 'switching', 'medications', 'and', 'prescription', 'prices', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['audio', 'books', 'vs', 'regular', 'books', 'i', 'suck', 'at', 'reading', 'books', '.', 'this', 'is', 'probably', 'pretty', 'common', 'for', 'people', 'here', '?', '.', '.', 'often', 'my', 'eyes', 'keep', 'moving', 'down', 'the', 'page', ',', 'but', 'it', \"'\", 's', 'like', 'i', \"'\", 'm', 'not', '\"', 'listening', '\"', 'to', 'myself', 'reading', 'in', 'my', 'head', 'or', 'something', '.', 'i', 'start', 'thinking', 'about', 'other', 'things', '.', 'i', 'have', 'to', 'keep', 'going', 'back', 'and', 're', '-', 'reading', 'many', 'of', 'the', 'paragraph', '##s', '(', 'maybe', 'most', 'of', 'them', '?', ')', '.', 'this', 'means', 'that', 'it', 'takes', 'me', 'a', 'very', 'long', 'time', 'to', 'complete', 'a', 'book', '.', 'usually', 'a', 'year', 'or', 'two', ',', 'also', 'due', 'to', 'the', 'fact', 'that', 'i', 'rarely', 'read', 'books', 'at', 'home', ',', 'usually', 'only', 'when', 'i', \"'\", 'm', 'on', 'a', 'bus', 'or', 'in', 'a', 'waiting', 'room', 'etc', '.', 'i', \"'\", 've', 'tried', 'listening', 'to', 'an', 'audio', 'history', 'documentary', 'and', 'some', 'podcast', '##s', 'on', 'subjects', 'that', 'interest', 'me', ',', 'and', 'i', 'think', 'for', 'me', 'it', 'is', 'easier', 'to', 'concentrate', 'on', 'listening', 'rather', 'than', 'reading', '.', 'i', 'sp', '##ose', 'after', 'all', ',', 'this', 'is', 'how', 'we', \"'\", 've', 'been', 'communicating', 'through', 'out', 'the', 'ages', 'well', 'before', 'the', 'printed', 'word', ',', 'so', 'from', 'a', 'genetic', 'point', 'of', 'view', 'it', 'makes', 'sense', 'for', 'it', 'to', 'be', 'easier', 'listening', 'to', 'someone', 'speaking', 'compared', 'to', 'translating', 'letters', 'on', 'a', 'page', 'into', '\"', 'talking', 'in', 'your', 'head', '\"', '.', 'one', 'problem', 'however', ',', 'is', 'that', 'with', 'audio', 'books', 'it', 'is', 'much', 'harder', 'to', 'go', 'back', '(', 're', '##wind', ')', 'after', 'laps', '##es', 'in', 'concentration', '.', 'it', 'is', 'much', 'easier', 'to', 'visually', 'scan', 'text', 'and', 'find', 'the', 'last', 'spot', 'you', 'remember', 'compared', 'to', 'guessing', 'how', 'far', 'to', 're', '##wind', 'an', 'audio', 'recording', 'on', 'an', 'ipod', '.', 'perhaps', 'splitting', 'up', 'audio', 'books', 'into', 'many', 'small', 'clips', '(', 'under', '5', 'min', '##s', 'each', ')', 'might', 'be', 'a', 'good', 'technique', 'to', 'overcome', 'this', '?', 'so', 'over', 'all', ',', 'is', 'it', 'still', 'easier', 'or', 'harder', 'to', 'listen', 'to', 'an', 'audio', 'book', '?', 'i', \"'\", 'm', 'not', 'sure', '.', '1', '.', 'what', \"'\", 's', 'your', 'opinion', '?', '2', '.', 'got', 'any', 'tips', 'for', 'concentrating', 'on', 'either', 'reading', 'or', 'audio', 'books', '?', '3', '.', 'got', 'any', 'good', 'sources', 'to', 'audio', 'books', 'you', 'can', 'link', 'to', '?']\n",
      "INFO:__main__:Number of tokens: 366\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['audio', 'books', 'vs', 'regular', 'books', 'i', 'suck', 'at', 'reading', 'books', '.', 'this', 'is', 'probably', 'pretty', 'common', 'for', 'people', 'here', '?', '.', '.', 'often', 'my', 'eyes', 'keep', 'moving', 'down', 'the', 'page', ',', 'but', 'it', \"'\", 's', 'like', 'i', \"'\", 'm', 'not', '\"', 'listening', '\"', 'to', 'myself', 'reading', 'in', 'my', 'head', 'or', 'something', '.', 'i', 'start', 'thinking', 'about', 'other', 'things', '.', 'i', 'have', 'to', 'keep', 'going', 'back', 'and', 're', '-', 'reading', 'many', 'of', 'the', 'paragraph', '##s', '(', 'maybe', 'most', 'of', 'them', '?', ')', '.', 'this', 'means', 'that', 'it', 'takes', 'me', 'a', 'very', 'long', 'time', 'to', 'complete', 'a', 'book', '.', 'usually', 'a', 'year', 'or', 'two', ',', 'also', 'due', 'to', 'the', 'fact', 'that', 'i', 'rarely', 'read', 'books', 'at', 'home', ',', 'usually', 'only', 'when', 'i', \"'\", 'm', 'on', 'a', 'bus', 'or', 'in', 'a', 'waiting', 'room', 'etc', '.', 'i', \"'\", 've', 'tried', 'listening', 'to', 'an', 'audio', 'history', 'documentary', 'and', 'some', 'podcast', '##s', 'on', 'subjects', 'that', 'interest', 'me', ',', 'and', 'i', 'think', 'for', 'me', 'it', 'is', 'easier', 'to', 'concentrate', 'on', 'listening', 'rather', 'than', 'reading', '.', 'i', 'sp', '##ose', 'after', 'all', ',', 'this', 'is', 'how', 'we', \"'\", 've', 'been', 'communicating', 'through', 'out', 'the', 'ages', 'well', 'before', 'the', 'printed', 'word', ',', 'so', 'from', 'a', 'genetic', 'point', 'of', 'view', 'it', 'makes', 'sense', 'for', 'it', 'to', 'be', 'easier', 'listening', 'to', 'someone', 'speaking', 'compared', 'to', 'translating', 'letters', 'on', 'a', 'page', 'into', '\"', 'talking', 'in', 'your', 'head', '\"', '.', 'one', 'problem', 'however', ',', 'is', 'that', 'with', 'audio', 'books', 'it', 'is', 'much', 'harder', 'to', 'go', 'back', '(', 're', '##wind', ')', 'after', 'laps', '##es', 'in', 'concentration', '.', 'it', 'is', 'much', 'easier', 'to', 'visually', 'scan', 'text', 'and', 'find', 'the', 'last', 'spot', 'you', 'remember', 'compared', 'to', 'guessing', 'how', 'far', 'to', 're', '##wind', 'an', 'audio', 'recording', 'on', 'an', 'ipod', '.', 'perhaps', 'splitting', 'up', 'audio', 'books', 'into', 'many', 'small', 'clips', '(', 'under', '5', 'min', '##s', 'each', ')', 'might', 'be', 'a', 'good', 'technique', 'to', 'overcome', 'this', '?', 'so', 'over', 'all', ',', 'is', 'it', 'still', 'easier', 'or', 'harder', 'to', 'listen', 'to', 'an', 'audio', 'book', '?', 'i', \"'\", 'm', 'not', 'sure', '.', '1', '.', 'what', \"'\", 's', 'your', 'opinion', '?', '2', '.', 'got', 'any', 'tips', 'for', 'concentrating', 'on', 'either', 'reading', 'or', 'audio', 'books', '?', '3', '.', 'got', 'any', 'good', 'sources', 'to', 'audio', 'books', 'you', 'can', 'link', 'to', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['pretty', 'much', 'how', 'i', 'feel', 'when', 'i', 'have', 'a', 'project', 'to', 'do', '(', 'from', '/', 'r', '/', 'design', ')']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['pretty', 'much', 'how', 'i', 'feel', 'when', 'i', 'have', 'a', 'project', 'to', 'do', '(', 'from', '/', 'r', '/', 'design', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['this', 'is', 'in', 'my', 'school', '.', 'is', 'this', 'true', '?', 'how', 'could', 'they', 'have', 'known', 'some', 'of', 'these', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['this', 'is', 'in', 'my', 'school', '.', 'is', 'this', 'true', '?', 'how', 'could', 'they', 'have', 'known', 'some', 'of', 'these', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ny', 'times', 'article', 'on', 'driving', '&', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ny', 'times', 'article', 'on', 'driving', '&', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', '.', 'd', '.', 'h', '.', 'd', '.', 'challenges', 'those', 'seeking', 'a', 'driver', '’', 's', 'license', '-', 'ny', '##time', '##s', '.', 'com']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', '.', 'd', '.', 'h', '.', 'd', '.', 'challenges', 'those', 'seeking', 'a', 'driver', '’', 's', 'license', '-', 'ny', '##time', '##s', '.', 'com']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'found', 'this', 'sub', '-', 'red', '##dit', '.', '.', '.', 'any', 'other', 'en', '##ts', 'with', 'ad', '##hd', 'here', '?', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'ad', '##hd', 'since', 'third', 'grade', ',', 'about', '13', 'years', 'ago', '.', 'i', \"'\", 've', 'been', 'med', '##icated', 'on', 'rita', '##lin', 'and', 'concert', '##a', 'until', 'about', 'two', 'years', 'ago', 'when', 'i', 'stopped', 'taking', 'my', 'medication', '.', 'when', 'i', 'started', 'to', 'slow', 'my', 'medicinal', 'intake', ',', 'i', 'also', 'increased', 'by', 'marijuana', 'intake', '.', 'i', 'used', 'to', 'be', 'an', 'o', '##cca', '##ssion', '##al', 'smoke', '##r', ',', 'but', 'around', 'my', 'sophomore', 'year', 'i', 'started', 'smoking', 'regularly', '.', 'now', ',', 'i', 'am', 'medication', 'free', ',', 'operating', 'at', 'peak', 'potential', 'with', 'no', 'side', 'effects', '.', 'all', 'i', 'do', 'is', 'smoke', 'trees', '!', 'anybody', 'with', 'a', 'similar', 'experience', 'or', 'anybody', 'looking', 'to', 'weigh', 'in', '?']\n",
      "INFO:__main__:Number of tokens: 133\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'found', 'this', 'sub', '-', 'red', '##dit', '.', '.', '.', 'any', 'other', 'en', '##ts', 'with', 'ad', '##hd', 'here', '?', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'ad', '##hd', 'since', 'third', 'grade', ',', 'about', '13', 'years', 'ago', '.', 'i', \"'\", 've', 'been', 'med', '##icated', 'on', 'rita', '##lin', 'and', 'concert', '##a', 'until', 'about', 'two', 'years', 'ago', 'when', 'i', 'stopped', 'taking', 'my', 'medication', '.', 'when', 'i', 'started', 'to', 'slow', 'my', 'medicinal', 'intake', ',', 'i', 'also', 'increased', 'by', 'marijuana', 'intake', '.', 'i', 'used', 'to', 'be', 'an', 'o', '##cca', '##ssion', '##al', 'smoke', '##r', ',', 'but', 'around', 'my', 'sophomore', 'year', 'i', 'started', 'smoking', 'regularly', '.', 'now', ',', 'i', 'am', 'medication', 'free', ',', 'operating', 'at', 'peak', 'potential', 'with', 'no', 'side', 'effects', '.', 'all', 'i', 'do', 'is', 'smoke', 'trees', '!', 'anybody', 'with', 'a', 'similar', 'experience', 'or', 'anybody', 'looking', 'to', 'weigh', 'in', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'deal', 'with', 'the', 'lack', 'of', 'motivation', '?', 'my', '(', 'ex', ')', 'girlfriend', 'broke', 'up', 'with', 'me', 'recently', 'and', 'i', 'really', 'want', 'to', 'use', 'this', 'time', 'to', 'turn', 'my', 'life', 'around', '.', 'i', 'haven', \"'\", 't', 'been', 'officially', 'diagnosed', 'with', 'ad', '##hd', 'but', 'i', \"'\", 'm', 'certain', 'i', 'have', 'it', '.', 'my', 'mother', 'and', 'two', 'brothers', 'have', 'been', 'diagnosed', 'and', 'i', 'share', 'a', 'lot', 'of', 'similar', 'symptoms', ',', 'and', 'symptoms', '(', 'that', 'i', \"'\", 've', 'discovered', 'through', 'my', 'research', ')', 'that', 'suggest', 'i', 'have', 'it', '.', 'i', \"'\", 'm', ',', 'if', 'anything', ',', 'really', 'nervous', 'to', 'actually', 'make', 'that', 'appointment', 'with', 'my', 'gp', '.', 'what', 'do', 'you', 'do', 'to', 'help', 'you', 'feel', 'motivated', '?', 'i', 'once', 'was', 'an', 'incredibly', 'fit', 'rower', 'and', 'rugby', 'player', ',', 'but', 'in', 'the', 'last', 'year', 'i', \"'\", 've', 'almost', 'completely', 'lost', 'my', 'way', '.', 'i', 'haven', \"'\", 't', 'been', 'for', 'a', 'run', 'in', 'months', 'and', 'my', 'diet', 'is', 'horrible', '.', 'i', 'started', 'a', 'new', 'job', 'and', 'i', 'absolutely', 'loved', 'it', 'in', 'it', \"'\", 's', 'early', 'days', ',', 'but', 'more', 'and', 'more', 'i', 'don', \"'\", 't', 'want', 'to', 'come', 'in', '.', 'if', 'i', 'wasn', \"'\", 't', 'in', 'my', 'supervisor', 'position', ',', 'and', 'didn', \"'\", 't', 'have', 'as', 'many', 'people', 'relying', 'on', 'me', 'to', 'be', 'here', 'i', 'wouldn', \"'\", 't', 'get', 'out', 'of', 'bed', 'in', 'the', 'morning', '.', 'i', 'really', 'want', 'to', 'be', 'able', 'to', 'do', 'all', 'of', 'these', 'things', ',', 'and', 'i', 'know', 'that', 'it', 'should', 'be', 'easier', '.', 'i', 'want', 'to', 'get', 'fit', 'again', '.', 'i', 'want', 'to', 'apply', 'for', 'a', 'more', 'taxi', '##ng', 'job', '.', 'i', 'want', 'to', 'rebuild', 'all', 'of', 'the', 'relationships', 'i', 'once', 'had', 'in', 'my', 'life', '.', 'i', 'just', 'don', \"'\", 't', 'know', 'how', '.', 'please', 'help', 'red', '##dit', '.', 'edit', ':', 'i', \"'\", 'd', 'like', 'to', 'add', ',', 'these', 'examples', 'are', 'just', 'the', 'tip', 'of', 'the', 'ice', '##berg', '.', 'my', 'lack', 'of', 'motivation', 'was', 'the', 'reason', 'i', 'failed', 'school', '.', 'i', \"'\", 'm', 'an', 'intelligent', 'person', ',', 'i', 'could', 'just', 'never', 'apply', 'myself', 'to', 'my', 'work', '.', 'it', \"'\", 's', 'the', 'reason', 'i', 'dropped', 'out', 'of', 'un', '##i', 'too', '.']\n",
      "INFO:__main__:Number of tokens: 352\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'deal', 'with', 'the', 'lack', 'of', 'motivation', '?', 'my', '(', 'ex', ')', 'girlfriend', 'broke', 'up', 'with', 'me', 'recently', 'and', 'i', 'really', 'want', 'to', 'use', 'this', 'time', 'to', 'turn', 'my', 'life', 'around', '.', 'i', 'haven', \"'\", 't', 'been', 'officially', 'diagnosed', 'with', 'ad', '##hd', 'but', 'i', \"'\", 'm', 'certain', 'i', 'have', 'it', '.', 'my', 'mother', 'and', 'two', 'brothers', 'have', 'been', 'diagnosed', 'and', 'i', 'share', 'a', 'lot', 'of', 'similar', 'symptoms', ',', 'and', 'symptoms', '(', 'that', 'i', \"'\", 've', 'discovered', 'through', 'my', 'research', ')', 'that', 'suggest', 'i', 'have', 'it', '.', 'i', \"'\", 'm', ',', 'if', 'anything', ',', 'really', 'nervous', 'to', 'actually', 'make', 'that', 'appointment', 'with', 'my', 'gp', '.', 'what', 'do', 'you', 'do', 'to', 'help', 'you', 'feel', 'motivated', '?', 'i', 'once', 'was', 'an', 'incredibly', 'fit', 'rower', 'and', 'rugby', 'player', ',', 'but', 'in', 'the', 'last', 'year', 'i', \"'\", 've', 'almost', 'completely', 'lost', 'my', 'way', '.', 'i', 'haven', \"'\", 't', 'been', 'for', 'a', 'run', 'in', 'months', 'and', 'my', 'diet', 'is', 'horrible', '.', 'i', 'started', 'a', 'new', 'job', 'and', 'i', 'absolutely', 'loved', 'it', 'in', 'it', \"'\", 's', 'early', 'days', ',', 'but', 'more', 'and', 'more', 'i', 'don', \"'\", 't', 'want', 'to', 'come', 'in', '.', 'if', 'i', 'wasn', \"'\", 't', 'in', 'my', 'supervisor', 'position', ',', 'and', 'didn', \"'\", 't', 'have', 'as', 'many', 'people', 'relying', 'on', 'me', 'to', 'be', 'here', 'i', 'wouldn', \"'\", 't', 'get', 'out', 'of', 'bed', 'in', 'the', 'morning', '.', 'i', 'really', 'want', 'to', 'be', 'able', 'to', 'do', 'all', 'of', 'these', 'things', ',', 'and', 'i', 'know', 'that', 'it', 'should', 'be', 'easier', '.', 'i', 'want', 'to', 'get', 'fit', 'again', '.', 'i', 'want', 'to', 'apply', 'for', 'a', 'more', 'taxi', '##ng', 'job', '.', 'i', 'want', 'to', 'rebuild', 'all', 'of', 'the', 'relationships', 'i', 'once', 'had', 'in', 'my', 'life', '.', 'i', 'just', 'don', \"'\", 't', 'know', 'how', '.', 'please', 'help', 'red', '##dit', '.', 'edit', ':', 'i', \"'\", 'd', 'like', 'to', 'add', ',', 'these', 'examples', 'are', 'just', 'the', 'tip', 'of', 'the', 'ice', '##berg', '.', 'my', 'lack', 'of', 'motivation', 'was', 'the', 'reason', 'i', 'failed', 'school', '.', 'i', \"'\", 'm', 'an', 'intelligent', 'person', ',', 'i', 'could', 'just', 'never', 'apply', 'myself', 'to', 'my', 'work', '.', 'it', \"'\", 's', 'the', 'reason', 'i', 'dropped', 'out', 'of', 'un', '##i', 'too', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['v', '##y', '##van', '##se', 'side', 'effect', ':', 'my', 'appetite', 'is', 'severely', 'reduced', 'to', 'the', 'point', 'where', 'eating', 'now', 'feels', 'like', 'a', 'cho', '##re', '.', 'any', 'tips', '?', 'hey', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', ',', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '(', 'ina', '##tten', '##tive', 'type', ')', 'about', '2', 'weeks', 'ago', 'and', 'was', 'prescribed', 'v', '##y', '##van', '##se', '.', 'psychological', '##ly', ',', 'it', \"'\", 's', 'worked', 'wonders', ';', 'i', \"'\", 'm', 'more', 'able', 'to', 'remember', 'and', 'complete', 'tasks', ',', 'focus', 'more', 'in', 'class', 'and', 'in', 'conversations', ',', 'and', 'just', 'generally', 'feel', 'less', 'stressed', 'out', 'and', 'more', 'in', 'control', 'of', 'my', 'life', '.', 'physically', ',', 'the', 'only', 'negative', 'effects', 'i', \"'\", 've', 'experienced', 'are', 'dry', 'mouth', 'and', 'loss', 'of', 'appetite', '.', 'the', 'dry', 'mouth', ',', 'while', 'not', 'pleasant', ',', 'is', 'something', 'i', 'can', 'deal', 'with', ',', 'but', 'the', 'appetite', 'loss', 'is', 'starting', 'to', 'become', 'a', 'problem', '.', 'it', \"'\", 's', 'not', 'that', 'i', 'don', \"'\", 't', 'feel', '*', 'hungry', '*', '-', '-', 'i', 'can', 'feel', 'my', 'stomach', 'growling', 'and', 'the', 'general', 'discomfort', 'associated', 'with', 'not', 'eating', '-', '-', 'but', 'even', 'when', 'i', 'recognize', 'this', ',', 'most', 'food', 'is', 'simply', 'not', 'app', '##eti', '##zing', 'anymore', 'and', 'finishing', 'a', 'healthy', 'meal', 'becomes', 'this', 'monumental', 'task', '.', 'perhaps', 'to', 'compensate', 'for', 'my', 'reduced', 'cal', '##ori', '##e', 'intake', ',', 'i', \"'\", 've', 'had', 'intense', 'craving', '##s', 'for', 'cal', '##oric', '##ally', '-', 'dense', 'foods', 'such', 'as', 'french', 'fries', ',', 'spring', 'rolls', 'and', 'ice', 'cream', ',', 'which', 'i', 'usually', 'ind', '##ul', '##ge', '.', 'i', 'realize', 'this', 'is', 'not', 'healthy', 'at', 'all', ',', 'but', 'for', 'the', 'short', '-', 'term', ',', 'it', 'feels', 'better', 'than', 'not', 'eating', 'anything', '.', 'prior', 'to', 'starting', 'the', 'medication', ',', 'i', 'had', 'an', 'ins', '##ati', '##able', 'appetite', ',', 'perhaps', 'even', 'approaching', 'the', 'point', 'of', '*', 'over', '*', 'eating', ',', 'though', 'i', \"'\", 've', 'never', 'had', 'any', 'issues', 'with', 'my', 'weight', '.', 'i', 'actually', 'think', 'the', 'over', '-', 'eating', 'may', 'have', 'been', 'a', 'manifestation', 'of', 'the', 'add', '(', 'i', 'would', 'snack', 'all', 'day', 'out', 'of', 'boredom', 'more', 'than', 'hunger', ')', '.', 'anyway', ',', 'while', 'i', 'could', 'deal', 'with', 'a', 'slight', 'appetite', 'reduction', ',', 'what', 'i', \"'\", 'm', 'experiencing', 'now', 'is', 'problematic', 'and', 'i', \"'\", 've', 'been', 'getting', 'headache', '##s', 'after', 'un', '##int', '##ent', '##ional', '##ly', 'skipping', 'meals', 'for', 'hours', '.', 'i', \"'\", 'd', 'also', 'like', 'to', 'stop', 'relying', 'on', 'junk', 'food', 'to', 'fill', 'me', 'up', ',', 'but', 'nothing', 'else', 'is', 'as', 'app', '##eti', '##zing', 'or', 'filling', '.', 'i', 'really', 'do', 'like', 'the', 'psychological', 'benefits', 'of', 'v', '##y', '##van', '##se', 'though', 'and', 'it', 'would', 'be', 'great', 'if', 'i', 'could', 'fix', 'this', 'problem', 'without', 'having', 'to', 'stop', 'taking', 'it', '.', 'so', ',', 'has', 'anyone', 'else', 'experienced', 'this', '?', 'if', 'so', ',', 'any', 'advice', 'on', 'how', 'to', 'deal', '?', 'i', 'am', 'going', 'to', 'discuss', 'this', 'with', 'my', 'doctor', 'of', 'course', ',', 'but', 'my', 'appointment', 'isn', \"'\", 't', 'for', 'another', 'couple', 'of', 'weeks', 'so', 'i', 'at', 'least', 'need', 'a', 'temporary', 'fix', 'in', 'the', 'meantime', '.', '*', '*', 'edit', '*', '*', ':', 'thanks', 'for', 'the', 'advice', ',', 'guys', '!', 'this', 'is', 'very', 'helpful', '!']\n",
      "INFO:__main__:Number of tokens: 512\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['v', '##y', '##van', '##se', 'side', 'effect', ':', 'my', 'appetite', 'is', 'severely', 'reduced', 'to', 'the', 'point', 'where', 'eating', 'now', 'feels', 'like', 'a', 'cho', '##re', '.', 'any', 'tips', '?', 'hey', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', ',', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '(', 'ina', '##tten', '##tive', 'type', ')', 'about', '2', 'weeks', 'ago', 'and', 'was', 'prescribed', 'v', '##y', '##van', '##se', '.', 'psychological', '##ly', ',', 'it', \"'\", 's', 'worked', 'wonders', ';', 'i', \"'\", 'm', 'more', 'able', 'to', 'remember', 'and', 'complete', 'tasks', ',', 'focus', 'more', 'in', 'class', 'and', 'in', 'conversations', ',', 'and', 'just', 'generally', 'feel', 'less', 'stressed', 'out', 'and', 'more', 'in', 'control', 'of', 'my', 'life', '.', 'physically', ',', 'the', 'only', 'negative', 'effects', 'i', \"'\", 've', 'experienced', 'are', 'dry', 'mouth', 'and', 'loss', 'of', 'appetite', '.', 'the', 'dry', 'mouth', ',', 'while', 'not', 'pleasant', ',', 'is', 'something', 'i', 'can', 'deal', 'with', ',', 'but', 'the', 'appetite', 'loss', 'is', 'starting', 'to', 'become', 'a', 'problem', '.', 'it', \"'\", 's', 'not', 'that', 'i', 'don', \"'\", 't', 'feel', '*', 'hungry', '*', '-', '-', 'i', 'can', 'feel', 'my', 'stomach', 'growling', 'and', 'the', 'general', 'discomfort', 'associated', 'with', 'not', 'eating', '-', '-', 'but', 'even', 'when', 'i', 'recognize', 'this', ',', 'most', 'food', 'is', 'simply', 'not', 'app', '##eti', '##zing', 'anymore', 'and', 'finishing', 'a', 'healthy', 'meal', 'becomes', 'this', 'monumental', 'task', '.', 'perhaps', 'to', 'compensate', 'for', 'my', 'reduced', 'cal', '##ori', '##e', 'intake', ',', 'i', \"'\", 've', 'had', 'intense', 'craving', '##s', 'for', 'cal', '##oric', '##ally', '-', 'dense', 'foods', 'such', 'as', 'french', 'fries', ',', 'spring', 'rolls', 'and', 'ice', 'cream', ',', 'which', 'i', 'usually', 'ind', '##ul', '##ge', '.', 'i', 'realize', 'this', 'is', 'not', 'healthy', 'at', 'all', ',', 'but', 'for', 'the', 'short', '-', 'term', ',', 'it', 'feels', 'better', 'than', 'not', 'eating', 'anything', '.', 'prior', 'to', 'starting', 'the', 'medication', ',', 'i', 'had', 'an', 'ins', '##ati', '##able', 'appetite', ',', 'perhaps', 'even', 'approaching', 'the', 'point', 'of', '*', 'over', '*', 'eating', ',', 'though', 'i', \"'\", 've', 'never', 'had', 'any', 'issues', 'with', 'my', 'weight', '.', 'i', 'actually', 'think', 'the', 'over', '-', 'eating', 'may', 'have', 'been', 'a', 'manifestation', 'of', 'the', 'add', '(', 'i', 'would', 'snack', 'all', 'day', 'out', 'of', 'boredom', 'more', 'than', 'hunger', ')', '.', 'anyway', ',', 'while', 'i', 'could', 'deal', 'with', 'a', 'slight', 'appetite', 'reduction', ',', 'what', 'i', \"'\", 'm', 'experiencing', 'now', 'is', 'problematic', 'and', 'i', \"'\", 've', 'been', 'getting', 'headache', '##s', 'after', 'un', '##int', '##ent', '##ional', '##ly', 'skipping', 'meals', 'for', 'hours', '.', 'i', \"'\", 'd', 'also', 'like', 'to', 'stop', 'relying', 'on', 'junk', 'food', 'to', 'fill', 'me', 'up', ',', 'but', 'nothing', 'else', 'is', 'as', 'app', '##eti', '##zing', 'or', 'filling', '.', 'i', 'really', 'do', 'like', 'the', 'psychological', 'benefits', 'of', 'v', '##y', '##van', '##se', 'though', 'and', 'it', 'would', 'be', 'great', 'if', 'i', 'could', 'fix', 'this', 'problem', 'without', 'having', 'to', 'stop', 'taking', 'it', '.', 'so', ',', 'has', 'anyone', 'else', 'experienced', 'this', '?', 'if', 'so', ',', 'any', 'advice', 'on', 'how', 'to', 'deal', '?', 'i', 'am', 'going', 'to', 'discuss', 'this', 'with', 'my', 'doctor', 'of', 'course', ',', 'but', 'my', 'appointment', 'isn', \"'\", 't', 'for', 'another', 'couple', 'of', 'weeks', 'so', 'i', 'at', 'least', 'need', 'a', 'temporary', 'fix', 'in', 'the', 'meantime', '.', '*', '*', 'edit', '*', '*', ':', 'thanks', 'for', 'the', 'advice', ',', 'guys', '!', 'this', 'is', 'very', 'helpful', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'i', 'know', 'what', 'to', 'attribute', 'to', 'possible', 'depression', 'and', 'what', 'to', 'add', '?', 'i', 'know', 'i', \"'\", 've', 'got', 'add', ',', 'but', 'sometimes', 'i', 'think', 'i', \"'\", 'm', 'depressed', 'as', 'well', '.', 'i', \"'\", 'm', 'not', 'sure', '.']\n",
      "INFO:__main__:Number of tokens: 40\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'i', 'know', 'what', 'to', 'attribute', 'to', 'possible', 'depression', 'and', 'what', 'to', 'add', '?', 'i', 'know', 'i', \"'\", 've', 'got', 'add', ',', 'but', 'sometimes', 'i', 'think', 'i', \"'\", 'm', 'depressed', 'as', 'well', '.', 'i', \"'\", 'm', 'not', 'sure', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'can', \"'\", 't', 'stress', 'out', ',', 'someone', 'recognizes', 'it', '?', 'i', \"'\", 've', 'never', 'experienced', 'stress', 'for', 'a', 'long', 'time', ',', 'i', \"'\", 'm', 'always', 'pretty', 'laid', 'back', '.', 'even', 'though', 'i', 'should', 'stress', 'out', ',', 'i', \"'\", 've', 'got', 'so', 'much', 'shit', 'on', 'me', '.', 'it', \"'\", 's', 'like', 'a', 'defense', 'mechanism', '.', 'i', 'wonder', 'if', 'anyone', 'ever', 'feels', 'the', 'same', '?', 'i', 'don', \"'\", 't', 'see', 'it', 'as', 'something', 'good', ',', 'because', 'it', 'doesn', \"'\", 't', 'mo', '##tiv', '##ate', 'me', 'to', 'do', 'stuff', '.', 'i', \"'\", 've', 'been', 'studying', 'for', 'five', 'years', 'instead', 'of', 'four', ',', 'and', 'i', 'still', 'have', 'a', 'lot', 'of', 'stuff', 'to', 'do', 'from', 'the', 'second', 'and', 'third', 'year', '.', 'i', \"'\", 'm', 'only', 'freaking', 'out', 'a', 'little', 'right', 'now', ',', 'because', 'i', 'just', 'realized', 'how', 'fucked', 'up', 'i', 'am', ',', 'but', 'i', 'know', 'it', 'will', 'be', 'over', 'in', '15', 'minutes', '.']\n",
      "INFO:__main__:Number of tokens: 148\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'can', \"'\", 't', 'stress', 'out', ',', 'someone', 'recognizes', 'it', '?', 'i', \"'\", 've', 'never', 'experienced', 'stress', 'for', 'a', 'long', 'time', ',', 'i', \"'\", 'm', 'always', 'pretty', 'laid', 'back', '.', 'even', 'though', 'i', 'should', 'stress', 'out', ',', 'i', \"'\", 've', 'got', 'so', 'much', 'shit', 'on', 'me', '.', 'it', \"'\", 's', 'like', 'a', 'defense', 'mechanism', '.', 'i', 'wonder', 'if', 'anyone', 'ever', 'feels', 'the', 'same', '?', 'i', 'don', \"'\", 't', 'see', 'it', 'as', 'something', 'good', ',', 'because', 'it', 'doesn', \"'\", 't', 'mo', '##tiv', '##ate', 'me', 'to', 'do', 'stuff', '.', 'i', \"'\", 've', 'been', 'studying', 'for', 'five', 'years', 'instead', 'of', 'four', ',', 'and', 'i', 'still', 'have', 'a', 'lot', 'of', 'stuff', 'to', 'do', 'from', 'the', 'second', 'and', 'third', 'year', '.', 'i', \"'\", 'm', 'only', 'freaking', 'out', 'a', 'little', 'right', 'now', ',', 'because', 'i', 'just', 'realized', 'how', 'fucked', 'up', 'i', 'am', ',', 'but', 'i', 'know', 'it', 'will', 'be', 'over', 'in', '15', 'minutes', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'to', 'accomplish', 'tuesday', ']', 'share', 'one', 'thing', 'you', 'would', 'like', 'to', 'accomplish', 'this', 'week', '.', 'we', 'will', 'support', 'and', 'keep', 'you', 'accountable', '!', '*', '*', 'you', 'aren', \"'\", 't', 'too', 'late', '!', 'feel', 'free', 'to', 'post', 'something', 'you', 'want', 'to', 'do', 'and', 'i', 'will', 'check', 'on', 'you', 'if', 'you', 'desire', '.', '*', '*', 'seems', 'like', 'this', 'really', 'helped', 'a', 'lot', 'of', 'people', 'make', 'some', 'progress', '(', 'even', 'if', 'they', 'didn', \"'\", 't', 'complete', '.', '.', '.', 'they', 'at', 'least', 'started', ')', '.', 'awesome', 'job', 'everyone', '!', '*', '*', '*', 'i', 'wanted', 'to', 'get', 'this', 'out', 'monday', '.', '.', '.', 'but', 'well', '.', '.', '.', 'i', 'forgot', '&', '#', '323', '##2', ';', '\\\\', '_', '&', '#', '323', '##2', ';', '.', 'this', 'might', 'be', 'a', 'weekly', 'thread', 'like', '*', '*', 'win', 'wednesday', '*', '*', 'if', 'people', 'like', 'it', '.', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', 'accountable', '(', '*', 'just', 'like', 'jing', '##o', '##32', '##3', 'who', 'replied', 'to', 'my', 'post', 'from', 'last', 'week', 'where', 'i', 'said', 'i', 'would', 'make', 'this', 'thread', 'by', 'yesterday', '*', '*', '*', 'thanks', 'for', 'reminding', 'me', '!', '*', '*', ')', '.', '*', '*', '*', '*', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '*', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'commit', '##ing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '*', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '*', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', 'tips', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', '*', '*', '*', '*', 'what', '*', '*', '-', '~', '~', 'i', 'would', 'like', 'to', 'finally', 'make', 'it', 'to', 'a', 'lau', '##nd', '##rom', '##at', 'and', 'do', 'some', 'loads', 'of', 'laundry', '.', 'we', 'have', 'a', 'small', 'wash', '##er', 'that', 'can', 'only', 'do', '2', 'jeans', 'at', 'once', '.', '~', '~', '*', '*', '*', 'when', '*', '*', '-', 'today', '.', 'hopefully', 'within', 'the', 'hour', '.', '*', '*', '*', 'check', 'in', 'on', 'me', '?', '*', '*', '-', 'yes', '~', '~', 'edit', ':', 'i', 'did', 'it', '!', 'when', 'you', 'have', 'done', 'your', 'task', 'you', 'can', 'cross', 'it', 'off', 'by', 'surrounding', 'your', 'task', 'with', '~', '~', 'on', 'each', 'side', '.', '*', '~', '~', 'next', 'goal', 'of', 'mine', 'is', 'to', 'actually', 'put', 'the', 'laundry', 'away', 'today', '.', '(', 'i', 'already', 'folded', 'it', 'all', 'at', 'the', 'lau', '##nd', '##rom', '##at', ')', 'i', 'have', 'a', 'coaching', 'appointment', 'in', '15', 'minutes', '.', '.', '.', 'then', 'might', 'go', 'to', 'yoga', '.', '.', '.', 'so', 'by', 'the', 'time', 'i', 'go', 'to', 'bed', 'i', 'want', 'all', 'the', 'laundry', 'in', 'drawers', '.', '~', '~', '*', '*', 'finished', 'before', 'wife', 'even', 'got', 'home', '!', '*', '*']\n",
      "INFO:__main__:Number of tokens: 617\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['[', 'to', 'accomplish', 'tuesday', ']', 'share', 'one', 'thing', 'you', 'would', 'like', 'to', 'accomplish', 'this', 'week', '.', 'we', 'will', 'support', 'and', 'keep', 'you', 'accountable', '!', '*', '*', 'you', 'aren', \"'\", 't', 'too', 'late', '!', 'feel', 'free', 'to', 'post', 'something', 'you', 'want', 'to', 'do', 'and', 'i', 'will', 'check', 'on', 'you', 'if', 'you', 'desire', '.', '*', '*', 'seems', 'like', 'this', 'really', 'helped', 'a', 'lot', 'of', 'people', 'make', 'some', 'progress', '(', 'even', 'if', 'they', 'didn', \"'\", 't', 'complete', '.', '.', '.', 'they', 'at', 'least', 'started', ')', '.', 'awesome', 'job', 'everyone', '!', '*', '*', '*', 'i', 'wanted', 'to', 'get', 'this', 'out', 'monday', '.', '.', '.', 'but', 'well', '.', '.', '.', 'i', 'forgot', '&', '#', '323', '##2', ';', '\\\\', '_', '&', '#', '323', '##2', ';', '.', 'this', 'might', 'be', 'a', 'weekly', 'thread', 'like', '*', '*', 'win', 'wednesday', '*', '*', 'if', 'people', 'like', 'it', '.', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', 'accountable', '(', '*', 'just', 'like', 'jing', '##o', '##32', '##3', 'who', 'replied', 'to', 'my', 'post', 'from', 'last', 'week', 'where', 'i', 'said', 'i', 'would', 'make', 'this', 'thread', 'by', 'yesterday', '*', '*', '*', 'thanks', 'for', 'reminding', 'me', '!', '*', '*', ')', '.', '*', '*', '*', '*', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '*', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'commit', '##ing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '*', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '*', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', 'tips', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', '*', '*', '*', '*', 'what', '*', '*', '-', '~', '~', 'i', 'would', 'like', 'to', 'finally', 'make', 'it', 'to', 'a', 'lau', '##nd', '##rom', '##at', 'and', 'do', 'some', 'loads', 'of', 'laundry', '.', 'we', 'have', 'a', 'small', 'wash', '##er', 'that', 'can', 'only', 'do', '2', 'jeans', 'at', 'once', '.', '~', '~', '*', '*', '*', 'when', '*', '*', '-', 'today', '.', 'hopefully', 'within', 'the', 'hour', '.', '*', '*', '*', 'check', 'in', 'on', 'me', '?', '*', '*', '-', 'yes', '~', '~'], ['edit', ':', 'i', 'did', 'it', '!', 'when', 'you', 'have', 'done', 'your', 'task', 'you', 'can', 'cross', 'it', 'off', 'by', 'surrounding', 'your', 'task', 'with', '~', '~', 'on', 'each', 'side', '.', '*', '~', '~', 'next', 'goal', 'of', 'mine', 'is', 'to', 'actually', 'put', 'the', 'laundry', 'away', 'today', '.', '(', 'i', 'already', 'folded', 'it', 'all', 'at', 'the', 'lau', '##nd', '##rom', '##at', ')', 'i', 'have', 'a', 'coaching', 'appointment', 'in', '15', 'minutes', '.', '.', '.', 'then', 'might', 'go', 'to', 'yoga', '.', '.', '.', 'so', 'by', 'the', 'time', 'i', 'go', 'to', 'bed', 'i', 'want', 'all', 'the', 'laundry', 'in', 'drawers', '.', '~', '~', '*', '*', 'finished', 'before', 'wife', 'even', 'got', 'home', '!', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sleeping', 'problem', 'and', 'skipping', 'classes', '.', 'so', 'i', 'am', 'seeing', 'a', 'psychiatrist', 'soon', 'but', 'i', \"'\", 'm', 'just', 'curious', 'if', 'anyone', 'else', 'feels', 'the', 'same', 'way', 'that', 'i', 'do', '.', 'over', 'the', 'past', 'couple', 'of', 'years', 'i', 'have', 'experienced', 'this', 'sleeping', 'thing', 'where', 'usually', 'after', 'eating', ',', 'during', 'class', ',', 'or', 'driving', 'i', 'would', 'feel', 'excessive', 'fatigue', 'almost', 'wanting', 'to', 'fall', 'asleep', '.', 'they', 'happen', 'randomly', 'so', '##mt', '##ime', '##s', 'when', 'i', \"'\", 'm', 'playing', 'wow', 'or', 'star', '##craft', 'i', 'just', 'fall', 'asleep', ',', 'it', \"'\", 's', 'so', 'strong', 'that', 'it', \"'\", 's', 'un', '##con', '##tro', '##lla', '##ble', '.', 'after', 'the', 'sleep', '##iness', 'sub', '##side', '##s', 'i', 'usually', 'feel', 'better', 'afterwards', '.', 'is', 'this', 'normal', ',', 'someone', 'told', 'me', 'it', \"'\", 's', 'just', 'ci', '##ca', '##dian', 'rhythm', '.', 'is', 'that', 'true', '.', 'i', 'looked', 'it', 'up', 'and', 'it', \"'\", 's', 'called', 'na', '##rce', '##lo', '##psy', '.', 'could', 'i', 'have', 'this', ',', 'along', 'with', 'my', 'other', 'ad', '##hd', 'symptoms', '.', 'in', 'the', 'beginning', 'of', 'the', 'year', 'i', 'had', 'quite', 'a', 'bit', 'of', 'motivation', 'so', 'i', 'go', 'to', 'class', ',', 'but', 'then', 'something', 'happens', 'like', 'anxiety', 'or', 'something', 'and', 'then', 'i', 'start', 'to', 'feel', 'uncomfortable', 'in', 'class', 'and', 'then', 'just', 'not', 'care', 'anymore', '.', 'this', 'happens', 'every', 'year', ',', 'through', '##tou', '##t', 'high', 'school', 'and', 'college', '.', 'does', 'anyone', 'else', 'with', 'ad', '##hd', 'used', 'to', 'skip', 'classes', 'when', 'un', '##tre', '##ated', 'or', 'still', 'do', 'under', 'treatment', '?', 'i', 'know', 'i', 'have', 'to', 'go', 'but', 'i', 'don', \"'\", 't', '.', 'now', 'i', 'just', 'feel', 'like', 'not', 'wanting', 'to', 'do', 'anything', ',', 'bored', 'of', 'the', 'video', 'games', ',', 'red', '##dit', ',', 'etc', '.', '.', '.', 'so', 'i', 'just', 'usually', 'find', 'something', 'pointless', 'to', 'waste', 'time', 'on', 'instead', 'of', 'studying', '.', 'or', 'i', 'just', 'lay', 'in', 'my', 'bed', '.', 'it', \"'\", 's', 'also', 'been', 'so', 'hard', 'la', '##tley', 'to', 'get', 'out', 'of', 'bed', '.', 'i', 'used', 'to', 'get', 'up', 'early', 'but', 'now', 'i', 'just', 'don', \"'\", 't', 'want', 'to', 'or', 'i', 'feel', 'tired', '.', 'anyone', 'else', 'have', 'this', '?', 'in', 'the', 'beginning', 'of', 'the', 'year', 'i', 'feel', 'fine', 'but', 'when', 'things', 'get', 'more', 'detailed', 'in', 'classes', 'i', 'get', 'overwhelmed', 'of', 'the', 'amount', 'of', 'organization', 'needed', 'and', 'studying', 'and', 'just', 'start', 'skipping', 'and', 'lose', 'all', 'motivation', '.', 'does', 'anyone', 'else', 'feel', 'this', 'way', '?', 'thanks', 'i', 'understand', 'i', 'shouldn', \"'\", 't', 'be', 'getting', 'help', 'from', 'here', 'but', 'i', 'am', 'seeing', 'a', 'psychologist', 'and', 'i', 'guess', 'this', 'is', 'one', 'way', 'to', 'waste', 'time', '.']\n",
      "INFO:__main__:Number of tokens: 407\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sleeping', 'problem', 'and', 'skipping', 'classes', '.', 'so', 'i', 'am', 'seeing', 'a', 'psychiatrist', 'soon', 'but', 'i', \"'\", 'm', 'just', 'curious', 'if', 'anyone', 'else', 'feels', 'the', 'same', 'way', 'that', 'i', 'do', '.', 'over', 'the', 'past', 'couple', 'of', 'years', 'i', 'have', 'experienced', 'this', 'sleeping', 'thing', 'where', 'usually', 'after', 'eating', ',', 'during', 'class', ',', 'or', 'driving', 'i', 'would', 'feel', 'excessive', 'fatigue', 'almost', 'wanting', 'to', 'fall', 'asleep', '.', 'they', 'happen', 'randomly', 'so', '##mt', '##ime', '##s', 'when', 'i', \"'\", 'm', 'playing', 'wow', 'or', 'star', '##craft', 'i', 'just', 'fall', 'asleep', ',', 'it', \"'\", 's', 'so', 'strong', 'that', 'it', \"'\", 's', 'un', '##con', '##tro', '##lla', '##ble', '.', 'after', 'the', 'sleep', '##iness', 'sub', '##side', '##s', 'i', 'usually', 'feel', 'better', 'afterwards', '.', 'is', 'this', 'normal', ',', 'someone', 'told', 'me', 'it', \"'\", 's', 'just', 'ci', '##ca', '##dian', 'rhythm', '.', 'is', 'that', 'true', '.', 'i', 'looked', 'it', 'up', 'and', 'it', \"'\", 's', 'called', 'na', '##rce', '##lo', '##psy', '.', 'could', 'i', 'have', 'this', ',', 'along', 'with', 'my', 'other', 'ad', '##hd', 'symptoms', '.', 'in', 'the', 'beginning', 'of', 'the', 'year', 'i', 'had', 'quite', 'a', 'bit', 'of', 'motivation', 'so', 'i', 'go', 'to', 'class', ',', 'but', 'then', 'something', 'happens', 'like', 'anxiety', 'or', 'something', 'and', 'then', 'i', 'start', 'to', 'feel', 'uncomfortable', 'in', 'class', 'and', 'then', 'just', 'not', 'care', 'anymore', '.', 'this', 'happens', 'every', 'year', ',', 'through', '##tou', '##t', 'high', 'school', 'and', 'college', '.', 'does', 'anyone', 'else', 'with', 'ad', '##hd', 'used', 'to', 'skip', 'classes', 'when', 'un', '##tre', '##ated', 'or', 'still', 'do', 'under', 'treatment', '?', 'i', 'know', 'i', 'have', 'to', 'go', 'but', 'i', 'don', \"'\", 't', '.', 'now', 'i', 'just', 'feel', 'like', 'not', 'wanting', 'to', 'do', 'anything', ',', 'bored', 'of', 'the', 'video', 'games', ',', 'red', '##dit', ',', 'etc', '.', '.', '.', 'so', 'i', 'just', 'usually', 'find', 'something', 'pointless', 'to', 'waste', 'time', 'on', 'instead', 'of', 'studying', '.', 'or', 'i', 'just', 'lay', 'in', 'my', 'bed', '.', 'it', \"'\", 's', 'also', 'been', 'so', 'hard', 'la', '##tley', 'to', 'get', 'out', 'of', 'bed', '.', 'i', 'used', 'to', 'get', 'up', 'early', 'but', 'now', 'i', 'just', 'don', \"'\", 't', 'want', 'to', 'or', 'i', 'feel', 'tired', '.', 'anyone', 'else', 'have', 'this', '?', 'in', 'the', 'beginning', 'of', 'the', 'year', 'i', 'feel', 'fine', 'but', 'when', 'things', 'get', 'more', 'detailed', 'in', 'classes', 'i', 'get', 'overwhelmed', 'of', 'the', 'amount', 'of', 'organization', 'needed', 'and', 'studying', 'and', 'just', 'start', 'skipping', 'and', 'lose', 'all', 'motivation', '.', 'does', 'anyone', 'else', 'feel', 'this', 'way', '?', 'thanks', 'i', 'understand', 'i', 'shouldn', \"'\", 't', 'be', 'getting', 'help', 'from', 'here', 'but', 'i', 'am', 'seeing', 'a', 'psychologist', 'and', 'i', 'guess', 'this', 'is', 'one', 'way', 'to', 'waste', 'time', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['young', 'adult', 'with', 'ad', '##hd', '.', 'i', 'need', 'tips', 'for', 'staying', 'off', 'of', 'red', '##dit', '/', 'the', 'internet', 'in', 'general', 'while', 'i', \"'\", 'm', 'at', 'work', 'i', 'have', 'a', 'job', 'that', 'requires', 'a', 'lot', 'of', 'autonomous', 'work', 'on', 'the', 'computer', ',', 'and', 'though', 'i', \"'\", 've', 'tried', 'to', 'will', 'through', 'it', ',', 'i', 'can', \"'\", 't', 'stop', 'zip', '##ping', 'back', 'to', 'the', 'internet', 'whenever', 'a', 'stray', 'though', 'warrant', '##s', 'it', '.', 'i', \"'\", 'm', 'wasting', 'a', 'ton', 'of', 'time', ',', 'and', 'don', \"'\", 't', 'have', 'a', 'good', 'method', 'to', 'mit', '##igate', 'it', ',', 'this', 'sort', 'of', 'stuff', 'is', 'new', 'to', 'me', '.', 'have', 'any', 'of', 'you', 'found', 'successful', 'ways', 'to', 'keep', 'your', 'internet', 'use', 'down', 'to', 'reasonable', 'levels', 'at', 'work', '?', 'edit', ':', 'thanks', 'all', 'for', 'the', 'quick', 'practical', 'awesome', 'advice', '.', 'it', \"'\", 's', 'a', 'crazy', 'feeling', 'discovering', 'that', 'there', 'are', 'so', 'many', 'people', 'going', 'through', 'such', 'a', 'similar', 'thing', '.', 'red', '##dit', 'is', 'great', 'in', 'that', 'respect', '.', 'i', \"'\", 'm', 'going', 'to', 'try', 'a', 'few', 'things', '-', '1', '.', 'set', 'up', 'clear', '/', 'realistic', 'work', '-', 'break', 'time', 'intervals', ',', 'use', 'a', 'computer', 'program', 'to', 'enforce', 'them', '2', '.', 'write', 'down', 'anything', 'i', 'think', 'to', 'use', 'the', 'internet', 'for', 'and', 'wait', 'until', 'a', 'break', 'to', 'ind', '##ul', '##ge', '3', '.', 'cut', 'down', 'on', 'the', 'fast', '/', 'exciting', 'music', '(', 'i', 'always', 'listen', 'to', 'music', 'when', 'i', 'work', ')', '4', '.', 'get', 'up', 'and', 'take', 'a', 'walk', 'every', '3', 'hours', 'if', 'i', 'can', \"'\", 't', 'keep', 'that', 'up', 'i', \"'\", 'll', 'have', 'to', 'just', 'di', '##sable', 'the', 'internet', '/', 'restrict', 'capabilities']\n",
      "INFO:__main__:Number of tokens: 263\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['young', 'adult', 'with', 'ad', '##hd', '.', 'i', 'need', 'tips', 'for', 'staying', 'off', 'of', 'red', '##dit', '/', 'the', 'internet', 'in', 'general', 'while', 'i', \"'\", 'm', 'at', 'work', 'i', 'have', 'a', 'job', 'that', 'requires', 'a', 'lot', 'of', 'autonomous', 'work', 'on', 'the', 'computer', ',', 'and', 'though', 'i', \"'\", 've', 'tried', 'to', 'will', 'through', 'it', ',', 'i', 'can', \"'\", 't', 'stop', 'zip', '##ping', 'back', 'to', 'the', 'internet', 'whenever', 'a', 'stray', 'though', 'warrant', '##s', 'it', '.', 'i', \"'\", 'm', 'wasting', 'a', 'ton', 'of', 'time', ',', 'and', 'don', \"'\", 't', 'have', 'a', 'good', 'method', 'to', 'mit', '##igate', 'it', ',', 'this', 'sort', 'of', 'stuff', 'is', 'new', 'to', 'me', '.', 'have', 'any', 'of', 'you', 'found', 'successful', 'ways', 'to', 'keep', 'your', 'internet', 'use', 'down', 'to', 'reasonable', 'levels', 'at', 'work', '?', 'edit', ':', 'thanks', 'all', 'for', 'the', 'quick', 'practical', 'awesome', 'advice', '.', 'it', \"'\", 's', 'a', 'crazy', 'feeling', 'discovering', 'that', 'there', 'are', 'so', 'many', 'people', 'going', 'through', 'such', 'a', 'similar', 'thing', '.', 'red', '##dit', 'is', 'great', 'in', 'that', 'respect', '.', 'i', \"'\", 'm', 'going', 'to', 'try', 'a', 'few', 'things', '-', '1', '.', 'set', 'up', 'clear', '/', 'realistic', 'work', '-', 'break', 'time', 'intervals', ',', 'use', 'a', 'computer', 'program', 'to', 'enforce', 'them', '2', '.', 'write', 'down', 'anything', 'i', 'think', 'to', 'use', 'the', 'internet', 'for', 'and', 'wait', 'until', 'a', 'break', 'to', 'ind', '##ul', '##ge', '3', '.', 'cut', 'down', 'on', 'the', 'fast', '/', 'exciting', 'music', '(', 'i', 'always', 'listen', 'to', 'music', 'when', 'i', 'work', ')', '4', '.', 'get', 'up', 'and', 'take', 'a', 'walk', 'every', '3', 'hours', 'if', 'i', 'can', \"'\", 't', 'keep', 'that', 'up', 'i', \"'\", 'll', 'have', 'to', 'just', 'di', '##sable', 'the', 'internet', '/', 'restrict', 'capabilities']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['screw', 'you', 'red', '##dit', ',', 'i', 'need', 'to', 'study', '.', 'unusual', 'tips', '?', 'so', 'this', 'week', 'is', 'an', 'insane', '##ly', 'busy', 'week', 'for', 'me', 'with', 'tests', '(', 'i', 'have', '5', 'tests', 'in', 'total', ',', 'and', 'i', 'need', 'to', 'do', 'well', 'in', 'them', ')', '.', 'we', 'all', 'know', 'how', 'it', 'is', 'though', ',', 'everything', 'seems', 'to', 'be', 'more', 'interesting', 'then', 'studying', '.', 'i', 'even', 'cleaned', 'my', 'room', 'and', 'made', 'up', 'the', 'excuse', 'it', 'was', '\"', 'distracting', '\"', 'me', '.', 'my', 'question', 'to', 'you', 'is', 'does', 'anyone', 'have', 'any', 'good', '*', '*', 'unusual', '*', '*', 'tips', '/', 'study', 'habits', 'that', 'they', 'would', 'want', 'to', 'share', '?', 'in', 'all', 'honesty', 'i', 'don', \"'\", 't', 'want', 'the', 'stereo', '##typical', '\"', 'get', 'off', 'the', 'computer', '\"', 'and', 'things', 'like', 'that', ',', 'i', 'do', 'those', 'things', 'already', '.', 'i', 'want', 'unique', 'study', 'ideas', 'and', 'habits', 'that', 'you', 'guys', 'use', '.', 'example', ':', 'in', 'finals', 'week', 'i', 'would', 'sit', 'in', 'my', 'room', 'and', 'lay', 'out', 'all', 'my', 'subjects', 'in', 'sections', '.', 'i', 'would', 'pick', 'up', 'one', 'and', 'start', 'studying', 'that', 'one', 'until', 'i', 'got', 'distracted', 'and', 'then', 'i', 'would', 'just', 'change', 'over', 'to', 'another', 'subject', '.', 'with', 'all', 'the', 'other', 'subjects', 'laying', 'out', 'i', 'would', 'usually', 'get', 'distracted', 'by', 'other', 'subjects', 'and', 'then', 'just', 'change', 'to', 'those', 'giving', 'me', 'a', 'positive', 'thing', 'to', 'distract', 'myself', 'with', '.']\n",
      "INFO:__main__:Number of tokens: 220\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['screw', 'you', 'red', '##dit', ',', 'i', 'need', 'to', 'study', '.', 'unusual', 'tips', '?', 'so', 'this', 'week', 'is', 'an', 'insane', '##ly', 'busy', 'week', 'for', 'me', 'with', 'tests', '(', 'i', 'have', '5', 'tests', 'in', 'total', ',', 'and', 'i', 'need', 'to', 'do', 'well', 'in', 'them', ')', '.', 'we', 'all', 'know', 'how', 'it', 'is', 'though', ',', 'everything', 'seems', 'to', 'be', 'more', 'interesting', 'then', 'studying', '.', 'i', 'even', 'cleaned', 'my', 'room', 'and', 'made', 'up', 'the', 'excuse', 'it', 'was', '\"', 'distracting', '\"', 'me', '.', 'my', 'question', 'to', 'you', 'is', 'does', 'anyone', 'have', 'any', 'good', '*', '*', 'unusual', '*', '*', 'tips', '/', 'study', 'habits', 'that', 'they', 'would', 'want', 'to', 'share', '?', 'in', 'all', 'honesty', 'i', 'don', \"'\", 't', 'want', 'the', 'stereo', '##typical', '\"', 'get', 'off', 'the', 'computer', '\"', 'and', 'things', 'like', 'that', ',', 'i', 'do', 'those', 'things', 'already', '.', 'i', 'want', 'unique', 'study', 'ideas', 'and', 'habits', 'that', 'you', 'guys', 'use', '.', 'example', ':', 'in', 'finals', 'week', 'i', 'would', 'sit', 'in', 'my', 'room', 'and', 'lay', 'out', 'all', 'my', 'subjects', 'in', 'sections', '.', 'i', 'would', 'pick', 'up', 'one', 'and', 'start', 'studying', 'that', 'one', 'until', 'i', 'got', 'distracted', 'and', 'then', 'i', 'would', 'just', 'change', 'over', 'to', 'another', 'subject', '.', 'with', 'all', 'the', 'other', 'subjects', 'laying', 'out', 'i', 'would', 'usually', 'get', 'distracted', 'by', 'other', 'subjects', 'and', 'then', 'just', 'change', 'to', 'those', 'giving', 'me', 'a', 'positive', 'thing', 'to', 'distract', 'myself', 'with', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['wait', 'til', 'you', 'see', 'the', 'line', 'to', 'the', 'pharmacy']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['wait', 'til', 'you', 'see', 'the', 'line', 'to', 'the', 'pharmacy']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ep', '##igen', '##etic', '##s', '.', 'at', 'a', 'lecture', '.', 'anyone', 'interested', 'in', 'notes', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ep', '##igen', '##etic', '##s', '.', 'at', 'a', 'lecture', '.', 'anyone', 'interested', 'in', 'notes', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['have', 'been', 'waiting', 'almost', '5', 'months', 'for', 'insurance', 'authorization', 'to', 'get', 'ne', '##uro', '##psy', '##ch', 'testing', 'for', 'add', '.', 'has', 'anyone', 'else', 'had', 'any', 'sort', 'of', 'problems', 'with', 'insurance', '?']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['have', 'been', 'waiting', 'almost', '5', 'months', 'for', 'insurance', 'authorization', 'to', 'get', 'ne', '##uro', '##psy', '##ch', 'testing', 'for', 'add', '.', 'has', 'anyone', 'else', 'had', 'any', 'sort', 'of', 'problems', 'with', 'insurance', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'let', \"'\", 's', 'celebrate', 'together', '!', '*', '*', 'even', 'though', 'wednesday', 'has', 'passed', ',', 'if', 'you', 'didn', \"'\", 't', 'have', 'a', 'chance', 'to', 'participate', 'this', 'week', 'please', 'keep', 'posting', 'your', 'wins', 'until', 'next', 'wednesday', '!', '*', '*', '*', '*', '*', 'welcome', 'to', 'the', '2nd', 'edition', 'of', 'win', 'wednesday', '!', 'we', 'had', '12', 'participants', 'last', 'week', '.', '.', '.', '*', '*', 'can', 'we', 'double', 'that', 'today', '*', '*', '?', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', 'so', 'here', 'if', 'your', 'chance', 'to', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', '*', 'some', 'examples', 'from', 'last', 'week', '*', 'finished', 'and', 'turned', 'in', 'homework', 'on', 'time', '*', 'left', 'the', 'house', 'every', 'day', '*', 'went', 'to', 'grocery', 'store', '*', 'cleaned', 'room', '(', 'or', 'part', 'of', 'room', ')', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', 'i', 'understand', 'it', 'might', 'be', 'hard', '(', 'initially', ')', 'to', 'think', 'of', 'something', '.', '*', '*', 'if', 'you', 'cannot', 'think', 'of', 'a', '\"', 'win', '\"', 'then', 'i', 'invite', 'you', 'to', 'name', '1', '(', 'or', 'more', ')', 'things', 'you', 'are', 'grateful', 'for', '.', '*', '*', '*', '*', '*', 'my', 'wins', 'to', 'start', '*', 'posted', '\"', 'win', 'wednesday', '\"', 'and', '\"', 'to', 'accomplish', 'tuesday', '\"', 'here', '*', 'went', 'to', 'lau', '##nd', '##rom', '##at', 'yesterday', ',', 'folded', ',', 'and', 'put', 'away', 'all', 'clothes', 'before', 'wife', 'got', 'home', 'from', 'work', '*', 'saw', 'hunger', 'games', 'and', 'didn', \"'\", 't', 'fi', '##dget', 'terribly', '(', 'the', 'rec', '##liner', 'seats', 'helped', ')', '*', 'finally', 'processed', 'my', \"'\", 'sort', \"'\", 'and', \"'\", 'action', \"'\", 'boxes', 'which', 'i', 'have', 'not', 'sorted', 'or', 'taken', 'any', 'action', 'on', 'for', 'the', 'past', '3', 'months', '(', 'various', 'papers', 'and', 'stuff', ')', '*', 'last', 'but', 'not', 'least', ',', 'put', 'all', 'my', 'taxes', 'documents', 'in', 'one', 'place', 'in', 'preparation', '.']\n",
      "INFO:__main__:Number of tokens: 459\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'let', \"'\", 's', 'celebrate', 'together', '!', '*', '*', 'even', 'though', 'wednesday', 'has', 'passed', ',', 'if', 'you', 'didn', \"'\", 't', 'have', 'a', 'chance', 'to', 'participate', 'this', 'week', 'please', 'keep', 'posting', 'your', 'wins', 'until', 'next', 'wednesday', '!', '*', '*', '*', '*', '*', 'welcome', 'to', 'the', '2nd', 'edition', 'of', 'win', 'wednesday', '!', 'we', 'had', '12', 'participants', 'last', 'week', '.', '.', '.', '*', '*', 'can', 'we', 'double', 'that', 'today', '*', '*', '?', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', 'so', 'here', 'if', 'your', 'chance', 'to', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', '*', 'some', 'examples', 'from', 'last', 'week', '*', 'finished', 'and', 'turned', 'in', 'homework', 'on', 'time', '*', 'left', 'the', 'house', 'every', 'day', '*', 'went', 'to', 'grocery', 'store', '*', 'cleaned', 'room', '(', 'or', 'part', 'of', 'room', ')', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', 'i', 'understand', 'it', 'might', 'be', 'hard', '(', 'initially', ')', 'to', 'think', 'of', 'something', '.', '*', '*', 'if', 'you', 'cannot', 'think', 'of', 'a', '\"', 'win', '\"', 'then', 'i', 'invite', 'you', 'to', 'name', '1', '(', 'or', 'more', ')', 'things', 'you', 'are', 'grateful', 'for', '.', '*', '*', '*', '*', '*', 'my', 'wins', 'to', 'start', '*', 'posted', '\"', 'win', 'wednesday', '\"', 'and', '\"', 'to', 'accomplish', 'tuesday', '\"', 'here', '*', 'went', 'to', 'lau', '##nd', '##rom', '##at', 'yesterday', ',', 'folded', ',', 'and', 'put', 'away', 'all', 'clothes', 'before', 'wife', 'got', 'home', 'from', 'work', '*', 'saw', 'hunger', 'games', 'and', 'didn', \"'\", 't', 'fi', '##dget', 'terribly', '(', 'the', 'rec', '##liner', 'seats', 'helped', ')', '*', 'finally', 'processed', 'my', \"'\", 'sort', \"'\", 'and', \"'\", 'action', \"'\", 'boxes', 'which', 'i', 'have', 'not', 'sorted', 'or', 'taken', 'any', 'action', 'on', 'for', 'the', 'past', '3', 'months', '(', 'various', 'papers', 'and', 'stuff', ')', '*', 'last', 'but', 'not', 'least', ',', 'put', 'all', 'my', 'taxes', 'documents', 'in', 'one', 'place', 'in', 'preparation', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['some', 'light', 'humour', '(', 'from', '/', 'r', '/', 'funny', ')']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['some', 'light', 'humour', '(', 'from', '/', 'r', '/', 'funny', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'happened', 'to', 'count', 'my', 'pills', 'the', 'other', 'day', 'and', 'found', 'they', 'came', 'up', 'short', '.', '.', '.', 'this', 'was', 'about', '4', 'days', 'after', 'i', 'got', 'the', 'sc', '##rip', 'filled', ',', 'and', 'i', \"'\", 've', 'ruled', 'out', 'the', 'possibility', 'of', 'mis', '##placed', '/', 'lost', 'pills', 'or', 'that', 'my', 'g', '##f', '(', 'my', 'only', 'roommate', ')', 'took', 'them', '.', 'so', 'i', \"'\", 'm', 'left', 'with', 'the', 'possibility', 'that', 'there', 'was', 'a', 'mis', '##co', '##unt', 'at', 'the', 'pharmacy', 'or', 'that', 'they', 'straight', 'up', 'stole', 'them', '.', 'anyone', 'have', 'any', 'experience', 'with', 'something', 'like', 'this', 'happening', '?', 'what', 'should', 'i', 'do', '?', '(', 'aside', 'from', 'counting', 'my', 'pills', 'at', 'the', 'pharmacy', 'whenever', 'i', 'pick', 'them', 'up', ')']\n",
      "INFO:__main__:Number of tokens: 114\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'happened', 'to', 'count', 'my', 'pills', 'the', 'other', 'day', 'and', 'found', 'they', 'came', 'up', 'short', '.', '.', '.', 'this', 'was', 'about', '4', 'days', 'after', 'i', 'got', 'the', 'sc', '##rip', 'filled', ',', 'and', 'i', \"'\", 've', 'ruled', 'out', 'the', 'possibility', 'of', 'mis', '##placed', '/', 'lost', 'pills', 'or', 'that', 'my', 'g', '##f', '(', 'my', 'only', 'roommate', ')', 'took', 'them', '.', 'so', 'i', \"'\", 'm', 'left', 'with', 'the', 'possibility', 'that', 'there', 'was', 'a', 'mis', '##co', '##unt', 'at', 'the', 'pharmacy', 'or', 'that', 'they', 'straight', 'up', 'stole', 'them', '.', 'anyone', 'have', 'any', 'experience', 'with', 'something', 'like', 'this', 'happening', '?', 'what', 'should', 'i', 'do', '?', '(', 'aside', 'from', 'counting', 'my', 'pills', 'at', 'the', 'pharmacy', 'whenever', 'i', 'pick', 'them', 'up', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['question', 'about', 'v', '##y', '##vance', 'so', 'i', 'was', 'having', 'problems', 'focusing', 'and', 'getting', 'things', 'done', 'at', 'work', '.', 'i', 'went', 'to', 'my', 'doctor', 'for', 'my', 'yearly', 'physical', ',', 'and', 'without', 'me', 'even', 'saying', 'anything', 'to', 'him', ',', 'he', 'asked', 'me', 'if', 'normally', 'had', 'trouble', 'making', 'eye', 'contact', ',', 'or', 'finishing', 'tasks', 'etc', '.', 'after', 'talking', 'with', 'him', 'for', 'a', 'little', 'while', 'he', 'referred', 'me', 'to', 'a', 'psychiatrist', 'to', 'talk', 'to', ',', 'since', 'he', 'had', 'a', 'suspicion', 'that', 'i', 'had', 'an', 'und', '##ia', '##gno', '##sed', 'case', 'of', 'add', '/', 'ad', '##hd', '.', 'i', 'am', '25', 'and', 'i', 'have', 'currently', 'been', 'on', 'v', '##y', '##vance', 'for', 'a', 'little', 'over', 'three', 'weeks', '.', 'my', 'question', 'is', ',', 'i', 'take', 'it', 'in', 'the', 'morning', ',', 'and', 'i', 'feel', 'great', '.', 'i', 'have', 'no', 'problems', 'remembering', 'things', ',', 'i', 'have', 'lots', 'of', 'energy', 'and', 'don', \"'\", 't', 'feel', 'so', 'gr', '##og', '##gy', '.', 'i', 'stay', 'focused', 'on', 'work', ',', 'all', 'is', 'well', '.', 'but', 'lately', ',', 'i', 'have', 'noticed', 'that', 'the', 'effects', 'seem', 'to', 'wear', 'off', 'around', '1', '##pm', ',', 'when', 'they', 'are', 'supposed', 'to', 'last', 'all', 'day', '.', 'i', 'work', 'until', '6', ',', 'so', 'having', 'this', '5', 'hour', 'gap', 'really', 'throws', 'me', 'off', '.', 'i', 'don', \"'\", 't', 'ever', 'take', 'more', 'than', 'is', 'prescribed', 'to', 'me', '(', 'currently', '40', '##mg', 'x', '##r', ')', ',', 'but', 'maybe', 'the', 'dos', '##age', 'is', 'a', 'little', 'to', 'low', '?', 'is', 'this', 'a', 'normal', 'kind', 'of', 'tolerance', 'build', '##up', '?', 'i', 'don', \"'\", 't', 'suspect', 'i', 'have', 'been', 'taking', 'it', 'long', 'enough', 'to', 'build', 'up', 'a', 'tolerance', ',', 'but', 'you', 'never', 'know', '.', 'i', 'know', 'this', 'is', 'something', 'that', 'i', 'should', 'and', 'will', 'talk', 'to', 'my', 'dr', 'about', 'as', 'well', ',', 'when', 'i', 'see', 'him', 'in', 'two', 'weeks', 'i', 'will', 'bring', 'it', 'up', '.', 'i', 'was', 'just', 'curious', 'if', 'anyone', 'else', 'had', 'heard', 'of', 'this', 'kind', 'of', 'thing', 'happening', 'before', '.']\n",
      "INFO:__main__:Number of tokens: 312\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['question', 'about', 'v', '##y', '##vance', 'so', 'i', 'was', 'having', 'problems', 'focusing', 'and', 'getting', 'things', 'done', 'at', 'work', '.', 'i', 'went', 'to', 'my', 'doctor', 'for', 'my', 'yearly', 'physical', ',', 'and', 'without', 'me', 'even', 'saying', 'anything', 'to', 'him', ',', 'he', 'asked', 'me', 'if', 'normally', 'had', 'trouble', 'making', 'eye', 'contact', ',', 'or', 'finishing', 'tasks', 'etc', '.', 'after', 'talking', 'with', 'him', 'for', 'a', 'little', 'while', 'he', 'referred', 'me', 'to', 'a', 'psychiatrist', 'to', 'talk', 'to', ',', 'since', 'he', 'had', 'a', 'suspicion', 'that', 'i', 'had', 'an', 'und', '##ia', '##gno', '##sed', 'case', 'of', 'add', '/', 'ad', '##hd', '.', 'i', 'am', '25', 'and', 'i', 'have', 'currently', 'been', 'on', 'v', '##y', '##vance', 'for', 'a', 'little', 'over', 'three', 'weeks', '.', 'my', 'question', 'is', ',', 'i', 'take', 'it', 'in', 'the', 'morning', ',', 'and', 'i', 'feel', 'great', '.', 'i', 'have', 'no', 'problems', 'remembering', 'things', ',', 'i', 'have', 'lots', 'of', 'energy', 'and', 'don', \"'\", 't', 'feel', 'so', 'gr', '##og', '##gy', '.', 'i', 'stay', 'focused', 'on', 'work', ',', 'all', 'is', 'well', '.', 'but', 'lately', ',', 'i', 'have', 'noticed', 'that', 'the', 'effects', 'seem', 'to', 'wear', 'off', 'around', '1', '##pm', ',', 'when', 'they', 'are', 'supposed', 'to', 'last', 'all', 'day', '.', 'i', 'work', 'until', '6', ',', 'so', 'having', 'this', '5', 'hour', 'gap', 'really', 'throws', 'me', 'off', '.', 'i', 'don', \"'\", 't', 'ever', 'take', 'more', 'than', 'is', 'prescribed', 'to', 'me', '(', 'currently', '40', '##mg', 'x', '##r', ')', ',', 'but', 'maybe', 'the', 'dos', '##age', 'is', 'a', 'little', 'to', 'low', '?', 'is', 'this', 'a', 'normal', 'kind', 'of', 'tolerance', 'build', '##up', '?', 'i', 'don', \"'\", 't', 'suspect', 'i', 'have', 'been', 'taking', 'it', 'long', 'enough', 'to', 'build', 'up', 'a', 'tolerance', ',', 'but', 'you', 'never', 'know', '.', 'i', 'know', 'this', 'is', 'something', 'that', 'i', 'should', 'and', 'will', 'talk', 'to', 'my', 'dr', 'about', 'as', 'well', ',', 'when', 'i', 'see', 'him', 'in', 'two', 'weeks', 'i', 'will', 'bring', 'it', 'up', '.', 'i', 'was', 'just', 'curious', 'if', 'anyone', 'else', 'had', 'heard', 'of', 'this', 'kind', 'of', 'thing', 'happening', 'before', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'time', 'taking', 'medication', 'in', 'about', '3', 'years', '.', 'any', 'advice', '?', '40', 'minutes', 'ago', 'i', 'took', 'the', 'first', 'pill', 'for', 'ad', '##hd', 'i', 'have', 'had', 'in', '3', 'years', '.', 'its', 'been', 'so', 'long', 'that', 'i', 'don', \"'\", 't', 'really', 'remember', 'what', 'the', 'side', 'effects', 'will', 'be', 'or', 'how', 'motivated', 'i', 'might', 'feel', '.', 'any', 'advice', 'on', 'what', 'i', 'should', 'do', 'or', 'how', 'to', 'manage', 'my', 'time', 'e', '##ffi', '##cent', '##ly', '?']\n",
      "INFO:__main__:Number of tokens: 72\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', 'time', 'taking', 'medication', 'in', 'about', '3', 'years', '.', 'any', 'advice', '?', '40', 'minutes', 'ago', 'i', 'took', 'the', 'first', 'pill', 'for', 'ad', '##hd', 'i', 'have', 'had', 'in', '3', 'years', '.', 'its', 'been', 'so', 'long', 'that', 'i', 'don', \"'\", 't', 'really', 'remember', 'what', 'the', 'side', 'effects', 'will', 'be', 'or', 'how', 'motivated', 'i', 'might', 'feel', '.', 'any', 'advice', 'on', 'what', 'i', 'should', 'do', 'or', 'how', 'to', 'manage', 'my', 'time', 'e', '##ffi', '##cent', '##ly', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['amp', '##het', '##amine', 'dos', '##age', 'plateau', '##s', ',', 'mostly', 'concerning', 'dex', '##ed', '##rine', 'and', 'add', '##eral', '##l', '.', 'i', 'have', 'had', 'a', 'prescription', 'to', 'dex', '##ed', '##rine', 'since', 'june', 'of', '2011', ',', 'following', 'a', 'month', 'of', 'prescribed', 'add', '##eral', '##l', 'x', '##r', 'and', 'a', 'few', 'months', 'of', 'discovering', 'that', 'v', '##y', '##van', '##se', 'and', 'add', '##eral', '##l', 'did', 'something', 'magical', 'for', 'my', 'brain', ',', 'and', 'that', 'i', 'had', 'better', 'ask', 'a', 'doctor', 'about', 'it', '.', 'i', 'have', 'considerable', 'experience', 'with', 'the', 'ir', 'and', 'er', 'formulation', '##s', 'of', 'both', 'types', 'of', 'amp', '##het', '##amine', '.', 'i', \"'\", 've', 'noticed', 'something', 'peculiar', 'about', 'my', 'experiences', 'and', 'i', 'want', 'to', 'see', 'if', 'anyone', 'else', 'has', 'a', 'similar', 'experience', '.', 'i', 'have', 'identified', 'roughly', 'three', '\"', 'plateau', '##s', '\"', 'or', 'dos', '##age', 'ranges', 'that', 'produce', 'different', 'effects', 'on', 'me', '.', 'it', 'is', 'a', 'predictable', 'effect', 'that', 'happens', 'at', 'least', '90', '%', 'of', 'the', 'time', 'within', 'these', 'ranges', '.', 'sub', '-', '10', '##mg', '(', 'ir', ')', 'or', 'sub', '-', '15', '##mg', '(', 'er', ')', '-', 'this', 'dos', '##age', 'only', 'gives', 'me', 'a', 'slight', 'energy', 'boost', 'equivalent', 'to', 'a', 'cup', 'of', 'coffee', ',', 'and', 'a', 'very', 'modest', 'improvement', 'in', 'my', 'capacity', 'to', 'focus', '.', 'my', 'capacity', 'to', 'ref', '##oc', '##us', ',', 'my', 'mood', '/', 'motivation', ',', 'and', 'my', 'mental', 'organization', 'are', 'barely', 'above', 'my', 'natural', 'levels', ',', 'if', 'at', 'all', '.', 'i', 'have', 'found', 'i', 'need', 'to', 'take', 'at', 'least', '5', '##mg', 'ir', 'or', '10', '##mg', 'er', 'to', 'avoid', 'amp', '##het', '##amine', 'withdrawal', 'symptoms', '.', '10', '-', '17', '.', '5', '##mg', '(', 'ir', ')', 'or', '15', '-', '25', '##mg', '(', 'er', ')', '-', 'i', 'have', 'noticeably', 'high', 'energy', ',', 'but', 'still', 'don', \"'\", 't', 'reach', 'that', '\"', 'sweet', 'spot', '\"', 'where', 'my', 'hyper', '##act', '##ivity', 'is', 'reduced', ',', 'so', 'the', 'physical', 'expressions', 'of', 'it', 'increase', 'slightly', '.', 'this', 'is', 'my', '\"', 'official', '\"', 'dos', '##age', 'and', 'the', 'range', 'in', 'which', 'my', 'ability', 'to', 'focus', 'is', 'consistent', ',', 'if', 'still', 'a', 'little', 'inferior', '.', 'in', 'addition', ',', 'within', 'this', 'range', 'i', 'gain', 'the', 'capacity', 'to', 'direct', 'my', 'thoughts', 'enough', 'to', 'ref', '##oc', '##us', 'and', 'not', 'rum', '##inate', '.', 'as', 'a', 'down', 'side', ',', 'i', 'begin', 'to', 'have', 'trouble', 'switching', 'tasks', 'within', 'the', 'upper', 'end', 'of', 'this', 'range', ',', 'which', 'has', 'led', 'to', 'more', 'than', 'one', 'night', 'of', 'expert', 'video', 'gaming', ':', '/', 'additionally', 'this', 'is', 'the', 'range', 'where', 'amp', '##het', '##amine', 'begins', 'to', 'make', 'the', 'world', 'artificial', '##ly', 'intense', 'for', 'me', '.', '20', '-', '30', '##mg', '(', 'ir', ')', 'or', '30', '##mg', '-', '40', '##mg', '(', 'er', ')', '-', 'this', 'range', 'is', 'what', 'i', 'consider', '\"', 'too', 'much', '\"', 'not', 'in', 'the', 'sense', 'that', 'it', 'is', 'uncomfortable', ',', 'but', 'that', 'it', 'alter', '##s', 'my', 'perception', 'enough', 'that', 'it', 'becomes', 'alteration', 'rather', 'than', 'supplement', '##ation', 'of', 'my', 'natural', 'state', '.', 'this', 'in', 'itself', 'is', 'not', 'bad', 'and', 'amp', '##het', '##amine', 'is', 'good', 'in', 'that', 'it', 'doesn', \"'\", 't', 'rob', 'you', 'of', 'your', 'perspective', 'until', 'you', \"'\", 've', 'taken', 'enough', 'that', 'you', 'probably', 'weren', \"'\", 't', 'planning', 'on', 'being', 'sober', 'anyway', '.', 'that', 'said', ',', 'this', 'is', 'also', 'the', 'range', 'in', 'which', 'i', 'begin', 'to', 'experience', 'the', '\"', 'full', 'suite', '\"', 'of', 'effects', 'that', 'an', 'ad', '##hd', 'brain', 'benefits', 'from', ',', 'such', 'as', 'increased', 'organization', ',', 'multi', '##tas', '##king', ',', 'goal', 'oriented', 'behavior', ',', 'desire', 'inhibition', ',', 'patience', ',', 'increased', 'productivity', ',', 'and', 'mood', 'lift', '/', 'stabilization', '.', 'i', 'generally', 'avoid', 'taking', 'this', 'much', ',', 'and', 'actually', 'have', 'had', 'to', 'insist', 'my', 'doctor', 'not', 'give', 'me', 'more', 'than', 'i', 'already', 'take', '.', 'if', 'i', 'have', 'a', 'large', 'task', 'to', 'complete', ',', 'such', 'as', 'an', 'essay', 'or', 'story', 'or', 'just', 'a', 'significant', 'amount', 'of', 'homework', 'in', 'general', ',', 'i', 'turn', 'to', 'this', 'dos', '##age', 'range', 'as', 'insurance', ',', 'as', 'it', 'does', 'basically', 'ensure', 'that', 'my', 'brain', 'will', 'go', 'until', 'i', 'am', 'done', 'with', 'everything', 'i', 'need', 'to', 'do', ',', 'mostly', 'through', 'completely', 'ne', '##gating', 'my', 'experience', 'of', '\"', 'add', 'over', '##w', '##helm', '\"', 'which', 'tends', 'to', 'foil', 'me', 'at', 'anything', 'less', 'than', '25', '##mg', ',', 'though', 'considerably', 'less', 'than', 'when', 'i', 'wasn', \"'\", 't', 'med', '##icated', '.', 'i', 'also', 'believe', '(', 'with', 'no', 'real', 'hard', 'evidence', ')', 'that', 'this', 'plateau', 'is', 'un', '##sus', '##tain', '##able', 'as', 'a', 'daily', 'dos', '##age', 'because', 'it', 'will', 'invariably', 'increase', 'tolerance', '.', 'the', 'main', 'cave', '##at', 'of', 'this', 'range', 'is', 'hyper', '##vi', '##gil', '##ance', ',', 'which', 'is', 'uncomfortable', 'when', 'i', 'have', 'nothing', 'to', 'focus', 'on', ',', 'and', 'also', 'leaves', 'me', 'a', 'little', 'emotionally', 'flattened', '.', 'this', 'is', 'also', 'the', 'range', 'where', 'the', 'crash', 'and', 'rebound', 'effects', 'are', 'severe', 'enough', 'that', 'i', 'am', 'significantly', 'worse', 'than', 'my', 'un', '##med', '##icated', 'norm', 'for', 'an', 'hour', 'or', 'two', 'after', 'it', 'wears', 'off', '.', 'above', '30', '##mg', 'ir', 'is', 'significantly', 'more', 'recreational', 'in', 'effects', 'and', 'while', 'it', 'doesn', \"'\", 't', 'become', 'md', '##ma', 'i', 'find', 'the', 'eu', '##ph', '##oria', 'annoying', 'and', 'distracting', 'rather', 'than', 'pleasant', '.', 'i', 'have', 'only', 'taken', '50', '##mg', 'of', 'the', 'er', 'once', ',', 'and', 'rather', 'than', 'eu', '##ph', '##oria', 'it', 'just', 'caused', 'anxiety', 'and', 'aka', '##thi', '##sia', 'that', 'worked', 'together', 'to', 'ne', '##gate', 'any', 'benefit', 'from', 'my', 'greatly', 'increased', 'focus', 'and', 'energy', '.', 'does', 'anyone', 'have', 'a', 'similar', 'experience', 'with', 'add', '##eral', '##l', ',', 'dex', '##ed', '##rine', ',', 'or', 'v', '##y', '##van', '##se', '?', 't', '##l', ';', 'dr', ':', 'i', 'experience', 'three', 'distinct', 'but', 'inter', '##rel', '##ated', 'sets', 'of', 'effects', 'from', 'three', 'different', 'dos', '##age', 'ranges', 'of', 'amp', '##het', '##amine', '.', '5', '-', '15', '##mg', '=', 'a', 'cup', 'of', 'coffee', '15', '-', '30', 'mg', '=', 'prescribed', 'dose', ',', 'stabilize', '##s', 'concentration', 'and', 'helps', 'ref', '##oc', '##us', ',', 'but', 'causes', 'ob', '##ses', '##sive', '##ness', '30', '-', '40', '##mg', '=', '\"', 'complete', '\"', 'positive', 'effects', 'profile', 'but', 'uncomfortable', 'side', 'effects', 'and', 'a', 'bad', 'rebound', 'period', '.']\n",
      "INFO:__main__:Number of tokens: 949\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['amp', '##het', '##amine', 'dos', '##age', 'plateau', '##s', ',', 'mostly', 'concerning', 'dex', '##ed', '##rine', 'and', 'add', '##eral', '##l', '.', 'i', 'have', 'had', 'a', 'prescription', 'to', 'dex', '##ed', '##rine', 'since', 'june', 'of', '2011', ',', 'following', 'a', 'month', 'of', 'prescribed', 'add', '##eral', '##l', 'x', '##r', 'and', 'a', 'few', 'months', 'of', 'discovering', 'that', 'v', '##y', '##van', '##se', 'and', 'add', '##eral', '##l', 'did', 'something', 'magical', 'for', 'my', 'brain', ',', 'and', 'that', 'i', 'had', 'better', 'ask', 'a', 'doctor', 'about', 'it', '.', 'i', 'have', 'considerable', 'experience', 'with', 'the', 'ir', 'and', 'er', 'formulation', '##s', 'of', 'both', 'types', 'of', 'amp', '##het', '##amine', '.', 'i', \"'\", 've', 'noticed', 'something', 'peculiar', 'about', 'my', 'experiences', 'and', 'i', 'want', 'to', 'see', 'if', 'anyone', 'else', 'has', 'a', 'similar', 'experience', '.', 'i', 'have', 'identified', 'roughly', 'three', '\"', 'plateau', '##s', '\"', 'or', 'dos', '##age', 'ranges', 'that', 'produce', 'different', 'effects', 'on', 'me', '.', 'it', 'is', 'a', 'predictable', 'effect', 'that', 'happens', 'at', 'least', '90', '%', 'of', 'the', 'time', 'within', 'these', 'ranges', '.', 'sub', '-', '10', '##mg', '(', 'ir', ')', 'or', 'sub', '-', '15', '##mg', '(', 'er', ')', '-', 'this', 'dos', '##age', 'only', 'gives', 'me', 'a', 'slight', 'energy', 'boost', 'equivalent', 'to', 'a', 'cup', 'of', 'coffee', ',', 'and', 'a', 'very', 'modest', 'improvement', 'in', 'my', 'capacity', 'to', 'focus', '.', 'my', 'capacity', 'to', 'ref', '##oc', '##us', ',', 'my', 'mood', '/', 'motivation', ',', 'and', 'my', 'mental', 'organization', 'are', 'barely', 'above', 'my', 'natural', 'levels', ',', 'if', 'at', 'all', '.', 'i', 'have', 'found', 'i', 'need', 'to', 'take', 'at', 'least', '5', '##mg', 'ir', 'or', '10', '##mg', 'er', 'to', 'avoid', 'amp', '##het', '##amine', 'withdrawal', 'symptoms', '.', '10', '-', '17', '.', '5', '##mg', '(', 'ir', ')', 'or', '15', '-', '25', '##mg', '(', 'er', ')', '-', 'i', 'have', 'noticeably', 'high', 'energy', ',', 'but', 'still', 'don', \"'\", 't', 'reach', 'that', '\"', 'sweet', 'spot', '\"', 'where', 'my', 'hyper', '##act', '##ivity', 'is', 'reduced', ',', 'so', 'the', 'physical', 'expressions', 'of', 'it', 'increase', 'slightly', '.', 'this', 'is', 'my', '\"', 'official', '\"', 'dos', '##age', 'and', 'the', 'range', 'in', 'which', 'my', 'ability', 'to', 'focus', 'is', 'consistent', ',', 'if', 'still', 'a', 'little', 'inferior', '.', 'in', 'addition', ',', 'within', 'this', 'range', 'i', 'gain', 'the', 'capacity', 'to', 'direct', 'my', 'thoughts', 'enough', 'to', 'ref', '##oc', '##us', 'and', 'not', 'rum', '##inate', '.', 'as', 'a', 'down', 'side', ',', 'i', 'begin', 'to', 'have', 'trouble', 'switching', 'tasks', 'within', 'the', 'upper', 'end', 'of', 'this', 'range', ',', 'which', 'has', 'led', 'to', 'more', 'than', 'one', 'night', 'of', 'expert', 'video', 'gaming', ':', '/', 'additionally', 'this', 'is', 'the', 'range', 'where', 'amp', '##het', '##amine', 'begins', 'to', 'make', 'the', 'world', 'artificial', '##ly', 'intense', 'for', 'me', '.', '20', '-', '30', '##mg', '(', 'ir', ')', 'or', '30', '##mg', '-', '40', '##mg', '(', 'er', ')', '-', 'this', 'range', 'is', 'what', 'i', 'consider', '\"', 'too', 'much', '\"', 'not', 'in', 'the', 'sense', 'that', 'it', 'is', 'uncomfortable', ',', 'but', 'that', 'it', 'alter', '##s', 'my', 'perception', 'enough', 'that', 'it', 'becomes', 'alteration', 'rather', 'than', 'supplement', '##ation', 'of', 'my', 'natural', 'state', '.', 'this', 'in', 'itself', 'is', 'not', 'bad', 'and', 'amp', '##het', '##amine', 'is', 'good', 'in', 'that', 'it', 'doesn', \"'\", 't', 'rob', 'you', 'of', 'your', 'perspective', 'until', 'you', \"'\", 've', 'taken', 'enough', 'that', 'you', 'probably', 'weren', \"'\", 't', 'planning', 'on', 'being', 'sober', 'anyway', '.', 'that', 'said', ',', 'this', 'is', 'also', 'the', 'range', 'in'], ['which', 'i', 'begin', 'to', 'experience', 'the', '\"', 'full', 'suite', '\"', 'of', 'effects', 'that', 'an', 'ad', '##hd', 'brain', 'benefits', 'from', ',', 'such', 'as', 'increased', 'organization', ',', 'multi', '##tas', '##king', ',', 'goal', 'oriented', 'behavior', ',', 'desire', 'inhibition', ',', 'patience', ',', 'increased', 'productivity', ',', 'and', 'mood', 'lift', '/', 'stabilization', '.', 'i', 'generally', 'avoid', 'taking', 'this', 'much', ',', 'and', 'actually', 'have', 'had', 'to', 'insist', 'my', 'doctor', 'not', 'give', 'me', 'more', 'than', 'i', 'already', 'take', '.', 'if', 'i', 'have', 'a', 'large', 'task', 'to', 'complete', ',', 'such', 'as', 'an', 'essay', 'or', 'story', 'or', 'just', 'a', 'significant', 'amount', 'of', 'homework', 'in', 'general', ',', 'i', 'turn', 'to', 'this', 'dos', '##age', 'range', 'as', 'insurance', ',', 'as', 'it', 'does', 'basically', 'ensure', 'that', 'my', 'brain', 'will', 'go', 'until', 'i', 'am', 'done', 'with', 'everything', 'i', 'need', 'to', 'do', ',', 'mostly', 'through', 'completely', 'ne', '##gating', 'my', 'experience', 'of', '\"', 'add', 'over', '##w', '##helm', '\"', 'which', 'tends', 'to', 'foil', 'me', 'at', 'anything', 'less', 'than', '25', '##mg', ',', 'though', 'considerably', 'less', 'than', 'when', 'i', 'wasn', \"'\", 't', 'med', '##icated', '.', 'i', 'also', 'believe', '(', 'with', 'no', 'real', 'hard', 'evidence', ')', 'that', 'this', 'plateau', 'is', 'un', '##sus', '##tain', '##able', 'as', 'a', 'daily', 'dos', '##age', 'because', 'it', 'will', 'invariably', 'increase', 'tolerance', '.', 'the', 'main', 'cave', '##at', 'of', 'this', 'range', 'is', 'hyper', '##vi', '##gil', '##ance', ',', 'which', 'is', 'uncomfortable', 'when', 'i', 'have', 'nothing', 'to', 'focus', 'on', ',', 'and', 'also', 'leaves', 'me', 'a', 'little', 'emotionally', 'flattened', '.', 'this', 'is', 'also', 'the', 'range', 'where', 'the', 'crash', 'and', 'rebound', 'effects', 'are', 'severe', 'enough', 'that', 'i', 'am', 'significantly', 'worse', 'than', 'my', 'un', '##med', '##icated', 'norm', 'for', 'an', 'hour', 'or', 'two', 'after', 'it', 'wears', 'off', '.', 'above', '30', '##mg', 'ir', 'is', 'significantly', 'more', 'recreational', 'in', 'effects', 'and', 'while', 'it', 'doesn', \"'\", 't', 'become', 'md', '##ma', 'i', 'find', 'the', 'eu', '##ph', '##oria', 'annoying', 'and', 'distracting', 'rather', 'than', 'pleasant', '.', 'i', 'have', 'only', 'taken', '50', '##mg', 'of', 'the', 'er', 'once', ',', 'and', 'rather', 'than', 'eu', '##ph', '##oria', 'it', 'just', 'caused', 'anxiety', 'and', 'aka', '##thi', '##sia', 'that', 'worked', 'together', 'to', 'ne', '##gate', 'any', 'benefit', 'from', 'my', 'greatly', 'increased', 'focus', 'and', 'energy', '.', 'does', 'anyone', 'have', 'a', 'similar', 'experience', 'with', 'add', '##eral', '##l', ',', 'dex', '##ed', '##rine', ',', 'or', 'v', '##y', '##van', '##se', '?', 't', '##l', ';', 'dr', ':', 'i', 'experience', 'three', 'distinct', 'but', 'inter', '##rel', '##ated', 'sets', 'of', 'effects', 'from', 'three', 'different', 'dos', '##age', 'ranges', 'of', 'amp', '##het', '##amine', '.', '5', '-', '15', '##mg', '=', 'a', 'cup', 'of', 'coffee', '15', '-', '30', 'mg', '=', 'prescribed', 'dose', ',', 'stabilize', '##s', 'concentration', 'and', 'helps', 'ref', '##oc', '##us', ',', 'but', 'causes', 'ob', '##ses', '##sive', '##ness', '30', '-', '40', '##mg', '=', '\"', 'complete', '\"', 'positive', 'effects', 'profile', 'but', 'uncomfortable', 'side', 'effects', 'and', 'a', 'bad', 'rebound', 'period', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['blame', 'it', 'on', 'my', 'add', ',', 'baby', '.']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['blame', 'it', 'on', 'my', 'add', ',', 'baby', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'questions', '(', 'adult', 'ad', '##hd', ',', 'etc', ')', 'hello', '/', 'r', '/', 'ad', '##hd', 'this', 'is', 'my', 'first', 'post', 'to', 'the', 'thread', ',', 'and', 'i', 'have', 'some', 'questions', '.', 'over', 'a', 'long', 'period', 'of', 'time', ',', 'trying', 'to', 'understand', 'things', 'in', 'my', 'life', ',', 'ie', 'difficulty', 'concentrating', ',', 'poor', 'grades', 'in', 'hs', '(', 'better', 'in', 'college', ')', 'despite', 'studying', 'hard', 'and', 'knowing', 'im', 'a', 'bright', 'kid', ',', 'distraction', '##s', 'of', 'mood', 'during', 'sex', '(', 'was', 'a', 'big', 'factor', 'in', 'trying', 'to', 'discover', 'what', \"'\", 's', 'up', ')', '.', 'and', 'realizing', 'most', 'my', 'whole', 'family', '(', 'brother', 'and', 'father', 'diagnosed', 'with', 'ad', '##hd', ',', 'mom', 'and', 'grandma', 'definitely', 'seem', 'to', 'have', 'it', '-', 'not', 'professionally', 'diagnosed', ')', ',', 'and', 'additionally', 'me', 'being', 'diagnosed', 'with', 'anxiety', 'and', 'depression', 'i', \"'\", 'm', 'beginning', 'to', 'think', 'i', 'might', 'have', 'add', '/', 'ad', '##hd', '.', 'i', \"'\", 'm', 'not', 'necessarily', 'hyper', '##active', ',', 'but', 'often', 'times', 'i', 'feel', 'like', 'just', 'doing', 'imp', '##ulsive', 'things', 'or', 'blur', '##ting', 'out', 'random', 'things', 'for', 'the', 'sake', 'of', 'being', 'dumb', '.', 'and', 'i', 'think', 'socially', 'it', \"'\", 's', 'become', 'acceptable', 'in', 'my', 'life', 'since', 'i', \"'\", 'm', 'know', 'as', 'the', 'goofy', 'person', ',', 'yet', 'often', 'shy', '.', 'does', 'any', 'of', 'this', 'seem', 'familiar', 'to', 'you', '?', 'how', 'did', 'you', 'go', 'about', 'getting', 'diagnosed', 'or', 'going', 'to', 'a', 'doctor', 'about', 'it', '?', 'edit', ':', 'i', 'also', 'lose', 'things', 'on', 'the', 'regular', '(', 'keys', ')', ',', 'get', 'distracted', 'to', 'do', 'things', 'while', 'im', 'heading', 'on', 'my', 'way', 'out', 'and', 'often', 'time', 'am', 'rushing', 'out', 'the', 'door', 'ha', '##ha', '-', '_', '-']\n",
      "INFO:__main__:Number of tokens: 262\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'questions', '(', 'adult', 'ad', '##hd', ',', 'etc', ')', 'hello', '/', 'r', '/', 'ad', '##hd', 'this', 'is', 'my', 'first', 'post', 'to', 'the', 'thread', ',', 'and', 'i', 'have', 'some', 'questions', '.', 'over', 'a', 'long', 'period', 'of', 'time', ',', 'trying', 'to', 'understand', 'things', 'in', 'my', 'life', ',', 'ie', 'difficulty', 'concentrating', ',', 'poor', 'grades', 'in', 'hs', '(', 'better', 'in', 'college', ')', 'despite', 'studying', 'hard', 'and', 'knowing', 'im', 'a', 'bright', 'kid', ',', 'distraction', '##s', 'of', 'mood', 'during', 'sex', '(', 'was', 'a', 'big', 'factor', 'in', 'trying', 'to', 'discover', 'what', \"'\", 's', 'up', ')', '.', 'and', 'realizing', 'most', 'my', 'whole', 'family', '(', 'brother', 'and', 'father', 'diagnosed', 'with', 'ad', '##hd', ',', 'mom', 'and', 'grandma', 'definitely', 'seem', 'to', 'have', 'it', '-', 'not', 'professionally', 'diagnosed', ')', ',', 'and', 'additionally', 'me', 'being', 'diagnosed', 'with', 'anxiety', 'and', 'depression', 'i', \"'\", 'm', 'beginning', 'to', 'think', 'i', 'might', 'have', 'add', '/', 'ad', '##hd', '.', 'i', \"'\", 'm', 'not', 'necessarily', 'hyper', '##active', ',', 'but', 'often', 'times', 'i', 'feel', 'like', 'just', 'doing', 'imp', '##ulsive', 'things', 'or', 'blur', '##ting', 'out', 'random', 'things', 'for', 'the', 'sake', 'of', 'being', 'dumb', '.', 'and', 'i', 'think', 'socially', 'it', \"'\", 's', 'become', 'acceptable', 'in', 'my', 'life', 'since', 'i', \"'\", 'm', 'know', 'as', 'the', 'goofy', 'person', ',', 'yet', 'often', 'shy', '.', 'does', 'any', 'of', 'this', 'seem', 'familiar', 'to', 'you', '?', 'how', 'did', 'you', 'go', 'about', 'getting', 'diagnosed', 'or', 'going', 'to', 'a', 'doctor', 'about', 'it', '?', 'edit', ':', 'i', 'also', 'lose', 'things', 'on', 'the', 'regular', '(', 'keys', ')', ',', 'get', 'distracted', 'to', 'do', 'things', 'while', 'im', 'heading', 'on', 'my', 'way', 'out', 'and', 'often', 'time', 'am', 'rushing', 'out', 'the', 'door', 'ha', '##ha', '-', '_', '-']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'long', 'do', 'you', 'take', 'to', 'recover', 'from', 'burn', 'out', 'r', '[', 'ad', '##hd', ']', 'its', '?', 'i', 'was', 'reading', 'the', 'comments', 'here', '-', 'http', ':', '/', '/', 'news', '.', 'y', '##comb', '##inator', '.', 'com', '/', 'item', '?', 'id', '=', '37', '##6', '##7', '##9', '##31', 'and', 'quite', 'a', 'few', 'are', 'really', 'clicking', '.', 'for', 'as', 'long', 'as', 'i', 'can', 'remember', ',', 'i', \"'\", 've', 'taken', 'on', 'many', 'projects', 'or', 'school', 'courses', ',', 'gotten', 'into', 'them', 'got', 'tired', 'of', 'them', 'and', 'dropped', 'off', '-', 'sometimes', 'because', 'i', 'got', 'distracted', ',', 'bored', ',', 'was', 'not', 'really', 'involved', 'with', 'the', 'material', 'or', 'project', '.', 'there', 'were', 'then', 'those', 'times', 'i', 'was', 'intensely', 'interested', ',', 'working', 'my', 'ass', 'off', 'but', 'the', 'rewards', 'just', 'weren', \"'\", 't', 'there', '.', 'there', 'were', 'always', 'people', 'who', 'could', 'sit', 'and', 'do', 'math', 'and', 'code', 'for', '5', 'hours', 'longer', ',', 'who', 'could', 'get', 'the', 'grades', 'because', 'they', 'were', 'more', 'focused', ',', 'a', 'sort', 'of', 'resentment', 'that', 'has', 'dogg', '##ed', 'me', 'for', 'years', '.', 'no', 'matter', 'how', 'hard', 'i', 'worked', ',', 'i', \"'\", 'd', 'not', 'see', 'it', 'yielding', 'the', 'results', 'it', 'yielded', 'for', 'those', 'without', 'the', 'add', '.', 'i', 'only', 'have', 'a', 'few', 'chances', 'left', 'at', 'college', 'and', 'i', \"'\", 'm', 'wasting', 'those', 'away', 'as', 'well', '-', 'suicidal', 'depression', '-', 'in', 'a', 'sense', ',', 'i', \"'\", 've', 'gotten', 'over', 'that', 'mostly', 'because', 'i', 'will', 'leave', 'behind', 'bigger', 'problems', 'for', 'my', 'family', 'and', 'in', 'the', 'short', 'term', 'i', 'kinda', 'want', 'to', 'watch', 'mi', '##b', '##3', ',', 'spider', '##man', '4', ',', 'avengers', 'and', 'the', 'next', 'batman', 'and', 'enjoy', 'life', 'a', 'bit', '(', 'misery', 'with', 'school', 'has', 'been', 'a', 'constant', 'from', '2003', ')', '.', 'for', 'info', '-', 'i', \"'\", 'm', 'not', 'med', '##icated', '[', 'too', 'poor', 'for', 'that', 'atm', ']', 'i', 'don', \"'\", 't', 'really', 'want', 'medical', 'advice', ',', 'just', 'general', 'life', 'tips', '-', 'how', 'you', 'recover', ',', 'how', 'you', 'find', 'projects', 'you', 'like', ',', 'how', 'you', 'sustain', 'interest', '.']\n",
      "INFO:__main__:Number of tokens: 317\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'long', 'do', 'you', 'take', 'to', 'recover', 'from', 'burn', 'out', 'r', '[', 'ad', '##hd', ']', 'its', '?', 'i', 'was', 'reading', 'the', 'comments', 'here', '-', 'http', ':', '/', '/', 'news', '.', 'y', '##comb', '##inator', '.', 'com', '/', 'item', '?', 'id', '=', '37', '##6', '##7', '##9', '##31', 'and', 'quite', 'a', 'few', 'are', 'really', 'clicking', '.', 'for', 'as', 'long', 'as', 'i', 'can', 'remember', ',', 'i', \"'\", 've', 'taken', 'on', 'many', 'projects', 'or', 'school', 'courses', ',', 'gotten', 'into', 'them', 'got', 'tired', 'of', 'them', 'and', 'dropped', 'off', '-', 'sometimes', 'because', 'i', 'got', 'distracted', ',', 'bored', ',', 'was', 'not', 'really', 'involved', 'with', 'the', 'material', 'or', 'project', '.', 'there', 'were', 'then', 'those', 'times', 'i', 'was', 'intensely', 'interested', ',', 'working', 'my', 'ass', 'off', 'but', 'the', 'rewards', 'just', 'weren', \"'\", 't', 'there', '.', 'there', 'were', 'always', 'people', 'who', 'could', 'sit', 'and', 'do', 'math', 'and', 'code', 'for', '5', 'hours', 'longer', ',', 'who', 'could', 'get', 'the', 'grades', 'because', 'they', 'were', 'more', 'focused', ',', 'a', 'sort', 'of', 'resentment', 'that', 'has', 'dogg', '##ed', 'me', 'for', 'years', '.', 'no', 'matter', 'how', 'hard', 'i', 'worked', ',', 'i', \"'\", 'd', 'not', 'see', 'it', 'yielding', 'the', 'results', 'it', 'yielded', 'for', 'those', 'without', 'the', 'add', '.', 'i', 'only', 'have', 'a', 'few', 'chances', 'left', 'at', 'college', 'and', 'i', \"'\", 'm', 'wasting', 'those', 'away', 'as', 'well', '-', 'suicidal', 'depression', '-', 'in', 'a', 'sense', ',', 'i', \"'\", 've', 'gotten', 'over', 'that', 'mostly', 'because', 'i', 'will', 'leave', 'behind', 'bigger', 'problems', 'for', 'my', 'family', 'and', 'in', 'the', 'short', 'term', 'i', 'kinda', 'want', 'to', 'watch', 'mi', '##b', '##3', ',', 'spider', '##man', '4', ',', 'avengers', 'and', 'the', 'next', 'batman', 'and', 'enjoy', 'life', 'a', 'bit', '(', 'misery', 'with', 'school', 'has', 'been', 'a', 'constant', 'from', '2003', ')', '.', 'for', 'info', '-', 'i', \"'\", 'm', 'not', 'med', '##icated', '[', 'too', 'poor', 'for', 'that', 'atm', ']', 'i', 'don', \"'\", 't', 'really', 'want', 'medical', 'advice', ',', 'just', 'general', 'life', 'tips', '-', 'how', 'you', 'recover', ',', 'how', 'you', 'find', 'projects', 'you', 'like', ',', 'how', 'you', 'sustain', 'interest', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', '-', 'c', '(', 'adult', 'late', 'diagnosis', ')', '.', 'severe', 'co', '##gna', '##tive', ',', 'impulse', 'and', 'memory', 'issues', '(', 'first', 'red', '##dit', 'post', ')', 'i', \"'\", 've', 'been', 'a', 'long', 'red', '##dit', 'reader', 'but', 'i', \"'\", 've', 'never', 'posted', 'anywhere', '.', 'i', \"'\", 've', 'reached', 'a', 'point', 'now', 'of', 'desperation', 'that', 'i', 'am', 'reaching', 'out', 'to', 'the', 'red', '##dit', 'community', 'for', 'input', 'and', 'some', 'guidance', '.', 't', '##ld', '##r', 'will', 'follow', '.', 'i', 'was', 'diagnosed', 'with', 'add', '/', 'ad', '##hd', 'about', '3', 'years', 'ago', 'at', 'the', 'age', 'of', '36', 'with', 'no', '\"', 'real', '\"', 'ex', '##plin', '##ation', 'by', 'our', 'family', 'm', '##ft', 'after', 'she', 'determined', 'that', 'my', 'teenage', 'daughter', 'was', 'add', '.', 'i', 'was', 'per', '##scribe', '##d', 'add', '##eral', '##l', 'at', 'low', 'dos', '##age', '(', '5', '##mg', 'ir', ')', 'which', 'has', 'been', 'increased', 'about', 'ever', '9', 'months', 'for', 'the', 'last', '3', 'years', '(', 'at', '25', '##mg', ',', '15', 'in', 'the', 'am', '10', 'in', 'the', 'pm', ')', '.', 'the', 'past', '4', 'years', 'have', 'been', 'a', 'living', 'nightmare', '.', '2008', '-', '09', 'my', 'mother', ',', 'grandmother', 'and', 'grandfather', 'all', 'pass', 'away', 'over', 'a', '12', 'month', 'span', '(', 'oct', '08', '-', 'oct', '09', ')', 'i', 'have', 'been', 'fired', 'from', '4', 'jobs', 'normally', 'stemming', 'from', 'incomplete', 'or', 'poor', 'quality', 'of', 'work', ',', 'we', \"'\", 've', 'been', 'evicted', 'from', '2', 'apartments', '(', 'even', 'though', 'my', 'income', 'is', 'in', 'the', 'low', 'six', 'figures', ')', '.', 'my', 'imp', '##uls', '##ivity', 'and', 'ne', '##gt', '##ive', 'be', '##ha', '##vo', '##ir', 'patterns', 'have', 'grown', 'to', 'extremely', 'dan', '##go', '##ures', 'and', 'di', '##sas', '##tour', '##us', 'levels', '(', 'lo', '##osing', '6', '##k', 'in', '1', 'night', 'of', 'gambling', ',', 'having', 'sex', 'with', 'as', 'many', 'as', '4', '-', '5', 'strangers', '/', 'prostitutes', 'in', 'a', 'day', ')', '.', 'my', 'co', '##gna', '##tive', 'and', 'memory', 'rent', '##ent', '##ion', 'have', 'also', 'been', 'slipping', 'drastic', '##ly', '.', 'at', 'times', 'basic', 'math', '##matic', '##s', 'can', 'be', 'a', 'huge', 'problem', '(', 'simple', 'addition', 'and', 'sub', '##tra', '##ction', ')', '.', 'memory', 'is', 'becoming', 'a', 'huge', 'issue', '.', 'example', 'being', 'i', 'want', 'something', 'from', 'another', 'room', '(', 'water', ',', 'a', 'lighter', ',', 'a', 'piece', 'of', 'paper', ')', ',', 'i', \"'\", 'll', 'get', 'up', 'and', 'walk', 'the', 'the', 'other', 'room', '10', 'feet', 'away', 'and', 'not', 'recall', 'what', 'it', 'was', 'i', 'needed', ';', 'return', 'back', 'to', 'my', 'computer', ',', 'remember', '10', '-', '15', 'minutes', 'later', 'and', 'repeat', 'the', 'process', 'as', 'many', 'as', '4', '-', '5', 'times', '(', 'like', 'the', 'movie', 'moment', '##o', ')', '.', 'i', \"'\", 've', 'had', 'to', 'start', 'training', 'myself', 'to', 'walk', 'through', 'simple', 'processes', 'mentally', 'one', 'minute', 'step', 'at', 'a', 'time', 'just', 'to', 'be', 'able', 'to', 'perform', 'simple', 'tasks', '.', 'i', \"'\", 'm', 'hungry', ',', 'i', 'should', 'make', 'something', 'too', 'eat', ',', 'i', \"'\", 'll', 'make', 'soup', ',', 'i', 'need', 'a', 'bow', '##el', 'it', \"'\", 's', 'located', 'in', 'that', 'this', 'cub', '##bard', ',', 'i', 'need', 'a', 'spoon', ',', 'i', 'need', 'a', 'pot', 'to', 'cook', 'it', 'in', ',', 'etc', 'etc', '.', '.', '.', 'if', 'i', 'don', \"'\", 't', 'put', 'a', 'road', 'map', 'together', ',', 'i', 'can', 'get', 'lost', 'in', 'the', 'middle', 'of', 'a', 'process', '.', '(', 'example', 'i', \"'\", 'm', '32', 'minutes', 'into', 'writing', 'this', 'post', ',', 'i', \"'\", 'm', 'having', 'to', 'google', 'words', 'fr', '##e', '##qua', '##ntly', 'to', 'remember', 'how', 'to', 'spell', 'them', ')', 'when', 'given', 'any', 'form', 'of', 'task', 'or', 'instruction', 'at', 'work', '(', 'programmer', 'by', 'trade', ')', 'i', 'have', 'to', 'write', 'cop', '##us', 'notes', ',', 'i', \"'\", 've', 'gotten', 'to', 'the', 'point', 'of', 'carrying', 'a', 'digital', 'recorder', 'just', 'to', 'save', 'time', 'and', 'make', 'sure', 'i', 'get', 'everything', '.', 'even', 'then', 'i', 'have', 'to', 'double', 'and', 'triple', 'check', 'both', 'my', 'work', 'and', 'my', 'notes', '.', 'long', 'term', 'memory', 'is', 'also', 'being', 'impacted', '.', 'i', 'don', \"'\", 't', 'recall', 'much', 'of', 'anything', 'before', 'the', 'age', 'of', '20', ',', 'recalling', 'specific', 'events', '/', 'people', 'take', 'heavy', 'concentration', '.', 'this', 'is', 'a', 'total', '180', 'of', 'how', 'i', 'use', 'to', 'be', ',', 'recalling', 'names', 'and', 'faces', 'from', '1', '-', '2nd', 'grade', '.', 'i', 'ran', 'into', 'a', 'friend', 'in', '06', 'from', 'element', '##ry', 'school', 'who', 'i', 'was', 'never', 'close', 'to', 'or', 'even', 'social', 'with', 'beyond', 'being', 'in', 'the', 'same', 'class', '.', 'the', 'person', 'is', 'now', 'transgender', '##ed', 'and', 'i', 'happen', 'to', 'see', 'her', 'in', 'passing', 'and', 'instantly', 'make', 'the', 'connection', 'to', 'who', 'she', 'use', 'to', 'be', '.', 'we', 'chat', '##ted', 'for', 'a', 'long', 'time', 'about', 'the', 'past', 'and', 'have', 'kept', 'in', 'touch', 'since', ',', 'so', 'there', 'is', 'no', 'false', 'identification', 'issue', '.', 'i', 'use', 'to', 'be', 'extremely', 'good', 'with', 'faces', 'and', 'memory', '.', 'when', 'many', 'of', 'these', 'things', 'happen', '(', 'memory', ',', 'co', '##gna', '##tive', 'processing', ',', 'lack', 'of', 'focus', 'on', 'any', 'task', ')', 'i', 'stop', 'the', 'task', 'and', 'go', 'into', 'a', 'negative', 'be', '##ha', '##vo', '##ir', '(', 'video', 'games', ',', 'gambling', ',', 'impulse', 'sex', ',', 'porn', ',', 'tv', ',', 'ditch', 'work', ',', 'etc', ')', '.', 'i', \"'\", 've', 'have', 'mentioned', 'these', 'things', 'to', 'both', 'my', 'm', '##ft', ',', 'my', 'psychiatric', 'mental', 'health', 'nurse', 'practitioner', ',', 'and', 'my', 'primary', 'care', 'dr', '.', 'the', 'res', '##pon', '##ce', 'i', 'typically', 'get', 'is', 'that', 'it', 'is', 'either', 'stress', ',', 'fatigue', ',', 'anxiety', ',', 'medication', 'side', 'effects', 'or', 'just', '\"', 'your', 'getting', 'older', '\"', '.', 'increasing', 'my', 'medication', 'doesn', \"'\", 't', 'really', 'seem', 'to', 'effect', 'my', 'mood', 'as', 'far', 'as', 'depression', 'or', 'mani', '##c', ',', 'but', 'it', 'does', 'dramatic', '##ly', 'effect', 'my', 'li', '##bid', '##o', '.', 'either', 'by', 'increasing', 'my', 'desire', 'levels', 'and', '/', 'or', 'impact', '##ing', 'my', 'ability', 'to', 'perform', '(', 'ed', ')', '.', 'i', 'have', 'noticed', 'that', 'different', 'types', 'of', 'specific', 'stress', 'also', 'are', 'effect', '##ing', 'which', 'impulse', '##s', 'i', 'act', 'on', '(', 'more', 'details', 'if', 'needed', ')', '.', 'i', \"'\", 'm', 'at', 'a', 'total', 'loss', 'as', 'to', 'what', 'to', 'do', ',', 'i', \"'\", 've', 'always', 'known', 'to', 'some', 'degree', 'that', 'i', 'was', 'add', '/', 'ad', '##hd', 'i', \"'\", 'm', 'at', 'a', 'loss', 'as', 'to', 'what', 'to', 'do', ',', 'we', \"'\", 've', 'reached', 'the', 'point', 'of', 'bankruptcy', ',', 'another', 'ev', '##iction', 'this', 'month', ',', 'criminal', 'charges', '(', 'case', 'pending', 'won', \"'\", 't', 'discuss', ')', 'and', 'on', 'the', 'verge', 'of', 'lo', '##osing', 'my', 'job', 'again', '.', 'note', 'the', 'bankruptcy', 'isn', \"'\", 't', 'due', 'to', 'gambling', 'losses', ',', 'i', 'ni', '##pped', 'that', 'in', 'the', 'butt', 'when', 'i', 'realized', 'exactly', 'what', 'was', 'going', 'on', 'and', 'stopped', '.', 'but', 'from', 'bad', 'money', 'management', ',', 'forget', '##ing', 'to', 'pay', 'bills', 'tell', 'things', 'are', 'turned', 'off', ',', 'overs', '##pen', '##ding', 'on', 'impulse', 'buys', ',', 'having', 'to', 'pay', 'off', 'past', 'mistakes', '/', 'loans', '(', '5', 'car', 'rep', '##oses', '##sion', 'in', '2', 'years', ')', ',', 'replacement', 'of', 'lost', 'items', 'because', 'i', 'would', 'forget', 'and', 'leave', 'them', 'somewhere', '(', 'i', 'lost', 'count', 'on', 'how', 'many', 'phones', '/', 'wallet', '##s', '/', 'cash', '/', 'ne', '##rch', '##end', '##ise', 'i', \"'\", 've', 'left', 'somewhere', ')', '.', 'i', \"'\", 've', 'tried', 'experimenting', 'with', 'the', 'dos', '##age', 'levels', 'to', 'see', 'if', 'i', 'can', 'find', 'an', 'impact', 'point', ',', 'increasing', 'and', 'decreasing', 'at', '2', '.', '5', '##mg', 'at', 'a', 'time', 'for', 'periods', 'of', 'time', 'trying', 'to', 'find', 'a', 'level', 'that', 'has', 'an', 'impact', 'with', 'as', 'few', 'side', 'effects', 'as', 'possible', 'with', 'no', 'real', 'results', '.', 'that', 'is', 'all', 'the', 'details', 'i', 'can', 'think', 'of', 'to', 'share', 'that', 'may', 'help', '.', 'i', 'will', 'answer', 'questions', 'if', 'more', 'information', 'is', 'needed', 'but', 'i', \"'\", 'm', 'just', 'hoping', 'someone', 'here', 'can', 'give', 'me', 'some', 'insight', 'or', 'direction', '.', 'td', '##lr', ';', 'ad', '##hd', '-', 'c', '(', 'adult', 'late', 'diagnosis', ')', '.', 'severe', 'co', '##gna', '##tive', ',', 'impulse', 'and', 'memory', 'issues', 'looking', 'for', 'advice', 'on', 'medication', 'adjustment', '(', 'increase', '/', 'decrease', '/', 'alternatives', ')', 'to', 'reduce', 'symptoms']\n",
      "INFO:__main__:Number of tokens: 1242\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['ad', '##hd', '-', 'c', '(', 'adult', 'late', 'diagnosis', ')', '.', 'severe', 'co', '##gna', '##tive', ',', 'impulse', 'and', 'memory', 'issues', '(', 'first', 'red', '##dit', 'post', ')', 'i', \"'\", 've', 'been', 'a', 'long', 'red', '##dit', 'reader', 'but', 'i', \"'\", 've', 'never', 'posted', 'anywhere', '.', 'i', \"'\", 've', 'reached', 'a', 'point', 'now', 'of', 'desperation', 'that', 'i', 'am', 'reaching', 'out', 'to', 'the', 'red', '##dit', 'community', 'for', 'input', 'and', 'some', 'guidance', '.', 't', '##ld', '##r', 'will', 'follow', '.', 'i', 'was', 'diagnosed', 'with', 'add', '/', 'ad', '##hd', 'about', '3', 'years', 'ago', 'at', 'the', 'age', 'of', '36', 'with', 'no', '\"', 'real', '\"', 'ex', '##plin', '##ation', 'by', 'our', 'family', 'm', '##ft', 'after', 'she', 'determined', 'that', 'my', 'teenage', 'daughter', 'was', 'add', '.', 'i', 'was', 'per', '##scribe', '##d', 'add', '##eral', '##l', 'at', 'low', 'dos', '##age', '(', '5', '##mg', 'ir', ')', 'which', 'has', 'been', 'increased', 'about', 'ever', '9', 'months', 'for', 'the', 'last', '3', 'years', '(', 'at', '25', '##mg', ',', '15', 'in', 'the', 'am', '10', 'in', 'the', 'pm', ')', '.', 'the', 'past', '4', 'years', 'have', 'been', 'a', 'living', 'nightmare', '.', '2008', '-', '09', 'my', 'mother', ',', 'grandmother', 'and', 'grandfather', 'all', 'pass', 'away', 'over', 'a', '12', 'month', 'span', '(', 'oct', '08', '-', 'oct', '09', ')', 'i', 'have', 'been', 'fired', 'from', '4', 'jobs', 'normally', 'stemming', 'from', 'incomplete', 'or', 'poor', 'quality', 'of', 'work', ',', 'we', \"'\", 've', 'been', 'evicted', 'from', '2', 'apartments', '(', 'even', 'though', 'my', 'income', 'is', 'in', 'the', 'low', 'six', 'figures', ')', '.', 'my', 'imp', '##uls', '##ivity', 'and', 'ne', '##gt', '##ive', 'be', '##ha', '##vo', '##ir', 'patterns', 'have', 'grown', 'to', 'extremely', 'dan', '##go', '##ures', 'and', 'di', '##sas', '##tour', '##us', 'levels', '(', 'lo', '##osing', '6', '##k', 'in', '1', 'night', 'of', 'gambling', ',', 'having', 'sex', 'with', 'as', 'many', 'as', '4', '-', '5', 'strangers', '/', 'prostitutes', 'in', 'a', 'day', ')', '.', 'my', 'co', '##gna', '##tive', 'and', 'memory', 'rent', '##ent', '##ion', 'have', 'also', 'been', 'slipping', 'drastic', '##ly', '.', 'at', 'times', 'basic', 'math', '##matic', '##s', 'can', 'be', 'a', 'huge', 'problem', '(', 'simple', 'addition', 'and', 'sub', '##tra', '##ction', ')', '.', 'memory', 'is', 'becoming', 'a', 'huge', 'issue', '.', 'example', 'being', 'i', 'want', 'something', 'from', 'another', 'room', '(', 'water', ',', 'a', 'lighter', ',', 'a', 'piece', 'of', 'paper', ')', ',', 'i', \"'\", 'll', 'get', 'up', 'and', 'walk', 'the', 'the', 'other', 'room', '10', 'feet', 'away', 'and', 'not', 'recall', 'what', 'it', 'was', 'i', 'needed', ';', 'return', 'back', 'to', 'my', 'computer', ',', 'remember', '10', '-', '15', 'minutes', 'later', 'and', 'repeat', 'the', 'process', 'as', 'many', 'as', '4', '-', '5', 'times', '(', 'like', 'the', 'movie', 'moment', '##o', ')', '.', 'i', \"'\", 've', 'had', 'to', 'start', 'training', 'myself', 'to', 'walk', 'through', 'simple', 'processes', 'mentally', 'one', 'minute', 'step', 'at', 'a', 'time', 'just', 'to', 'be', 'able', 'to', 'perform', 'simple', 'tasks', '.', 'i', \"'\", 'm', 'hungry', ',', 'i', 'should', 'make', 'something', 'too', 'eat', ',', 'i', \"'\", 'll', 'make', 'soup', ',', 'i', 'need', 'a', 'bow', '##el', 'it', \"'\", 's', 'located', 'in', 'that', 'this', 'cub', '##bard', ',', 'i', 'need', 'a', 'spoon', ',', 'i', 'need', 'a', 'pot', 'to', 'cook', 'it', 'in', ',', 'etc', 'etc', '.', '.', '.', 'if', 'i', 'don', \"'\", 't', 'put', 'a', 'road', 'map', 'together', ',', 'i', 'can', 'get', 'lost', 'in', 'the', 'middle', 'of', 'a', 'process', '.', '(', 'example', 'i', \"'\", 'm', '32', 'minutes', 'into'], ['writing', 'this', 'post', ',', 'i', \"'\", 'm', 'having', 'to', 'google', 'words', 'fr', '##e', '##qua', '##ntly', 'to', 'remember', 'how', 'to', 'spell', 'them', ')', 'when', 'given', 'any', 'form', 'of', 'task', 'or', 'instruction', 'at', 'work', '(', 'programmer', 'by', 'trade', ')', 'i', 'have', 'to', 'write', 'cop', '##us', 'notes', ',', 'i', \"'\", 've', 'gotten', 'to', 'the', 'point', 'of', 'carrying', 'a', 'digital', 'recorder', 'just', 'to', 'save', 'time', 'and', 'make', 'sure', 'i', 'get', 'everything', '.', 'even', 'then', 'i', 'have', 'to', 'double', 'and', 'triple', 'check', 'both', 'my', 'work', 'and', 'my', 'notes', '.', 'long', 'term', 'memory', 'is', 'also', 'being', 'impacted', '.', 'i', 'don', \"'\", 't', 'recall', 'much', 'of', 'anything', 'before', 'the', 'age', 'of', '20', ',', 'recalling', 'specific', 'events', '/', 'people', 'take', 'heavy', 'concentration', '.', 'this', 'is', 'a', 'total', '180', 'of', 'how', 'i', 'use', 'to', 'be', ',', 'recalling', 'names', 'and', 'faces', 'from', '1', '-', '2nd', 'grade', '.', 'i', 'ran', 'into', 'a', 'friend', 'in', '06', 'from', 'element', '##ry', 'school', 'who', 'i', 'was', 'never', 'close', 'to', 'or', 'even', 'social', 'with', 'beyond', 'being', 'in', 'the', 'same', 'class', '.', 'the', 'person', 'is', 'now', 'transgender', '##ed', 'and', 'i', 'happen', 'to', 'see', 'her', 'in', 'passing', 'and', 'instantly', 'make', 'the', 'connection', 'to', 'who', 'she', 'use', 'to', 'be', '.', 'we', 'chat', '##ted', 'for', 'a', 'long', 'time', 'about', 'the', 'past', 'and', 'have', 'kept', 'in', 'touch', 'since', ',', 'so', 'there', 'is', 'no', 'false', 'identification', 'issue', '.', 'i', 'use', 'to', 'be', 'extremely', 'good', 'with', 'faces', 'and', 'memory', '.', 'when', 'many', 'of', 'these', 'things', 'happen', '(', 'memory', ',', 'co', '##gna', '##tive', 'processing', ',', 'lack', 'of', 'focus', 'on', 'any', 'task', ')', 'i', 'stop', 'the', 'task', 'and', 'go', 'into', 'a', 'negative', 'be', '##ha', '##vo', '##ir', '(', 'video', 'games', ',', 'gambling', ',', 'impulse', 'sex', ',', 'porn', ',', 'tv', ',', 'ditch', 'work', ',', 'etc', ')', '.', 'i', \"'\", 've', 'have', 'mentioned', 'these', 'things', 'to', 'both', 'my', 'm', '##ft', ',', 'my', 'psychiatric', 'mental', 'health', 'nurse', 'practitioner', ',', 'and', 'my', 'primary', 'care', 'dr', '.', 'the', 'res', '##pon', '##ce', 'i', 'typically', 'get', 'is', 'that', 'it', 'is', 'either', 'stress', ',', 'fatigue', ',', 'anxiety', ',', 'medication', 'side', 'effects', 'or', 'just', '\"', 'your', 'getting', 'older', '\"', '.', 'increasing', 'my', 'medication', 'doesn', \"'\", 't', 'really', 'seem', 'to', 'effect', 'my', 'mood', 'as', 'far', 'as', 'depression', 'or', 'mani', '##c', ',', 'but', 'it', 'does', 'dramatic', '##ly', 'effect', 'my', 'li', '##bid', '##o', '.', 'either', 'by', 'increasing', 'my', 'desire', 'levels', 'and', '/', 'or', 'impact', '##ing', 'my', 'ability', 'to', 'perform', '(', 'ed', ')', '.', 'i', 'have', 'noticed', 'that', 'different', 'types', 'of', 'specific', 'stress', 'also', 'are', 'effect', '##ing', 'which', 'impulse', '##s', 'i', 'act', 'on', '(', 'more', 'details', 'if', 'needed', ')', '.', 'i', \"'\", 'm', 'at', 'a', 'total', 'loss', 'as', 'to', 'what', 'to', 'do', ',', 'i', \"'\", 've', 'always', 'known', 'to', 'some', 'degree', 'that', 'i', 'was', 'add', '/', 'ad', '##hd', 'i', \"'\", 'm', 'at', 'a', 'loss', 'as', 'to', 'what', 'to', 'do', ',', 'we', \"'\", 've', 'reached', 'the', 'point', 'of', 'bankruptcy', ',', 'another', 'ev', '##iction', 'this', 'month', ',', 'criminal', 'charges', '(', 'case', 'pending', 'won', \"'\", 't', 'discuss', ')', 'and', 'on', 'the', 'verge', 'of', 'lo', '##osing', 'my', 'job', 'again', '.', 'note', 'the', 'bankruptcy', 'isn', \"'\", 't', 'due', 'to', 'gambling', 'losses', ',', 'i', 'ni', '##pped', 'that', 'in', 'the', 'butt', 'when', 'i', 'realized', 'exactly', 'what', 'was', 'going'], ['on', 'and', 'stopped', '.', 'but', 'from', 'bad', 'money', 'management', ',', 'forget', '##ing', 'to', 'pay', 'bills', 'tell', 'things', 'are', 'turned', 'off', ',', 'overs', '##pen', '##ding', 'on', 'impulse', 'buys', ',', 'having', 'to', 'pay', 'off', 'past', 'mistakes', '/', 'loans', '(', '5', 'car', 'rep', '##oses', '##sion', 'in', '2', 'years', ')', ',', 'replacement', 'of', 'lost', 'items', 'because', 'i', 'would', 'forget', 'and', 'leave', 'them', 'somewhere', '(', 'i', 'lost', 'count', 'on', 'how', 'many', 'phones', '/', 'wallet', '##s', '/', 'cash', '/', 'ne', '##rch', '##end', '##ise', 'i', \"'\", 've', 'left', 'somewhere', ')', '.', 'i', \"'\", 've', 'tried', 'experimenting', 'with', 'the', 'dos', '##age', 'levels', 'to', 'see', 'if', 'i', 'can', 'find', 'an', 'impact', 'point', ',', 'increasing', 'and', 'decreasing', 'at', '2', '.', '5', '##mg', 'at', 'a', 'time', 'for', 'periods', 'of', 'time', 'trying', 'to', 'find', 'a', 'level', 'that', 'has', 'an', 'impact', 'with', 'as', 'few', 'side', 'effects', 'as', 'possible', 'with', 'no', 'real', 'results', '.', 'that', 'is', 'all', 'the', 'details', 'i', 'can', 'think', 'of', 'to', 'share', 'that', 'may', 'help', '.', 'i', 'will', 'answer', 'questions', 'if', 'more', 'information', 'is', 'needed', 'but', 'i', \"'\", 'm', 'just', 'hoping', 'someone', 'here', 'can', 'give', 'me', 'some', 'insight', 'or', 'direction', '.', 'td', '##lr', ';', 'ad', '##hd', '-', 'c', '(', 'adult', 'late', 'diagnosis', ')', '.', 'severe', 'co', '##gna', '##tive', ',', 'impulse', 'and', 'memory', 'issues', 'looking', 'for', 'advice', 'on', 'medication', 'adjustment', '(', 'increase', '/', 'decrease', '/', 'alternatives', ')', 'to', 'reduce', 'symptoms']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'over', '##com', '##mit', '##ment', 'a', 'common', 'issue', 'related', 'to', 'ad', '##hd', ',', 'and', 'if', 'so', 'how', 'do', 'you', 'deal', 'with', 'it', '?', 'i', 'am', 'a', 'senior', 'in', 'college', 'studying', 'mechanical', 'engineering', '(', 'hopefully', 'graduating', 'in', 'august', '!', ')', ',', 'and', 'i', 'would', 'say', 'that', 'without', 'ex', '##agger', '##ation', 'i', 'have', 'sep', '##nt', '4', 'out', 'of', '5', 'schools', 'days', 'on', 'campus', 'working', 'from', '8', '##am', 'till', '1', 'am', 'for', 'the', 'last', '2', 'months', ',', 'and', 'its', 'getting', 'worse', 'as', 'the', 'end', 'of', 'the', 'semester', 'approaches', '(', 'i', 'have', 'projects', 'in', '4', 'of', 'my', '6', 'classes', ')', '.', 'i', 'am', 'aware', 'that', 'part', 'of', 'the', 'issue', 'lies', 'with', 'my', 'stubborn', '##ness', ',', 'an', 'inability', 'to', 'stop', 'working', ',', 'and', 'the', 'nature', 'of', 'my', 'medication', '(', 'i', 'take', 'day', '##tra', '##na', 'patches', 'that', 'are', 'supposed', 'to', 'last', '9', 'hours', ',', 'but', 'often', 'last', 'longer', ')', '.', 'however', ',', 'i', 'also', 'know', 'that', 'a', 'main', 'chunk', 'of', 'the', 'issue', 'is', 'that', 'i', 'took', 'on', 'more', 'classes', 'than', 'i', 'should', 'have', '.', 'in', 'this', 'case', 'i', 'needed', 'to', 'take', 'a', '1', 'credit', 'lab', 'that', 'is', 'impacted', '(', 'it', \"'\", 's', 'supposed', 'to', 'be', 'taken', 'fall', 'of', 'junior', 'year', ',', 'but', 'they', 'only', 'offer', 'half', 'as', 'many', 'seats', 'as', 'there', 'are', 'students', 'who', 'need', 'to', 'take', 'it', ')', ',', 'and', 'is', 'easily', '3', 'credits', 'worth', 'of', 'work', '.', 'the', 'main', 'reason', 'i', 'am', 'mentioning', 'all', 'of', 'this', 'is', 'that', 'i', 'know', 'i', 'have', 'had', 'problems', 'in', 'the', 'past', 'with', 'taking', 'on', 'extra', 'work', 'because', 'i', 'like', 'helping', 'others', ',', 'and', 'i', 'always', 'under', '##est', '##imate', 'how', 'long', 'stuff', 'will', 'take', 'me', 'to', 'do', '.', 'i', 'believe', 'that', 'this', 'is', 'related', 'to', 'impulse', 'control', 'difficulties', ',', 'and', 'am', 'curious', 'to', 'know', 'if', 'this', 'is', 'a', 'common', 'problem', 'among', 'people', 'with', 'ad', '##hd', ',', 'and', 'how', 'you', 'have', 'managed', 'to', 'deal', 'with', 'it', '.', 'right', 'now', 'i', 'am', 'very', 'worried', 'that', 'once', 'i', 'get', 'a', 'job', 'i', 'will', 'put', 'myself', 'under', 'too', 'much', 'stress', 'because', 'i', 'imp', '##ulsive', '##ly', 'take', 'on', 'extra', 'work', ',', 'then', 'am', 'too', 'proud', 'to', 'say', 'i', 'don', \"'\", 't', 'have', 'time', 'to', 'do', 'the', 'work', '.', '(', 'sorry', 'if', 'this', 'is', 'too', 'much', 'ram', '##bling', ',', 'i', 'will', 'try', 'to', 'find', 'time', 'tomorrow', 'to', 'clarify', 'any', 'questions', '.', ')', 'thank', 'you', 'for', 'your', 'time', '.', 'edit', ':', 'thank', 'you', 'all', 'for', 'the', 'info', '.', 'it', 'is', 'comforting', 'to', 'know', 'that', 'i', 'am', 'not', 'the', 'only', 'one', 'dealing', 'with', 'this', ',', 'and', 'i', 'think', 'i', 'will', 'be', 'able', 'to', 'make', 'some', 'good', 'rules', 'for', 'my', 'self', 'regarding', 'how', 'much', 'i', 'take', 'on', 'once', 'i', 'graduate', '.']\n",
      "INFO:__main__:Number of tokens: 433\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'over', '##com', '##mit', '##ment', 'a', 'common', 'issue', 'related', 'to', 'ad', '##hd', ',', 'and', 'if', 'so', 'how', 'do', 'you', 'deal', 'with', 'it', '?', 'i', 'am', 'a', 'senior', 'in', 'college', 'studying', 'mechanical', 'engineering', '(', 'hopefully', 'graduating', 'in', 'august', '!', ')', ',', 'and', 'i', 'would', 'say', 'that', 'without', 'ex', '##agger', '##ation', 'i', 'have', 'sep', '##nt', '4', 'out', 'of', '5', 'schools', 'days', 'on', 'campus', 'working', 'from', '8', '##am', 'till', '1', 'am', 'for', 'the', 'last', '2', 'months', ',', 'and', 'its', 'getting', 'worse', 'as', 'the', 'end', 'of', 'the', 'semester', 'approaches', '(', 'i', 'have', 'projects', 'in', '4', 'of', 'my', '6', 'classes', ')', '.', 'i', 'am', 'aware', 'that', 'part', 'of', 'the', 'issue', 'lies', 'with', 'my', 'stubborn', '##ness', ',', 'an', 'inability', 'to', 'stop', 'working', ',', 'and', 'the', 'nature', 'of', 'my', 'medication', '(', 'i', 'take', 'day', '##tra', '##na', 'patches', 'that', 'are', 'supposed', 'to', 'last', '9', 'hours', ',', 'but', 'often', 'last', 'longer', ')', '.', 'however', ',', 'i', 'also', 'know', 'that', 'a', 'main', 'chunk', 'of', 'the', 'issue', 'is', 'that', 'i', 'took', 'on', 'more', 'classes', 'than', 'i', 'should', 'have', '.', 'in', 'this', 'case', 'i', 'needed', 'to', 'take', 'a', '1', 'credit', 'lab', 'that', 'is', 'impacted', '(', 'it', \"'\", 's', 'supposed', 'to', 'be', 'taken', 'fall', 'of', 'junior', 'year', ',', 'but', 'they', 'only', 'offer', 'half', 'as', 'many', 'seats', 'as', 'there', 'are', 'students', 'who', 'need', 'to', 'take', 'it', ')', ',', 'and', 'is', 'easily', '3', 'credits', 'worth', 'of', 'work', '.', 'the', 'main', 'reason', 'i', 'am', 'mentioning', 'all', 'of', 'this', 'is', 'that', 'i', 'know', 'i', 'have', 'had', 'problems', 'in', 'the', 'past', 'with', 'taking', 'on', 'extra', 'work', 'because', 'i', 'like', 'helping', 'others', ',', 'and', 'i', 'always', 'under', '##est', '##imate', 'how', 'long', 'stuff', 'will', 'take', 'me', 'to', 'do', '.', 'i', 'believe', 'that', 'this', 'is', 'related', 'to', 'impulse', 'control', 'difficulties', ',', 'and', 'am', 'curious', 'to', 'know', 'if', 'this', 'is', 'a', 'common', 'problem', 'among', 'people', 'with', 'ad', '##hd', ',', 'and', 'how', 'you', 'have', 'managed', 'to', 'deal', 'with', 'it', '.', 'right', 'now', 'i', 'am', 'very', 'worried', 'that', 'once', 'i', 'get', 'a', 'job', 'i', 'will', 'put', 'myself', 'under', 'too', 'much', 'stress', 'because', 'i', 'imp', '##ulsive', '##ly', 'take', 'on', 'extra', 'work', ',', 'then', 'am', 'too', 'proud', 'to', 'say', 'i', 'don', \"'\", 't', 'have', 'time', 'to', 'do', 'the', 'work', '.', '(', 'sorry', 'if', 'this', 'is', 'too', 'much', 'ram', '##bling', ',', 'i', 'will', 'try', 'to', 'find', 'time', 'tomorrow', 'to', 'clarify', 'any', 'questions', '.', ')', 'thank', 'you', 'for', 'your', 'time', '.', 'edit', ':', 'thank', 'you', 'all', 'for', 'the', 'info', '.', 'it', 'is', 'comforting', 'to', 'know', 'that', 'i', 'am', 'not', 'the', 'only', 'one', 'dealing', 'with', 'this', ',', 'and', 'i', 'think', 'i', 'will', 'be', 'able', 'to', 'make', 'some', 'good', 'rules', 'for', 'my', 'self', 'regarding', 'how', 'much', 'i', 'take', 'on', 'once', 'i', 'graduate', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['un', '##tre', '##ated', 'childhood', 'ad', '##hd', 'rear', '##ing', 'its', 'ugly', 'head', '?', 'perhaps', '.']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['un', '##tre', '##ated', 'childhood', 'ad', '##hd', 'rear', '##ing', 'its', 'ugly', 'head', '?', 'perhaps', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'have', 'experience', 'with', 'a', 'ne', '##uro', '##logist', '?', 'i', 'am', 'going', 'to', 'be', 'seeing', 'a', 'ne', '##uro', '##logist', 'for', 'a', 'possible', 'sleep', 'disorder', 'and', 'i', 'was', 'wondering', 'if', 'he', 'could', 'be', 'able', 'to', 'test', 'for', 'ad', '##hd', 'or', 'treat', 'it', '?', 'what', 'i', \"'\", 've', 'been', 'telling', 'my', 'ps', '##ych', 'is', 'that', 'recently', 'i', \"'\", 've', 'had', 'small', 'sleep', 'attacks', 'that', 'sometimes', 'come', 'during', 'driving', '(', 'i', 'start', 'to', 'sw', '##ir', '##ve', 'but', 'i', 'never', 'actually', 'fall', 'asleep', ',', 'it', \"'\", 's', 'getting', 'a', 'lot', 'worse', ')', '.', 'sometimes', 'these', 'happen', 'a', 'lot', 'during', 'school', 'and', 'classes', ',', 'even', 'during', 'tests', '.', 'i', 'also', 'have', 'been', 'feeling', 'dr', '##ows', '##y', 'a', 'lot', '.', 'so', 'i', 'think', 'it', 'could', 'be', 'na', '##rco', '##le', '##psy', '.', 'but', 'i', 'feel', 'as', 'though', 'i', 'have', 'add', '/', 'ad', '##hd', 'as', 'well', '.', 'my', 'main', 'question', 'is', 'has', 'anyone', 'had', 'a', 'ne', '##uro', '##logist', 'dia', '##gno', '##se', 'them', 'with', 'add', '/', 'ad', '##hd', ',', 'how', 'long', 'was', 'the', 'process', 'or', 'test', '?']\n",
      "INFO:__main__:Number of tokens: 168\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'have', 'experience', 'with', 'a', 'ne', '##uro', '##logist', '?', 'i', 'am', 'going', 'to', 'be', 'seeing', 'a', 'ne', '##uro', '##logist', 'for', 'a', 'possible', 'sleep', 'disorder', 'and', 'i', 'was', 'wondering', 'if', 'he', 'could', 'be', 'able', 'to', 'test', 'for', 'ad', '##hd', 'or', 'treat', 'it', '?', 'what', 'i', \"'\", 've', 'been', 'telling', 'my', 'ps', '##ych', 'is', 'that', 'recently', 'i', \"'\", 've', 'had', 'small', 'sleep', 'attacks', 'that', 'sometimes', 'come', 'during', 'driving', '(', 'i', 'start', 'to', 'sw', '##ir', '##ve', 'but', 'i', 'never', 'actually', 'fall', 'asleep', ',', 'it', \"'\", 's', 'getting', 'a', 'lot', 'worse', ')', '.', 'sometimes', 'these', 'happen', 'a', 'lot', 'during', 'school', 'and', 'classes', ',', 'even', 'during', 'tests', '.', 'i', 'also', 'have', 'been', 'feeling', 'dr', '##ows', '##y', 'a', 'lot', '.', 'so', 'i', 'think', 'it', 'could', 'be', 'na', '##rco', '##le', '##psy', '.', 'but', 'i', 'feel', 'as', 'though', 'i', 'have', 'add', '/', 'ad', '##hd', 'as', 'well', '.', 'my', 'main', 'question', 'is', 'has', 'anyone', 'had', 'a', 'ne', '##uro', '##logist', 'dia', '##gno', '##se', 'them', 'with', 'add', '/', 'ad', '##hd', ',', 'how', 'long', 'was', 'the', 'process', 'or', 'test', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'think', 'that', 'i', 'have', 'could', 'possibly', 'have', 'inn', '##att', '##ent', '##ive', 'add', ',', 'but', 'i', 'have', 'some', 'questions', 'about', 'diagnosis', 'and', 'whether', 'or', 'not', 'i', 'truly', 'have', 'add']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'think', 'that', 'i', 'have', 'could', 'possibly', 'have', 'inn', '##att', '##ent', '##ive', 'add', ',', 'but', 'i', 'have', 'some', 'questions', 'about', 'diagnosis', 'and', 'whether', 'or', 'not', 'i', 'truly', 'have', 'add']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mental', 'awareness', 'week', 'just', 'wanted', 'to', 'say', '<', '3', 'to', 'all', 'of', 'you', 'for', 'being', 'able', 'to', 'recognize', 'your', 'condition', 'and', 'withstand', '##ing', 'all', 'the', 'pressure', 'that', 'society', 'puts', 'us', 'in', '.', 'if', 'they', 'were', 'aware', 'as', 'well', '.', '.', '.', 'they', 'would', 'understand', '.', 'keep', 'on', 'keep', '##in', 'on', 'guys', '.']\n",
      "INFO:__main__:Number of tokens: 52\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mental', 'awareness', 'week', 'just', 'wanted', 'to', 'say', '<', '3', 'to', 'all', 'of', 'you', 'for', 'being', 'able', 'to', 'recognize', 'your', 'condition', 'and', 'withstand', '##ing', 'all', 'the', 'pressure', 'that', 'society', 'puts', 'us', 'in', '.', 'if', 'they', 'were', 'aware', 'as', 'well', '.', '.', '.', 'they', 'would', 'understand', '.', 'keep', 'on', 'keep', '##in', 'on', 'guys', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'can', 'i', 'read', 'a', 'book', 'without', 'getting', 'distracted', '/', 'thinking', 'of', 'myself', 'reading', 'the', 'book', '?']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'can', 'i', 'read', 'a', 'book', 'without', 'getting', 'distracted', '/', 'thinking', 'of', 'myself', 'reading', 'the', 'book', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'adult', 'int', '##uni', '##v', 'users', 'out', 'there', '?', 'question', 'about', 'anxiety', 'and', 'interactions', 'with', 'other', 'med', '##s', 'i', 'take', '70', '##mg', 'of', 'v', '##y', '##van', '##se', 'with', '4', '##mg', 'of', 'int', '##uni', '##v', 'and', 'have', 'found', 'myself', 'feeling', 'more', 'and', 'more', 'anxious', 'and', 'depressed', 'than', 'i', 'used', 'to', 'be', '.', 'i', \"'\", 'm', 'wondering', 'if', 'any', 'of', 'you', 'have', 'taken', 'int', '##uni', '##v', 'and', 'had', 'any', 'similar', 'effects', '?', 'did', 'your', 'doctor', 'reduce', 'a', 'st', '##im', '##ula', '##nt', 'if', 'you', 'were', 'taking', 'it', 'with', 'the', 'int', '##uni', '##v', '?', 'i', 'was', 'on', 'the', 'v', '##y', '##van', '##se', 'for', 'over', 'a', 'year', 'but', 'the', 'int', '##uni', '##v', 'is', 'newer', '(', '2', 'months', '.', ')']\n",
      "INFO:__main__:Number of tokens: 114\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'adult', 'int', '##uni', '##v', 'users', 'out', 'there', '?', 'question', 'about', 'anxiety', 'and', 'interactions', 'with', 'other', 'med', '##s', 'i', 'take', '70', '##mg', 'of', 'v', '##y', '##van', '##se', 'with', '4', '##mg', 'of', 'int', '##uni', '##v', 'and', 'have', 'found', 'myself', 'feeling', 'more', 'and', 'more', 'anxious', 'and', 'depressed', 'than', 'i', 'used', 'to', 'be', '.', 'i', \"'\", 'm', 'wondering', 'if', 'any', 'of', 'you', 'have', 'taken', 'int', '##uni', '##v', 'and', 'had', 'any', 'similar', 'effects', '?', 'did', 'your', 'doctor', 'reduce', 'a', 'st', '##im', '##ula', '##nt', 'if', 'you', 'were', 'taking', 'it', 'with', 'the', 'int', '##uni', '##v', '?', 'i', 'was', 'on', 'the', 'v', '##y', '##van', '##se', 'for', 'over', 'a', 'year', 'but', 'the', 'int', '##uni', '##v', 'is', 'newer', '(', '2', 'months', '.', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['project', 'help', 'if', 'someone', 'who', 'has', 'been', 'diagnosed', 'with', 'ad', '##hd', 'would', 'be', 'willing', 'could', 'you', 'make', 'a', 'video', 'describing', 'any', 'troubles', 'you', 'have', 'had', 'with', 'this', 'disorder', '?', 'specifically', 'in', 'relation', 'to', 'having', 'a', 'lack', 'of', 'free', 'will', 'of', 'some', 'kind', '.', 'if', 'you', 'have', 'any', 'questions', 'about', 'the', 'project', ',', 'please', 'message', 'me', 'and', 'i', 'will', 'be', 'happy', 'to', 'provide', 'more', 'information', '.', 'i', 'would', 'be', 'perfectly', 'willing', 'to', 'give', 'you', 'credit', '.', 'this', 'isn', \"'\", 't', 'graduate', 'research', 'or', 'anything', 'just', 'a', 'college', 'project', '.']\n",
      "INFO:__main__:Number of tokens: 88\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['project', 'help', 'if', 'someone', 'who', 'has', 'been', 'diagnosed', 'with', 'ad', '##hd', 'would', 'be', 'willing', 'could', 'you', 'make', 'a', 'video', 'describing', 'any', 'troubles', 'you', 'have', 'had', 'with', 'this', 'disorder', '?', 'specifically', 'in', 'relation', 'to', 'having', 'a', 'lack', 'of', 'free', 'will', 'of', 'some', 'kind', '.', 'if', 'you', 'have', 'any', 'questions', 'about', 'the', 'project', ',', 'please', 'message', 'me', 'and', 'i', 'will', 'be', 'happy', 'to', 'provide', 'more', 'information', '.', 'i', 'would', 'be', 'perfectly', 'willing', 'to', 'give', 'you', 'credit', '.', 'this', 'isn', \"'\", 't', 'graduate', 'research', 'or', 'anything', 'just', 'a', 'college', 'project', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['diagnosed', 'with', 'ad', '##hd', '-', 'pi', 'after', '2', 'months', 'of', 'trying', 'to', 'figure', 'out', 'what', 'was', 'wrong', ',', 'filled', 'my', 'first', 'prescription', 'today', '.', 'not', 'sure', 'what', \"'\", 's', 'to', 'come', 'but', 'just', 'want', 'you', 'thank', 'you', 'guys', 'and', 'ask', 'that', 'you', 'wish', 'me', 'luck', '.']\n",
      "INFO:__main__:Number of tokens: 46\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['diagnosed', 'with', 'ad', '##hd', '-', 'pi', 'after', '2', 'months', 'of', 'trying', 'to', 'figure', 'out', 'what', 'was', 'wrong', ',', 'filled', 'my', 'first', 'prescription', 'today', '.', 'not', 'sure', 'what', \"'\", 's', 'to', 'come', 'but', 'just', 'want', 'you', 'thank', 'you', 'guys', 'and', 'ask', 'that', 'you', 'wish', 'me', 'luck', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['music', 'what', 'kind', 'of', 'music', 'do', 'you', 'all', 'listen', 'to', '?', 'i', 'find', 'stuff', 'like', '[', 'explosions', 'in', 'the', 'sky', ']', '(', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'v', '-', 'h', '##vr', '##0s', '##5', '##x', '##ee', ')', 'or', '[', 'si', '##gur', 'ro', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'nec', '##f', '##n', '-', 'cf', '##wl', '##k', ')', 'can', 'help', 'quiet', 'my', 'brain', 'down', 'amazingly', ',', 'like', 'a', 'damp', 'cloth', 'on', 'a', 'burn', '.']\n",
      "INFO:__main__:Number of tokens: 87\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['music', 'what', 'kind', 'of', 'music', 'do', 'you', 'all', 'listen', 'to', '?', 'i', 'find', 'stuff', 'like', '[', 'explosions', 'in', 'the', 'sky', ']', '(', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'v', '-', 'h', '##vr', '##0s', '##5', '##x', '##ee', ')', 'or', '[', 'si', '##gur', 'ro', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'nec', '##f', '##n', '-', 'cf', '##wl', '##k', ')', 'can', 'help', 'quiet', 'my', 'brain', 'down', 'amazingly', ',', 'like', 'a', 'damp', 'cloth', 'on', 'a', 'burn', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['changed', 'med', '##s', 'yesterday', ';', 'advice', '?', 'so', 'yesterday', 'i', 'saw', 'my', 'ps', '##ych', ';', 'explained', 'how', 'the', 'rita', '##lin', 'ir', \"'\", 's', 'i', \"'\", 'd', 'been', 'prescribed', 'last', 'month', 'were', 'working', 'better', 'than', 'i', 'expected', '.', 'unfortunately', 'when', 'the', 'effects', 'of', 'the', 'rita', '##lin', 'wore', 'off', 'i', 'experienced', 'mig', '##raine', '##s', '.', 'i', 'have', 'now', 'been', 'swapped', 'to', 'dex', '##amp', '##het', '##amine', 'and', 'i', 'can', \"'\", 't', 'believe', 'how', 'little', 'it', \"'\", 's', 'helping', '!', ':', '/', 'i', 'was', 'taking', 'on', 'average', '20', '##mg', 'rita', '##lin', 'in', 'two', 'doses', 'over', 'the', 'day', 'when', 'i', 'am', 'at', 'university', '.', 'now', 'yesterday', 'i', 'tried', 'taking', '5', '##mg', 'of', 'dex', '##amp', '##het', '##amine', 'only', 'to', 'find', 'that', 'it', 'barely', 'did', 'anything', '!', 'now', 'today', 'i', 'have', 'tried', 'taking', '10', '##mg', '(', '2', 'tablets', ')', 'at', 'once', 'and', 'i', 'did', 'experience', 'mild', 'stimulation', 'and', 'a', 'little', 'better', 'concentration', 'but', 'nothing', 'akin', 'to', 'that', 'of', 'the', 'rita', '##lin', 'dos', '##age', 'of', '10', '##mg', '(', '1', 'tablet', ')', '.', 'i', \"'\", 'm', 'tempted', 'to', 'try', 'and', 'study', 'tomorrow', 'and', 'take', '12', '.', '5', 'or', 'maybe', '15', '##mg', '(', 'the', 'maximum', 'in', 'which', 'my', 'ps', '##ych', 'suggested', ')', '.', 'have', 'any', 'of', 'you', 'experienced', 'dex', '##amp', '##het', '##amine', '(', 'dex', '##ed', '##rine', 'in', 'the', 'us', 'i', 'think', ')', 'to', 'be', 'less', 'effective', 'at', 'making', 'you', 'sit', 'down', 'and', 'get', 'shit', 'done', '?', 'either', 'way', ',', 'if', 'i', 'do', 'end', 'up', 'trying', 'the', '15', '##mg', 'dose', 'and', 'it', 'doesn', \"'\", 't', 'help', 'i', 'might', 'have', 'to', 'consult', 'my', 'ps', '##ych', 'because', 'i', \"'\", 'm', 'frankly', 'a', 'little', 'worried', 'about', 'dos', '##ing', 'this', 'high', 'when', 'i', \"'\", 've', 'just', 'been', 'prescribed', 'a', 'new', 'medication', '.', 'any', 'help', 'would', 'be', 'greatly', 'appreciated', ':', ')']\n",
      "INFO:__main__:Number of tokens: 285\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['changed', 'med', '##s', 'yesterday', ';', 'advice', '?', 'so', 'yesterday', 'i', 'saw', 'my', 'ps', '##ych', ';', 'explained', 'how', 'the', 'rita', '##lin', 'ir', \"'\", 's', 'i', \"'\", 'd', 'been', 'prescribed', 'last', 'month', 'were', 'working', 'better', 'than', 'i', 'expected', '.', 'unfortunately', 'when', 'the', 'effects', 'of', 'the', 'rita', '##lin', 'wore', 'off', 'i', 'experienced', 'mig', '##raine', '##s', '.', 'i', 'have', 'now', 'been', 'swapped', 'to', 'dex', '##amp', '##het', '##amine', 'and', 'i', 'can', \"'\", 't', 'believe', 'how', 'little', 'it', \"'\", 's', 'helping', '!', ':', '/', 'i', 'was', 'taking', 'on', 'average', '20', '##mg', 'rita', '##lin', 'in', 'two', 'doses', 'over', 'the', 'day', 'when', 'i', 'am', 'at', 'university', '.', 'now', 'yesterday', 'i', 'tried', 'taking', '5', '##mg', 'of', 'dex', '##amp', '##het', '##amine', 'only', 'to', 'find', 'that', 'it', 'barely', 'did', 'anything', '!', 'now', 'today', 'i', 'have', 'tried', 'taking', '10', '##mg', '(', '2', 'tablets', ')', 'at', 'once', 'and', 'i', 'did', 'experience', 'mild', 'stimulation', 'and', 'a', 'little', 'better', 'concentration', 'but', 'nothing', 'akin', 'to', 'that', 'of', 'the', 'rita', '##lin', 'dos', '##age', 'of', '10', '##mg', '(', '1', 'tablet', ')', '.', 'i', \"'\", 'm', 'tempted', 'to', 'try', 'and', 'study', 'tomorrow', 'and', 'take', '12', '.', '5', 'or', 'maybe', '15', '##mg', '(', 'the', 'maximum', 'in', 'which', 'my', 'ps', '##ych', 'suggested', ')', '.', 'have', 'any', 'of', 'you', 'experienced', 'dex', '##amp', '##het', '##amine', '(', 'dex', '##ed', '##rine', 'in', 'the', 'us', 'i', 'think', ')', 'to', 'be', 'less', 'effective', 'at', 'making', 'you', 'sit', 'down', 'and', 'get', 'shit', 'done', '?', 'either', 'way', ',', 'if', 'i', 'do', 'end', 'up', 'trying', 'the', '15', '##mg', 'dose', 'and', 'it', 'doesn', \"'\", 't', 'help', 'i', 'might', 'have', 'to', 'consult', 'my', 'ps', '##ych', 'because', 'i', \"'\", 'm', 'frankly', 'a', 'little', 'worried', 'about', 'dos', '##ing', 'this', 'high', 'when', 'i', \"'\", 've', 'just', 'been', 'prescribed', 'a', 'new', 'medication', '.', 'any', 'help', 'would', 'be', 'greatly', 'appreciated', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['switching', 'from', 'methyl', '##ph', '##eni', '##date', 'to', 'dex', '##tro', '##amp', '##het', '##amine', '.', 'what', 'can', 'i', 'expect', 'and', 'what', 'should', 'i', 'know', '?']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['switching', 'from', 'methyl', '##ph', '##eni', '##date', 'to', 'dex', '##tro', '##amp', '##het', '##amine', '.', 'what', 'can', 'i', 'expect', 'and', 'what', 'should', 'i', 'know', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sc', '##umb', '##ag', 'sub', '##red', '##dit', '.', 'source', 'of', 'support', 'for', 'those', 'with', 'add', '/', 'ad', '##hd', 'distract', 'me', 'from', 'my', 'work', '.']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sc', '##umb', '##ag', 'sub', '##red', '##dit', '.', 'source', 'of', 'support', 'for', 'those', 'with', 'add', '/', 'ad', '##hd', 'distract', 'me', 'from', 'my', 'work', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', \"'\", 't', 'keep', 'ignoring', 'it', ':', 'my', 'ad', '##hd', 'story']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', \"'\", 't', 'keep', 'ignoring', 'it', ':', 'my', 'ad', '##hd', 'story']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'jobs', 'have', 'you', 'guys', 'enjoyed', '?', 'right', 'now', 'i', 'am', 'working', 'with', 'my', 'family', 'and', 'it', 'can', 'be', 'a', 'real', 'struggle', 'because', 'i', 'have', 'to', 'do', 'a', 'lot', 'of', 'ted', '##ious', 'paperwork', '(', 'which', 'i', \"'\", 'm', 'sure', 'already', 'has', 'some', 'of', 'you', 'cr', '##inging', ')', '.', 'but', 'i', 'think', 'the', 'pay', '##off', 'will', 'be', 'worth', 'it', 'so', 'i', 'pl', '##ow', 'forward', '.', 'one', 'year', 'i', 'took', 'some', 'time', 'off', 'from', 'college', ',', 'i', 'had', 'a', 'job', 'as', 'a', 'move', '##r', '.', 'i', 'worked', '7', 'days', 'a', 'week', 'sometime', '12', '+', 'hours', '.', 'i', 'loved', 'it', '.', 'it', 'was', 'ti', '##ring', 'but', 'it', 'was', 'so', 'easy', 'to', 'see', 'the', 'progress', 'we', 'were', 'making', 'that', 'i', 'was', 'always', 'motivated', 'to', 'keep', 'moving', 'stuff', '.', 'and', 'the', 'days', 'just', 'flew', 'buy', '.', 'i', 'also', 'happened', 'to', 'be', 'working', 'right', 'before', 'the', 'bubble', 'burst', 'so', 'people', 'were', 'spending', 'money', 'like', 'mad', '##men', '.', 'so', 'how', 'about', 'it', '?', 'what', 'jobs', 'have', 'you', 'guys', 'enjoyed', '?']\n",
      "INFO:__main__:Number of tokens: 163\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'jobs', 'have', 'you', 'guys', 'enjoyed', '?', 'right', 'now', 'i', 'am', 'working', 'with', 'my', 'family', 'and', 'it', 'can', 'be', 'a', 'real', 'struggle', 'because', 'i', 'have', 'to', 'do', 'a', 'lot', 'of', 'ted', '##ious', 'paperwork', '(', 'which', 'i', \"'\", 'm', 'sure', 'already', 'has', 'some', 'of', 'you', 'cr', '##inging', ')', '.', 'but', 'i', 'think', 'the', 'pay', '##off', 'will', 'be', 'worth', 'it', 'so', 'i', 'pl', '##ow', 'forward', '.', 'one', 'year', 'i', 'took', 'some', 'time', 'off', 'from', 'college', ',', 'i', 'had', 'a', 'job', 'as', 'a', 'move', '##r', '.', 'i', 'worked', '7', 'days', 'a', 'week', 'sometime', '12', '+', 'hours', '.', 'i', 'loved', 'it', '.', 'it', 'was', 'ti', '##ring', 'but', 'it', 'was', 'so', 'easy', 'to', 'see', 'the', 'progress', 'we', 'were', 'making', 'that', 'i', 'was', 'always', 'motivated', 'to', 'keep', 'moving', 'stuff', '.', 'and', 'the', 'days', 'just', 'flew', 'buy', '.', 'i', 'also', 'happened', 'to', 'be', 'working', 'right', 'before', 'the', 'bubble', 'burst', 'so', 'people', 'were', 'spending', 'money', 'like', 'mad', '##men', '.', 'so', 'how', 'about', 'it', '?', 'what', 'jobs', 'have', 'you', 'guys', 'enjoyed', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'read', 'books', '?', 'please', ',', 'i', 'do', 'not', 'need', 'any', 'smart', '##ass', 'answers', 'like', ':', '\"', 'well', 'you', 'learn', 'the', 'alphabet', '.', '.', '.', '.', 'and', 'then', '.', '.', '.', '.', '\"', 'etc', '.', ':', ')', 'just', 'want', 'to', 'know', 'whether', 'or', 'not', 'you', 'guys', 'have', 'an', 'effective', 'method', 'for', 'reading', 'a', 'book', 'without', 'interruption', '.']\n",
      "INFO:__main__:Number of tokens: 59\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'read', 'books', '?', 'please', ',', 'i', 'do', 'not', 'need', 'any', 'smart', '##ass', 'answers', 'like', ':', '\"', 'well', 'you', 'learn', 'the', 'alphabet', '.', '.', '.', '.', 'and', 'then', '.', '.', '.', '.', '\"', 'etc', '.', ':', ')', 'just', 'want', 'to', 'know', 'whether', 'or', 'not', 'you', 'guys', 'have', 'an', 'effective', 'method', 'for', 'reading', 'a', 'book', 'without', 'interruption', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'i', 'find', 'out', 'which', 'ph', '##arm', '##acies', 'in', 'my', 'area', 'have', 'generic', 'add', '##eral', '##l', 'and', 'in', 'my', 'dos', '##age', '(', '15', '##mg', 'x', '##r', ')', '?', 'this', 'shortage', 'business', 'is', 'a', 'pain', 'in', 'the', 'ass', '.', 'and', 'there', \"'\", 's', 'no', 'way', 'i', 'can', 'continue', 'to', 'do', 'without', ',', 'or', 'to', 'pay', '$', '280', 'a', 'month', '.']\n",
      "INFO:__main__:Number of tokens: 60\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'i', 'find', 'out', 'which', 'ph', '##arm', '##acies', 'in', 'my', 'area', 'have', 'generic', 'add', '##eral', '##l', 'and', 'in', 'my', 'dos', '##age', '(', '15', '##mg', 'x', '##r', ')', '?', 'this', 'shortage', 'business', 'is', 'a', 'pain', 'in', 'the', 'ass', '.', 'and', 'there', \"'\", 's', 'no', 'way', 'i', 'can', 'continue', 'to', 'do', 'without', ',', 'or', 'to', 'pay', '$', '280', 'a', 'month', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['are', 'my', '\"', 'hearing', '\"', 'problems', 'add', 'or', 'ad', '##hd', '?', 'so', 'i', 'researched', 'add', ',', 'and', 'it', 'seems', 'like', 'i', 'only', 'have', 'two', 'symptoms', '.', 'one', 'is', 'i', 'can', 'hyper', '-', 'focus', 'when', 'i', 'play', 'computer', 'games', '/', 'watch', 'tv', '/', 'read', 'or', 'do', 'homework', 'i', \"'\", 'm', 'interested', 'in', '.', 'the', 'other', 'sy', '##mpt', '##om', ',', 'which', 'has', 'the', 'most', 'negative', 'effect', 'on', 'my', 'life', ',', 'is', 'my', 'inability', 'to', 'listen', 'to', 'people', '.', 'if', 'my', 'husband', 'talks', 'for', 'too', 'long', ',', 'i', 'zone', 'out', '.', 'if', 'he', 'starts', 'talking', 'to', 'me', 'while', 'i', \"'\", 'm', 'playing', 'a', 'video', 'game', ',', 'i', 'can', \"'\", 't', 'hear', 'him', '.', 'when', 'i', \"'\", 'm', 'at', 'work', 'and', 'someone', 'is', 'speaking', 'to', 'me', ',', 'even', 'if', 'it', \"'\", 's', 'important', 'and', 'i', \"'\", 'm', 'in', 'a', 'meeting', ',', 'i', 'often', 'ask', ',', '\"', 'could', 'you', 'repeat', 'what', 'you', 'just', 'said', '?', '\"', 'that', \"'\", 's', 'really', 'em', '##bara', '##ssing', '.', 'to', 'add', ':', 'this', 'has', 'been', 'going', 'on', 'since', 'i', 'was', 'a', 'kid', '.', 'my', 'first', 'grade', 'teacher', 'sent', 'me', 'to', 'get', 'my', 'hearing', 'tested', '.', 'my', 'kindergarten', 'teacher', 'said', 'on', 'my', 'report', 'card', ',', '\"', 'she', 'thinks', 'my', 'directions', 'in', 'class', 'don', \"'\", 't', 'apply', 'to', 'her', '.', '\"', 'in', 'high', 'school', ',', 'my', 'family', 'would', 'get', 'super', 'pissed', 'at', 'me', 'because', 'the', 'phone', 'would', 'ring', ',', 'and', 'i', 'wouldn', \"'\", 't', 'pick', 'it', 'up', '.', 'i', 'told', 'them', 'i', 'couldn', \"'\", 't', 'hear', 'it', ',', 'and', 'they', \"'\", 'd', 'call', 'me', 'a', 'liar', 'because', 'it', 'was', 'right', 'next', 'door', 'and', 'loud', '.', 'i', 'just', 'couldn', \"'\", 't', 'hear', 'it', 'because', 'i', 'was', 'watching', 'tv', ',', 'but', 'they', 'thought', 'that', 'was', 'a', 'bad', 'excuse', '.', 'should', 'i', 'go', 'ahead', 'and', 'see', 'a', 'psychiatrist', 'to', 'diagnosis', 'this', '?', 'does', 'this', 'even', 'seem', 'like', 'add', '?']\n",
      "INFO:__main__:Number of tokens: 304\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['are', 'my', '\"', 'hearing', '\"', 'problems', 'add', 'or', 'ad', '##hd', '?', 'so', 'i', 'researched', 'add', ',', 'and', 'it', 'seems', 'like', 'i', 'only', 'have', 'two', 'symptoms', '.', 'one', 'is', 'i', 'can', 'hyper', '-', 'focus', 'when', 'i', 'play', 'computer', 'games', '/', 'watch', 'tv', '/', 'read', 'or', 'do', 'homework', 'i', \"'\", 'm', 'interested', 'in', '.', 'the', 'other', 'sy', '##mpt', '##om', ',', 'which', 'has', 'the', 'most', 'negative', 'effect', 'on', 'my', 'life', ',', 'is', 'my', 'inability', 'to', 'listen', 'to', 'people', '.', 'if', 'my', 'husband', 'talks', 'for', 'too', 'long', ',', 'i', 'zone', 'out', '.', 'if', 'he', 'starts', 'talking', 'to', 'me', 'while', 'i', \"'\", 'm', 'playing', 'a', 'video', 'game', ',', 'i', 'can', \"'\", 't', 'hear', 'him', '.', 'when', 'i', \"'\", 'm', 'at', 'work', 'and', 'someone', 'is', 'speaking', 'to', 'me', ',', 'even', 'if', 'it', \"'\", 's', 'important', 'and', 'i', \"'\", 'm', 'in', 'a', 'meeting', ',', 'i', 'often', 'ask', ',', '\"', 'could', 'you', 'repeat', 'what', 'you', 'just', 'said', '?', '\"', 'that', \"'\", 's', 'really', 'em', '##bara', '##ssing', '.', 'to', 'add', ':', 'this', 'has', 'been', 'going', 'on', 'since', 'i', 'was', 'a', 'kid', '.', 'my', 'first', 'grade', 'teacher', 'sent', 'me', 'to', 'get', 'my', 'hearing', 'tested', '.', 'my', 'kindergarten', 'teacher', 'said', 'on', 'my', 'report', 'card', ',', '\"', 'she', 'thinks', 'my', 'directions', 'in', 'class', 'don', \"'\", 't', 'apply', 'to', 'her', '.', '\"', 'in', 'high', 'school', ',', 'my', 'family', 'would', 'get', 'super', 'pissed', 'at', 'me', 'because', 'the', 'phone', 'would', 'ring', ',', 'and', 'i', 'wouldn', \"'\", 't', 'pick', 'it', 'up', '.', 'i', 'told', 'them', 'i', 'couldn', \"'\", 't', 'hear', 'it', ',', 'and', 'they', \"'\", 'd', 'call', 'me', 'a', 'liar', 'because', 'it', 'was', 'right', 'next', 'door', 'and', 'loud', '.', 'i', 'just', 'couldn', \"'\", 't', 'hear', 'it', 'because', 'i', 'was', 'watching', 'tv', ',', 'but', 'they', 'thought', 'that', 'was', 'a', 'bad', 'excuse', '.', 'should', 'i', 'go', 'ahead', 'and', 'see', 'a', 'psychiatrist', 'to', 'diagnosis', 'this', '?', 'does', 'this', 'even', 'seem', 'like', 'add', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'many', 'ho', '##bbies', 'do', 'you', 'guys', 'have', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'many', 'ho', '##bbies', 'do', 'you', 'guys', 'have', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'know', 'anything', 'about', 'slug', '##gis', '##h', 'cognitive', 'tempo', 'disorder', '?', '(', 'sc', '##t', ')', 'i', 'have', 'been', 'watching', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##3d', '##1', '##sw', '##ux', '##mc', '##0', '&', 'feature', '=', 'player', '_', 'embedded', 'and', 'i', 'can', 'relate', 'to', 'a', 'lot', 'of', 'what', 'dr', '.', 'bark', '##ley', 'says', ',', 'does', 'anyone', 'else', 'out', 'there', 'think', 'they', 'may', 'have', 'sc', '##t', 'and', 'not', 'ad', '##hd', '-', 'pi', '?']\n",
      "INFO:__main__:Number of tokens: 80\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'know', 'anything', 'about', 'slug', '##gis', '##h', 'cognitive', 'tempo', 'disorder', '?', '(', 'sc', '##t', ')', 'i', 'have', 'been', 'watching', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##3d', '##1', '##sw', '##ux', '##mc', '##0', '&', 'feature', '=', 'player', '_', 'embedded', 'and', 'i', 'can', 'relate', 'to', 'a', 'lot', 'of', 'what', 'dr', '.', 'bark', '##ley', 'says', ',', 'does', 'anyone', 'else', 'out', 'there', 'think', 'they', 'may', 'have', 'sc', '##t', 'and', 'not', 'ad', '##hd', '-', 'pi', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['med', '##s', 'don', \"'\", 't', 'work', '.', 'why', '?', '?', 'hey', 'red', '##dit', '##ors', '.', 'so', 'recently', 'i', 'got', 'diagnosed', 'with', 'add', '.', 'its', 'not', 'as', 'bad', 'as', 'some', 'of', 'you', 'all', 'seem', 'to', 'have', ',', 'but', 'i', 'can', \"'\", 't', 'focus', 'in', 'class', 'or', 'on', 'homework', ',', 'you', 'know', 'with', 'stuff', 'i', \"'\", 'm', 'just', 'not', 'interested', 'in', '.', 'anyway', '##s', 'they', 'prescribed', 'my', 'some', 'med', '##s', 'which', 'i', 'spent', 'a', 'couple', 'months', 'mess', '##in', 'with', ',', '2', 'different', 'types', 'and', 'various', 'dos', '##ages', 'up', 'to', '20', '##mg', ',', 'but', 'with', 'very', 'little', 'effects', '.', 'a', 'few', 'times', 'while', 'i', 'was', 'on', '15', '##mg', 'of', 'focal', '##in', 'my', 'brain', 'seemed', 'to', 'turn', 'on', 'for', 'maybe', 'half', 'an', 'hour', 'in', 'the', 'morning', 'and', 'i', 'hyper', '##fo', '##cus', '##ed', 'in', 'on', 'a', 'math', 'lecture', '.', 'other', 'than', 'that', ',', 'which', 'had', 'never', 'happened', 'before', 'on', 'or', 'off', 'med', '##s', ',', 'nothing', '.', 'the', 'only', 'purpose', 'i', 'have', 'for', 'them', 'is', 'that', 'they', 'stop', 'me', 'from', 'falling', 'asleep', 'for', 'their', 'duration', '.', 'the', 'doc', '##s', 'are', 'per', '##plex', '##ed', ',', 'they', 'said', 'mess', 'around', 'more', 'with', 'the', 'drugs', ',', 'and', 'also', 'that', 'cases', 'like', 'this', 'pop', 'up', 'a', 'good', 'amount', '.', 'somebody', 'else', 'has', 'to', 'be', 'experiencing', 'this', '.', 'am', 'i', 'not', 'add', '?', 'should', 'i', 'go', 'higher', 'doses', '?', 't', '##ld', '##r', '(', 'i', 'imagine', 'this', 'is', 'more', 'necessary', 'on', 'a', 'sub', '##red', '##dit', 'like', 'this', ':', 'd', ')', '-', 'diagnosed', 'with', 'not', 'to', 'terrible', 'add', ',', 'but', 'after', 'messing', 'with', 'med', '##s', 'nothing', 'improves', 'focus', '.', 'why', '?']\n",
      "INFO:__main__:Number of tokens: 258\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['med', '##s', 'don', \"'\", 't', 'work', '.', 'why', '?', '?', 'hey', 'red', '##dit', '##ors', '.', 'so', 'recently', 'i', 'got', 'diagnosed', 'with', 'add', '.', 'its', 'not', 'as', 'bad', 'as', 'some', 'of', 'you', 'all', 'seem', 'to', 'have', ',', 'but', 'i', 'can', \"'\", 't', 'focus', 'in', 'class', 'or', 'on', 'homework', ',', 'you', 'know', 'with', 'stuff', 'i', \"'\", 'm', 'just', 'not', 'interested', 'in', '.', 'anyway', '##s', 'they', 'prescribed', 'my', 'some', 'med', '##s', 'which', 'i', 'spent', 'a', 'couple', 'months', 'mess', '##in', 'with', ',', '2', 'different', 'types', 'and', 'various', 'dos', '##ages', 'up', 'to', '20', '##mg', ',', 'but', 'with', 'very', 'little', 'effects', '.', 'a', 'few', 'times', 'while', 'i', 'was', 'on', '15', '##mg', 'of', 'focal', '##in', 'my', 'brain', 'seemed', 'to', 'turn', 'on', 'for', 'maybe', 'half', 'an', 'hour', 'in', 'the', 'morning', 'and', 'i', 'hyper', '##fo', '##cus', '##ed', 'in', 'on', 'a', 'math', 'lecture', '.', 'other', 'than', 'that', ',', 'which', 'had', 'never', 'happened', 'before', 'on', 'or', 'off', 'med', '##s', ',', 'nothing', '.', 'the', 'only', 'purpose', 'i', 'have', 'for', 'them', 'is', 'that', 'they', 'stop', 'me', 'from', 'falling', 'asleep', 'for', 'their', 'duration', '.', 'the', 'doc', '##s', 'are', 'per', '##plex', '##ed', ',', 'they', 'said', 'mess', 'around', 'more', 'with', 'the', 'drugs', ',', 'and', 'also', 'that', 'cases', 'like', 'this', 'pop', 'up', 'a', 'good', 'amount', '.', 'somebody', 'else', 'has', 'to', 'be', 'experiencing', 'this', '.', 'am', 'i', 'not', 'add', '?', 'should', 'i', 'go', 'higher', 'doses', '?', 't', '##ld', '##r', '(', 'i', 'imagine', 'this', 'is', 'more', 'necessary', 'on', 'a', 'sub', '##red', '##dit', 'like', 'this', ':', 'd', ')', '-', 'diagnosed', 'with', 'not', 'to', 'terrible', 'add', ',', 'but', 'after', 'messing', 'with', 'med', '##s', 'nothing', 'improves', 'focus', '.', 'why', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['questions', 'for', 'over', '##weight', 'ad', '##hd', '##ers', 'i', \"'\", 'm', 'talking', 'to', 'my', 'primary', 'care', 'doctor', 'about', 'the', 'symptoms', 'i', \"'\", 've', 'been', 'having', 'since', 'i', 'was', 'very', 'young', '.', '*', '*', 'are', 'there', 'any', 'concerns', 'a', 'doctor', 'may', 'have', 'specifically', 'for', 'over', '##weight', 'patients', 'related', 'to', 'this', ',', 'such', 'as', 'possible', 'other', 'causes', 'or', 'medication', 'concerns', '?', 'also', ',', 'do', 'you', 'feel', 'like', 'the', 'smile', '##r', 'stigma', '##s', 'about', 'the', 'symptoms', 'of', 'ad', '##hd', 'and', 'being', 'fat', '(', 'ie', 'lazy', ',', 'stupid', ',', 'no', 'self', 'control', ')', 'have', 'hit', 'you', 'especially', 'hard', '?', '*', '*', 'i', 'know', 'that', 'before', 'i', 'thought', 'of', 'this', 'as', 'a', 'possibility', ',', 'i', 'always', 'felt', 'like', 'my', 'weight', 'was', 'a', 'physical', 'representation', 'of', 'my', 'own', 'stupidity', ',', 'even', 'knowing', 'from', 'standardized', 'tests', 'and', 'my', 'own', 'way', 'of', 'thinking', 'that', 'i', 'am', 'well', '-', 'above', 'average', '.']\n",
      "INFO:__main__:Number of tokens: 142\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['questions', 'for', 'over', '##weight', 'ad', '##hd', '##ers', 'i', \"'\", 'm', 'talking', 'to', 'my', 'primary', 'care', 'doctor', 'about', 'the', 'symptoms', 'i', \"'\", 've', 'been', 'having', 'since', 'i', 'was', 'very', 'young', '.', '*', '*', 'are', 'there', 'any', 'concerns', 'a', 'doctor', 'may', 'have', 'specifically', 'for', 'over', '##weight', 'patients', 'related', 'to', 'this', ',', 'such', 'as', 'possible', 'other', 'causes', 'or', 'medication', 'concerns', '?', 'also', ',', 'do', 'you', 'feel', 'like', 'the', 'smile', '##r', 'stigma', '##s', 'about', 'the', 'symptoms', 'of', 'ad', '##hd', 'and', 'being', 'fat', '(', 'ie', 'lazy', ',', 'stupid', ',', 'no', 'self', 'control', ')', 'have', 'hit', 'you', 'especially', 'hard', '?', '*', '*', 'i', 'know', 'that', 'before', 'i', 'thought', 'of', 'this', 'as', 'a', 'possibility', ',', 'i', 'always', 'felt', 'like', 'my', 'weight', 'was', 'a', 'physical', 'representation', 'of', 'my', 'own', 'stupidity', ',', 'even', 'knowing', 'from', 'standardized', 'tests', 'and', 'my', 'own', 'way', 'of', 'thinking', 'that', 'i', 'am', 'well', '-', 'above', 'average', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', '&', 'well', '##bu', '##tri', '##n', 'don', \"'\", 't', 'play', 'nicely', 'together', '.', 'i', \"'\", 'm', 'newly', 'diagnosed', 'with', 'add', ',', 'and', 'have', 'been', 'on', 'well', '##bu', '##tri', '##n', 'for', '10', 'years', '.', 'i', \"'\", 've', 'been', 'taking', '300', '##mg', 'well', '##bu', '##tri', '##nx', '##l', 'and', 'my', 'doctor', 'started', 'me', 'on', '20', '##mg', 'add', '##eral', '##l', '(', 'ir', ')', '.', 'the', 'paperwork', 'warned', 'of', 'an', 'interaction', 'with', 'well', '##bu', '##tri', '##n', '(', 'bu', '##pro', '##pr', '##ion', ')', 'and', 'they', 'weren', \"'\", 't', 'kidding', '.', 'i', 'thought', 'that', 'it', 'wouldn', \"'\", 't', 'be', 'a', 'big', 'deal', ',', 'especially', 'with', 'the', 'ir', 'dose', '.', 'nope', '.', '20', '##mg', 'add', '##eral', '##l', 'taken', 'at', '6', 'am', 'keep', 'me', 'wide', 'awake', 'until', '2', 'am', '.', 'i', 'felt', 'absolutely', 'terrible', 'last', 'night', '.', 'brain', 'was', 'on', 'fire', '.', 'physical', 'pain', 'ran', 'in', 'bands', 'across', 'my', 'head', '.', 'had', 'to', 'use', 'an', 'ice', 'pack', 'to', 'get', 'any', 'sort', 'of', 'relief', '.', 'could', 'this', 'have', 'been', 'withdrawal', '?', 'seems', 'unlikely', 'with', 'only', 'two', 'doses', '.', 'anyone', 'here', 'have', 'experience', 'with', 'add', '##eral', '##l', 'and', 'well', '##bu', '##tri', '##n', ',', 'and', 'have', 'any', 'advice', '?', 'skipped', 'my', 'well', '##bu', '##tri', '##n', 'today', ',', 'and', 'i', \"'\", 'll', 'see', 'what', 'that', 'does', '.', 'but', 'it', 'does', 'seem', 'to', 'work', ',', 'at', 'least', 'during', 'the', 'day', '.', 'i', \"'\", 've', 'been', 'productive', '.', 'i', \"'\", 'm', 'getting', 'stuff', 'done', '.', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 234\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', '&', 'well', '##bu', '##tri', '##n', 'don', \"'\", 't', 'play', 'nicely', 'together', '.', 'i', \"'\", 'm', 'newly', 'diagnosed', 'with', 'add', ',', 'and', 'have', 'been', 'on', 'well', '##bu', '##tri', '##n', 'for', '10', 'years', '.', 'i', \"'\", 've', 'been', 'taking', '300', '##mg', 'well', '##bu', '##tri', '##nx', '##l', 'and', 'my', 'doctor', 'started', 'me', 'on', '20', '##mg', 'add', '##eral', '##l', '(', 'ir', ')', '.', 'the', 'paperwork', 'warned', 'of', 'an', 'interaction', 'with', 'well', '##bu', '##tri', '##n', '(', 'bu', '##pro', '##pr', '##ion', ')', 'and', 'they', 'weren', \"'\", 't', 'kidding', '.', 'i', 'thought', 'that', 'it', 'wouldn', \"'\", 't', 'be', 'a', 'big', 'deal', ',', 'especially', 'with', 'the', 'ir', 'dose', '.', 'nope', '.', '20', '##mg', 'add', '##eral', '##l', 'taken', 'at', '6', 'am', 'keep', 'me', 'wide', 'awake', 'until', '2', 'am', '.', 'i', 'felt', 'absolutely', 'terrible', 'last', 'night', '.', 'brain', 'was', 'on', 'fire', '.', 'physical', 'pain', 'ran', 'in', 'bands', 'across', 'my', 'head', '.', 'had', 'to', 'use', 'an', 'ice', 'pack', 'to', 'get', 'any', 'sort', 'of', 'relief', '.', 'could', 'this', 'have', 'been', 'withdrawal', '?', 'seems', 'unlikely', 'with', 'only', 'two', 'doses', '.', 'anyone', 'here', 'have', 'experience', 'with', 'add', '##eral', '##l', 'and', 'well', '##bu', '##tri', '##n', ',', 'and', 'have', 'any', 'advice', '?', 'skipped', 'my', 'well', '##bu', '##tri', '##n', 'today', ',', 'and', 'i', \"'\", 'll', 'see', 'what', 'that', 'does', '.', 'but', 'it', 'does', 'seem', 'to', 'work', ',', 'at', 'least', 'during', 'the', 'day', '.', 'i', \"'\", 've', 'been', 'productive', '.', 'i', \"'\", 'm', 'getting', 'stuff', 'done', '.', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '/', 'ad', '##hd', ',', 'attention', 'disorders', 'eyes', '##ight', ',', 'vision', ',', 'diagnosis', ',', 'treatments']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '/', 'ad', '##hd', ',', 'attention', 'disorders', 'eyes', '##ight', ',', 'vision', ',', 'diagnosis', ',', 'treatments']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'help', 'with', 'my', 'habits', '.', 'i', 'have', 'tried', 'so', 'many', 'different', 'ways', 'to', 'forge', 'good', 'habits', 'and', 'break', 'bad', 'ones', ',', 'but', 'it', 'is', 'so', 'difficult', '.', 'i', \"'\", 'm', 'certain', 'that', 'ad', '##hd', 'plays', 'a', 'role', 'in', 'the', 'difficulty', 'i', 'have', 'with', 'habits', ',', 'so', 'i', 'wanted', 'to', 'ask', 'you', 'all', 'about', 'techniques', 'you', \"'\", 've', 'found', 'useful', '.', 'how', 'can', 'i', 'create', 'good', 'habits', 'for', 'myself', 'in', 'a', 'strategic', 'way', '?']\n",
      "INFO:__main__:Number of tokens: 75\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'help', 'with', 'my', 'habits', '.', 'i', 'have', 'tried', 'so', 'many', 'different', 'ways', 'to', 'forge', 'good', 'habits', 'and', 'break', 'bad', 'ones', ',', 'but', 'it', 'is', 'so', 'difficult', '.', 'i', \"'\", 'm', 'certain', 'that', 'ad', '##hd', 'plays', 'a', 'role', 'in', 'the', 'difficulty', 'i', 'have', 'with', 'habits', ',', 'so', 'i', 'wanted', 'to', 'ask', 'you', 'all', 'about', 'techniques', 'you', \"'\", 've', 'found', 'useful', '.', 'how', 'can', 'i', 'create', 'good', 'habits', 'for', 'myself', 'in', 'a', 'strategic', 'way', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dealing', 'with', 'people', 'who', 'think', 'ad', '##hd', 'is', 'fake', 'and', 'other', 'problems', '?']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dealing', 'with', 'people', 'who', 'think', 'ad', '##hd', 'is', 'fake', 'and', 'other', 'problems', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mod', '##af', '##ini', '##l', 'is', 'a', 'drug', 'that', 'might', 'be', 'effective', 'for', 'treating', 'ad', '##hd', '.', 'no', 'prescription', 'required', '(', 'grey', 'market', ')', '.', 'if', 'you', \"'\", 've', 'tried', 'it', ',', 'share', 'your', 'stories', '!', '(', 'click', 'x', '-', 'post', 'link', ')']\n",
      "INFO:__main__:Number of tokens: 42\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mod', '##af', '##ini', '##l', 'is', 'a', 'drug', 'that', 'might', 'be', 'effective', 'for', 'treating', 'ad', '##hd', '.', 'no', 'prescription', 'required', '(', 'grey', 'market', ')', '.', 'if', 'you', \"'\", 've', 'tried', 'it', ',', 'share', 'your', 'stories', '!', '(', 'click', 'x', '-', 'post', 'link', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hil']\n",
      "INFO:__main__:Number of tokens: 2\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hil']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['an', 'interesting', 'study', 'related', 'to', 'add', '##eral', '##l', '?', 'the', 'lev', '##o', 'en', '##ant', '##iom', '##er', 'of', 'amp', '##het', '##amine', 'increases', 'memory', 'consolidation', 'and', 'gene', 'expression', 'in', 'the', 'hip', '##po', '##camp', '##us', 'without', 'producing', 'loco', '##moto', '##r', 'stimulation', '.']\n",
      "INFO:__main__:Number of tokens: 39\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['an', 'interesting', 'study', 'related', 'to', 'add', '##eral', '##l', '?', 'the', 'lev', '##o', 'en', '##ant', '##iom', '##er', 'of', 'amp', '##het', '##amine', 'increases', 'memory', 'consolidation', 'and', 'gene', 'expression', 'in', 'the', 'hip', '##po', '##camp', '##us', 'without', 'producing', 'loco', '##moto', '##r', 'stimulation', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 'off', 'of', 'the', 'med', '##s']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 'off', 'of', 'the', 'med', '##s']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', 'over', '-', 'med', '##icate', '?', 'i', \"'\", 've', 'noticed', 'i', 'tend', 'to', 'over', '-', 'med', '##icate', 'quite', 'a', 'lot', 'when', 'i', \"'\", 'm', 'working', 'on', 'an', 'important', 'or', 'high', '-', 'profile', 'project', 'that', 'requires', 'an', 'insane', 'amount', 'of', 'focus', '(', 'i', \"'\", 'm', 'a', 'software', 'engineer', '-', '-', 'if', 'i', 'can', \"'\", 't', 'focus', '/', 'concentrate', ',', 'i', \"'\", 'm', 'useless', ')', '.', 'it', 'always', 'seems', 'like', 'a', 'good', 'idea', 'at', 'the', 'time', ':', '*', '\"', 'just', 'another', 'half', ',', 'just', 'to', 'get', 'a', 'bit', 'more', 'of', 'a', 'peak', 'to', 'keep', 'working', '.', '.', '.', '\"', '*', 'but', 'it', 'always', 'comes', 'back', 'to', 'bite', 'me', 'in', 'the', 'ass', '.', 'you', 'tend', 'to', 'chase', 'an', 'endless', 'high', ',', 'then', 'realize', 'you', \"'\", 've', 'been', 'awake', 'for', '30', 'hours', 'and', 'can', \"'\", 't', 'sleep', 'for', 'another', '15', '.', 'i', \"'\", 've', 'started', 'taking', 'my', 'exact', 'daily', 'prescription', '(', '3', '##x', '##20', '##mg', 'add', '##eral', '##l', ',', 'i', 'take', 'one', 'after', 'each', 'meal', ')', 'to', 'work', '(', 'instead', 'of', 'keeping', 'my', 'entire', 'bottle', 'in', 'my', 'bag', ')', '.', 'it', \"'\", 's', 'incredibly', 'frustrating', 'towards', 'the', 'end', 'of', 'the', 'evening', '/', 'day', 'when', 'i', \"'\", 'm', 'out', 'of', 'med', '##s', 'but', 'get', 'that', '*', 'ar', '##gh', ',', 'if', 'only', 'i', 'had', 'another', 'half', 'i', 'could', 'get', 'a', 'few', 'more', 'hours', 'work', 'done', '!', '*', 'feeling', ',', 'but', 'it', \"'\", 's', 'definitely', 'better', 'if', 'you', 'view', 'my', 'productivity', 'from', 'the', 'perspective', 'of', 'the', 'entire', 'week', '.', 'does', 'anyone', 'else', 'consciously', 'or', 'inadvertently', 'over', '-', 'med', '##icate', '?']\n",
      "INFO:__main__:Number of tokens: 254\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', 'over', '-', 'med', '##icate', '?', 'i', \"'\", 've', 'noticed', 'i', 'tend', 'to', 'over', '-', 'med', '##icate', 'quite', 'a', 'lot', 'when', 'i', \"'\", 'm', 'working', 'on', 'an', 'important', 'or', 'high', '-', 'profile', 'project', 'that', 'requires', 'an', 'insane', 'amount', 'of', 'focus', '(', 'i', \"'\", 'm', 'a', 'software', 'engineer', '-', '-', 'if', 'i', 'can', \"'\", 't', 'focus', '/', 'concentrate', ',', 'i', \"'\", 'm', 'useless', ')', '.', 'it', 'always', 'seems', 'like', 'a', 'good', 'idea', 'at', 'the', 'time', ':', '*', '\"', 'just', 'another', 'half', ',', 'just', 'to', 'get', 'a', 'bit', 'more', 'of', 'a', 'peak', 'to', 'keep', 'working', '.', '.', '.', '\"', '*', 'but', 'it', 'always', 'comes', 'back', 'to', 'bite', 'me', 'in', 'the', 'ass', '.', 'you', 'tend', 'to', 'chase', 'an', 'endless', 'high', ',', 'then', 'realize', 'you', \"'\", 've', 'been', 'awake', 'for', '30', 'hours', 'and', 'can', \"'\", 't', 'sleep', 'for', 'another', '15', '.', 'i', \"'\", 've', 'started', 'taking', 'my', 'exact', 'daily', 'prescription', '(', '3', '##x', '##20', '##mg', 'add', '##eral', '##l', ',', 'i', 'take', 'one', 'after', 'each', 'meal', ')', 'to', 'work', '(', 'instead', 'of', 'keeping', 'my', 'entire', 'bottle', 'in', 'my', 'bag', ')', '.', 'it', \"'\", 's', 'incredibly', 'frustrating', 'towards', 'the', 'end', 'of', 'the', 'evening', '/', 'day', 'when', 'i', \"'\", 'm', 'out', 'of', 'med', '##s', 'but', 'get', 'that', '*', 'ar', '##gh', ',', 'if', 'only', 'i', 'had', 'another', 'half', 'i', 'could', 'get', 'a', 'few', 'more', 'hours', 'work', 'done', '!', '*', 'feeling', ',', 'but', 'it', \"'\", 's', 'definitely', 'better', 'if', 'you', 'view', 'my', 'productivity', 'from', 'the', 'perspective', 'of', 'the', 'entire', 'week', '.', 'does', 'anyone', 'else', 'consciously', 'or', 'inadvertently', 'over', '-', 'med', '##icate', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'help', 'finding', 'online', 'readings', 'for', 'using', 'marijuana', 'to', 'help', 'with', 'add', 'after', 'reading', 'about', 'add', 'i', 'came', 'to', 'the', 'conclusion', 'that', 'i', 'have', 'a', 'problem', '.', 'weird', ',', 'because', 'i', \"'\", 've', 'been', 'using', 'marijuana', 'and', 'i', 'have', 'clearly', 'noticed', 'how', 'it', 'can', 'help', 'me', 'feel', 'normal', 'when', 'it', 'comes', 'to', 'concentration', 'yet', ',', 'i', 'clearly', 'never', 'wanted', 'to', 'accept', 'it', '.', 'i', 'would', 'like', 'help', 'finding', 'information', 'on', 'how', 'to', 'use', 'marijuana', 'to', 'best', 'help', 'my', 'illness', '.', 'add', 'not', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 85\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'help', 'finding', 'online', 'readings', 'for', 'using', 'marijuana', 'to', 'help', 'with', 'add', 'after', 'reading', 'about', 'add', 'i', 'came', 'to', 'the', 'conclusion', 'that', 'i', 'have', 'a', 'problem', '.', 'weird', ',', 'because', 'i', \"'\", 've', 'been', 'using', 'marijuana', 'and', 'i', 'have', 'clearly', 'noticed', 'how', 'it', 'can', 'help', 'me', 'feel', 'normal', 'when', 'it', 'comes', 'to', 'concentration', 'yet', ',', 'i', 'clearly', 'never', 'wanted', 'to', 'accept', 'it', '.', 'i', 'would', 'like', 'help', 'finding', 'information', 'on', 'how', 'to', 'use', 'marijuana', 'to', 'best', 'help', 'my', 'illness', '.', 'add', 'not', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', \"'\", 's', 'going', 'on', 'with', 'brand', 'name', 'vs', 'generic', 'add', '##eral', '##l', 'and', 'insurance', '?', 'today', ',', 'i', 'went', 'to', 'pick', 'up', 'a', 'prescription', 'and', 'this', 'time', ',', 'for', 'some', 'reason', ',', 'insurance', 'wasn', \"'\", 't', 'going', 'to', 'cover', 'a', '60', 'day', 'supply', 'of', 'generic', 'add', '##eral', '##l', '.', 'however', ',', 'brand', 'name', 'add', '##eral', '##l', 'was', 'covered', 'and', 'i', 'got', 'a', '60', 'day', 'supply', 'for', '$', '40', '.', 'i', 'was', 'kind', 'of', 'confused', 'because', 'the', 'last', 'time', 'i', 'got', 'a', '60', 'day', 'supply', ',', 'insurance', 'had', 'covered', 'the', 'generic', '.', 'the', 'time', 'before', 'that', ',', 'insurance', 'only', 'covered', 'brand', 'name', '.', 'out', 'of', 'curiosity', ',', 'i', 'asked', 'how', 'much', 'the', 'generic', 'would', 'cost', 'me', '.', 'the', 'ph', '##arm', '##ac', '##ist', 'said', 'it', 'would', 'cost', 'me', 'upwards', 'of', '$', '300', 'only', 'pay', '##able', 'in', 'cash', '(', 'w', '##tf', '?', ')', '.', 'is', 'my', 'insurance', 'switching', 'my', 'coverage', 'every', '2', 'months', 'or', 'is', 'wal', '##gree', '##ns', 'just', 'mis', '##in', '##formed', '?', 't', '##l', ';', 'dr', ':', 'insurance', 'keeps', 'changing', 'its', 'mind', 'about', 'whether', 'or', 'not', 'it', 'will', 'cover', 'generic', 'add', '##eral', '##l', '.']\n",
      "INFO:__main__:Number of tokens: 183\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', \"'\", 's', 'going', 'on', 'with', 'brand', 'name', 'vs', 'generic', 'add', '##eral', '##l', 'and', 'insurance', '?', 'today', ',', 'i', 'went', 'to', 'pick', 'up', 'a', 'prescription', 'and', 'this', 'time', ',', 'for', 'some', 'reason', ',', 'insurance', 'wasn', \"'\", 't', 'going', 'to', 'cover', 'a', '60', 'day', 'supply', 'of', 'generic', 'add', '##eral', '##l', '.', 'however', ',', 'brand', 'name', 'add', '##eral', '##l', 'was', 'covered', 'and', 'i', 'got', 'a', '60', 'day', 'supply', 'for', '$', '40', '.', 'i', 'was', 'kind', 'of', 'confused', 'because', 'the', 'last', 'time', 'i', 'got', 'a', '60', 'day', 'supply', ',', 'insurance', 'had', 'covered', 'the', 'generic', '.', 'the', 'time', 'before', 'that', ',', 'insurance', 'only', 'covered', 'brand', 'name', '.', 'out', 'of', 'curiosity', ',', 'i', 'asked', 'how', 'much', 'the', 'generic', 'would', 'cost', 'me', '.', 'the', 'ph', '##arm', '##ac', '##ist', 'said', 'it', 'would', 'cost', 'me', 'upwards', 'of', '$', '300', 'only', 'pay', '##able', 'in', 'cash', '(', 'w', '##tf', '?', ')', '.', 'is', 'my', 'insurance', 'switching', 'my', 'coverage', 'every', '2', 'months', 'or', 'is', 'wal', '##gree', '##ns', 'just', 'mis', '##in', '##formed', '?', 't', '##l', ';', 'dr', ':', 'insurance', 'keeps', 'changing', 'its', 'mind', 'about', 'whether', 'or', 'not', 'it', 'will', 'cover', 'generic', 'add', '##eral', '##l', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'did', 'you', 'feel', 'the', 'first', 'time', 'you', 'took', 'your', 'ad', '##hd', 'medicine', '?', 'currently', 'took', 'methyl', '##ph', '##eni', '##date', '10', '##mg', 'and', 'feel', 'very', 'zone', '##d', 'in', 'it', \"'\", 's', 'almost', 'an', 'odd', 'feeling', '.', 'family', 'called', 'when', 'i', 'was', 'on', 'computer', 'and', 'was', 'almost', 'had', 'to', 'talk', 'since', 'i', 'was', 'previously', 'so', 'engaged', 'in', 'researching', 'online', '.', 'im', 'sort', 'of', 'excited', '!']\n",
      "INFO:__main__:Number of tokens: 64\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'did', 'you', 'feel', 'the', 'first', 'time', 'you', 'took', 'your', 'ad', '##hd', 'medicine', '?', 'currently', 'took', 'methyl', '##ph', '##eni', '##date', '10', '##mg', 'and', 'feel', 'very', 'zone', '##d', 'in', 'it', \"'\", 's', 'almost', 'an', 'odd', 'feeling', '.', 'family', 'called', 'when', 'i', 'was', 'on', 'computer', 'and', 'was', 'almost', 'had', 'to', 'talk', 'since', 'i', 'was', 'previously', 'so', 'engaged', 'in', 'researching', 'online', '.', 'im', 'sort', 'of', 'excited', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'feel', 'like', 'i', 'have', 'no', 'control', '.', 'hi', 'r', '/', 'ad', '##hd', ',', 'i', 'just', 'found', 'this', 'sub', '##red', '##dit', 'and', 'i', \"'\", 'm', 'so', 'happy', 'it', 'exists', '.', 'using', 'a', 'new', 'account', '/', 'throw', '##away', 'because', 'all', 'of', 'this', 'is', 'pretty', 'personal', '.', 'i', \"'\", 'm', 'in', 'my', '20', \"'\", 's', 'and', 'have', 'ad', '##hd', ',', 'depression', 'and', 'anxiety', '(', 'awesome', 'combination', ',', 'right', '?', 'i', \"'\", 've', 'really', 'hit', 'the', 'jack', '##pot', '.', '.', '.', ')', '.', 'they', 'have', 'always', 'been', 'a', 'struggle', 'for', 'me', ',', 'but', 'often', 'i', \"'\", 'm', 'proud', 'of', 'how', 'well', 'i', \"'\", 've', 'been', 'able', 'to', 'handle', 'them', ',', 'especially', 'without', 'medication', 'or', 'anything', 'in', 'recent', 'years', '.', '.', '.', 'and', 'yet', 'lately', 'at', 'work', ',', 'just', 'when', 'i', 'feel', 'like', 'i', \"'\", 'm', 'doing', 'great', 'and', 'i', \"'\", 've', 'come', 'so', 'far', ',', 'these', 'things', 'will', 'sometimes', 'creep', 'back', 'up', 'on', 'me', ',', 'and', 'suddenly', 'i', 'can', \"'\", 't', 'focus', 'at', 'all', '.', 'i', 'feel', 'like', 'the', '\"', 'dumb', '\"', 'kid', 'in', 'school', 'again', '.', 'i', 'can', 'keep', 'a', 'calm', 'exterior', ',', 'but', 'inwardly', 'my', 'mind', 'is', 'collapsing', '.', 'i', 'can', \"'\", 't', 'think', 'straight', ',', 'and', 'i', 'can', \"'\", 't', 'talk', 'myself', 'back', 'into', 'confidence', 'to', 'make', 'it', 'better', ',', 'and', 'feel', 'like', 'anything', 'i', 'say', 'comes', 'out', 'sounding', 'stupid', '.', 'i', 'don', \"'\", 't', 'want', 'ad', '##hd', 'or', 'anything', 'else', 'to', 'affect', 'my', 'job', 'performance', ',', 'or', 'make', 'me', 'feel', 'like', 'crap', 'anymore', '.', 'i', 'need', 'to', 'do', 'something', '.', 'medication', 'didn', \"'\", 't', 'do', 'anything', 'for', 'me', 'as', 'a', 'kid', ',', 'i', 'stopped', 'taking', 'it', 'early', 'on', 'in', 'my', 'first', 'year', 'of', 'college', 'and', 'realized', 'it', 'wasn', \"'\", 't', 'making', 'any', 'significant', 'difference', ',', 'so', 'i', 'never', 'went', 'back', 'to', 'it', '.', 'but', 'these', 'days', 'i', \"'\", 'm', 'thinking', 'that', 'maybe', 'i', 'should', 'try', 'something', 'new', '.', 'does', 'anyone', 'have', 'any', 'advice', 'for', 'me', '?', 'can', 'you', 'recommend', 'any', 'medications', 'that', 'won', \"'\", 't', 'have', 'fucked', 'up', 'side', '-', 'effects', '?', 'i', 'know', 'i', 'could', 'google', 'any', 'number', 'of', 'them', ',', 'but', 'i', \"'\", 'd', 'much', 'rather', 'hear', 'from', 'people', 'here', 'with', 'experience', 'in', 'taking', 'them', ',', 'or', 'doing', 'well', 'without', 'them', ',', 'or', 'handling', 'it', 'however', 'you', 'do', '.']\n",
      "INFO:__main__:Number of tokens: 372\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'feel', 'like', 'i', 'have', 'no', 'control', '.', 'hi', 'r', '/', 'ad', '##hd', ',', 'i', 'just', 'found', 'this', 'sub', '##red', '##dit', 'and', 'i', \"'\", 'm', 'so', 'happy', 'it', 'exists', '.', 'using', 'a', 'new', 'account', '/', 'throw', '##away', 'because', 'all', 'of', 'this', 'is', 'pretty', 'personal', '.', 'i', \"'\", 'm', 'in', 'my', '20', \"'\", 's', 'and', 'have', 'ad', '##hd', ',', 'depression', 'and', 'anxiety', '(', 'awesome', 'combination', ',', 'right', '?', 'i', \"'\", 've', 'really', 'hit', 'the', 'jack', '##pot', '.', '.', '.', ')', '.', 'they', 'have', 'always', 'been', 'a', 'struggle', 'for', 'me', ',', 'but', 'often', 'i', \"'\", 'm', 'proud', 'of', 'how', 'well', 'i', \"'\", 've', 'been', 'able', 'to', 'handle', 'them', ',', 'especially', 'without', 'medication', 'or', 'anything', 'in', 'recent', 'years', '.', '.', '.', 'and', 'yet', 'lately', 'at', 'work', ',', 'just', 'when', 'i', 'feel', 'like', 'i', \"'\", 'm', 'doing', 'great', 'and', 'i', \"'\", 've', 'come', 'so', 'far', ',', 'these', 'things', 'will', 'sometimes', 'creep', 'back', 'up', 'on', 'me', ',', 'and', 'suddenly', 'i', 'can', \"'\", 't', 'focus', 'at', 'all', '.', 'i', 'feel', 'like', 'the', '\"', 'dumb', '\"', 'kid', 'in', 'school', 'again', '.', 'i', 'can', 'keep', 'a', 'calm', 'exterior', ',', 'but', 'inwardly', 'my', 'mind', 'is', 'collapsing', '.', 'i', 'can', \"'\", 't', 'think', 'straight', ',', 'and', 'i', 'can', \"'\", 't', 'talk', 'myself', 'back', 'into', 'confidence', 'to', 'make', 'it', 'better', ',', 'and', 'feel', 'like', 'anything', 'i', 'say', 'comes', 'out', 'sounding', 'stupid', '.', 'i', 'don', \"'\", 't', 'want', 'ad', '##hd', 'or', 'anything', 'else', 'to', 'affect', 'my', 'job', 'performance', ',', 'or', 'make', 'me', 'feel', 'like', 'crap', 'anymore', '.', 'i', 'need', 'to', 'do', 'something', '.', 'medication', 'didn', \"'\", 't', 'do', 'anything', 'for', 'me', 'as', 'a', 'kid', ',', 'i', 'stopped', 'taking', 'it', 'early', 'on', 'in', 'my', 'first', 'year', 'of', 'college', 'and', 'realized', 'it', 'wasn', \"'\", 't', 'making', 'any', 'significant', 'difference', ',', 'so', 'i', 'never', 'went', 'back', 'to', 'it', '.', 'but', 'these', 'days', 'i', \"'\", 'm', 'thinking', 'that', 'maybe', 'i', 'should', 'try', 'something', 'new', '.', 'does', 'anyone', 'have', 'any', 'advice', 'for', 'me', '?', 'can', 'you', 'recommend', 'any', 'medications', 'that', 'won', \"'\", 't', 'have', 'fucked', 'up', 'side', '-', 'effects', '?', 'i', 'know', 'i', 'could', 'google', 'any', 'number', 'of', 'them', ',', 'but', 'i', \"'\", 'd', 'much', 'rather', 'hear', 'from', 'people', 'here', 'with', 'experience', 'in', 'taking', 'them', ',', 'or', 'doing', 'well', 'without', 'them', ',', 'or', 'handling', 'it', 'however', 'you', 'do', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['porn', '##o', 'animal']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['porn', '##o', 'animal']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'i', 'was', 'finally', 'able', 'to', 'work', 'hard', 'for', 'something', '.', '.', '.', 'and', 'i', 'flu', '##nk', '##ed', 'it', ',', 'i', 'went', 'into', 'the', 'test', '.', '.', '.', 'sat', 'down', ',', 'and', 'i', 'knew', 'nothing', '.', 'all', 'the', 'things', 'i', 'had', 'worked', 'so', 'so', 'hard', 'on', 'disappeared', 'from', 'my', 'mind', '.', 'i', 'just', 'can', \"'\", 't', 'even', 'be', 'bothered', 'carrying', 'on', 'with', 'bothering', 'to', 'go', 'to', 'university', 'if', 'i', \"'\", 'm', 'too', 'dumb', 'to', 'even', 'pass', 'a', 'mid', '##ter', '##m', 'test', '.', 'just', 'wanted', 'to', 'say', 'this', 'here', 'since', 'i', 'really', 'don', \"'\", 't', 'have', 'anywhere', 'else', 'to', 'say', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 101\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', 'i', 'was', 'finally', 'able', 'to', 'work', 'hard', 'for', 'something', '.', '.', '.', 'and', 'i', 'flu', '##nk', '##ed', 'it', ',', 'i', 'went', 'into', 'the', 'test', '.', '.', '.', 'sat', 'down', ',', 'and', 'i', 'knew', 'nothing', '.', 'all', 'the', 'things', 'i', 'had', 'worked', 'so', 'so', 'hard', 'on', 'disappeared', 'from', 'my', 'mind', '.', 'i', 'just', 'can', \"'\", 't', 'even', 'be', 'bothered', 'carrying', 'on', 'with', 'bothering', 'to', 'go', 'to', 'university', 'if', 'i', \"'\", 'm', 'too', 'dumb', 'to', 'even', 'pass', 'a', 'mid', '##ter', '##m', 'test', '.', 'just', 'wanted', 'to', 'say', 'this', 'here', 'since', 'i', 'really', 'don', \"'\", 't', 'have', 'anywhere', 'else', 'to', 'say', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['could', 'biting', 'yourself', 'to', 'focus', 'have', 'any', 'negative', 'effects', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['could', 'biting', 'yourself', 'to', 'focus', 'have', 'any', 'negative', 'effects', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['describing', 'the', 'elephant', ':', 'behavior', 'problems', 'and', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['describing', 'the', 'elephant', ':', 'behavior', 'problems', 'and', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['energy', 'drinks', 'and', 'ad', '##hd', '?', 'hey', 'guys', ',', 'do', 'energy', 'drinks', 'help', 'performance', 'or', 'concentration', '.', 'i', 'take', 'nos', 'but', 'i', 'feel', 'like', 'it', 'just', 'increases', 'the', 'ad', '##hd', 'symptoms', '.', 'sort', 'of', 'helps', 'a', 'little', 'when', 'i', 'try', 'to', 'focus', 'on', 'something', 'more', 'interesting', '.', 'does', 'anyone', 'else', 'have', 'different', 'experiences', '?', 'might', 'be', 'a', 'rep', '##ost', 'but', 'i', \"'\", 'm', 'just', 'curious', '.']\n",
      "INFO:__main__:Number of tokens: 66\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['energy', 'drinks', 'and', 'ad', '##hd', '?', 'hey', 'guys', ',', 'do', 'energy', 'drinks', 'help', 'performance', 'or', 'concentration', '.', 'i', 'take', 'nos', 'but', 'i', 'feel', 'like', 'it', 'just', 'increases', 'the', 'ad', '##hd', 'symptoms', '.', 'sort', 'of', 'helps', 'a', 'little', 'when', 'i', 'try', 'to', 'focus', 'on', 'something', 'more', 'interesting', '.', 'does', 'anyone', 'else', 'have', 'different', 'experiences', '?', 'might', 'be', 'a', 'rep', '##ost', 'but', 'i', \"'\", 'm', 'just', 'curious', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['using', 'caf', '##fe', '##ine', 'to', 'control', 'ad', '##hd', 'symptoms', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['using', 'caf', '##fe', '##ine', 'to', 'control', 'ad', '##hd', 'symptoms', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['setting', 'up', 'an', 'ap', '##oint', '##ment', 'so', 'im', 'a', '20', '##yr', 'old', 'male', 'and', 'after', 'talking', 'with', 'my', 'dad', 'we', 'decided', 'that', 'i', 'should', 'see', 'a', 'psychiatrist', 'about', 'possibly', 'having', 'adult', 'add', '.', 'i', 'have', 'had', 'co', '##un', '##sl', '##ors', ',', 'taken', 'classes', 'for', 'organ', '##zation', ',', 'study', '##ind', 'and', 'have', 'had', 'numerous', 'tutor', '##s', 'over', 'the', 'years', 'and', 'nothing', 'has', 'seemed', 'to', 'help', '.', 'i', 'found', 'the', 'number', 'of', 'a', 'local', 'psychiatrist', ',', 'but', 'i', 'don', '##t', 'know', 'if', 'she', 'specializes', 'in', 'add', ',', 'will', 'she', 'still', 'be', 'able', 'to', 'diagnosis', '?', 'and', 'when', 'i', 'call', 'the', 'office', 'what', 'should', 'say', 'to', 'the', 'receptionist', 'when', 'making', 'the', 'appointment', '?', 'thank', 'you', 'for', 'any', 'and', 'all', 'advice']\n",
      "INFO:__main__:Number of tokens: 117\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['setting', 'up', 'an', 'ap', '##oint', '##ment', 'so', 'im', 'a', '20', '##yr', 'old', 'male', 'and', 'after', 'talking', 'with', 'my', 'dad', 'we', 'decided', 'that', 'i', 'should', 'see', 'a', 'psychiatrist', 'about', 'possibly', 'having', 'adult', 'add', '.', 'i', 'have', 'had', 'co', '##un', '##sl', '##ors', ',', 'taken', 'classes', 'for', 'organ', '##zation', ',', 'study', '##ind', 'and', 'have', 'had', 'numerous', 'tutor', '##s', 'over', 'the', 'years', 'and', 'nothing', 'has', 'seemed', 'to', 'help', '.', 'i', 'found', 'the', 'number', 'of', 'a', 'local', 'psychiatrist', ',', 'but', 'i', 'don', '##t', 'know', 'if', 'she', 'specializes', 'in', 'add', ',', 'will', 'she', 'still', 'be', 'able', 'to', 'diagnosis', '?', 'and', 'when', 'i', 'call', 'the', 'office', 'what', 'should', 'say', 'to', 'the', 'receptionist', 'when', 'making', 'the', 'appointment', '?', 'thank', 'you', 'for', 'any', 'and', 'all', 'advice']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['stopped', 'taking', 'add', '##eral', '##l', 'because', 'it', 'overwhelmed', 'me', ',', 'dr', 'now', 'prescribed', 'me', 'v', '##y', '##vance', '.', 'will', 'there', 'be', 'a', 'difference', '?']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['stopped', 'taking', 'add', '##eral', '##l', 'because', 'it', 'overwhelmed', 'me', ',', 'dr', 'now', 'prescribed', 'me', 'v', '##y', '##vance', '.', 'will', 'there', 'be', 'a', 'difference', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'turn', 'your', 'excitement', 'in', 'to', 'action', '?', 'hello', ',', 'my', 'name', 'is', 'aw', '##just', '##ag', '##uy', ',', 'and', 'like', 'many', 'of', 'you', ',', 'i', 'suffer', 'from', 'ad', '##hd', '.', 'now', ',', 'we', 'all', 'know', 'how', 'hard', 'it', 'is', 'to', 'do', 'something', 'that', 'isn', \"'\", 't', 'exciting', 'or', 'interesting', '.', 'however', ',', 'we', 'also', 'know', 'how', 'easy', 'it', 'is', 'to', 'spend', 'hours', 'upon', 'hours', 'of', 'hard', ',', 'and', 'amazing', ',', 'work', 'towards', 'something', 'that', 'is', 'exciting', ',', 'like', 'a', 'video', 'game', 'or', 'relationship', 'or', 'whatever', 'your', 'poison', 'is', '.', 'i', \"'\", 've', 'recently', 'given', 'up', 'on', 'trying', 'so', 'hard', 'to', 'fight', 'up', '##hill', 'on', 'une', '##x', '##cit', '##ing', 'work', '.', 'i', 'just', 'can', \"'\", 't', 'look', 'forward', 'to', 'a', 'life', 'where', 'i', \"'\", 'm', 'always', 'struggling', 'to', 'do', 'the', 'normal', 'things', 'that', 'people', 'do', '.', 'i', \"'\", 'm', 'wanting', 'to', 'do', 'the', 'things', 'i', 'find', 'exciting', '.', 'here', \"'\", 's', 'the', 'issue', '.', 'whenever', 'i', 'think', 'about', 'something', 'i', 'am', 'really', 'excited', 'about', '(', 'making', 'a', 'website', ',', 'programming', 'a', 'game', ',', 'writing', 'a', 'book', ',', 'etc', ')', ',', 'the', 'idea', 'is', 'very', 'mo', '##tiv', '##ating', '.', 'however', ',', 'when', 'it', \"'\", 's', 'time', 'to', 'actually', 'sit', 'down', 'and', 'everything', 'slow', '##s', 'down', ',', 'i', 'can', \"'\", 't', 'get', 'myself', 'to', 'stick', 'to', 'it', '.', 'what', 'are', 'some', 'ways', 'that', 'you', 'guys', 'manage', 'your', 'excitement', 'to', 'last', 'through', 'a', 'goal', '?']\n",
      "INFO:__main__:Number of tokens: 233\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'turn', 'your', 'excitement', 'in', 'to', 'action', '?', 'hello', ',', 'my', 'name', 'is', 'aw', '##just', '##ag', '##uy', ',', 'and', 'like', 'many', 'of', 'you', ',', 'i', 'suffer', 'from', 'ad', '##hd', '.', 'now', ',', 'we', 'all', 'know', 'how', 'hard', 'it', 'is', 'to', 'do', 'something', 'that', 'isn', \"'\", 't', 'exciting', 'or', 'interesting', '.', 'however', ',', 'we', 'also', 'know', 'how', 'easy', 'it', 'is', 'to', 'spend', 'hours', 'upon', 'hours', 'of', 'hard', ',', 'and', 'amazing', ',', 'work', 'towards', 'something', 'that', 'is', 'exciting', ',', 'like', 'a', 'video', 'game', 'or', 'relationship', 'or', 'whatever', 'your', 'poison', 'is', '.', 'i', \"'\", 've', 'recently', 'given', 'up', 'on', 'trying', 'so', 'hard', 'to', 'fight', 'up', '##hill', 'on', 'une', '##x', '##cit', '##ing', 'work', '.', 'i', 'just', 'can', \"'\", 't', 'look', 'forward', 'to', 'a', 'life', 'where', 'i', \"'\", 'm', 'always', 'struggling', 'to', 'do', 'the', 'normal', 'things', 'that', 'people', 'do', '.', 'i', \"'\", 'm', 'wanting', 'to', 'do', 'the', 'things', 'i', 'find', 'exciting', '.', 'here', \"'\", 's', 'the', 'issue', '.', 'whenever', 'i', 'think', 'about', 'something', 'i', 'am', 'really', 'excited', 'about', '(', 'making', 'a', 'website', ',', 'programming', 'a', 'game', ',', 'writing', 'a', 'book', ',', 'etc', ')', ',', 'the', 'idea', 'is', 'very', 'mo', '##tiv', '##ating', '.', 'however', ',', 'when', 'it', \"'\", 's', 'time', 'to', 'actually', 'sit', 'down', 'and', 'everything', 'slow', '##s', 'down', ',', 'i', 'can', \"'\", 't', 'get', 'myself', 'to', 'stick', 'to', 'it', '.', 'what', 'are', 'some', 'ways', 'that', 'you', 'guys', 'manage', 'your', 'excitement', 'to', 'last', 'through', 'a', 'goal', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['it', \"'\", 's', 'really', 'difficult', 'for', 'me', 'to', 'visual', '##ize', 'i', 'read', 'about', 'an', 'exercise', 'where', 'you', 'can', 'kind', 'of', 'test', 'to', 'see', 'if', 'you', \"'\", 're', 'in', 'control', 'of', 'your', 'brain', 'or', 'not', '.', 'basically', 'you', 'just', 'close', 'your', 'eyes', 'and', 'try', 'to', 'see', 'if', 'you', 'can', 'make', '10', 'shots', 'in', 'a', 'row', '.', 'so', 'i', 'tried', 'this', 'just', 'now', 'and', 'was', 'able', 'to', 'make', 'all', '10', 'shots', ',', 'however', 'they', 'weren', \"'\", 't', 'all', 'sw', '##ish', '##es', 'and', 'there', 'were', 'a', 'lot', 'of', 'non', '##sen', '##sic', '##al', 'things', 'going', 'on', 'that', 'wouldn', \"'\", 't', 'really', 'be', 'there', 'like', 'weird', 'shapes', 'and', 'colors', 'and', 'changing', '\"', 'camera', 'angles', '\"', 'probably', 'due', 'to', 'my', 'add', '.', 'i', \"'\", 'm', 'thinking', 'if', 'i', 'could', 'somehow', 'practice', 'and', 'be', 'able', 'to', 'get', 'rid', 'of', 'all', 'the', 'random', 'mental', 'distraction', '##s', 'when', 'visual', '##izing', ',', 'it', 'would', 'help', 'my', 'thinking', 'and', 'my', 'life', 'in', 'a', 'lot', 'of', 'ways', '.', 'this', 'doesn', \"'\", 't', 'happen', 'only', 'when', 'i', \"'\", 'm', 'trying', 'to', 'visual', '##ize', 'of', 'course', ',', 'it', 'happens', 'when', 'i', \"'\", 'm', 'doing', 'tasks', 'such', 'as', 'memo', '##riz', '##ing', 'as', 'well', '.', 'does', 'anyone', 'have', 'any', 'suggestions', 'for', 'me', 'or', 'find', 'themselves', 'in', 'the', 'same', 'boat', '?', 'additional', 'information', ':', 'i', \"'\", 'm', '22', 'years', 'old', 'and', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', 'in', '3rd', 'grade', 'and', 'took', 'rita', '##lin', 'for', 'about', 'a', 'year', 'and', 'my', 'parents', 'and', 'i', 'decided', 'i', 'would', 'be', 'better', 'off', 'not', 'taking', 'it', 'because', 'of', 'how', 'much', 'it', 'altered', 'my', 'personality', ',', 'etc', '.', 'thanks', '.', 'edit', ':', 'if', 'anyone', \"'\", 's', 'wondering', ',', 'i', 'will', 'start', 'doing', 'meditation', 'and', 'following', '[', 'these', 'exercises', '.', ']', '(', 'http', ':', '/', '/', 'lit', '##emi', '##nd', '.', 'com', '/', 'how', '-', 'to', '-', 'develop', '-', 'visual', '##ization', '-', 'skill', '/', ')']\n",
      "INFO:__main__:Number of tokens: 303\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['it', \"'\", 's', 'really', 'difficult', 'for', 'me', 'to', 'visual', '##ize', 'i', 'read', 'about', 'an', 'exercise', 'where', 'you', 'can', 'kind', 'of', 'test', 'to', 'see', 'if', 'you', \"'\", 're', 'in', 'control', 'of', 'your', 'brain', 'or', 'not', '.', 'basically', 'you', 'just', 'close', 'your', 'eyes', 'and', 'try', 'to', 'see', 'if', 'you', 'can', 'make', '10', 'shots', 'in', 'a', 'row', '.', 'so', 'i', 'tried', 'this', 'just', 'now', 'and', 'was', 'able', 'to', 'make', 'all', '10', 'shots', ',', 'however', 'they', 'weren', \"'\", 't', 'all', 'sw', '##ish', '##es', 'and', 'there', 'were', 'a', 'lot', 'of', 'non', '##sen', '##sic', '##al', 'things', 'going', 'on', 'that', 'wouldn', \"'\", 't', 'really', 'be', 'there', 'like', 'weird', 'shapes', 'and', 'colors', 'and', 'changing', '\"', 'camera', 'angles', '\"', 'probably', 'due', 'to', 'my', 'add', '.', 'i', \"'\", 'm', 'thinking', 'if', 'i', 'could', 'somehow', 'practice', 'and', 'be', 'able', 'to', 'get', 'rid', 'of', 'all', 'the', 'random', 'mental', 'distraction', '##s', 'when', 'visual', '##izing', ',', 'it', 'would', 'help', 'my', 'thinking', 'and', 'my', 'life', 'in', 'a', 'lot', 'of', 'ways', '.', 'this', 'doesn', \"'\", 't', 'happen', 'only', 'when', 'i', \"'\", 'm', 'trying', 'to', 'visual', '##ize', 'of', 'course', ',', 'it', 'happens', 'when', 'i', \"'\", 'm', 'doing', 'tasks', 'such', 'as', 'memo', '##riz', '##ing', 'as', 'well', '.', 'does', 'anyone', 'have', 'any', 'suggestions', 'for', 'me', 'or', 'find', 'themselves', 'in', 'the', 'same', 'boat', '?', 'additional', 'information', ':', 'i', \"'\", 'm', '22', 'years', 'old', 'and', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', 'in', '3rd', 'grade', 'and', 'took', 'rita', '##lin', 'for', 'about', 'a', 'year', 'and', 'my', 'parents', 'and', 'i', 'decided', 'i', 'would', 'be', 'better', 'off', 'not', 'taking', 'it', 'because', 'of', 'how', 'much', 'it', 'altered', 'my', 'personality', ',', 'etc', '.', 'thanks', '.', 'edit', ':', 'if', 'anyone', \"'\", 's', 'wondering', ',', 'i', 'will', 'start', 'doing', 'meditation', 'and', 'following', '[', 'these', 'exercises', '.', ']', '(', 'http', ':', '/', '/', 'lit', '##emi', '##nd', '.', 'com', '/', 'how', '-', 'to', '-', 'develop', '-', 'visual', '##ization', '-', 'skill', '/', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['quit', '##ting', 'smoking', 'for', 'any', 'former', 'smoke', '##rs', 'with', 'ad', '##hd', 'that', 'quit', 'after', 'starting', 'medication', '.', 'did', 'your', 'craving', '##s', 'drop', 'off', 'or', 'was', 'it', 'as', 'challenging', 'as', 'without', 'med', '##s', '?']\n",
      "INFO:__main__:Number of tokens: 33\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['quit', '##ting', 'smoking', 'for', 'any', 'former', 'smoke', '##rs', 'with', 'ad', '##hd', 'that', 'quit', 'after', 'starting', 'medication', '.', 'did', 'your', 'craving', '##s', 'drop', 'off', 'or', 'was', 'it', 'as', 'challenging', 'as', 'without', 'med', '##s', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['r', '/', 'ad', '##hd', 'needs', 'shorter', 'posts', 'and', 'comments', 'or', 'more', 't', '##l', ';', 'dr', 'versions', '!', 'i', 'mean', 'guys', ',', 'seriously', ',', 'you', 'are', 'writing', 'for', 'other', '*', '*', 'ad', '##hd', '*', '*', 'people', '.', '.', '.', 'like', ',', 'a', 'few', 'minutes', 'ago', 'i', 'have', 'been', 'reading', 'this', 'good', 'but', 'really', 'long', 'comment', 'here', 'on', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'and', 'i', 'didn', \"'\", 't', 'finish', 'it', 'because', 'my', 'train', 'of', 'thought', 'slipped', 'and', 'i', 'started', 'doing', 'something', 'else', ',', 'anyway', ',', 'i', 'think', 'it', 'was', 'something', 'about', '.', '.', '.', 'hmm', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 105\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['r', '/', 'ad', '##hd', 'needs', 'shorter', 'posts', 'and', 'comments', 'or', 'more', 't', '##l', ';', 'dr', 'versions', '!', 'i', 'mean', 'guys', ',', 'seriously', ',', 'you', 'are', 'writing', 'for', 'other', '*', '*', 'ad', '##hd', '*', '*', 'people', '.', '.', '.', 'like', ',', 'a', 'few', 'minutes', 'ago', 'i', 'have', 'been', 'reading', 'this', 'good', 'but', 'really', 'long', 'comment', 'here', 'on', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'and', 'i', 'didn', \"'\", 't', 'finish', 'it', 'because', 'my', 'train', 'of', 'thought', 'slipped', 'and', 'i', 'started', 'doing', 'something', 'else', ',', 'anyway', ',', 'i', 'think', 'it', 'was', 'something', 'about', '.', '.', '.', 'hmm', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: [\"'\", 'what', 'should', 'we', 'add', 'to', 'the', 'fa', '##q', '?', \"'\", 'or', \"'\", 'i', \"'\", 'm', 'tired', 'of', 're', '##ty', '##ping', 'the', 'same', 'things', '.', \"'\", 'i', 'find', 'myself', 're', '-', 'typing', 'the', 'same', 'helpful', 'hints', 'on', 'ad', '##hd', 'and', 'school', 'or', 'eating', 'right', 'or', 'exercise', ',', 'etc', '.', 'i', \"'\", 'd', 'like', 'to', 'gather', 'this', 'all', 'up', 'so', 'the', 'fa', '##q', 'gods', 'and', 'mod', '##s', 'have', 'an', 'easy', 'time', 'adding', 'it', 'to', 'the', 'fa', '##q', ',', 'and', 'new', 'subscribers', 'have', 'an', 'easy', 'and', 'instant', 'way', 'to', 'find', 'the', 'generic', 'stuff', '.', '*', 'please', 'start', '*', '*', '*', 'sub', '##th', '##rea', '##ds', 'for', 'each', 'separate', 'topic', '*', '*', '*', '(', 'i', \"'\", 'll', 'demonstrate', ')', 'and', 'include', 'only', 'comments', 'on', 'that', 'subject', 'there', ',', 'to', 'make', 'it', 'easier', 'to', 'conde', '##nse', 'and', 'minimize', 'repetition', '.', '*']\n",
      "INFO:__main__:Number of tokens: 136\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [[\"'\", 'what', 'should', 'we', 'add', 'to', 'the', 'fa', '##q', '?', \"'\", 'or', \"'\", 'i', \"'\", 'm', 'tired', 'of', 're', '##ty', '##ping', 'the', 'same', 'things', '.', \"'\", 'i', 'find', 'myself', 're', '-', 'typing', 'the', 'same', 'helpful', 'hints', 'on', 'ad', '##hd', 'and', 'school', 'or', 'eating', 'right', 'or', 'exercise', ',', 'etc', '.', 'i', \"'\", 'd', 'like', 'to', 'gather', 'this', 'all', 'up', 'so', 'the', 'fa', '##q', 'gods', 'and', 'mod', '##s', 'have', 'an', 'easy', 'time', 'adding', 'it', 'to', 'the', 'fa', '##q', ',', 'and', 'new', 'subscribers', 'have', 'an', 'easy', 'and', 'instant', 'way', 'to', 'find', 'the', 'generic', 'stuff', '.', '*', 'please', 'start', '*', '*', '*', 'sub', '##th', '##rea', '##ds', 'for', 'each', 'separate', 'topic', '*', '*', '*', '(', 'i', \"'\", 'll', 'demonstrate', ')', 'and', 'include', 'only', 'comments', 'on', 'that', 'subject', 'there', ',', 'to', 'make', 'it', 'easier', 'to', 'conde', '##nse', 'and', 'minimize', 'repetition', '.', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'and', 'periods', '?', 'please', 'help', '.', 'i', \"'\", 've', 'been', 'prescribed', 'add', '##eral', '##l', 'and', 'have', 'been', 'taking', 'it', 'for', 'about', '10', 'days', '.', 'a', 'few', 'days', 'ago', ',', 'i', 'noticed', 'some', 'spotting', ',', 'which', 'swiftly', 'blossom', '##ed', 'into', 'a', 'full', '-', 'blown', 'period', '.', 'two', 'weeks', 'before', 'my', 'downs', '##tai', '##r', \"'\", 's', 'regularly', 'scheduled', 'programming', '.', 'this', 'is', 'tr', '##ou', '##bling', 'for', 'me', ',', 'as', 'i', 'am', 'on', 'the', 'pill', ',', 'and', 'for', 'the', 'past', 'ten', '-', 'is', '##h', 'years', ',', 'my', 'periods', 'have', 'been', 'like', 'clock', '##work', 'due', 'to', 'all', 'that', 'pill', '-', 'taking', '.', 'has', 'anyone', 'else', 'had', 'this', 'issue', 'or', 'something', 'similar', '?', 'my', 'ob', '##ses', '##sive', 'internet', 'research', 'seems', 'to', 'conclude', 'that', 'many', 'women', 'have', 'the', 'opposite', 'problem', ',', 'where', 'they', 'don', \"'\", 't', 'get', 'their', 'periods', 'on', 'time', ',', 'are', 'a', 'month', 'late', ',', 'etc', '.', 'what', 'about', 'more', 'frequent', '/', 'longer', 'bleed', '##s', 'on', 'add', '##eral', '##l', '?', 'drugs', '.', 'com', 'only', 'mentions', 'd', '##ys', '##men', '##or', '##rh', '##ea', '.', 'add', '##eral', '##l', 'has', 'been', 'super', 'effective', 'to', 'restore', 'my', 'focus', ',', 'discourage', 'my', 'sc', '##atter', '-', 'brain', '##ed', '##ness', 'and', 'deter', 'hyper', '##fo', '##cus', '##ing', '.', '.', '.', '.', 'i', 'am', 'not', 'opposed', 'to', 'trying', 'something', 'new', ',', 'but', 'the', 'trial', 'and', 'error', 'approach', 'to', 'medications', 'has', 'been', 'really', 'stress', '##ful', 'and', 'would', 'prefer', 'not', 'to', 'switch', 'from', 'something', 'that', 'seems', 'to', 'be', 'working', '.', 'speaking', 'of', 'which', ',', 'the', 'past', 'couple', 'weeks', 'have', 'been', 'a', 've', '##rita', '##ble', 'stress', '-', 'fest', 'all', 'by', 'themselves', ',', 'which', 'has', 'occasionally', 'rev', '##ved', 'up', 'my', 'period', 'a', 'day', 'or', 'two', 'early', '-', '-', 'but', 'not', 'weeks', '!', 'i', 'am', 'going', 'to', 'call', 'my', 'doc', 'to', 'ask', 'about', 'this', ',', 'but', 'i', \"'\", 'm', 'at', 'the', 'office', 'and', 'don', \"'\", 't', 'really', 'want', 'to', 'announce', 'my', 'surprise', 'men', '##ses', 'to', 'all', 'my', 'employees', '.', 'my', 'doc', 'started', 'me', 'out', 'on', 'v', '##y', '##vance', ',', 'but', 'it', 'didn', \"'\", 't', 'do', 'anything', 'for', 'me', ',', 'and', 'not', 'having', 'a', 'generic', 'form', 'was', 'going', 'to', 'find', 'me', 'paying', 'out', 'of', 'pocket', 'every', 'month', '.', 'not', 'taking', 'any', 'illegal', 'drugs', '(', 'except', 'for', 'the', 'add', '##eral', '##l', ',', 'but', 'i', 'have', 'a', 'script', 'for', 'it', ')', 'or', 'drinking', 'alcohol', '.', '(', 'this', 'is', 'an', 'x', '-', 'post', 'from', '2', '##x', '-', '-', 'it', 'got', 'down', '##vot', '##ed', 'immediately', 'after', 'hitting', 'the', 'save', '/', 'submit', 'button', ':', '(', 'hurt', 'my', 'feel', '##bad', '##s', '.', 'is', 'sister', '##hood', 'so', 'powerful', 'that', 'it', 'takes', 'up', 'all', 'the', 'space', ',', 'with', 'no', 'room', 'for', 'lady', '-', 'related', 'questions', 'about', 'add', 'med', '##s', '?', 'what', '##ev', '##s', '.', ')']\n",
      "INFO:__main__:Number of tokens: 440\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'and', 'periods', '?', 'please', 'help', '.', 'i', \"'\", 've', 'been', 'prescribed', 'add', '##eral', '##l', 'and', 'have', 'been', 'taking', 'it', 'for', 'about', '10', 'days', '.', 'a', 'few', 'days', 'ago', ',', 'i', 'noticed', 'some', 'spotting', ',', 'which', 'swiftly', 'blossom', '##ed', 'into', 'a', 'full', '-', 'blown', 'period', '.', 'two', 'weeks', 'before', 'my', 'downs', '##tai', '##r', \"'\", 's', 'regularly', 'scheduled', 'programming', '.', 'this', 'is', 'tr', '##ou', '##bling', 'for', 'me', ',', 'as', 'i', 'am', 'on', 'the', 'pill', ',', 'and', 'for', 'the', 'past', 'ten', '-', 'is', '##h', 'years', ',', 'my', 'periods', 'have', 'been', 'like', 'clock', '##work', 'due', 'to', 'all', 'that', 'pill', '-', 'taking', '.', 'has', 'anyone', 'else', 'had', 'this', 'issue', 'or', 'something', 'similar', '?', 'my', 'ob', '##ses', '##sive', 'internet', 'research', 'seems', 'to', 'conclude', 'that', 'many', 'women', 'have', 'the', 'opposite', 'problem', ',', 'where', 'they', 'don', \"'\", 't', 'get', 'their', 'periods', 'on', 'time', ',', 'are', 'a', 'month', 'late', ',', 'etc', '.', 'what', 'about', 'more', 'frequent', '/', 'longer', 'bleed', '##s', 'on', 'add', '##eral', '##l', '?', 'drugs', '.', 'com', 'only', 'mentions', 'd', '##ys', '##men', '##or', '##rh', '##ea', '.', 'add', '##eral', '##l', 'has', 'been', 'super', 'effective', 'to', 'restore', 'my', 'focus', ',', 'discourage', 'my', 'sc', '##atter', '-', 'brain', '##ed', '##ness', 'and', 'deter', 'hyper', '##fo', '##cus', '##ing', '.', '.', '.', '.', 'i', 'am', 'not', 'opposed', 'to', 'trying', 'something', 'new', ',', 'but', 'the', 'trial', 'and', 'error', 'approach', 'to', 'medications', 'has', 'been', 'really', 'stress', '##ful', 'and', 'would', 'prefer', 'not', 'to', 'switch', 'from', 'something', 'that', 'seems', 'to', 'be', 'working', '.', 'speaking', 'of', 'which', ',', 'the', 'past', 'couple', 'weeks', 'have', 'been', 'a', 've', '##rita', '##ble', 'stress', '-', 'fest', 'all', 'by', 'themselves', ',', 'which', 'has', 'occasionally', 'rev', '##ved', 'up', 'my', 'period', 'a', 'day', 'or', 'two', 'early', '-', '-', 'but', 'not', 'weeks', '!', 'i', 'am', 'going', 'to', 'call', 'my', 'doc', 'to', 'ask', 'about', 'this', ',', 'but', 'i', \"'\", 'm', 'at', 'the', 'office', 'and', 'don', \"'\", 't', 'really', 'want', 'to', 'announce', 'my', 'surprise', 'men', '##ses', 'to', 'all', 'my', 'employees', '.', 'my', 'doc', 'started', 'me', 'out', 'on', 'v', '##y', '##vance', ',', 'but', 'it', 'didn', \"'\", 't', 'do', 'anything', 'for', 'me', ',', 'and', 'not', 'having', 'a', 'generic', 'form', 'was', 'going', 'to', 'find', 'me', 'paying', 'out', 'of', 'pocket', 'every', 'month', '.', 'not', 'taking', 'any', 'illegal', 'drugs', '(', 'except', 'for', 'the', 'add', '##eral', '##l', ',', 'but', 'i', 'have', 'a', 'script', 'for', 'it', ')', 'or', 'drinking', 'alcohol', '.', '(', 'this', 'is', 'an', 'x', '-', 'post', 'from', '2', '##x', '-', '-', 'it', 'got', 'down', '##vot', '##ed', 'immediately', 'after', 'hitting', 'the', 'save', '/', 'submit', 'button', ':', '(', 'hurt', 'my', 'feel', '##bad', '##s', '.', 'is', 'sister', '##hood', 'so', 'powerful', 'that', 'it', 'takes', 'up', 'all', 'the', 'space', ',', 'with', 'no', 'room', 'for', 'lady', '-', 'related', 'questions', 'about', 'add', 'med', '##s', '?', 'what', '##ev', '##s', '.', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'wrote', 'this', 'while', 'i', 'was', 'on', 'add', '##y', 'and', 'it', 'was', 'by', 'far', 'one', 'of', 'the', 'best', 'papers', 'i', \"'\", 've', 'written', 'in', 'college', '.']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'wrote', 'this', 'while', 'i', 'was', 'on', 'add', '##y', 'and', 'it', 'was', 'by', 'far', 'one', 'of', 'the', 'best', 'papers', 'i', \"'\", 've', 'written', 'in', 'college', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'and', 'physical', 'energy', 'i', 'recently', 'started', 'taking', 'add', '##eral', '##l', 'ir', '.', 'i', 'take', '20', '##mg', 'at', '5', ':', '40', '##am', ',', 'and', 'then', 'another', '10', 'or', '20', 'mg', 'around', '11', '##am', '.', 'it', 'seems', 'to', 'be', 'helping', 'me', 'with', 'focus', 'at', 'work', 'and', 'feel', 'less', 'sc', '##atter', '##bra', '##ined', 'in', 'general', '.', 'one', 'problem', 'i', \"'\", 've', 'noticed', 'is', 'that', 'i', 'feel', 'physically', 'weak', '.', 'i', 'used', 'to', 'be', 'able', 'to', 'exercise', 'regularly', '(', 'go', 'jogging', 'a', 'few', 'times', 'a', 'week', 'and', 'take', 'a', 'few', 'body', '##pu', '##mp', 'classes', '.', '.', '.', 'nothing', 'extreme', ')', '.', 'i', 'have', 'lost', 'about', '10', 'lbs', 'pretty', 'quickly', ',', 'and', 'i', \"'\", 'm', 'thinking', 'that', 'may', 'be', 'why', '.', 'i', \"'\", 'm', 'still', 'eating', 'plenty', 'of', 'healthy', 'foods', ',', 'but', 'the', 'add', '##eral', '##l', 'makes', 'everything', '.', '.', '.', 'go', 'right', 'through', 'me', '.', 'i', 'don', \"'\", 't', 'mind', 'the', 'weight', 'loss', 'as', 'now', 'i', \"'\", 'm', 'at', 'my', 'ideal', 'weight', 'and', 'i', 'am', 'not', 'too', 'thin', ',', 'but', 'i', 'would', 'love', 'some', 'of', 'my', 'energy', 'back', '.', 'i', 'just', 'tried', 'my', 'usual', 'jogging', 'route', 'and', 'i', 'had', 'to', 'walk', 'more', 'than', 'half', '!', 'i', \"'\", 'm', 'running', 'a', '5', '##k', 'in', 'a', 'month', ',', 'and', 'i', 'need', 'to', 'get', 'past', 'this', '!', '*', '*', 'edit', ':', 'also', ',', 'at', 'what', 'point', 'should', 'i', 'worry', 'about', 'the', 'weight', 'loss', '?', 'i', \"'\", 'm', 'a', '5', \"'\", '7', '\"', 'female', 'and', 'i', 'started', 'at', '~', '133', 'lbs', '.', 'i', \"'\", 'm', 'down', 'to', '123', 'lbs', 'after', 'two', 'weeks', '.', 'thanks', 'in', 'advance', 'for', 'any', 'advice', 'you', 'can', 'give', '!']\n",
      "INFO:__main__:Number of tokens: 268\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'and', 'physical', 'energy', 'i', 'recently', 'started', 'taking', 'add', '##eral', '##l', 'ir', '.', 'i', 'take', '20', '##mg', 'at', '5', ':', '40', '##am', ',', 'and', 'then', 'another', '10', 'or', '20', 'mg', 'around', '11', '##am', '.', 'it', 'seems', 'to', 'be', 'helping', 'me', 'with', 'focus', 'at', 'work', 'and', 'feel', 'less', 'sc', '##atter', '##bra', '##ined', 'in', 'general', '.', 'one', 'problem', 'i', \"'\", 've', 'noticed', 'is', 'that', 'i', 'feel', 'physically', 'weak', '.', 'i', 'used', 'to', 'be', 'able', 'to', 'exercise', 'regularly', '(', 'go', 'jogging', 'a', 'few', 'times', 'a', 'week', 'and', 'take', 'a', 'few', 'body', '##pu', '##mp', 'classes', '.', '.', '.', 'nothing', 'extreme', ')', '.', 'i', 'have', 'lost', 'about', '10', 'lbs', 'pretty', 'quickly', ',', 'and', 'i', \"'\", 'm', 'thinking', 'that', 'may', 'be', 'why', '.', 'i', \"'\", 'm', 'still', 'eating', 'plenty', 'of', 'healthy', 'foods', ',', 'but', 'the', 'add', '##eral', '##l', 'makes', 'everything', '.', '.', '.', 'go', 'right', 'through', 'me', '.', 'i', 'don', \"'\", 't', 'mind', 'the', 'weight', 'loss', 'as', 'now', 'i', \"'\", 'm', 'at', 'my', 'ideal', 'weight', 'and', 'i', 'am', 'not', 'too', 'thin', ',', 'but', 'i', 'would', 'love', 'some', 'of', 'my', 'energy', 'back', '.', 'i', 'just', 'tried', 'my', 'usual', 'jogging', 'route', 'and', 'i', 'had', 'to', 'walk', 'more', 'than', 'half', '!', 'i', \"'\", 'm', 'running', 'a', '5', '##k', 'in', 'a', 'month', ',', 'and', 'i', 'need', 'to', 'get', 'past', 'this', '!', '*', '*', 'edit', ':', 'also', ',', 'at', 'what', 'point', 'should', 'i', 'worry', 'about', 'the', 'weight', 'loss', '?', 'i', \"'\", 'm', 'a', '5', \"'\", '7', '\"', 'female', 'and', 'i', 'started', 'at', '~', '133', 'lbs', '.', 'i', \"'\", 'm', 'down', 'to', '123', 'lbs', 'after', 'two', 'weeks', '.', 'thanks', 'in', 'advance', 'for', 'any', 'advice', 'you', 'can', 'give', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'to', 'do', 'when', 'your', 'ad', '##hd', 'is', 'really', 'acting', 'up', 'and', 'you', 'need', 'to', 'study', '?', 'hey', ',', 'i', \"'\", 'm', 'attempting', 'to', 'study', 'for', 'a', 'test', 'right', 'now', 'and', 'my', 'ad', '##hd', 'literally', 'will', 'not', 'allow', 'me', 'to', 'do', 'so', '.', 'i', 'can', \"'\", 't', 'focus', 'to', 'save', 'my', 'life', ',', 'i', 'took', 'my', 'med', '##s', 'today', 'so', 'i', 'should', 'be', 'fine', 'on', 'that', 'part', ',', 'but', 'i', 'can', \"'\", 't', 'seem', 'to', 'wrap', 'my', 'mind', 'around', 'studying', '.', 'music', 'isn', \"'\", 't', 'helping', 'either', '.', 'all', 'of', 'my', 'go', 'to', 'things', 'are', 'failing', 'me', 'right', 'now', '.', 'i', \"'\", 'm', 'in', 'a', '\"', 'study', 'room', '\"', 'on', 'campus', 'where', 'i', \"'\", 'm', 'alone', ',', 'i', 'have', 'my', 'computer', 'open', 'but', 'pushed', 'way', 'to', 'the', 'side', 'and', 'sleep', 'mode', 'on', 'so', 'it', 'won', \"'\", 't', 'distract', 'me', 'but', 'i', 'can', \"'\", 't', 'seem', 'to', 'study', '.']\n",
      "INFO:__main__:Number of tokens: 148\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'to', 'do', 'when', 'your', 'ad', '##hd', 'is', 'really', 'acting', 'up', 'and', 'you', 'need', 'to', 'study', '?', 'hey', ',', 'i', \"'\", 'm', 'attempting', 'to', 'study', 'for', 'a', 'test', 'right', 'now', 'and', 'my', 'ad', '##hd', 'literally', 'will', 'not', 'allow', 'me', 'to', 'do', 'so', '.', 'i', 'can', \"'\", 't', 'focus', 'to', 'save', 'my', 'life', ',', 'i', 'took', 'my', 'med', '##s', 'today', 'so', 'i', 'should', 'be', 'fine', 'on', 'that', 'part', ',', 'but', 'i', 'can', \"'\", 't', 'seem', 'to', 'wrap', 'my', 'mind', 'around', 'studying', '.', 'music', 'isn', \"'\", 't', 'helping', 'either', '.', 'all', 'of', 'my', 'go', 'to', 'things', 'are', 'failing', 'me', 'right', 'now', '.', 'i', \"'\", 'm', 'in', 'a', '\"', 'study', 'room', '\"', 'on', 'campus', 'where', 'i', \"'\", 'm', 'alone', ',', 'i', 'have', 'my', 'computer', 'open', 'but', 'pushed', 'way', 'to', 'the', 'side', 'and', 'sleep', 'mode', 'on', 'so', 'it', 'won', \"'\", 't', 'distract', 'me', 'but', 'i', 'can', \"'\", 't', 'seem', 'to', 'study', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dad', 'needs', 'advice', 'about', 'son', '(', '5', 'y', '##r', 'old', ')', 'being', 'tested', 'for', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dad', 'needs', 'advice', 'about', 'son', '(', '5', 'y', '##r', 'old', ')', 'being', 'tested', 'for', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'just', 'diagnosed', 'myself', 'with', 'add', ',', 'i', 'feel', 'like', 'crying', 'and', 'ripping', 'my', 'hair', 'out', 'i', 'just', 'found', 'out', 'i', 'have', 'add', '.', 'no', 'i', 'didn', \"'\", 't', 'recently', 'acquire', 'the', 'symptoms', ',', 'rather', 'i', 'just', 'now', 'discovered', 'that', 'my', 'collective', 'symptoms', 'have', 'a', 'name', '.', 'i', 'suppose', 'the', 'next', 'step', 'is', 'to', 'schedule', 'an', 'appointment', 'with', 'my', 'worthless', 'kaiser', 'psychiatrist', '(', 'aka', 'ph', '##arm', '##ac', '##ist', ')', '.', 'but', 'what', 'really', 'makes', 'me', 'want', 'to', 'tear', 'out', 'my', 'hair', 'is', 'that', 'i', \"'\", 've', 'struggled', 'so', 'much', 'the', 'past', '2', 'and', 'a', 'half', 'years', 'at', 'my', 'community', 'college', ',', 'i', 'have', 'failed', 'countless', 'courses', '.', 'and', 'if', 'i', 'only', 'would', 'have', 'discovered', 'a', 'treatment', 'plan', 'earlier', 'my', 'entire', 'life', 'would', 'probably', 'be', 'better', 'off', '.', 'so', 'for', 'the', 'rest', 'of', 'my', 'life', 'i', 'have', 'to', 'struggle', 'with', 'this', '?', 'is', 'there', 'anything', 'i', 'can', 'do', 'with', 'my', 'school', 'grades', '?', 'i', \"'\", 've', 'cried', 'so', 'much', 'these', 'past', 'few', 'days', 'because', 'so', 'much', 'of', 'my', 'life', 'makes', 'more', 'sense', 'now', '.', 'i', \"'\", 'm', '26', 'and', 'i', 'feel', 'like', 'such', 'a', 'failure', '.', 'i', 'guess', 'i', 'just', 'wanted', 'some', 'support', 'from', 'people', 'that', 'struggle', 'with', 'this', ',', 'my', 'wife', 'can', \"'\", 't', 'even', 'wrap', 'her', 'head', 'around', 'it', '.', 'it', \"'\", 's', 'sad', 'because', 'she', \"'\", 's', 'my', 'only', 'friend', '.']\n",
      "INFO:__main__:Number of tokens: 223\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'just', 'diagnosed', 'myself', 'with', 'add', ',', 'i', 'feel', 'like', 'crying', 'and', 'ripping', 'my', 'hair', 'out', 'i', 'just', 'found', 'out', 'i', 'have', 'add', '.', 'no', 'i', 'didn', \"'\", 't', 'recently', 'acquire', 'the', 'symptoms', ',', 'rather', 'i', 'just', 'now', 'discovered', 'that', 'my', 'collective', 'symptoms', 'have', 'a', 'name', '.', 'i', 'suppose', 'the', 'next', 'step', 'is', 'to', 'schedule', 'an', 'appointment', 'with', 'my', 'worthless', 'kaiser', 'psychiatrist', '(', 'aka', 'ph', '##arm', '##ac', '##ist', ')', '.', 'but', 'what', 'really', 'makes', 'me', 'want', 'to', 'tear', 'out', 'my', 'hair', 'is', 'that', 'i', \"'\", 've', 'struggled', 'so', 'much', 'the', 'past', '2', 'and', 'a', 'half', 'years', 'at', 'my', 'community', 'college', ',', 'i', 'have', 'failed', 'countless', 'courses', '.', 'and', 'if', 'i', 'only', 'would', 'have', 'discovered', 'a', 'treatment', 'plan', 'earlier', 'my', 'entire', 'life', 'would', 'probably', 'be', 'better', 'off', '.', 'so', 'for', 'the', 'rest', 'of', 'my', 'life', 'i', 'have', 'to', 'struggle', 'with', 'this', '?', 'is', 'there', 'anything', 'i', 'can', 'do', 'with', 'my', 'school', 'grades', '?', 'i', \"'\", 've', 'cried', 'so', 'much', 'these', 'past', 'few', 'days', 'because', 'so', 'much', 'of', 'my', 'life', 'makes', 'more', 'sense', 'now', '.', 'i', \"'\", 'm', '26', 'and', 'i', 'feel', 'like', 'such', 'a', 'failure', '.', 'i', 'guess', 'i', 'just', 'wanted', 'some', 'support', 'from', 'people', 'that', 'struggle', 'with', 'this', ',', 'my', 'wife', 'can', \"'\", 't', 'even', 'wrap', 'her', 'head', 'around', 'it', '.', 'it', \"'\", 's', 'sad', 'because', 'she', \"'\", 's', 'my', 'only', 'friend', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'to', 'approach', 'spouse', 'about', 'getting', 'evaluated', '?', 'my', 'husband', 'is', 'textbook', 'adult', 'ad', '##hd', ',', 'but', 'neither', 'of', 'us', 'entertained', 'the', 'possibility', 'for', 'all', 'of', 'these', 'years', ',', 'and', 'our', 'marriage', 'suffered', 'greatly', 'as', 'a', 'result', '.', 'we', 'are', 'separated', 'now', 'and', 'headed', 'toward', 'divorce', ',', 'and', 'my', 'therapist', 'suggested', 'that', 'this', 'is', 'likely', 'one', 'of', 'his', 'issues', '.', 'i', \"'\", 've', 'been', 'doing', 'a', 'lot', 'of', 'reading', ',', 'and', 'it', 'was', 'rev', '##ela', '##tory', '.', 'i', 'think', 'we', 'have', 'a', 'chance', 'at', 'reconciliation', 'if', 'he', 'is', 'willing', 'to', 'be', 'evaluated', 'and', 'embark', 'on', 'a', 'treatment', 'plan', ',', 'but', 'i', \"'\", 'm', 'unsure', 'how', 'to', 'bring', 'this', 'up', 'without', 'making', 'him', 'feel', 'like', 'everything', 'is', 'his', 'fault', '(', 'which', 'it', \"'\", 's', 'not', 'by', 'a', 'long', 'shot', ')', '.', 'how', 'can', 'i', 'bro', '##ach', 'the', 'topic', 'with', 'sensitivity', ',', 'do', 'you', 'think', '?']\n",
      "INFO:__main__:Number of tokens: 144\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'to', 'approach', 'spouse', 'about', 'getting', 'evaluated', '?', 'my', 'husband', 'is', 'textbook', 'adult', 'ad', '##hd', ',', 'but', 'neither', 'of', 'us', 'entertained', 'the', 'possibility', 'for', 'all', 'of', 'these', 'years', ',', 'and', 'our', 'marriage', 'suffered', 'greatly', 'as', 'a', 'result', '.', 'we', 'are', 'separated', 'now', 'and', 'headed', 'toward', 'divorce', ',', 'and', 'my', 'therapist', 'suggested', 'that', 'this', 'is', 'likely', 'one', 'of', 'his', 'issues', '.', 'i', \"'\", 've', 'been', 'doing', 'a', 'lot', 'of', 'reading', ',', 'and', 'it', 'was', 'rev', '##ela', '##tory', '.', 'i', 'think', 'we', 'have', 'a', 'chance', 'at', 'reconciliation', 'if', 'he', 'is', 'willing', 'to', 'be', 'evaluated', 'and', 'embark', 'on', 'a', 'treatment', 'plan', ',', 'but', 'i', \"'\", 'm', 'unsure', 'how', 'to', 'bring', 'this', 'up', 'without', 'making', 'him', 'feel', 'like', 'everything', 'is', 'his', 'fault', '(', 'which', 'it', \"'\", 's', 'not', 'by', 'a', 'long', 'shot', ')', '.', 'how', 'can', 'i', 'bro', '##ach', 'the', 'topic', 'with', 'sensitivity', ',', 'do', 'you', 'think', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'to', 'accomplish', 'tuesday', ']', 'share', 'one', 'thing', 'you', 'would', 'like', 'to', 'accomplish', 'this', 'week', '.', 'we', 'will', 'support', 'and', 'keep', 'you', 'accountable', '!', '[', 'part', 'deux', ']', '*', '*', 'even', 'though', 'it', 'is', 'past', 'tuesday', 'please', 'feel', 'free', 'to', 'post', 'what', 'you', 'would', 'like', 'to', 'accomplish', 'throughout', 'the', 'week', '.', 'we', 'have', 'until', 'next', 'tuesday', '.', 'i', 'promise', 'i', 'will', 'follow', 'up', 'on', 'you', 'if', 'you', 'give', 'me', 'a', 'time', 'to', 'do', 'so', '.', '.', '.', '*', '*', '*', '*', 'welcome', 'to', 'part', 'ii', 'of', 'to', 'accomplish', 'tuesday', '!', 'we', 'had', '*', '*', '14', 'people', '*', '*', 'accomplish', 'many', 'many', 'tasks', 'each', '.', '(', '[', 'check', 'out', 'last', 'weeks', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'r', '##g', '##1', '##q', '##f', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', ')', '*', '*', 'can', 'we', 'get', '25', 'people', 'this', 'week', '?', '*', '*', '*', 'we', 'can', 'get', 'at', 'least', '25', 'up', '##vot', '##es', '.', '.', '.', 'right', 'guys', '(', 'and', 'girls', ')', '?', '*', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '*', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '*', '*', '*', '*', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '*', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'commit', '##ing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '*', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '*', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', '*', '*', 'tips', '*', '*', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', 'one', 'hour', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '.', '*', '*', '*', '*', '*', 'examples', 'from', 'last', 'week', ':', '*', '*', '*', 'un', '##load', 'the', 'dish', '##wash', '##er', '*', 'pack', 'for', 'moving', 'early', '*', 'email', 'professor', '*', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '*', 'make', 'an', 'appointment', 'with', 'doctor', '*', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '*', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '*', '*', '*', 'what', '*', '*', '-', '~', '~', 'i', 'want', 'to', '*', '*', 'get', 'started', 'on', 'my', 'taxes', '*', '*', '.', '*', 'if', 'i', 'get', 'them', 'done', 'this', 'week', 'my', 'wife', 'said', 'i', 'can', 'buy', 'a', 'ds', '##lr', '!', '*', '~', '~', '*', '*', '*', 'how', 'much', 'time', '*', '*', '-', 'just', '15', 'minutes', '*', '*', '*', 'when', '*', '*', '-', 'start', 'today', '.', 'finish', 'by', 'saturday', '.', '*', '*', '*', 'check', 'in', 'on', 'me', '?', '*', '*', '-', 'yes', '.', '5', 'hours', 'from', 'now', '.', '5', '##pm', 'ps', '##t', '.', 'tired', 'today', '!', '*', 'i', 'installed', 'turbo', '##ta', '##x', ',', 'found', 'out', 'how', 'much', 'i', 'made', 'last', 'year', '(', 'self', '-', 'employed', ')', 'and', 'got', 'tax', 'forms', 'together', '.', 'took', 'more', 'than', '15', 'minutes', 'but', 'at', 'least', 'i', 'will', 'feel', 'much', 'more', 'prepared', 'next', 'time', '!', '*', '~', '~', '*', 'go', 'to', 'tea', 'shop', 'and', 'pick', 'up', 'some', 'green', 'tea', '~', '~', '*', '*', '*', 'sit', 'down', 'for', '20', 'minutes', 'of', 'meditation', 'and', 'enjoy', 'my', '29th', 'birthday', '!', '*', '*']\n",
      "INFO:__main__:Number of tokens: 859\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['[', 'to', 'accomplish', 'tuesday', ']', 'share', 'one', 'thing', 'you', 'would', 'like', 'to', 'accomplish', 'this', 'week', '.', 'we', 'will', 'support', 'and', 'keep', 'you', 'accountable', '!', '[', 'part', 'deux', ']', '*', '*', 'even', 'though', 'it', 'is', 'past', 'tuesday', 'please', 'feel', 'free', 'to', 'post', 'what', 'you', 'would', 'like', 'to', 'accomplish', 'throughout', 'the', 'week', '.', 'we', 'have', 'until', 'next', 'tuesday', '.', 'i', 'promise', 'i', 'will', 'follow', 'up', 'on', 'you', 'if', 'you', 'give', 'me', 'a', 'time', 'to', 'do', 'so', '.', '.', '.', '*', '*', '*', '*', 'welcome', 'to', 'part', 'ii', 'of', 'to', 'accomplish', 'tuesday', '!', 'we', 'had', '*', '*', '14', 'people', '*', '*', 'accomplish', 'many', 'many', 'tasks', 'each', '.', '(', '[', 'check', 'out', 'last', 'weeks', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'r', '##g', '##1', '##q', '##f', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', ')', '*', '*', 'can', 'we', 'get', '25', 'people', 'this', 'week', '?', '*', '*', '*', 'we', 'can', 'get', 'at', 'least', '25', 'up', '##vot', '##es', '.', '.', '.', 'right', 'guys', '(', 'and', 'girls', ')', '?', '*', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '*', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '*', '*', '*', '*', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '*', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'commit', '##ing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '*', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '*', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', '*', '*', 'tips', '*', '*', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes'], ['or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', 'one', 'hour', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '.', '*', '*', '*', '*', '*', 'examples', 'from', 'last', 'week', ':', '*', '*', '*', 'un', '##load', 'the', 'dish', '##wash', '##er', '*', 'pack', 'for', 'moving', 'early', '*', 'email', 'professor', '*', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '*', 'make', 'an', 'appointment', 'with', 'doctor', '*', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '*', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '*', '*', '*', 'what', '*', '*', '-', '~', '~', 'i', 'want', 'to', '*', '*', 'get', 'started', 'on', 'my', 'taxes', '*', '*', '.', '*', 'if', 'i', 'get', 'them', 'done', 'this', 'week', 'my', 'wife', 'said', 'i', 'can', 'buy', 'a', 'ds', '##lr', '!', '*', '~', '~', '*', '*', '*', 'how', 'much', 'time', '*', '*', '-', 'just', '15', 'minutes', '*', '*', '*', 'when', '*', '*', '-', 'start', 'today', '.', 'finish', 'by', 'saturday', '.', '*', '*', '*', 'check', 'in', 'on', 'me', '?', '*', '*', '-', 'yes', '.', '5', 'hours', 'from', 'now', '.', '5', '##pm', 'ps', '##t', '.', 'tired', 'today', '!', '*', 'i', 'installed', 'turbo', '##ta', '##x', ',', 'found', 'out', 'how', 'much', 'i', 'made', 'last', 'year', '(', 'self', '-', 'employed', ')', 'and', 'got', 'tax', 'forms', 'together', '.', 'took', 'more', 'than', '15', 'minutes', 'but', 'at', 'least', 'i', 'will', 'feel', 'much', 'more', 'prepared', 'next', 'time', '!', '*', '~', '~', '*', 'go', 'to', 'tea', 'shop', 'and', 'pick', 'up', 'some', 'green', 'tea', '~', '~', '*', '*', '*', 'sit', 'down', 'for', '20', 'minutes', 'of', 'meditation', 'and', 'enjoy', 'my', '29th', 'birthday', '!', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['medication', 'recommendation', '?']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['medication', 'recommendation', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'drinking', 'water', 'enough', 'to', 'activate', 'the', 'digest', '##ion', 'processes', 'that', 'trigger', 'v', '##y', '##van', '##se', '?', 'squirrel', '!']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'drinking', 'water', 'enough', 'to', 'activate', 'the', 'digest', '##ion', 'processes', 'that', 'trigger', 'v', '##y', '##van', '##se', '?', 'squirrel', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', 'tips', 'for', 'stimulating', 'appetite', 'while', 'on', 'add', '##eral', '##l', '.', 'i', \"'\", 'm', 'down', 'to', '95', '##lb', '##s', '(', '5', \"'\", '2', '\"', ',', 'so', 'not', 'starving', 'but', 'very', 'close', 'to', 'under', '##weight', ')', 'and', 'my', 'co', '-', 'workers', 'are', 'starting', 'to', 'comment', 'on', 'how', 'thin', 'i', 'look', '.', 'i', 'never', 'had', 'much', 'interest', 'in', 'food', 'to', 'begin', 'with', 'but', 'on', 'add', '##eral', '##l', 'i', 'have', 'to', 'literally', 'choke', 'down', 'every', 'meal', '.', 'i', 'already', 'carry', 'around', 'snacks', 'and', 'have', 'a', 'food', 'alarm', 'on', 'my', 'cell', 'phone', 'to', 'remind', 'me', 'to', 'eat', 'something', 'every', 'hour', 'or', 'so', '(', 'i', 'tend', 'to', 'ignore', 'it', 'by', 'accident', 'more', 'often', 'than', 'not', ',', 'but', 'hey', '.', ')', 'what', 'i', \"'\", 'd', 'like', 'to', 'know', 'is', 'any', 'methods', 'anyone', 'might', 'know', 'of', 'to', 'stimulate', 'the', 'actual', 'desire', 'to', 'eat', '.', 'medications', ',', 'certain', 'kinds', 'of', 'food', ',', 'exercises', '.', '.', '.', 'i', 'don', \"'\", 't', 'know', '.', 'i', \"'\", 'm', 'just', 'seriously', 'tired', 'of', 'people', 'assuming', 'i', \"'\", 'm', 'an', '##ore', '##xi', '##c', 'and', 'i', \"'\", 'd', 'rather', 'not', 'have', 'to', 'go', 'through', 'every', 'meal', 'feeling', 'like', 'i', \"'\", 'm', 'being', 'tortured', '.', 'also', ':', 'marijuana', 'does', 'make', 'me', 'eat', 'more', ',', 'but', 'i', 'can', \"'\", 't', 'use', 'it', 'because', 'of', 'my', 'workplace', \"'\", 's', 'drug', 'policy', '.', 'anyone', 'know', 'if', 'it', \"'\", 's', 'possible', 'to', 'get', 'a', 'prescription', 'for', 'th', '##c', 'and', 'have', 'it', 'approved', 'by', 'an', 'employer', 'like', 'they', 'do', 'for', 'amp', '##het', '##amine', '##s', '?', 'i', 'figure', 'if', 'they', \"'\", 're', 'already', 'fine', 'with', 'me', 'being', 'on', 'legal', 'speed', 'they', \"'\", 'll', 'probably', 'be', 'okay', 'with', 'pseudo', '-', 'pot', '.']\n",
      "INFO:__main__:Number of tokens: 270\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', 'tips', 'for', 'stimulating', 'appetite', 'while', 'on', 'add', '##eral', '##l', '.', 'i', \"'\", 'm', 'down', 'to', '95', '##lb', '##s', '(', '5', \"'\", '2', '\"', ',', 'so', 'not', 'starving', 'but', 'very', 'close', 'to', 'under', '##weight', ')', 'and', 'my', 'co', '-', 'workers', 'are', 'starting', 'to', 'comment', 'on', 'how', 'thin', 'i', 'look', '.', 'i', 'never', 'had', 'much', 'interest', 'in', 'food', 'to', 'begin', 'with', 'but', 'on', 'add', '##eral', '##l', 'i', 'have', 'to', 'literally', 'choke', 'down', 'every', 'meal', '.', 'i', 'already', 'carry', 'around', 'snacks', 'and', 'have', 'a', 'food', 'alarm', 'on', 'my', 'cell', 'phone', 'to', 'remind', 'me', 'to', 'eat', 'something', 'every', 'hour', 'or', 'so', '(', 'i', 'tend', 'to', 'ignore', 'it', 'by', 'accident', 'more', 'often', 'than', 'not', ',', 'but', 'hey', '.', ')', 'what', 'i', \"'\", 'd', 'like', 'to', 'know', 'is', 'any', 'methods', 'anyone', 'might', 'know', 'of', 'to', 'stimulate', 'the', 'actual', 'desire', 'to', 'eat', '.', 'medications', ',', 'certain', 'kinds', 'of', 'food', ',', 'exercises', '.', '.', '.', 'i', 'don', \"'\", 't', 'know', '.', 'i', \"'\", 'm', 'just', 'seriously', 'tired', 'of', 'people', 'assuming', 'i', \"'\", 'm', 'an', '##ore', '##xi', '##c', 'and', 'i', \"'\", 'd', 'rather', 'not', 'have', 'to', 'go', 'through', 'every', 'meal', 'feeling', 'like', 'i', \"'\", 'm', 'being', 'tortured', '.', 'also', ':', 'marijuana', 'does', 'make', 'me', 'eat', 'more', ',', 'but', 'i', 'can', \"'\", 't', 'use', 'it', 'because', 'of', 'my', 'workplace', \"'\", 's', 'drug', 'policy', '.', 'anyone', 'know', 'if', 'it', \"'\", 's', 'possible', 'to', 'get', 'a', 'prescription', 'for', 'th', '##c', 'and', 'have', 'it', 'approved', 'by', 'an', 'employer', 'like', 'they', 'do', 'for', 'amp', '##het', '##amine', '##s', '?', 'i', 'figure', 'if', 'they', \"'\", 're', 'already', 'fine', 'with', 'me', 'being', 'on', 'legal', 'speed', 'they', \"'\", 'll', 'probably', 'be', 'okay', 'with', 'pseudo', '-', 'pot', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'doesn', \"'\", 't', 'seem', 'to', 'kick', 'in', 'till', 'the', 'afternoon', '.', 'i', \"'\", 've', 'been', 'on', 'add', '##eral', '##l', 'x', '##r', '20', '##mg', 'for', 'a', 'week', 'now', 'and', 'am', 'generally', 'pleased', 'with', 'the', 'results', '.', 'but', 'it', 'seems', 'to', 'take', 'forever', 'before', 'i', 'feel', 'calm', 'and', 'productive', '.', 'i', 'take', 'it', 'at', '7', '##am', 'and', 'i', 'don', \"'\", 't', 'seem', 'to', 'feel', 'its', 'effects', 'until', 'the', 'afternoon', '.', 'anyone', 'have', 'any', 'thoughts', 'or', 'suggestions', '?', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 80\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'doesn', \"'\", 't', 'seem', 'to', 'kick', 'in', 'till', 'the', 'afternoon', '.', 'i', \"'\", 've', 'been', 'on', 'add', '##eral', '##l', 'x', '##r', '20', '##mg', 'for', 'a', 'week', 'now', 'and', 'am', 'generally', 'pleased', 'with', 'the', 'results', '.', 'but', 'it', 'seems', 'to', 'take', 'forever', 'before', 'i', 'feel', 'calm', 'and', 'productive', '.', 'i', 'take', 'it', 'at', '7', '##am', 'and', 'i', 'don', \"'\", 't', 'seem', 'to', 'feel', 'its', 'effects', 'until', 'the', 'afternoon', '.', 'anyone', 'have', 'any', 'thoughts', 'or', 'suggestions', '?', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['thought', 'this', 'quick', 'video', 'would', 'help', 'people', '(', 'if', 'your', 'interested', ')', 'understand', 'how', 'ad', '##hd', 'affects', 'our', 'brain', \"'\", 's', 'emotional', 'control', '.', 'might', 'explain', 'a', 'few', 'things', 'about', 'our', 'behaviour', ':', ')']\n",
      "INFO:__main__:Number of tokens: 34\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['thought', 'this', 'quick', 'video', 'would', 'help', 'people', '(', 'if', 'your', 'interested', ')', 'understand', 'how', 'ad', '##hd', 'affects', 'our', 'brain', \"'\", 's', 'emotional', 'control', '.', 'might', 'explain', 'a', 'few', 'things', 'about', 'our', 'behaviour', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['medication', 'dos', '##ages', 'if', 'it', 'were', 'up', 'to', 'you', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['medication', 'dos', '##ages', 'if', 'it', 'were', 'up', 'to', 'you', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'is', 'making', 'me', 'tired', 'during', 'the', 'day', 'and', 'unable', 'to', 'sleep', 'at', 'night', 'any', 'tips', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'is', 'making', 'me', 'tired', 'during', 'the', 'day', 'and', 'unable', 'to', 'sleep', 'at', 'night', 'any', 'tips', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'let', \"'\", 's', 'celebrate', 'together', '!', '[', 'week', '3', ']', 'even', 'though', 'wednesday', 'has', 'passed', 'i', 'encourage', 'you', 'to', 'share', 'your', 'wins', 'throughout', 'the', 'week', '.', '*', '*', 'you', 'are', 'not', 'late', '!', '*', '*', 'this', 'is', 'a', 'great', 'awareness', 'builder', '!', '*', '*', '*', '*', '*', 'welcome', 'to', 'the', '3rd', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', 'it', \"'\", 's', 'the', 'moment', 'that', '3', 'of', 'you', 'have', 'been', 'waiting', 'for', '!', 'we', 'had', '12', 'participants', 'the', 'first', 'week', ',', '24', 'last', 'week', '.', '.', '.', '*', '*', 'can', 'we', 'keep', 'doubling', 'that', 'today', '*', '*', '?', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', 'so', 'here', 'if', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', '*', 'some', 'examples', 'from', 'last', 'week', '*', 'started', 'taking', 'ad', '##hd', 'medication', 'x', '##2', '*', 'called', 'doctor', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '*', 'working', 'out', 'and', 'eating', 'a', 'healthy', 'diet', '*', 'got', 'to', 'work', '.', '.', '.', 'on', 'time', '!', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', 'i', 'understand', 'it', 'might', 'be', 'hard', '(', 'initially', ')', 'to', 'think', 'of', 'something', '.', '*', '*', 'if', 'you', 'cannot', 'think', 'of', 'a', '\"', 'win', '\"', 'then', 'i', 'invite', 'you', 'to', 'name', '1', '(', 'or', 'more', ')', 'things', 'you', 'are', 'grateful', 'for', '.', '*', '*', '*', '*', '*', 'my', 'wins', ':', '*', 'celebrated', 'my', '29th', 'birthday', 'with', 'close', 'friends', '*', 'spent', '15', 'minutes', 'starting', 'my', 'taxes', '*', 'got', 'to', 'sleep', 'before', 'midnight', 'every', 'day', '(', 'except', 'b', '##day', 'yesterday', ')', '*', 'spent', 'a', 'lot', 'of', 'time', 'answering', 'comments', 'in', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', '*', 'went', 'for', '2', 'runs', 'this', 'week']\n",
      "INFO:__main__:Number of tokens: 455\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'let', \"'\", 's', 'celebrate', 'together', '!', '[', 'week', '3', ']', 'even', 'though', 'wednesday', 'has', 'passed', 'i', 'encourage', 'you', 'to', 'share', 'your', 'wins', 'throughout', 'the', 'week', '.', '*', '*', 'you', 'are', 'not', 'late', '!', '*', '*', 'this', 'is', 'a', 'great', 'awareness', 'builder', '!', '*', '*', '*', '*', '*', 'welcome', 'to', 'the', '3rd', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', 'it', \"'\", 's', 'the', 'moment', 'that', '3', 'of', 'you', 'have', 'been', 'waiting', 'for', '!', 'we', 'had', '12', 'participants', 'the', 'first', 'week', ',', '24', 'last', 'week', '.', '.', '.', '*', '*', 'can', 'we', 'keep', 'doubling', 'that', 'today', '*', '*', '?', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', 'so', 'here', 'if', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', '*', 'some', 'examples', 'from', 'last', 'week', '*', 'started', 'taking', 'ad', '##hd', 'medication', 'x', '##2', '*', 'called', 'doctor', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '*', 'working', 'out', 'and', 'eating', 'a', 'healthy', 'diet', '*', 'got', 'to', 'work', '.', '.', '.', 'on', 'time', '!', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', 'i', 'understand', 'it', 'might', 'be', 'hard', '(', 'initially', ')', 'to', 'think', 'of', 'something', '.', '*', '*', 'if', 'you', 'cannot', 'think', 'of', 'a', '\"', 'win', '\"', 'then', 'i', 'invite', 'you', 'to', 'name', '1', '(', 'or', 'more', ')', 'things', 'you', 'are', 'grateful', 'for', '.', '*', '*', '*', '*', '*', 'my', 'wins', ':', '*', 'celebrated', 'my', '29th', 'birthday', 'with', 'close', 'friends', '*', 'spent', '15', 'minutes', 'starting', 'my', 'taxes', '*', 'got', 'to', 'sleep', 'before', 'midnight', 'every', 'day', '(', 'except', 'b', '##day', 'yesterday', ')', '*', 'spent', 'a', 'lot', 'of', 'time', 'answering', 'comments', 'in', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', '*', 'went', 'for', '2', 'runs', 'this', 'week']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['when', 'i', 'stop', 'taking', 'my', 'concert', '##a', 'i', 'tend', 'to', 'suffer', 'from', 'really', 'poor', 'control', 'of', 'my', 'emotional', 'state', '.', 'is', 'this', 'common', '?', 'i', 'find', 'that', 'when', 'i', 'forget', 'to', 'take', 'concert', '##a', 'in', 'the', 'morning', 'i', \"'\", 'm', 'prone', 'to', 'sudden', 'bouts', 'of', 'very', 'intense', ',', 'anxious', 'energy', 'and', 'a', 'desire', 'to', 'commit', 'violence', 'towards', 'myself', 'or', 'others', '.', 'like', 'smash', 'my', 'computer', 'apart', 'with', 'my', 'forehead', 'or', 'bite', 'my', 'cow', '##or', '##ker', \"'\", 's', 'throat', '##s', '.', 'i', \"'\", 'm', 'obviously', 'able', 'to', 'suppress', 'these', 'urges', ',', 'but', 'i', \"'\", 've', 'always', 'wondered', 'if', 'this', 'is', 'a', 'common', 'thing', 'or', 'a', 'sign', 'that', 'maybe', 'concert', '##a', 'is', 'suppress', '##ing', 'my', 'emotions', 'in', 'a', 'big', 'way', '.']\n",
      "INFO:__main__:Number of tokens: 119\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['when', 'i', 'stop', 'taking', 'my', 'concert', '##a', 'i', 'tend', 'to', 'suffer', 'from', 'really', 'poor', 'control', 'of', 'my', 'emotional', 'state', '.', 'is', 'this', 'common', '?', 'i', 'find', 'that', 'when', 'i', 'forget', 'to', 'take', 'concert', '##a', 'in', 'the', 'morning', 'i', \"'\", 'm', 'prone', 'to', 'sudden', 'bouts', 'of', 'very', 'intense', ',', 'anxious', 'energy', 'and', 'a', 'desire', 'to', 'commit', 'violence', 'towards', 'myself', 'or', 'others', '.', 'like', 'smash', 'my', 'computer', 'apart', 'with', 'my', 'forehead', 'or', 'bite', 'my', 'cow', '##or', '##ker', \"'\", 's', 'throat', '##s', '.', 'i', \"'\", 'm', 'obviously', 'able', 'to', 'suppress', 'these', 'urges', ',', 'but', 'i', \"'\", 've', 'always', 'wondered', 'if', 'this', 'is', 'a', 'common', 'thing', 'or', 'a', 'sign', 'that', 'maybe', 'concert', '##a', 'is', 'suppress', '##ing', 'my', 'emotions', 'in', 'a', 'big', 'way', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'research', 'shows', 'ad', '##hd', 'is', 'not', 'one', 'disorder', ',', 'but', 'a', 'family', 'of', 'disorders']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'research', 'shows', 'ad', '##hd', 'is', 'not', 'one', 'disorder', ',', 'but', 'a', 'family', 'of', 'disorders']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'can', 'i', 'get', 'my', 'boyfriend', 'to', 'consider', 'medication', 'for', 'his', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'can', 'i', 'get', 'my', 'boyfriend', 'to', 'consider', 'medication', 'for', 'his', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'get', 'tingling', 'lips', 'and', 'teeth', 'on', 'add', '##eral', '##l', '?']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'get', 'tingling', 'lips', 'and', 'teeth', 'on', 'add', '##eral', '##l', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['for', 'those', 'with', 'di', '##sb', '##eli', '##eving', 'parents', ':', '\"', 'i', \"'\", 'm', 'christian', ',', 'unless', 'you', \"'\", 're', '.', '.', '.', '\"']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['for', 'those', 'with', 'di', '##sb', '##eli', '##eving', 'parents', ':', '\"', 'i', \"'\", 'm', 'christian', ',', 'unless', 'you', \"'\", 're', '.', '.', '.', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['question', 'about', 'a', 'weird', 'fee', 'my', 'ca', 'doctor', 'charged', 'for', 'writing', 'my', 'prescription', '.', 'i', 'live', 'in', 'california', '.', 'i', 'recently', 'got', 'diagnosed', 'and', 'was', 'prescribed', 'dex', '##ed', '##rine', '.', 'my', 'doctor', 'told', 'me', 'he', 'had', 'to', 'charge', 'me', '$', '20', 'to', '\"', 'write', 'it', 'on', 'this', 'special', 'pad', '\"', 'and', 'that', 'it', 'wasn', \"'\", 't', 'covered', 'by', 'my', 'insurance', '.', 'so', 'it', 'had', 'to', 'be', '100', '%', 'out', 'of', 'pocket', '.', 'then', 'i', 'took', 'my', \"'\", 'script', 'to', 'the', 'pharmacy', ',', 'so', 'it', \"'\", 's', 'not', 'like', 'the', 'charge', 'was', 'to', 'actually', 'fill', 'the', 'prescription', '.', 'this', 'is', 'completely', 'new', 'and', 'very', 'strange', 'to', 'me', 'as', 'i', 'have', 'really', 'amazing', 'insurance', '(', '$', '0', 'co', '-', 'pay', 'for', 'visits', 'or', 'medication', ')', '.', 'i', 'tried', 'to', 'google', ',', 'but', 'it', \"'\", 's', 'pretty', 'hard', 'to', 'find', 'any', 'related', 'information', 'and', 'i', 'don', \"'\", 't', 'know', 'anyone', 'else', 'in', 'ca', 'that', 'will', 'openly', 'discuss', 'their', 'ad', '##hd', 'diagnosis', 'or', 'medication', '.', 'is', 'this', 'normal', ',', 'just', 'a', 'california', 'thing', ',', 'or', 'is', 'my', 'doctor', 'up', 'to', 'something', 'du', '##pl', '##ici', '##tou', '##s', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'is', 'my', 'california', 'doctor', 'jerking', 'me', 'around', 'by', 'charging', 'me', 'extra', 'to', 'write', 'prescription', '##s', '?', '*', '*']\n",
      "INFO:__main__:Number of tokens: 208\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['question', 'about', 'a', 'weird', 'fee', 'my', 'ca', 'doctor', 'charged', 'for', 'writing', 'my', 'prescription', '.', 'i', 'live', 'in', 'california', '.', 'i', 'recently', 'got', 'diagnosed', 'and', 'was', 'prescribed', 'dex', '##ed', '##rine', '.', 'my', 'doctor', 'told', 'me', 'he', 'had', 'to', 'charge', 'me', '$', '20', 'to', '\"', 'write', 'it', 'on', 'this', 'special', 'pad', '\"', 'and', 'that', 'it', 'wasn', \"'\", 't', 'covered', 'by', 'my', 'insurance', '.', 'so', 'it', 'had', 'to', 'be', '100', '%', 'out', 'of', 'pocket', '.', 'then', 'i', 'took', 'my', \"'\", 'script', 'to', 'the', 'pharmacy', ',', 'so', 'it', \"'\", 's', 'not', 'like', 'the', 'charge', 'was', 'to', 'actually', 'fill', 'the', 'prescription', '.', 'this', 'is', 'completely', 'new', 'and', 'very', 'strange', 'to', 'me', 'as', 'i', 'have', 'really', 'amazing', 'insurance', '(', '$', '0', 'co', '-', 'pay', 'for', 'visits', 'or', 'medication', ')', '.', 'i', 'tried', 'to', 'google', ',', 'but', 'it', \"'\", 's', 'pretty', 'hard', 'to', 'find', 'any', 'related', 'information', 'and', 'i', 'don', \"'\", 't', 'know', 'anyone', 'else', 'in', 'ca', 'that', 'will', 'openly', 'discuss', 'their', 'ad', '##hd', 'diagnosis', 'or', 'medication', '.', 'is', 'this', 'normal', ',', 'just', 'a', 'california', 'thing', ',', 'or', 'is', 'my', 'doctor', 'up', 'to', 'something', 'du', '##pl', '##ici', '##tou', '##s', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'is', 'my', 'california', 'doctor', 'jerking', 'me', 'around', 'by', 'charging', 'me', 'extra', 'to', 'write', 'prescription', '##s', '?', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'i', 'deal', 'with', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'i', 'deal', 'with', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'mo', '##tiv', '##ates', 'us', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'mo', '##tiv', '##ates', 'us', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['good', 'supplements', ',', 'non', '-', 'prescription', 'drugs', 'or', 'anything', 'to', 'help', 'with', 'concentration', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['good', 'supplements', ',', 'non', '-', 'prescription', 'drugs', 'or', 'anything', 'to', 'help', 'with', 'concentration', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'ability', 'to', 'focus', 'is', 'seemingly', 'under', 'my', 'control', ',', 'but', 'i', 'just', 'don', '##t', '\"', 'choose', '\"', 'to', 'focus', '.', 'anyone', 'else', 'feel', 'like', 'this', '?', 'the', 'thing', 'is', ',', 'i', \"'\", 've', 'had', 'symptoms', 'of', 'add', 'my', 'whole', 'life', ',', 'and', 'some', 'of', 'them', 'really', 'do', 'affect', 'me', '.', 'i', 'have', 'no', 'ability', 'to', 'focus', 'on', 'the', 'simplest', 'of', 'things', ',', 'like', 'putting', 'laundry', 'away', 'for', 'folding', 'it', 'or', 'even', 'doing', 'it', ',', 'preparing', 'a', 'meal', ',', 'hygiene', ',', 'etc', '.', 'see', ',', 'i', 'know', 'my', 'add', 'is', 'bad', 'because', 'i', 'can', 'let', 'everything', 'pile', 'up', ':', 'school', 'work', ',', 'things', ',', 'clothes', ',', 'and', 'i', 'don', \"'\", 't', 'care', '.', 'i', 'just', 'keep', 'imp', '##ulsive', '##ly', 'doing', 'what', 'i', 'want', 'to', 'do', ',', 'i', 'feel', 'lazy', 'more', 'often', 'than', 'i', 'feel', 'distracted', ',', 'and', 'just', 'not', 'motivated', 'to', 'do', 'basic', 'things', '.', '\"', 'forgetting', '\"', 'to', 'take', 'my', 'medication', 'for', 'my', 'cr', '##oh', '##ns', 'disease', 'has', 'had', 'the', 'biggest', 'impact', 'on', 'my', 'life', 'and', 'i', 'must', 'actively', 'fight', 'against', 'the', 'constant', 'desire', 'to', 'just', 'not', 'take', 'my', 'medicine', '.', 'it', \"'\", 's', 'not', 'because', 'i', 'don', \"'\", 't', 'want', 'to', 'do', 'it', 'for', 'any', 'particular', 'reason', '.', 'i', 'just', 'have', 'had', 'issues', 'with', 'wanting', 'to', 'take', 'it', 'at', 'all', 'my', 'whole', 'life', '.', 'its', 'been', 'very', 'bad', 'for', 'my', 'health', '.', 'at', 'work', 'and', 'school', ',', 'i', 'will', 'pro', '##cr', '##ast', '##inate', 'beyond', 'reason', 'at', 'times', 'and', 'neglect', 'to', 'do', 'work', 'because', 'i', 'do', 'it', '\"', 'at', 'the', 'last', 'minute', '\"', 'always', '.', 'in', '##fa', '##ct', ',', 'im', 'neglect', '##ing', 'stuff', 'right', 'now', 'and', 'i', 'just', 'remembered', 'that', ',', 'so', 'i', \"'\", 'll', 'have', 'to', 'wrap', 'this', 'up', '.', 'i', 'just', 'feel', 'like', 'my', 'add', 'isn', \"'\", 't', 'a', 'huge', 'problem', 'because', 'i', 'can', 'think', '/', 'focus', 'on', 'what', 'i', 'find', 'really', 'interesting', 'for', 'hours', '.', 'i', 'neglect', 'other', 'responsibilities', 'just', 'to', 'do', 'what', 'i', 'find', 'naturally', 'interesting', ',', 'which', 'is', 'usually', ',', 'thinking', 'and', 'analyzing', 'situations', '.', 'i', 'like', 'reading', 'certain', 'books', ',', 'surfing', 'certain', 'sub', '##red', '##dit', '##s', ',', 'and', 'thinking', 'deeply', 'about', 'subjects', 'normal', 'people', 'never', 'think', 'about', 'ever', ',', 'let', 'alone', 'every', 'day', '.', 'i', 'always', 'feel', 'like', 'im', 'not', 'a', '\"', 'real', '\"', 'suffer', '##er', 'of', 'add', '.', 'i', 'don', \"'\", 't', 'even', 'know', 'what', 'kind', 'of', 'add', 'this', 'really', 'is', '.', 'does', 'it', 'really', 'impact', 'my', 'distract', '##ability', 'as', 'much', 'as', 'it', 'is', 'claimed', 'to', '?', 'because', 'i', 'write', 'prolific', '##ly', ',', 'i', 'have', 'so', 'many', 'thoughts', 'i', 'love', 'sharing', ',', 'and', 'i', 'can', 'focus', 'on', 'writing', 'and', 'talking', 'about', 'them', 'easily', ',', 'even', 'though', 'i', 'tend', 'to', 'jump', 'from', 'one', 'thought', 'to', 'another', 'rather', 'quickly', '.', 'what', 'are', 'your', 'thoughts', '?', '*', '*', 't', '##l', ';', 'dr', 'although', 'i', 'clearly', 'show', 'symptoms', 'that', 'affect', 'my', 'ability', 'to', 'do', 'mundane', 'things', ',', 'my', 'focus', 'is', 'pretty', 'good', 'when', 'i', \"'\", 'm', 'writing', 'or', 'thinking', 'about', 'something', 'i', 'enjoy', '.', 'i', 'know', 'my', 'add', 'is', 'real', ',', 'but', 'deep', 'down', 'inside', 'i', 'question', 'whether', 'or', 'not', 'i', 'have', 'the', 'power', 'to', 'change', 'what', 'i', 'find', 'interesting', 'and', 'focus', 'on', 'that', '.', 'i', 'try', 'and', 'fail', 'at', 'that', 'too', '.', 'what', 'kind', 'of', 'thoughts', 'do', 'you', 'all', 'have', '?', '*', '*']\n",
      "INFO:__main__:Number of tokens: 538\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['my', 'ability', 'to', 'focus', 'is', 'seemingly', 'under', 'my', 'control', ',', 'but', 'i', 'just', 'don', '##t', '\"', 'choose', '\"', 'to', 'focus', '.', 'anyone', 'else', 'feel', 'like', 'this', '?', 'the', 'thing', 'is', ',', 'i', \"'\", 've', 'had', 'symptoms', 'of', 'add', 'my', 'whole', 'life', ',', 'and', 'some', 'of', 'them', 'really', 'do', 'affect', 'me', '.', 'i', 'have', 'no', 'ability', 'to', 'focus', 'on', 'the', 'simplest', 'of', 'things', ',', 'like', 'putting', 'laundry', 'away', 'for', 'folding', 'it', 'or', 'even', 'doing', 'it', ',', 'preparing', 'a', 'meal', ',', 'hygiene', ',', 'etc', '.', 'see', ',', 'i', 'know', 'my', 'add', 'is', 'bad', 'because', 'i', 'can', 'let', 'everything', 'pile', 'up', ':', 'school', 'work', ',', 'things', ',', 'clothes', ',', 'and', 'i', 'don', \"'\", 't', 'care', '.', 'i', 'just', 'keep', 'imp', '##ulsive', '##ly', 'doing', 'what', 'i', 'want', 'to', 'do', ',', 'i', 'feel', 'lazy', 'more', 'often', 'than', 'i', 'feel', 'distracted', ',', 'and', 'just', 'not', 'motivated', 'to', 'do', 'basic', 'things', '.', '\"', 'forgetting', '\"', 'to', 'take', 'my', 'medication', 'for', 'my', 'cr', '##oh', '##ns', 'disease', 'has', 'had', 'the', 'biggest', 'impact', 'on', 'my', 'life', 'and', 'i', 'must', 'actively', 'fight', 'against', 'the', 'constant', 'desire', 'to', 'just', 'not', 'take', 'my', 'medicine', '.', 'it', \"'\", 's', 'not', 'because', 'i', 'don', \"'\", 't', 'want', 'to', 'do', 'it', 'for', 'any', 'particular', 'reason', '.', 'i', 'just', 'have', 'had', 'issues', 'with', 'wanting', 'to', 'take', 'it', 'at', 'all', 'my', 'whole', 'life', '.', 'its', 'been', 'very', 'bad', 'for', 'my', 'health', '.', 'at', 'work', 'and', 'school', ',', 'i', 'will', 'pro', '##cr', '##ast', '##inate', 'beyond', 'reason', 'at', 'times', 'and', 'neglect', 'to', 'do', 'work', 'because', 'i', 'do', 'it', '\"', 'at', 'the', 'last', 'minute', '\"', 'always', '.', 'in', '##fa', '##ct', ',', 'im', 'neglect', '##ing', 'stuff', 'right', 'now', 'and', 'i', 'just', 'remembered', 'that', ',', 'so', 'i', \"'\", 'll', 'have', 'to', 'wrap', 'this', 'up', '.', 'i', 'just', 'feel', 'like', 'my', 'add', 'isn', \"'\", 't', 'a', 'huge', 'problem', 'because', 'i', 'can', 'think', '/', 'focus', 'on', 'what', 'i', 'find', 'really', 'interesting', 'for', 'hours', '.', 'i', 'neglect', 'other', 'responsibilities', 'just', 'to', 'do', 'what', 'i', 'find', 'naturally', 'interesting', ',', 'which', 'is', 'usually', ',', 'thinking', 'and', 'analyzing', 'situations', '.', 'i', 'like', 'reading', 'certain', 'books', ',', 'surfing', 'certain', 'sub', '##red', '##dit', '##s', ',', 'and', 'thinking', 'deeply', 'about', 'subjects', 'normal', 'people', 'never', 'think', 'about', 'ever', ',', 'let', 'alone', 'every', 'day', '.', 'i', 'always', 'feel', 'like', 'im', 'not', 'a', '\"', 'real', '\"', 'suffer', '##er', 'of', 'add', '.', 'i', 'don', \"'\", 't', 'even', 'know', 'what', 'kind', 'of', 'add', 'this', 'really', 'is', '.', 'does', 'it', 'really', 'impact', 'my', 'distract', '##ability', 'as', 'much', 'as', 'it', 'is', 'claimed', 'to', '?', 'because', 'i', 'write', 'prolific', '##ly', ',', 'i', 'have', 'so', 'many', 'thoughts', 'i', 'love', 'sharing', ',', 'and', 'i', 'can', 'focus', 'on', 'writing', 'and', 'talking', 'about', 'them', 'easily', ',', 'even', 'though', 'i', 'tend', 'to', 'jump', 'from', 'one', 'thought', 'to', 'another', 'rather', 'quickly', '.', 'what', 'are', 'your', 'thoughts', '?', '*', '*', 't', '##l', ';', 'dr', 'although', 'i', 'clearly', 'show', 'symptoms', 'that', 'affect', 'my', 'ability', 'to', 'do', 'mundane', 'things', ',', 'my', 'focus', 'is', 'pretty', 'good', 'when', 'i', \"'\", 'm', 'writing', 'or', 'thinking', 'about', 'something', 'i', 'enjoy', '.', 'i', 'know', 'my', 'add', 'is', 'real', ',', 'but', 'deep', 'down', 'inside', 'i', 'question', 'whether', 'or', 'not', 'i', 'have', 'the', 'power', 'to', 'change', 'what', 'i'], ['find', 'interesting', 'and', 'focus', 'on', 'that', '.', 'i', 'try', 'and', 'fail', 'at', 'that', 'too', '.', 'what', 'kind', 'of', 'thoughts', 'do', 'you', 'all', 'have', '?', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['if', 'ph', '##arm', '##a', 'made', 'tri', '##kes', '-', '\"', 'buyer', 'be', '##ware', '\"']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['if', 'ph', '##arm', '##a', 'made', 'tri', '##kes', '-', '\"', 'buyer', 'be', '##ware', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['skin', 'pick', '##er', 'so', 'on', 'the', 'v', '##y', '##van', '##se', 'the', 'side', 'effect', 'is', 'skin', 'picking', '.', 'i', 'can', '##t', 'stop', 'my', 'face', 'is', 'all', 'scarred', 'up', 'now', 'because', 'i', 'can', '##t', 'stop', 'picking', '.', 'how', 'do', 'i', 'stop', 'without', 'changing', 'my', 'med', '##s', '.', 'i', 'keep', 'finding', 'imperfect', '##ions', 'and', 'pick', 'and', 'pick', 'and', 'pick', '.']\n",
      "INFO:__main__:Number of tokens: 57\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['skin', 'pick', '##er', 'so', 'on', 'the', 'v', '##y', '##van', '##se', 'the', 'side', 'effect', 'is', 'skin', 'picking', '.', 'i', 'can', '##t', 'stop', 'my', 'face', 'is', 'all', 'scarred', 'up', 'now', 'because', 'i', 'can', '##t', 'stop', 'picking', '.', 'how', 'do', 'i', 'stop', 'without', 'changing', 'my', 'med', '##s', '.', 'i', 'keep', 'finding', 'imperfect', '##ions', 'and', 'pick', 'and', 'pick', 'and', 'pick', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['v', '##y', '##van', '##se', 'users', ',', 'do', 'you', 'take', 'it', 'every', 'day', '?', 'need', 'advice', '.', 'i', \"'\", 've', 'been', 'on', '30', '##mg', 'of', 'v', '##y', '##van', '##se', 'for', 'about', 'a', 'year', 'now', '.', 'at', 'this', 'point', 'i', 'generally', 'take', 'it', 'the', '5', 'days', 'during', 'the', 'week', ',', 'but', 'not', 'the', 'weekend', '.', 'i', 'have', 'realized', 'this', 'may', 'be', 'changing', 'its', 'effectiveness', ',', 'and', 'possibly', 'making', 'things', 'harder', 'for', 'me', '.', 'does', 'anyone', 'have', 'any', 'information', 'to', 'share', '?']\n",
      "INFO:__main__:Number of tokens: 79\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['v', '##y', '##van', '##se', 'users', ',', 'do', 'you', 'take', 'it', 'every', 'day', '?', 'need', 'advice', '.', 'i', \"'\", 've', 'been', 'on', '30', '##mg', 'of', 'v', '##y', '##van', '##se', 'for', 'about', 'a', 'year', 'now', '.', 'at', 'this', 'point', 'i', 'generally', 'take', 'it', 'the', '5', 'days', 'during', 'the', 'week', ',', 'but', 'not', 'the', 'weekend', '.', 'i', 'have', 'realized', 'this', 'may', 'be', 'changing', 'its', 'effectiveness', ',', 'and', 'possibly', 'making', 'things', 'harder', 'for', 'me', '.', 'does', 'anyone', 'have', 'any', 'information', 'to', 'share', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['went', 'to', 'a', 'ne', '##uro', '##logist', 'for', 'memory', 'problems', ',', 'getting', 'tested', ',', 'online', 'test', 'says', 'probably', '>', 'results', 'of', 'your', '>', 'attention', 'deficit', 'disorder', 'quiz', '>', '>', 'you', 'scored', 'a', 'total', 'of', '79', '>', '>', 'it', 'is', 'highly', 'likely', 'that', 'you', 'are', 'presently', 'suffering', 'from', 'adult', 'attention', 'deficit', 'disorder', ',', 'according', 'to', 'your', 'responses', 'on', 'this', 'self', '-', 'report', 'question', '##naire', '.', 'you', 'should', 'not', 'take', 'this', 'as', 'a', 'diagnosis', 'of', 'any', 'sort', ',', 'or', 'a', 'recommendation', 'for', 'treatment', '.', 'however', ',', 'it', 'would', 'be', 'ad', '##vis', '##able', 'and', 'likely', 'beneficial', 'for', 'you', 'to', 'seek', 'further', 'diagnosis', 'from', 'a', 'trained', 'mental', 'health', 'professional', 'immediately', '.', 'it', 'was', 'this', '[', 'test', ']', '(', 'http', ':', '/', '/', 'ps', '##ych', '##cent', '##ral', '.', 'com', '/', 'add', '##qui', '##z', '.', 'h', '##tm', ')', 'ok', 'so', 'the', 'online', 'test', 'thing', 'is', 'stupid', 'and', 'not', 'scientific', 'at', 'all', ',', 'i', 'get', 'it', '.', 'just', 'thought', 'that', 'it', 'was', 'pretty', 'telling', 'that', 'i', 'scored', '79', 'and', 'i', 'sent', 'it', 'to', 'two', 'of', 'my', '\"', 'normal', '\"', 'friends', 'and', 'they', 'were', 'both', 'in', 'the', '20s', 'moving', 'on', ',', 'i', 'have', 'some', 'long', 'term', 'memory', '\"', 'issues', '.', '\"', 'i', 'can', \"'\", 't', 'seem', 'to', 'recall', 'large', 'portions', 'of', 'my', 'childhood', 'and', 'other', 'more', 'recent', 'periods', '.', 'i', \"'\", 'm', 'getting', 'an', 'mri', 'to', 'rule', 'out', 'anything', 'physical', '(', 'doc', 'says', 'most', 'likely', 'not', ')', 'but', 'the', 'more', 'we', 'dug', 'into', 'the', 'simple', 'tests', ',', 'he', 'asked', 'if', 'i', 'was', 'ever', 'diagnosed', 'with', 'add', 'and', 'says', 'that', 'he', 'has', 'a', 'sinking', 'suspicion', 'that', 'i', 'might', 'have', 'it', '.', 'he', 'also', 'went', 'on', 'to', 'talk', 'about', 'how', 'add', 'drugs', 'tend', 'to', 'be', 'over', 'prescribed', 'and', 'the', 'add', 'diagnosis', 'seems', 'to', 'be', 'given', 'out', 'too', 'often', 'w', '/', 'o', 'proper', 'testing', ',', 'so', 'i', 'have', 'the', 'feeling', 'he', \"'\", 's', 'not', 'just', 'trying', 'to', 'med', '##icate', 'me', 'and', 'move', 'on', 'with', 'life', '.', 'so', 'here', 'goes', ',', 'time', 'to', 'make', 'my', 'app', '##t', 'for', 'the', '4', '-', '6', 'hour', 'testing', '.', 'i', 'should', 'have', 'some', 'sort', 'of', 'insight', 'soon', '.', 'any', 'advice', 'for', 'a', 'potential', 'new', 'member', 'to', 'the', 'club', '?', 'i', \"'\", 've', 'added', 'a', 'few', 'books', 'to', 'my', 'amazon', 'cart', ',', 'going', 'to', 'try', 'to', 'read', 'up', 'a', 'little', '.', 't', '##l', ';', 'dr', ':', 'i', 'probably', 'have', 'adult', 'add', ',', 'ok', 'now', 'what', '?']\n",
      "INFO:__main__:Number of tokens: 388\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['went', 'to', 'a', 'ne', '##uro', '##logist', 'for', 'memory', 'problems', ',', 'getting', 'tested', ',', 'online', 'test', 'says', 'probably', '>', 'results', 'of', 'your', '>', 'attention', 'deficit', 'disorder', 'quiz', '>', '>', 'you', 'scored', 'a', 'total', 'of', '79', '>', '>', 'it', 'is', 'highly', 'likely', 'that', 'you', 'are', 'presently', 'suffering', 'from', 'adult', 'attention', 'deficit', 'disorder', ',', 'according', 'to', 'your', 'responses', 'on', 'this', 'self', '-', 'report', 'question', '##naire', '.', 'you', 'should', 'not', 'take', 'this', 'as', 'a', 'diagnosis', 'of', 'any', 'sort', ',', 'or', 'a', 'recommendation', 'for', 'treatment', '.', 'however', ',', 'it', 'would', 'be', 'ad', '##vis', '##able', 'and', 'likely', 'beneficial', 'for', 'you', 'to', 'seek', 'further', 'diagnosis', 'from', 'a', 'trained', 'mental', 'health', 'professional', 'immediately', '.', 'it', 'was', 'this', '[', 'test', ']', '(', 'http', ':', '/', '/', 'ps', '##ych', '##cent', '##ral', '.', 'com', '/', 'add', '##qui', '##z', '.', 'h', '##tm', ')', 'ok', 'so', 'the', 'online', 'test', 'thing', 'is', 'stupid', 'and', 'not', 'scientific', 'at', 'all', ',', 'i', 'get', 'it', '.', 'just', 'thought', 'that', 'it', 'was', 'pretty', 'telling', 'that', 'i', 'scored', '79', 'and', 'i', 'sent', 'it', 'to', 'two', 'of', 'my', '\"', 'normal', '\"', 'friends', 'and', 'they', 'were', 'both', 'in', 'the', '20s', 'moving', 'on', ',', 'i', 'have', 'some', 'long', 'term', 'memory', '\"', 'issues', '.', '\"', 'i', 'can', \"'\", 't', 'seem', 'to', 'recall', 'large', 'portions', 'of', 'my', 'childhood', 'and', 'other', 'more', 'recent', 'periods', '.', 'i', \"'\", 'm', 'getting', 'an', 'mri', 'to', 'rule', 'out', 'anything', 'physical', '(', 'doc', 'says', 'most', 'likely', 'not', ')', 'but', 'the', 'more', 'we', 'dug', 'into', 'the', 'simple', 'tests', ',', 'he', 'asked', 'if', 'i', 'was', 'ever', 'diagnosed', 'with', 'add', 'and', 'says', 'that', 'he', 'has', 'a', 'sinking', 'suspicion', 'that', 'i', 'might', 'have', 'it', '.', 'he', 'also', 'went', 'on', 'to', 'talk', 'about', 'how', 'add', 'drugs', 'tend', 'to', 'be', 'over', 'prescribed', 'and', 'the', 'add', 'diagnosis', 'seems', 'to', 'be', 'given', 'out', 'too', 'often', 'w', '/', 'o', 'proper', 'testing', ',', 'so', 'i', 'have', 'the', 'feeling', 'he', \"'\", 's', 'not', 'just', 'trying', 'to', 'med', '##icate', 'me', 'and', 'move', 'on', 'with', 'life', '.', 'so', 'here', 'goes', ',', 'time', 'to', 'make', 'my', 'app', '##t', 'for', 'the', '4', '-', '6', 'hour', 'testing', '.', 'i', 'should', 'have', 'some', 'sort', 'of', 'insight', 'soon', '.', 'any', 'advice', 'for', 'a', 'potential', 'new', 'member', 'to', 'the', 'club', '?', 'i', \"'\", 've', 'added', 'a', 'few', 'books', 'to', 'my', 'amazon', 'cart', ',', 'going', 'to', 'try', 'to', 'read', 'up', 'a', 'little', '.', 't', '##l', ';', 'dr', ':', 'i', 'probably', 'have', 'adult', 'add', ',', 'ok', 'now', 'what', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', 'question', 'about', 'med', '##s', 'my', 'six', 'year', 'old', 'daughter', 'has', 'been', 'taking', 'v', '##y', '##van', '##se', 'and', 'was', 'just', 'prescribed', 'ten', '##ex', 'for', 'some', 'anger', 'and', 'ag', '##ress', '##ion', 'issues', '.', 'so', 'my', 'question', 'is', ',', 'does', 'it', 'take', 'awhile', 'for', 'the', 'ten', '##ex', 'to', 'start', 'working', '?', 'she', 'has', 'been', 'taking', 'it', 'for', 'a', 'couple', 'days', 'and', 'i', 'haven', \"'\", 't', 'noticed', 'anything', '.']\n",
      "INFO:__main__:Number of tokens: 66\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', 'question', 'about', 'med', '##s', 'my', 'six', 'year', 'old', 'daughter', 'has', 'been', 'taking', 'v', '##y', '##van', '##se', 'and', 'was', 'just', 'prescribed', 'ten', '##ex', 'for', 'some', 'anger', 'and', 'ag', '##ress', '##ion', 'issues', '.', 'so', 'my', 'question', 'is', ',', 'does', 'it', 'take', 'awhile', 'for', 'the', 'ten', '##ex', 'to', 'start', 'working', '?', 'she', 'has', 'been', 'taking', 'it', 'for', 'a', 'couple', 'days', 'and', 'i', 'haven', \"'\", 't', 'noticed', 'anything', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['disgusting', '##ly', 'un', '##mot', '##ivated']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['disgusting', '##ly', 'un', '##mot', '##ivated']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['more', 'in', 'touch', 'with', 'my', 'emotions', 'with', '20', '##mg', 'add', '##eral', '##l', 'x', '##r', ',', 'anyone', 'else', 'have', 'this', 'reaction', '?', 'also', 'flat', 'feeling', 'at', 'night', 'after', 'it', 'wears', 'off', 'a', 'few', 'hours', 'before', 'i', 'go', 'to', 'sleep', '?', 'so', 'i', 'tried', 'st', '##rate', '##rra', 'and', 'had', 'bouts', 'of', 'crying', 'and', 'es', '##cala', '##ting', 'suicidal', 'thoughts', 'so', 'i', 'went', 'to', 'the', 'hospital', 'and', 'got', 'put', 'in', 'the', 'ps', '##ych', 'ward', 'for', 'a', 'few', 'days', '.', 'i', 'never', 'had', 'thoughts', 'like', 'that', 'before', 'so', 'i', 'am', 'fairly', 'confident', 'it', 'was', 'due', 'to', 'the', 'st', '##rat', '##tera', '.', 'i', 'have', 'been', 'crying', 'on', 'the', 'add', '##eral', '##l', 'but', 'it', 'is', 'all', 'related', 'to', 'my', 'shitty', 'childhood', 'and', 'i', 'don', \"'\", 't', 'have', 'any', 'suicidal', 'thoughts', 'so', 'i', 'know', 'these', 'feelings', 'are', 'not', 'irrational', 'or', 'harmful', '.', 'before', 'i', 'would', 'get', 'upset', 'about', 'more', 'minor', 'things', 'and', 'not', 'have', 'any', 'emotional', 'reaction', 'to', 'the', 'larger', 'problems', 'that', 'logical', '##ly', 'i', 'knew', 'i', 'should', 'and', 'that', 'other', 'people', 'were', 'surprised', 'i', 'could', 'be', 'so', 'calm', 'about', '.', 'its', 'in', 'a', 'way', 'a', 'good', 'thing', 'because', 'i', 'can', 'finally', 'start', 'dealing', 'with', 'these', 'su', '##pressed', 'feelings', 'but', 'on', 'the', 'other', 'hand', 'the', 'reason', 'i', 'am', 'taking', 'add', '##eral', '##l', 'is', 'because', 'of', 'how', 'impossible', 'for', 'me', 'to', 'focus', 'in', 'school', ',', 'and', 'even', 'though', 'i', 'think', 'this', 'effect', 'might', 'be', 'the', '##ra', '##put', '##ic', 'if', 'i', 'explore', 'it', 'with', 'my', 'therapist', ',', 'emotional', 'trauma', 'crop', '##ping', 'up', 'from', 'childhood', 'isn', \"'\", 't', 'exactly', 'convenient', '.', 'also', 'i', 'have', 'realized', 'a', 'lot', 'of', 'things', 'about', 'my', 'family', \"'\", 's', 'dynamic', 'that', 'i', 'never', 'saw', 'before', 'and', 'i', 'am', 'now', 'coming', 'to', 'terms', 'with', 'the', 'fact', 'that', 'my', 'dad', 'is', 'mentally', 'ill', 'and', 'very', 'mani', '##pu', '##lative', 'and', 'that', 'it', 'isn', \"'\", 't', 'my', 'responsibility', 'to', 'take', 'care', 'of', 'him', 'which', 'com', '##pl', '##icate', '##s', 'matters', 'because', 'i', 'now', 'don', \"'\", 't', 'think', 'that', 'i', 'can', 'have', 'any', 'relationship', 'with', 'him', ',', 'but', 'i', 'think', 'he', 'might', 'do', 'something', 'crazy', 'because', 'he', 'has', 'no', 'one', 'else', 'and', 'his', 'pattern', 'of', 'behavior', 'is', 'to', 'go', 'bat', 'shit', 'crazy', 'when', 'someone', 'tries', 'to', 'cut', 'ties', 'with', 'him', '.', 'basically', 'i', 'would', 'really', 'appreciate', 'some', 'opinions', 'about', 'whether', 'or', 'not', 'these', 'feelings', 'sound', 'harmful', 'or', 'ill', '##ogical', 'and', 'maybe', 'what', 'i', 'should', 'do', 'about', 'it', '.', 'i', 'think', 'the', 'focusing', 'effect', 'in', 'add', '##eral', '##l', 'is', 'making', 'it', 'impossible', 'to', 'avoid', 'feelings', 'that', 'before', 'i', 'just', 'gloss', '##ed', 'over', 'and', 'i', 'think', 'it', 'is', 'also', 'helping', 'me', 'organize', 'my', 'thou', '##ht', '##s', 'while', 'being', 'emotional', '.', 'usually', 'i', 'can', \"'\", 't', 'unite', 'those', 'two', 'sides', 'of', 'me', '.', 'i', 'had', 'a', 'feeling', 'of', 'flat', '##ness', 'and', 'ap', '##athy', 'after', '8', '##pm', 'last', 'night', 'but', 'i', 'didn', \"'\", 't', 'want', 'to', 'go', 'to', 'sleep', 'then', 'so', 'i', 'took', 'another', '20', '##mg', 'to', 'see', 'how', 'that', 'would', 'effect', 'it', '.', 'not', 'a', 'great', 'idea', '.', 'stayed', 'up', 'all', 'night', 'and', 'had', 'jaw', 'stiff', '##ness', 'and', 'teeth', 'grinding', '.', 'i', 'have', 'an', 'appointment', 'with', 'my', 'doc', 'next', 'week', ',', 'should', 'i', 'ask', 'for', 'add', '##eral', '##l', 'ir', 'to', 'get', 'me', 'through', 'the', 'last', 'few', 'hours', 'of', 'the', 'day', '?', 'or', 'maybe', '2', 'smaller', 'doses', 'during', 'the', 'day', '?', 'sorry', 'for', 'the', 'ridiculous', 'length', 'but', 'i', 'worry', ':', 'p', 't', '##l', ';', 'dr', 'the', 'title', 'is', 'a', 'good', 'summer', '##y', 'so', 'what', 'do', 'you', 'guys', 'think', '?', 'any', 'similar', 'reactions', '/', 'solutions', '?']\n",
      "INFO:__main__:Number of tokens: 569\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['more', 'in', 'touch', 'with', 'my', 'emotions', 'with', '20', '##mg', 'add', '##eral', '##l', 'x', '##r', ',', 'anyone', 'else', 'have', 'this', 'reaction', '?', 'also', 'flat', 'feeling', 'at', 'night', 'after', 'it', 'wears', 'off', 'a', 'few', 'hours', 'before', 'i', 'go', 'to', 'sleep', '?', 'so', 'i', 'tried', 'st', '##rate', '##rra', 'and', 'had', 'bouts', 'of', 'crying', 'and', 'es', '##cala', '##ting', 'suicidal', 'thoughts', 'so', 'i', 'went', 'to', 'the', 'hospital', 'and', 'got', 'put', 'in', 'the', 'ps', '##ych', 'ward', 'for', 'a', 'few', 'days', '.', 'i', 'never', 'had', 'thoughts', 'like', 'that', 'before', 'so', 'i', 'am', 'fairly', 'confident', 'it', 'was', 'due', 'to', 'the', 'st', '##rat', '##tera', '.', 'i', 'have', 'been', 'crying', 'on', 'the', 'add', '##eral', '##l', 'but', 'it', 'is', 'all', 'related', 'to', 'my', 'shitty', 'childhood', 'and', 'i', 'don', \"'\", 't', 'have', 'any', 'suicidal', 'thoughts', 'so', 'i', 'know', 'these', 'feelings', 'are', 'not', 'irrational', 'or', 'harmful', '.', 'before', 'i', 'would', 'get', 'upset', 'about', 'more', 'minor', 'things', 'and', 'not', 'have', 'any', 'emotional', 'reaction', 'to', 'the', 'larger', 'problems', 'that', 'logical', '##ly', 'i', 'knew', 'i', 'should', 'and', 'that', 'other', 'people', 'were', 'surprised', 'i', 'could', 'be', 'so', 'calm', 'about', '.', 'its', 'in', 'a', 'way', 'a', 'good', 'thing', 'because', 'i', 'can', 'finally', 'start', 'dealing', 'with', 'these', 'su', '##pressed', 'feelings', 'but', 'on', 'the', 'other', 'hand', 'the', 'reason', 'i', 'am', 'taking', 'add', '##eral', '##l', 'is', 'because', 'of', 'how', 'impossible', 'for', 'me', 'to', 'focus', 'in', 'school', ',', 'and', 'even', 'though', 'i', 'think', 'this', 'effect', 'might', 'be', 'the', '##ra', '##put', '##ic', 'if', 'i', 'explore', 'it', 'with', 'my', 'therapist', ',', 'emotional', 'trauma', 'crop', '##ping', 'up', 'from', 'childhood', 'isn', \"'\", 't', 'exactly', 'convenient', '.', 'also', 'i', 'have', 'realized', 'a', 'lot', 'of', 'things', 'about', 'my', 'family', \"'\", 's', 'dynamic', 'that', 'i', 'never', 'saw', 'before', 'and', 'i', 'am', 'now', 'coming', 'to', 'terms', 'with', 'the', 'fact', 'that', 'my', 'dad', 'is', 'mentally', 'ill', 'and', 'very', 'mani', '##pu', '##lative', 'and', 'that', 'it', 'isn', \"'\", 't', 'my', 'responsibility', 'to', 'take', 'care', 'of', 'him', 'which', 'com', '##pl', '##icate', '##s', 'matters', 'because', 'i', 'now', 'don', \"'\", 't', 'think', 'that', 'i', 'can', 'have', 'any', 'relationship', 'with', 'him', ',', 'but', 'i', 'think', 'he', 'might', 'do', 'something', 'crazy', 'because', 'he', 'has', 'no', 'one', 'else', 'and', 'his', 'pattern', 'of', 'behavior', 'is', 'to', 'go', 'bat', 'shit', 'crazy', 'when', 'someone', 'tries', 'to', 'cut', 'ties', 'with', 'him', '.', 'basically', 'i', 'would', 'really', 'appreciate', 'some', 'opinions', 'about', 'whether', 'or', 'not', 'these', 'feelings', 'sound', 'harmful', 'or', 'ill', '##ogical', 'and', 'maybe', 'what', 'i', 'should', 'do', 'about', 'it', '.', 'i', 'think', 'the', 'focusing', 'effect', 'in', 'add', '##eral', '##l', 'is', 'making', 'it', 'impossible', 'to', 'avoid', 'feelings', 'that', 'before', 'i', 'just', 'gloss', '##ed', 'over', 'and', 'i', 'think', 'it', 'is', 'also', 'helping', 'me', 'organize', 'my', 'thou', '##ht', '##s', 'while', 'being', 'emotional', '.', 'usually', 'i', 'can', \"'\", 't', 'unite', 'those', 'two', 'sides', 'of', 'me', '.', 'i', 'had', 'a', 'feeling', 'of', 'flat', '##ness', 'and', 'ap', '##athy', 'after', '8', '##pm', 'last', 'night', 'but', 'i', 'didn', \"'\", 't', 'want', 'to', 'go', 'to', 'sleep', 'then', 'so', 'i', 'took', 'another', '20', '##mg', 'to', 'see', 'how', 'that', 'would', 'effect', 'it', '.', 'not', 'a', 'great', 'idea', '.', 'stayed', 'up', 'all', 'night', 'and', 'had', 'jaw', 'stiff', '##ness', 'and', 'teeth', 'grinding', '.', 'i', 'have', 'an', 'appointment', 'with', 'my', 'doc', 'next', 'week', ',', 'should', 'i', 'ask', 'for', 'add', '##eral'], ['##l', 'ir', 'to', 'get', 'me', 'through', 'the', 'last', 'few', 'hours', 'of', 'the', 'day', '?', 'or', 'maybe', '2', 'smaller', 'doses', 'during', 'the', 'day', '?', 'sorry', 'for', 'the', 'ridiculous', 'length', 'but', 'i', 'worry', ':', 'p', 't', '##l', ';', 'dr', 'the', 'title', 'is', 'a', 'good', 'summer', '##y', 'so', 'what', 'do', 'you', 'guys', 'think', '?', 'any', 'similar', 'reactions', '/', 'solutions', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['running', 'and', 'ad', '##hd', 'i', 'decided', 'i', 'want', 'to', 'start', 'running', ',', 'i', \"'\", 've', 'already', 'got', 'a', 'program', 'which', 'requires', 'me', 'to', 'run', '3', 'times', 'a', 'week', '.', 'i', \"'\", 've', 'got', 'plenty', 'of', 'time', ',', 'that', \"'\", 's', 'not', 'a', 'problem', '.', 'but', 'how', 'will', 'i', 'keep', 'motivated', '?', 'i', 'tend', 'to', 'start', 'to', 'care', 'less', 'about', 'stuff', ',', 'but', 'i', 'want', 'to', 'keep', 'this', 'up', 'because', 'i', 'think', 'i', \"'\", 'll', 'benefit', 'from', 'it', 'a', 'lot', '.', 'not', 'only', 'physically', ',', 'i', 'hope', 'to', 'get', 'some', 'rest', 'in', 'my', 'mind', '.', 'anyone', 'have', 'any', 'experience', 'with', 'running', '?', 'how', 'did', 'it', 'help', 'you', 'and', 'how', 'did', 'you', 'stay', 'motivated', '?']\n",
      "INFO:__main__:Number of tokens: 113\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['running', 'and', 'ad', '##hd', 'i', 'decided', 'i', 'want', 'to', 'start', 'running', ',', 'i', \"'\", 've', 'already', 'got', 'a', 'program', 'which', 'requires', 'me', 'to', 'run', '3', 'times', 'a', 'week', '.', 'i', \"'\", 've', 'got', 'plenty', 'of', 'time', ',', 'that', \"'\", 's', 'not', 'a', 'problem', '.', 'but', 'how', 'will', 'i', 'keep', 'motivated', '?', 'i', 'tend', 'to', 'start', 'to', 'care', 'less', 'about', 'stuff', ',', 'but', 'i', 'want', 'to', 'keep', 'this', 'up', 'because', 'i', 'think', 'i', \"'\", 'll', 'benefit', 'from', 'it', 'a', 'lot', '.', 'not', 'only', 'physically', ',', 'i', 'hope', 'to', 'get', 'some', 'rest', 'in', 'my', 'mind', '.', 'anyone', 'have', 'any', 'experience', 'with', 'running', '?', 'how', 'did', 'it', 'help', 'you', 'and', 'how', 'did', 'you', 'stay', 'motivated', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'wanna', 'get', 'off', 'v', '##y', '##vance', 'and', 'try', 'to', 'treat', 'my', 'add', 'with', 'diet', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'wanna', 'get', 'off', 'v', '##y', '##vance', 'and', 'try', 'to', 'treat', 'my', 'add', 'with', 'diet', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['must', 'watch', ':', 'an', 'excellent', 'ad', '##hd', 'researcher', 'dr', '.', 'russ', '##el', 'bark', '##ley', 'explain', 'almost', 'everything', 'you', 'need', 'to', 'know', 'about', 'ad', '##hd', 'in', 'few', 'short', 'videos', '!', '(', 'links', 'in', 'the', 'post', ')', '[', 'it', \"'\", 's', 'a', 'trap', '!', ']', '(', 'http', ':', '/', '/', 'cd', '##n', '.', 'instant', '##tra', '##p', '.', 'com', '/', 'trap', '.', 'jp', '##g', ')', 'ok', ',', 'it', \"'\", 's', 'more', 'than', 'just', 'a', 'few', 'videos', 'but', 'they', 'are', 'all', 'short', '(', '1', '-', '6', 'min', '##s', ')', 'and', 'they', 'are', 'very', 'inform', '##ative', ',', 'my', 'understanding', 'of', 'ad', '##hd', 'was', 'more', 'improved', 'by', 'these', 'several', 'videos', 'than', 'most', 'of', 'the', 'other', 'ad', '##hd', 'material', 'i', 'have', 'seen', '.', '[', 'ad', '##hd', 'emotional', 'regulation', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '7', '##c', '##w', '##8', '##j', '##hu', '##kh', '##ia', ')', '[', 'ad', '##hd', 'intention', 'deficit', 'disorder', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'w', '##f', '##1', '##yre', '##8', '##ff', '##1', '##g', ')', '[', 'ad', '##hd', 'more', 'than', 'an', 'attention', 'problem', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'k', '##q', '##c', '-', 'nk', '##5', '##oof', '##e', ')', '[', 'ad', '##hd', 'is', 'not', 'a', 'gift', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '4', '##x', '##pe', '##be', '##9', '##vd', '##w', '##w', ')', '[', 'ad', '##hd', 'co', '-', 'mor', '##bid', '##ity', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '415', '##q', '##ya', '##q', '##cic', '##q', ')', '[', 'ad', '##hd', 'more', 'accountability', ',', 'not', 'less', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'z', '##y', '##5', '##h', '-', 'l', '##hin', '##u', '##4', ')', '[', 'ad', '##hd', 'motivation', 'deficit', 'disorder', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##3', '##vu', '##v', '##5', '##j', '##va', '##z', '##s', ')', '[', 'ad', '##hd', 'hyper', '##act', '##ivity', '&', 'multi', '##tas', '##king', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'p', '##w', '##v', '##0', '##pa', '##m', '##43', '##j', '##4', ')', '[', 'ad', '##hd', 'not', 'different', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '4', '##gh', '##d', '##2', '##q', '##ma', '##q', '##ji', ')', '[', 'ad', '##hd', 'diagnosis', 'acceptance', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##gun', '##b', '##4', '##b', '##36', '##vo', ')', '[', 'ad', '##hd', 'family', 'genetics', '&', 'traits', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '273', '##os', '##7', '##4', '##rt', '##w', '##4', ')', '[', 'ad', '##hd', '\"', 'hyper', '##fo', '##cus', '\"', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'y', '##fk', '##g', '##0', '##v', '##w', '##x', '##3', '##rm', ')', '[', 'add', ',', 'odd', ',', 'emotional', 'imp', '##ulsive', '##ness', ',', 'and', 'relationships', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'rc', '##w', '##p', '##9', '##t', '##3', '##z', '##nc', '##m', ')', 'edit', ':', '*', 'explains', '-', 'don', \"'\", 't', 'burn', 'me', 'please', 'edit', ':', 'all', 'of', 'them', 'compiled', 'into', 'a', 'play', '##list', 'by', 'red', '##dit', '##or', 'computer', '##psy', '##ch', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'play', '##list', '?', 'list', '=', 'pl', '##55', '##8', '##b', '##39', '##75', '##fc', '##49', '##c', '##01', '##c', '*', '*', 'edit', ':', 'even', 'though', 'i', 'agree', 'with', 'everything', 'dr', '.', 'bark', '##ley', 'says', ',', 'viewer', 'discretion', 'is', 'advised', '.', 'i', 'personally', 'do', 'not', 'think', 'that', 'ad', '##hd', 'has', 'only', 'downs', '##ide', 'and', 'i', 'don', \"'\", 't', 'think', 'he', 'does', 'either', '.', 'but', 'you', 'have', 'to', 'understand', 'that', 'first', 'of', 'all', ',', 'he', 'is', 'a', 'doctor', 'and', 'a', 'researcher', 'and', 'that', 'is', 'the', 'attitude', 'in', 'which', 'he', 'approaches', 'ad', '##hd', ',', 'he', 'knows', 'what', 'he', 'needs', 'to', 'say', 'in', 'order', 'to', 'get', 'the', 'support', 'and', 'funding', 'to', 'help', 'ad', '##hd', 'people', '.', 'the', 'problem', 'is', 'not', 'that', 'ad', '##hd', 'is', 'not', 'anything', 'more', 'than', 'just', 'a', 'set', 'of', 'disorders', ',', 'the', 'problem', 'is', 'that', 'it', 'is', 'perceived', 'to', 'be', 'so', 'in', 'today', \"'\", 's', 'educational', ',', 'social', 'and', 'occupational', 'system', ',', 'where', 'the', 'upside', '##s', 'of', 'ad', '##hd', 'overlooked', 'and', 'disregard', '##ed', '.', '*', '*', 'for', 'example', ':', '*', 'majority', 'of', 'the', 'population', 'are', 'early', 'birds', 'whereas', 'ad', '##hd', 'people', 'are', 'usually', 'night', 'owls', '*', 'majority', 'of', 'the', 'population', 'can', 'focus', 'on', 'and', 'do', 'well', 'what', 'they', '*', 'need', '*', ',', 'ad', '##hd', 'people', 'can', 'only', 'focus', 'on', 'and', 'do', 'well', 'what', 'they', '*', 'like', '*', ',', 'but', 'if', 'they', 'are', 'enabled', 'to', 'do', 'so', ',', 'they', 'are', 'really', ',', 'really', 'good', 'at', 'it', '*', 'normal', 'brain', 'can', 'only', 'focus', 'on', 'what', 'is', 'the', 'actual', 'issue', 'at', 'the', 'given', 'moment', ',', 'ad', '##hd', 'brain', 'tends', 'to', 'process', 'multiple', 'information', 'at', 'once', 'and', 'many', 'more', '.', '.', '.', 'the', 'thing', 'is', 'that', 'ad', '##hd', 'is', 'not', 'that', 'much', 'of', 'a', '*', 'serious', 'disorder', '*', ',', 'the', 'thing', 'is', 'that', 'it', 'is', 'manufactured', 'that', 'way', 'by', 'simple', 'fucked', 'up', '-', 'one', '-', 'way', 'system', 'focused', 'on', 'as', 'much', 'regular', '##ity', 'as', 'possible', ',', 'st', '##omp', '##ing', 'individual', '##ity', 'and', 'creativity', ',', 'to', 'better', 'understand', 'this', ',', '*', '*', 'i', 'highly', 'recommend', 'you', 'guys', 'to', 'go', 'watch', 'these', 'videos', ',', 'they', 'are', 'essential', 'for', 'understanding', 'of', 'what', 'ad', '##hd', 'people', 'need', 'from', 'education', '*', '*', ':', '*', '[', 'sir', 'ken', 'robinson', '-', 'school', 'kills', 'creativity', ']', '(', 'http', ':', '/', '/', 'www', '.', 'ted', '.', 'com', '/', 'talks', '/', 'ken', '_', 'robinson', '_', 'says', '_', 'schools', '_', 'kill', '_', 'creativity', '.', 'html', ')', '*', '[', 'sir', 'ken', 'robinson', '-', 'bring', 'on', 'the', 'revolution', ']', '(', 'http', ':', '/', '/', 'www', '.', 'ted', '.', 'com', '/', 'talks', '/', 'sir', '_', 'ken', '_', 'robinson', '_', 'bring', '_', 'on', '_', 'the', '_', 'revolution', '.', 'html', ')', '*', '[', 'sir', 'ken', 'robinson', '-', 'changing', 'education', 'paradigm', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'z', '##d', '##z', '##fc', '##d', '##gp', '##l', '##4', '##u', ')', '*', '[', 'the', 'young', 'turks', 'coverage', '-', 'finland', \"'\", 's', 'revolutionary', 'education', 'program', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##lo', '##f', '##z', '##l', '_', 'j', '##5', '##fo', '&', 'feature', '=', 'plc', '##p', '&', 'context', '=', 'c', '##4', '##ce', '##59', '##1', '##b', '##vd', '##v', '##j', '##v', '##qa', '##1', '##pp', '##c', '##fp', '##a', '##90', '##q', '##x', '##7', '##3', '##v', '##4', '##n', '##8', '##d', '_', 'y', '##x', '##fu', '##oh', '##ay', '##ph', '##w', '##jo', '##c', '##9', '##i', '##5', '##iy', '=', ')', '*', '[', 'more', 'about', 'finland', \"'\", 's', 'educational', 'system', '\\\\', '(', 'long', 'article', 'alert', '\\\\', ')', ']', '(', 'http', ':', '/', '/', 'www', '.', 'the', '##at', '##lan', '##tic', '.', 'com', '/', 'national', '/', 'archive', '/', '2011', '/', '12', '/', 'what', '-', 'americans', '-', 'keep', '-', 'ignoring', '-', 'about', '-', 'finland', '##s', '-', 'school', '-', 'success', '/', '250', '##56', '##4', '/', ')', 'if', 'those', 'ideas', 'got', 'implemented', ',', 'the', 'educational', 'system', 'and', 'future', 'life', 'would', 'improve', 'drastically', 'for', 'all', 'ad', '##hd', 'people', 'living', 'in', 'the', 'given', 'country', '.']\n",
      "INFO:__main__:Number of tokens: 1214\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['must', 'watch', ':', 'an', 'excellent', 'ad', '##hd', 'researcher', 'dr', '.', 'russ', '##el', 'bark', '##ley', 'explain', 'almost', 'everything', 'you', 'need', 'to', 'know', 'about', 'ad', '##hd', 'in', 'few', 'short', 'videos', '!', '(', 'links', 'in', 'the', 'post', ')', '[', 'it', \"'\", 's', 'a', 'trap', '!', ']', '(', 'http', ':', '/', '/', 'cd', '##n', '.', 'instant', '##tra', '##p', '.', 'com', '/', 'trap', '.', 'jp', '##g', ')', 'ok', ',', 'it', \"'\", 's', 'more', 'than', 'just', 'a', 'few', 'videos', 'but', 'they', 'are', 'all', 'short', '(', '1', '-', '6', 'min', '##s', ')', 'and', 'they', 'are', 'very', 'inform', '##ative', ',', 'my', 'understanding', 'of', 'ad', '##hd', 'was', 'more', 'improved', 'by', 'these', 'several', 'videos', 'than', 'most', 'of', 'the', 'other', 'ad', '##hd', 'material', 'i', 'have', 'seen', '.', '[', 'ad', '##hd', 'emotional', 'regulation', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '7', '##c', '##w', '##8', '##j', '##hu', '##kh', '##ia', ')', '[', 'ad', '##hd', 'intention', 'deficit', 'disorder', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'w', '##f', '##1', '##yre', '##8', '##ff', '##1', '##g', ')', '[', 'ad', '##hd', 'more', 'than', 'an', 'attention', 'problem', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'k', '##q', '##c', '-', 'nk', '##5', '##oof', '##e', ')', '[', 'ad', '##hd', 'is', 'not', 'a', 'gift', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '4', '##x', '##pe', '##be', '##9', '##vd', '##w', '##w', ')', '[', 'ad', '##hd', 'co', '-', 'mor', '##bid', '##ity', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '415', '##q', '##ya', '##q', '##cic', '##q', ')', '[', 'ad', '##hd', 'more', 'accountability', ',', 'not', 'less', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'z', '##y', '##5', '##h', '-', 'l', '##hin', '##u', '##4', ')', '[', 'ad', '##hd', 'motivation', 'deficit', 'disorder', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##3', '##vu', '##v', '##5', '##j', '##va', '##z', '##s', ')', '[', 'ad', '##hd', 'hyper', '##act', '##ivity', '&', 'multi', '##tas', '##king', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'p', '##w', '##v', '##0', '##pa', '##m', '##43', '##j', '##4', ')', '[', 'ad', '##hd', 'not', 'different', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '4', '##gh', '##d', '##2', '##q', '##ma', '##q', '##ji', ')', '[', 'ad', '##hd', 'diagnosis', 'acceptance', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##gun', '##b', '##4', '##b', '##36', '##vo', ')', '[', 'ad', '##hd', 'family', 'genetics', '&', 'traits', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '273', '##os', '##7', '##4', '##rt', '##w', '##4', ')', '[', 'ad', '##hd', '\"', 'hyper', '##fo', '##cus', '\"', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'y', '##fk', '##g', '##0', '##v', '##w', '##x', '##3', '##rm', ')', '[', 'add', ',', 'odd', ',', 'emotional', 'imp', '##ulsive', '##ness', ',', 'and', 'relationships', ']'], ['(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'rc', '##w', '##p', '##9', '##t', '##3', '##z', '##nc', '##m', ')', 'edit', ':', '*', 'explains', '-', 'don', \"'\", 't', 'burn', 'me', 'please', 'edit', ':', 'all', 'of', 'them', 'compiled', 'into', 'a', 'play', '##list', 'by', 'red', '##dit', '##or', 'computer', '##psy', '##ch', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'play', '##list', '?', 'list', '=', 'pl', '##55', '##8', '##b', '##39', '##75', '##fc', '##49', '##c', '##01', '##c', '*', '*', 'edit', ':', 'even', 'though', 'i', 'agree', 'with', 'everything', 'dr', '.', 'bark', '##ley', 'says', ',', 'viewer', 'discretion', 'is', 'advised', '.', 'i', 'personally', 'do', 'not', 'think', 'that', 'ad', '##hd', 'has', 'only', 'downs', '##ide', 'and', 'i', 'don', \"'\", 't', 'think', 'he', 'does', 'either', '.', 'but', 'you', 'have', 'to', 'understand', 'that', 'first', 'of', 'all', ',', 'he', 'is', 'a', 'doctor', 'and', 'a', 'researcher', 'and', 'that', 'is', 'the', 'attitude', 'in', 'which', 'he', 'approaches', 'ad', '##hd', ',', 'he', 'knows', 'what', 'he', 'needs', 'to', 'say', 'in', 'order', 'to', 'get', 'the', 'support', 'and', 'funding', 'to', 'help', 'ad', '##hd', 'people', '.', 'the', 'problem', 'is', 'not', 'that', 'ad', '##hd', 'is', 'not', 'anything', 'more', 'than', 'just', 'a', 'set', 'of', 'disorders', ',', 'the', 'problem', 'is', 'that', 'it', 'is', 'perceived', 'to', 'be', 'so', 'in', 'today', \"'\", 's', 'educational', ',', 'social', 'and', 'occupational', 'system', ',', 'where', 'the', 'upside', '##s', 'of', 'ad', '##hd', 'overlooked', 'and', 'disregard', '##ed', '.', '*', '*', 'for', 'example', ':', '*', 'majority', 'of', 'the', 'population', 'are', 'early', 'birds', 'whereas', 'ad', '##hd', 'people', 'are', 'usually', 'night', 'owls', '*', 'majority', 'of', 'the', 'population', 'can', 'focus', 'on', 'and', 'do', 'well', 'what', 'they', '*', 'need', '*', ',', 'ad', '##hd', 'people', 'can', 'only', 'focus', 'on', 'and', 'do', 'well', 'what', 'they', '*', 'like', '*', ',', 'but', 'if', 'they', 'are', 'enabled', 'to', 'do', 'so', ',', 'they', 'are', 'really', ',', 'really', 'good', 'at', 'it', '*', 'normal', 'brain', 'can', 'only', 'focus', 'on', 'what', 'is', 'the', 'actual', 'issue', 'at', 'the', 'given', 'moment', ',', 'ad', '##hd', 'brain', 'tends', 'to', 'process', 'multiple', 'information', 'at', 'once', 'and', 'many', 'more', '.', '.', '.', 'the', 'thing', 'is', 'that', 'ad', '##hd', 'is', 'not', 'that', 'much', 'of', 'a', '*', 'serious', 'disorder', '*', ',', 'the', 'thing', 'is', 'that', 'it', 'is', 'manufactured', 'that', 'way', 'by', 'simple', 'fucked', 'up', '-', 'one', '-', 'way', 'system', 'focused', 'on', 'as', 'much', 'regular', '##ity', 'as', 'possible', ',', 'st', '##omp', '##ing', 'individual', '##ity', 'and', 'creativity', ',', 'to', 'better', 'understand', 'this', ',', '*', '*', 'i', 'highly', 'recommend', 'you', 'guys', 'to', 'go', 'watch', 'these', 'videos', ',', 'they', 'are', 'essential', 'for', 'understanding', 'of', 'what', 'ad', '##hd', 'people', 'need', 'from', 'education', '*', '*', ':', '*', '[', 'sir', 'ken', 'robinson', '-', 'school', 'kills', 'creativity', ']', '(', 'http', ':', '/', '/', 'www', '.', 'ted', '.', 'com', '/', 'talks', '/', 'ken', '_', 'robinson', '_', 'says', '_', 'schools', '_', 'kill', '_', 'creativity', '.', 'html', ')', '*', '[', 'sir', 'ken', 'robinson', '-', 'bring', 'on', 'the', 'revolution', ']', '(', 'http', ':', '/', '/', 'www', '.', 'ted', '.', 'com', '/', 'talks', '/', 'sir', '_', 'ken', '_', 'robinson', '_', 'bring', '_', 'on', '_', 'the', '_', 'revolution', '.', 'html', ')', '*', '[', 'sir', 'ken', 'robinson', '-', 'changing', 'education', 'paradigm', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com'], ['/', 'watch', '?', 'v', '=', 'z', '##d', '##z', '##fc', '##d', '##gp', '##l', '##4', '##u', ')', '*', '[', 'the', 'young', 'turks', 'coverage', '-', 'finland', \"'\", 's', 'revolutionary', 'education', 'program', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##lo', '##f', '##z', '##l', '_', 'j', '##5', '##fo', '&', 'feature', '=', 'plc', '##p', '&', 'context', '=', 'c', '##4', '##ce', '##59', '##1', '##b', '##vd', '##v', '##j', '##v', '##qa', '##1', '##pp', '##c', '##fp', '##a', '##90', '##q', '##x', '##7', '##3', '##v', '##4', '##n', '##8', '##d', '_', 'y', '##x', '##fu', '##oh', '##ay', '##ph', '##w', '##jo', '##c', '##9', '##i', '##5', '##iy', '=', ')', '*', '[', 'more', 'about', 'finland', \"'\", 's', 'educational', 'system', '\\\\', '(', 'long', 'article', 'alert', '\\\\', ')', ']', '(', 'http', ':', '/', '/', 'www', '.', 'the', '##at', '##lan', '##tic', '.', 'com', '/', 'national', '/', 'archive', '/', '2011', '/', '12', '/', 'what', '-', 'americans', '-', 'keep', '-', 'ignoring', '-', 'about', '-', 'finland', '##s', '-', 'school', '-', 'success', '/', '250', '##56', '##4', '/', ')', 'if', 'those', 'ideas', 'got', 'implemented', ',', 'the', 'educational', 'system', 'and', 'future', 'life', 'would', 'improve', 'drastically', 'for', 'all', 'ad', '##hd', 'people', 'living', 'in', 'the', 'given', 'country', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'switching', 'from', 'v', '##y', '##van', '##se', 'to', 'day', '##tra', '##na', 'soon', 'and', 'i', 'have', 'some', 'questions', '.', '.', '.', 'so', 'i', \"'\", 'm', '20', 'and', 'i', \"'\", 've', 'taken', 'v', '##y', '##van', '##se', 'for', 'about', '2', 'years', 'with', 'both', 'success', 'and', 'failure', ',', 'mainly', 'dealing', 'with', 'some', 'gi', 'issues', 'and', 'sleeping', 'issues', '.', 'so', 'my', 'doctor', 'suggested', 'i', 'try', 'day', '##tra', '##na', 'because', 'it', 'enters', 'the', 'body', 'through', 'the', 'skin', 'and', 'you', 'can', 'control', 'how', 'long', 'you', 'want', 'to', 'have', 'medicine', '(', 'to', 'some', 'extent', ')', '.', 'so', 'my', 'questions', 'are', 'these', ':', '1', '.', 'i', 'weigh', 'about', '165', 'and', 'i', \"'\", 'm', 'currently', 'on', 'the', '20', '##mg', 'patch', '.', 'i', 'know', 'that', 'dos', '##age', 'var', '##ries', 'from', 'person', 'to', 'person', 'but', 'does', 'that', 'sound', 'about', 'right', '?', '2', '.', 'it', 'says', 'to', 'only', 'leave', 'on', 'for', '9', 'hours', ',', 'but', 'is', 'it', 'okay', 'to', 'leave', 'it', 'on', 'for', 'longer', '?', 'is', 'the', 'only', 'thing', 'that', 'changes', 'just', 'how', 'much', 'you', 'get', '?', '3', '.', 'v', '##y', '##van', '##se', ',', 'to', 'me', 'at', 'least', ',', 'had', 'a', 'very', 'distinct', 'feeling', 'when', 'it', 'started', 'to', 'kick', 'in', ',', 'like', 'someone', 'dr', '##ap', '##ing', 'a', 'layer', 'of', 'cloth', 'over', 'you', 'and', 'that', 'made', 'everything', 'so', 'much', 'clearer', '.', 'is', 'it', 'like', 'v', '##y', '##van', '##se', 'where', 'you', 'feel', 'it', 'start', 'or', 'is', 'it', 'a', 'sneaking', 'feeling', '?', 'just', 'found', 'this', 'sub', '-', 'red', '##dit', 'today', 'and', 'i', \"'\", 'm', 'already', 'hooked', '!', 'thanks', 'guys', 'for', 'the', 'help', '!']\n",
      "INFO:__main__:Number of tokens: 249\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'switching', 'from', 'v', '##y', '##van', '##se', 'to', 'day', '##tra', '##na', 'soon', 'and', 'i', 'have', 'some', 'questions', '.', '.', '.', 'so', 'i', \"'\", 'm', '20', 'and', 'i', \"'\", 've', 'taken', 'v', '##y', '##van', '##se', 'for', 'about', '2', 'years', 'with', 'both', 'success', 'and', 'failure', ',', 'mainly', 'dealing', 'with', 'some', 'gi', 'issues', 'and', 'sleeping', 'issues', '.', 'so', 'my', 'doctor', 'suggested', 'i', 'try', 'day', '##tra', '##na', 'because', 'it', 'enters', 'the', 'body', 'through', 'the', 'skin', 'and', 'you', 'can', 'control', 'how', 'long', 'you', 'want', 'to', 'have', 'medicine', '(', 'to', 'some', 'extent', ')', '.', 'so', 'my', 'questions', 'are', 'these', ':', '1', '.', 'i', 'weigh', 'about', '165', 'and', 'i', \"'\", 'm', 'currently', 'on', 'the', '20', '##mg', 'patch', '.', 'i', 'know', 'that', 'dos', '##age', 'var', '##ries', 'from', 'person', 'to', 'person', 'but', 'does', 'that', 'sound', 'about', 'right', '?', '2', '.', 'it', 'says', 'to', 'only', 'leave', 'on', 'for', '9', 'hours', ',', 'but', 'is', 'it', 'okay', 'to', 'leave', 'it', 'on', 'for', 'longer', '?', 'is', 'the', 'only', 'thing', 'that', 'changes', 'just', 'how', 'much', 'you', 'get', '?', '3', '.', 'v', '##y', '##van', '##se', ',', 'to', 'me', 'at', 'least', ',', 'had', 'a', 'very', 'distinct', 'feeling', 'when', 'it', 'started', 'to', 'kick', 'in', ',', 'like', 'someone', 'dr', '##ap', '##ing', 'a', 'layer', 'of', 'cloth', 'over', 'you', 'and', 'that', 'made', 'everything', 'so', 'much', 'clearer', '.', 'is', 'it', 'like', 'v', '##y', '##van', '##se', 'where', 'you', 'feel', 'it', 'start', 'or', 'is', 'it', 'a', 'sneaking', 'feeling', '?', 'just', 'found', 'this', 'sub', '-', 'red', '##dit', 'today', 'and', 'i', \"'\", 'm', 'already', 'hooked', '!', 'thanks', 'guys', 'for', 'the', 'help', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'here', 'a', 'fan', 'of', 'y', '##tm', '##nd', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'here', 'a', 'fan', 'of', 'y', '##tm', '##nd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'think', 'i', 'have', 'add', '/', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'think', 'i', 'have', 'add', '/', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['amp', '##het', '##amine', '##s', 'and', 'ssr', '##is', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['amp', '##het', '##amine', '##s', 'and', 'ssr', '##is', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['emotions', 'and', 'stress', 'require', 'energy', 'throw', 'off', 'bio', '##chemical', 'home', '##osta', '##sis', ',', 'requiring', 'more', 'energy', 'to', 'compensate', ',', 'making', 'you', '\"', 'tired', '.', '\"', '[', 'one', 'to', 'watch', 'from', 'asks', '##cie', '##nce', ']']\n",
      "INFO:__main__:Number of tokens: 34\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['emotions', 'and', 'stress', 'require', 'energy', 'throw', 'off', 'bio', '##chemical', 'home', '##osta', '##sis', ',', 'requiring', 'more', 'energy', 'to', 'compensate', ',', 'making', 'you', '\"', 'tired', '.', '\"', '[', 'one', 'to', 'watch', 'from', 'asks', '##cie', '##nce', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['concert', '##a', 'reservations', 'i', 'went', 'to', 'the', 'psychiatrist', 'yesterday', ',', 'and', 'she', 'prescribed', 'me', '18', '##mg', 'concert', '##a', ',', 'and', 'told', 'me', 'that', 'if', 'i', 'didn', \"'\", 't', 'notice', 'any', 'negative', 'side', 'effects', 'the', 'first', 'day', ',', 'i', 'should', 'take', 'two', 'a', 'day', 'because', '18', 'wouldn', \"'\", 't', 'be', 'enough', '.', 'now', ',', 'concert', '##a', 'is', 'one', 'i', \"'\", 've', 'heard', 'fairly', 'little', 'about', 'on', 'this', 'sub', '##red', '##dit', '.', 'i', 'have', 'a', 'friend', 'who', 'takes', 'it', ',', 'and', 'she', 'says', 'she', 'stopped', 'taking', 'it', 'because', 'it', 'made', 'her', 'depressed', '(', 'but', 'she', 'lies', 'about', 'almost', 'anything', 'she', 'can', 'for', 'attention', 'so', 'i', 'take', 'anything', 'she', 'says', 'with', 'a', 'heap', '##ing', 'handful', 'of', 'salt', ')', '.', 'i', 'didn', \"'\", 't', 'to', 'mention', 'my', 'reservations', ',', 'first', 'of', 'all', 'because', 'i', 'didn', \"'\", 't', 'want', 'to', 'seem', 'like', 'i', 'was', 'just', 'trying', 'to', 'get', 'certain', 'drugs', 'for', 'recreation', ',', 'but', 'also', 'because', 'my', 'mom', 'was', 'i', 'the', 'room', 'and', 'i', 'feel', 'like', 'i', 'can', \"'\", 't', 'freely', 'speak', 'my', 'opinion', 'around', 'her', '.']\n",
      "INFO:__main__:Number of tokens: 172\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['concert', '##a', 'reservations', 'i', 'went', 'to', 'the', 'psychiatrist', 'yesterday', ',', 'and', 'she', 'prescribed', 'me', '18', '##mg', 'concert', '##a', ',', 'and', 'told', 'me', 'that', 'if', 'i', 'didn', \"'\", 't', 'notice', 'any', 'negative', 'side', 'effects', 'the', 'first', 'day', ',', 'i', 'should', 'take', 'two', 'a', 'day', 'because', '18', 'wouldn', \"'\", 't', 'be', 'enough', '.', 'now', ',', 'concert', '##a', 'is', 'one', 'i', \"'\", 've', 'heard', 'fairly', 'little', 'about', 'on', 'this', 'sub', '##red', '##dit', '.', 'i', 'have', 'a', 'friend', 'who', 'takes', 'it', ',', 'and', 'she', 'says', 'she', 'stopped', 'taking', 'it', 'because', 'it', 'made', 'her', 'depressed', '(', 'but', 'she', 'lies', 'about', 'almost', 'anything', 'she', 'can', 'for', 'attention', 'so', 'i', 'take', 'anything', 'she', 'says', 'with', 'a', 'heap', '##ing', 'handful', 'of', 'salt', ')', '.', 'i', 'didn', \"'\", 't', 'to', 'mention', 'my', 'reservations', ',', 'first', 'of', 'all', 'because', 'i', 'didn', \"'\", 't', 'want', 'to', 'seem', 'like', 'i', 'was', 'just', 'trying', 'to', 'get', 'certain', 'drugs', 'for', 'recreation', ',', 'but', 'also', 'because', 'my', 'mom', 'was', 'i', 'the', 'room', 'and', 'i', 'feel', 'like', 'i', 'can', \"'\", 't', 'freely', 'speak', 'my', 'opinion', 'around', 'her', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'ad', '##hd', 'over', '##dia', '##gno', '##sed', '?', 'modern', 'lifestyle', 'vs', '.', 'brain', 'chemistry', 'vs', '.', 'general', 'negligence']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'ad', '##hd', 'over', '##dia', '##gno', '##sed', '?', 'modern', 'lifestyle', 'vs', '.', 'brain', 'chemistry', 'vs', '.', 'general', 'negligence']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'what', 'now', '?', 'so', 'i', 'was', 'diagnosed', 'about', '2', 'months', 'ago', 'as', 'having', 'adult', 'ad', '##hd', '.', 'i', 'have', 'since', 'been', 'put', 'on', '10', 'mg', 'dex', '##ed', '##rine', 'spans', '##ules', 'and', 'it', 'seems', 'to', 'be', 'helping', 'with', 'motivation', 'and', 'concentration', 'which', 'has', 'been', 'fantastic', '.', 'i', 'have', 'also', 'been', 'learning', 'a', 'great', 'deal', 'about', 'myself', 'and', 'ad', '##hd', '(', 'i', 'am', 'primarily', 'ina', '##tten', '##tive', ')', 'and', 'what', 'it', 'has', 'cost', 'me', 'over', 'the', 'years', '.', 'one', 'of', 'the', 'things', 'that', 'i', 'have', 'a', 'really', 'hard', 'time', 'with', 'is', 'emotional', 'control', '.', 'i', 'am', 'quick', 'to', 'anger', ',', 'i', 'take', 'criticism', 'very', 'poorly', 'and', 'have', 'had', 'really', 'bad', 'bouts', 'of', 'depression', 'and', 'anxiety', 'in', 'the', 'past', 'and', 'i', 'am', 'currently', 'being', 'treated', 'for', 'depression', '/', 'anxiety', 'as', 'well', 'as', 'the', 'ad', '##hd', 'now', '.', 'i', 'am', 'an', 'adult', 'student', 'and', 'the', 'stress', 'is', 'really', 'hard', 'to', 'take', 'sometimes', '.', 'i', 'am', 'paranoid', 'that', 'i', 'get', 'by', 'based', 'on', 'instructors', 'just', 'taking', 'pity', 'on', 'me', 'or', 'feeling', 'sorry', 'for', 'me', '.', 'i', 'have', 'never', 'felt', 'like', 'any', 'successes', 'have', 'been', 'mine', 'alone', '.', 'i', 'feel', 'like', 'it', \"'\", 's', 'always', 'because', 'of', 'someone', 'or', 'something', 'else', \"'\", 's', 'influence', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'feel', 'this', 'way', 'anymore', '.', 'i', 'want', 'to', 'find', 'my', 'own', 'success', 'or', 'at', 'least', 'recognize', 'that', 'i', 'have', 'had', 'some', 'success', 'in', 'this', 'world', '.']\n",
      "INFO:__main__:Number of tokens: 232\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', 'what', 'now', '?', 'so', 'i', 'was', 'diagnosed', 'about', '2', 'months', 'ago', 'as', 'having', 'adult', 'ad', '##hd', '.', 'i', 'have', 'since', 'been', 'put', 'on', '10', 'mg', 'dex', '##ed', '##rine', 'spans', '##ules', 'and', 'it', 'seems', 'to', 'be', 'helping', 'with', 'motivation', 'and', 'concentration', 'which', 'has', 'been', 'fantastic', '.', 'i', 'have', 'also', 'been', 'learning', 'a', 'great', 'deal', 'about', 'myself', 'and', 'ad', '##hd', '(', 'i', 'am', 'primarily', 'ina', '##tten', '##tive', ')', 'and', 'what', 'it', 'has', 'cost', 'me', 'over', 'the', 'years', '.', 'one', 'of', 'the', 'things', 'that', 'i', 'have', 'a', 'really', 'hard', 'time', 'with', 'is', 'emotional', 'control', '.', 'i', 'am', 'quick', 'to', 'anger', ',', 'i', 'take', 'criticism', 'very', 'poorly', 'and', 'have', 'had', 'really', 'bad', 'bouts', 'of', 'depression', 'and', 'anxiety', 'in', 'the', 'past', 'and', 'i', 'am', 'currently', 'being', 'treated', 'for', 'depression', '/', 'anxiety', 'as', 'well', 'as', 'the', 'ad', '##hd', 'now', '.', 'i', 'am', 'an', 'adult', 'student', 'and', 'the', 'stress', 'is', 'really', 'hard', 'to', 'take', 'sometimes', '.', 'i', 'am', 'paranoid', 'that', 'i', 'get', 'by', 'based', 'on', 'instructors', 'just', 'taking', 'pity', 'on', 'me', 'or', 'feeling', 'sorry', 'for', 'me', '.', 'i', 'have', 'never', 'felt', 'like', 'any', 'successes', 'have', 'been', 'mine', 'alone', '.', 'i', 'feel', 'like', 'it', \"'\", 's', 'always', 'because', 'of', 'someone', 'or', 'something', 'else', \"'\", 's', 'influence', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'feel', 'this', 'way', 'anymore', '.', 'i', 'want', 'to', 'find', 'my', 'own', 'success', 'or', 'at', 'least', 'recognize', 'that', 'i', 'have', 'had', 'some', 'success', 'in', 'this', 'world', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anybody', 'else', 'have', 'ape', '##tite', 'issues', 'with', 'v', '##y', '##van', '##se', '?', 'how', 'do', 'i', 'combat', 'it', '?', 'im', 'getting', 'really', 'skinny', '.', 'im', 'taking', '40', 'mg', 'and', 'when', 'i', 'take', 'it', 'i', 'cannot', 'eat', '.', 'i', 'know', 'that', 'im', 'hungry', 'but', 'i', 'don', '##t', 'have', 'an', 'any', 'appetite', 'and', 'eating', 'something', 'would', 'make', 'me', 'feel', 'like', 'im', 'going', 'to', 'throw', 'up', 'and', 'i', 'have', 'to', 'shove', 'things', 'down', 'my', 'throat', '.', 'can', 'somebody', 'help', 'me', 'out', '?']\n",
      "INFO:__main__:Number of tokens: 79\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anybody', 'else', 'have', 'ape', '##tite', 'issues', 'with', 'v', '##y', '##van', '##se', '?', 'how', 'do', 'i', 'combat', 'it', '?', 'im', 'getting', 'really', 'skinny', '.', 'im', 'taking', '40', 'mg', 'and', 'when', 'i', 'take', 'it', 'i', 'cannot', 'eat', '.', 'i', 'know', 'that', 'im', 'hungry', 'but', 'i', 'don', '##t', 'have', 'an', 'any', 'appetite', 'and', 'eating', 'something', 'would', 'make', 'me', 'feel', 'like', 'im', 'going', 'to', 'throw', 'up', 'and', 'i', 'have', 'to', 'shove', 'things', 'down', 'my', 'throat', '.', 'can', 'somebody', 'help', 'me', 'out', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['low', 'brain', 'g', '##ly', '##co', '##gen', 'dim', '##ini', '##sh', '##es', 'executive', 'function', ';', 'brain', 'draws', 'plasma', 'glucose', 'to', 'sustain', 'itself', ';', 'stress', '##ors', 'burn', 'through', 'glucose', 'stores', 'faster']\n",
      "INFO:__main__:Number of tokens: 28\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['low', 'brain', 'g', '##ly', '##co', '##gen', 'dim', '##ini', '##sh', '##es', 'executive', 'function', ';', 'brain', 'draws', 'plasma', 'glucose', 'to', 'sustain', 'itself', ';', 'stress', '##ors', 'burn', 'through', 'glucose', 'stores', 'faster']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'day', 'on', 'med', '##s', 'after', 'diagnosis', '.']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', 'day', 'on', 'med', '##s', 'after', 'diagnosis', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'st', '##rat', '##tera', 'effective', 'if', 'taken', 'at', 'night', '?', 'just', 'started', 'it', 'today', '.', 'after', 'i', 'took', 'it', 'this', 'morning', 'i', 'slept', 'for', 'a', 'few', 'hours', '.', 'not', 'really', 'something', 'i', \"'\", 'm', 'gonna', 'be', 'able', 'to', 'keep', 'doing', '.', 'but', 'i', 'already', 'feel', 'better', 'and', 'less', 'anxious', '.', 'i', 'don', \"'\", 't', 'really', 'know', 'if', 'my', 'concentration', 'is', 'up', 'at', 'all', '.', 'i', 'feel', 'like', 'i', \"'\", 've', 'been', 'able', 'to', 'read', 'more', 'at', 'once', ',', 'but', 'that', 'may', 'just', 'be', 'a', 'place', '##bo', 'effect', '.', 'so', 'if', 'you', 'take', 'it', ',', 'what', 'is', 'your', 'dose', 'and', 'when', 'do', 'you', 'take', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 105\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'st', '##rat', '##tera', 'effective', 'if', 'taken', 'at', 'night', '?', 'just', 'started', 'it', 'today', '.', 'after', 'i', 'took', 'it', 'this', 'morning', 'i', 'slept', 'for', 'a', 'few', 'hours', '.', 'not', 'really', 'something', 'i', \"'\", 'm', 'gonna', 'be', 'able', 'to', 'keep', 'doing', '.', 'but', 'i', 'already', 'feel', 'better', 'and', 'less', 'anxious', '.', 'i', 'don', \"'\", 't', 'really', 'know', 'if', 'my', 'concentration', 'is', 'up', 'at', 'all', '.', 'i', 'feel', 'like', 'i', \"'\", 've', 'been', 'able', 'to', 'read', 'more', 'at', 'once', ',', 'but', 'that', 'may', 'just', 'be', 'a', 'place', '##bo', 'effect', '.', 'so', 'if', 'you', 'take', 'it', ',', 'what', 'is', 'your', 'dose', 'and', 'when', 'do', 'you', 'take', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'ad', '##hd', '-', 'healthy', 'rewards', 'you', 'can', 'or', 'do', 'make', 'for', 'yourself', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'ad', '##hd', '-', 'healthy', 'rewards', 'you', 'can', 'or', 'do', 'make', 'for', 'yourself', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'feel', ',', 'when', 'they', 'were', 'younger', 'or', 'now', ',', 'that', 'they', 'wanted', 'to', 'do', 'something', '\"', 'big', '\"', 'that', 'would', 'change', 'the', 'world', 'forever', '?', '*', '*', 'update', ':', '25', '##ske', '##do', '##o', 'made', 'it', 'clearer', 'to', 'me', 'that', 'this', 'is', 'also', 'part', 'of', 'always', 'focusing', 'on', 'the', '\"', 'bigger', 'picture', '\"', 'than', 'on', 'the', 'details', ',', 'just', 'to', 'relate', 'it', 'more', 'to', 'you', 'folks', 'who', 'may', 'not', 'necessarily', 'want', 'to', '\"', 'change', 'the', 'world', '\"', '*', '*', 'well', 'now', 'that', 'i', 'know', 'i', 'certainly', 'do', 'share', 'a', 'lot', 'of', 'qualities', 'with', 'you', 'people', '-', 'since', 'we', 'all', 'are', 'ad', '##hd', 'here', '-', 'i', 'wonder', 'if', 'any', 'one', 'else', 'ever', 'had', 'this', 'deep', 'desire', 'within', 'them', ',', 'young', ',', 'teenage', ',', 'or', 'even', 'now', '?', 'i', \"'\", 've', 'always', 'had', 'the', 'cr', '##azi', '##est', 'aspirations', 'and', 'desires', '.', 'especially', 'throughout', 'high', 'school', ',', 'i', 'imagined', 'myself', 'either', 'discovering', 'new', 'laws', 'in', 'natural', 'physics', 'or', 'creating', 'the', 'perfect', '\"', 'government', '\"', 'system', '.', 'even', 'now', ',', 'my', 'life', 'still', 'seems', 'to', 'be', 'drawn', 'to', 'some', '\"', 'big', '\"', 'goal', 'in', 'my', 'future', '.', '.', '.', 'which', 'now', 'involves', 'creating', 'brain', 'computer', 'interfaces', '.', 'i', 'wonder', 'if', 'this', 'is', 'something', 'anyone', 'else', 'ever', 'felt', '.', '.', '.', 'that', 'they', 'wanted', 'the', 'world', 'to', 'be', 'drastically', 'different', 'in', 'some', 'way', 'and', 'that', 'they', 'wanted', 'to', 'do', 'something', 'in', 'their', 'life', 'that', 'would', 'make', 'a', 'difference', '.', 'i', 'feel', 'like', 'you', 'fellow', 'ad', '##hd', '##ers', 'would', 'understand', ',', 'since', 'you', ',', 'like', 'me', ',', 'are', 'probably', 'always', 'distracted', 'most', 'by', 'the', 'thoughts', 'in', 'your', 'own', 'mind', '.', 'day', 'dreamer', '##s', ',', 'etc', ',', 'but', 'this', 'isn', \"'\", 't', 'a', 'very', '\"', 'typical', '\"', 'thing', 'for', 'people', 'to', 'want', '.', 'most', 'people', 'i', 'know', '(', 'and', 'most', 'people', 'i', 'know', 'do', 'not', 'have', 'ad', '##hd', ')', 'have', 'no', 'such', 'aspirations', 'or', 'goals', 'that', 'involve', 'not', 'only', 'success', 'for', 'themselves', ',', 'but', 'literally', ',', 'bringing', 'about', 'a', 'huge', 'change', '.', 'i', 'really', 'want', 'to', 'know', 'if', 'anyone', 'else', 'here', 'has', 'ever', 'felt', 'this', 'or', 'feels', 'like', 'this', 'describes', 'them', '!']\n",
      "INFO:__main__:Number of tokens: 346\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'feel', ',', 'when', 'they', 'were', 'younger', 'or', 'now', ',', 'that', 'they', 'wanted', 'to', 'do', 'something', '\"', 'big', '\"', 'that', 'would', 'change', 'the', 'world', 'forever', '?', '*', '*', 'update', ':', '25', '##ske', '##do', '##o', 'made', 'it', 'clearer', 'to', 'me', 'that', 'this', 'is', 'also', 'part', 'of', 'always', 'focusing', 'on', 'the', '\"', 'bigger', 'picture', '\"', 'than', 'on', 'the', 'details', ',', 'just', 'to', 'relate', 'it', 'more', 'to', 'you', 'folks', 'who', 'may', 'not', 'necessarily', 'want', 'to', '\"', 'change', 'the', 'world', '\"', '*', '*', 'well', 'now', 'that', 'i', 'know', 'i', 'certainly', 'do', 'share', 'a', 'lot', 'of', 'qualities', 'with', 'you', 'people', '-', 'since', 'we', 'all', 'are', 'ad', '##hd', 'here', '-', 'i', 'wonder', 'if', 'any', 'one', 'else', 'ever', 'had', 'this', 'deep', 'desire', 'within', 'them', ',', 'young', ',', 'teenage', ',', 'or', 'even', 'now', '?', 'i', \"'\", 've', 'always', 'had', 'the', 'cr', '##azi', '##est', 'aspirations', 'and', 'desires', '.', 'especially', 'throughout', 'high', 'school', ',', 'i', 'imagined', 'myself', 'either', 'discovering', 'new', 'laws', 'in', 'natural', 'physics', 'or', 'creating', 'the', 'perfect', '\"', 'government', '\"', 'system', '.', 'even', 'now', ',', 'my', 'life', 'still', 'seems', 'to', 'be', 'drawn', 'to', 'some', '\"', 'big', '\"', 'goal', 'in', 'my', 'future', '.', '.', '.', 'which', 'now', 'involves', 'creating', 'brain', 'computer', 'interfaces', '.', 'i', 'wonder', 'if', 'this', 'is', 'something', 'anyone', 'else', 'ever', 'felt', '.', '.', '.', 'that', 'they', 'wanted', 'the', 'world', 'to', 'be', 'drastically', 'different', 'in', 'some', 'way', 'and', 'that', 'they', 'wanted', 'to', 'do', 'something', 'in', 'their', 'life', 'that', 'would', 'make', 'a', 'difference', '.', 'i', 'feel', 'like', 'you', 'fellow', 'ad', '##hd', '##ers', 'would', 'understand', ',', 'since', 'you', ',', 'like', 'me', ',', 'are', 'probably', 'always', 'distracted', 'most', 'by', 'the', 'thoughts', 'in', 'your', 'own', 'mind', '.', 'day', 'dreamer', '##s', ',', 'etc', ',', 'but', 'this', 'isn', \"'\", 't', 'a', 'very', '\"', 'typical', '\"', 'thing', 'for', 'people', 'to', 'want', '.', 'most', 'people', 'i', 'know', '(', 'and', 'most', 'people', 'i', 'know', 'do', 'not', 'have', 'ad', '##hd', ')', 'have', 'no', 'such', 'aspirations', 'or', 'goals', 'that', 'involve', 'not', 'only', 'success', 'for', 'themselves', ',', 'but', 'literally', ',', 'bringing', 'about', 'a', 'huge', 'change', '.', 'i', 'really', 'want', 'to', 'know', 'if', 'anyone', 'else', 'here', 'has', 'ever', 'felt', 'this', 'or', 'feels', 'like', 'this', 'describes', 'them', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 've', 'long', 'suspected', 'i', 'had', 'ad', '##hd', ',', 'but', 'i', \"'\", 'm', 'hesitant', 'to', 'get', 'tested', '.', 'what', 'made', 'you', 'get', 'tested', 'for', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 28\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 've', 'long', 'suspected', 'i', 'had', 'ad', '##hd', ',', 'but', 'i', \"'\", 'm', 'hesitant', 'to', 'get', 'tested', '.', 'what', 'made', 'you', 'get', 'tested', 'for', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'love', 'all', 'you', 'guys', '/', 'r', '/', 'ad', '##hd', '!', 'enjoy', 'the', 'cake', 'with', 'me', '.', 'almost', 'gave', 'up', 'karma', 'who', '##ring', 'for', 'lent', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 28\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'love', 'all', 'you', 'guys', '/', 'r', '/', 'ad', '##hd', '!', 'enjoy', 'the', 'cake', 'with', 'me', '.', 'almost', 'gave', 'up', 'karma', 'who', '##ring', 'for', 'lent', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'you', 'do', '?', '(', 'job', ',', 'career', ',', 'hobby', ',', 'etc', '.', ')', 'so', 'after', 'intending', 'to', 'write', 'this', 'post', 'for', 'the', 'last', '2', 'hours', '(', 'oh', 'the', '\"', 'irony', '\"', 'of', 'getting', 'distracted', 'by', 'the', 'ad', '##hd', 'sub', '##red', '##dit', ')', ',', 'i', 'have', 'finally', 'gotten', 'ar', '##oud', '##n', 'to', 'posting', '!', 'i', 'am', 'curious', 'to', 'know', 'what', 'jobs', 'you', 'guys', 'and', 'gal', '##s', 'have', '?', 'do', 'you', 'know', 'others', 'at', 'work', 'or', 'in', 'your', 'field', 'with', 'ad', '##hd', '?', 'are', 'there', 'any', 'fields', 'that', 'seem', 'to', 'attract', 'those', 'of', 'us', 'with', 'ad', '##hd', '?', 'also', ',', 'i', 'just', 'want', 'to', 'get', 'to', 'know', 'the', 'people', 'on', 'here', '.', 'to', 'start', ',', 'i', 'am', 'a', 'graduating', 'senior', 'in', 'mechanical', 'engineering', ',', 'and', 'my', 'main', 'ho', '##bbies', 'include', 'tin', '##ker', '##ing', 'with', 'computers', ',', 'bicycles', ',', 'and', 'taking', 'stuff', 'apart', '.']\n",
      "INFO:__main__:Number of tokens: 143\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'you', 'do', '?', '(', 'job', ',', 'career', ',', 'hobby', ',', 'etc', '.', ')', 'so', 'after', 'intending', 'to', 'write', 'this', 'post', 'for', 'the', 'last', '2', 'hours', '(', 'oh', 'the', '\"', 'irony', '\"', 'of', 'getting', 'distracted', 'by', 'the', 'ad', '##hd', 'sub', '##red', '##dit', ')', ',', 'i', 'have', 'finally', 'gotten', 'ar', '##oud', '##n', 'to', 'posting', '!', 'i', 'am', 'curious', 'to', 'know', 'what', 'jobs', 'you', 'guys', 'and', 'gal', '##s', 'have', '?', 'do', 'you', 'know', 'others', 'at', 'work', 'or', 'in', 'your', 'field', 'with', 'ad', '##hd', '?', 'are', 'there', 'any', 'fields', 'that', 'seem', 'to', 'attract', 'those', 'of', 'us', 'with', 'ad', '##hd', '?', 'also', ',', 'i', 'just', 'want', 'to', 'get', 'to', 'know', 'the', 'people', 'on', 'here', '.', 'to', 'start', ',', 'i', 'am', 'a', 'graduating', 'senior', 'in', 'mechanical', 'engineering', ',', 'and', 'my', 'main', 'ho', '##bbies', 'include', 'tin', '##ker', '##ing', 'with', 'computers', ',', 'bicycles', ',', 'and', 'taking', 'stuff', 'apart', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['submissions', 'from', 'clearly', 'anti', '-', 'psychiatry', '/', 'scientology', 'websites', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['submissions', 'from', 'clearly', 'anti', '-', 'psychiatry', '/', 'scientology', 'websites', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['important', ':', 'what', 'ad', '##hd', 'people', 'need', 'in', 'education', '.', '.', '.', 'the', 'thing', 'is', 'that', 'ad', '##hd', 'is', 'not', 'that', 'much', 'of', 'a', '*', 'serious', 'disorder', '*', ',', 'the', 'thing', 'is', 'that', 'it', 'is', 'manufactured', 'that', 'way', 'by', 'simple', 'fucked', 'up', '-', 'one', '-', 'way', 'system', 'focused', 'on', 'as', 'much', 'regular', '##ity', 'as', 'possible', ',', 'st', '##omp', '##ing', 'individual', '##ity', 'and', 'creativity', ',', 'to', 'better', 'understand', 'this', ',', '*', '*', 'i', 'highly', 'recommend', 'you', 'guys', 'to', 'go', 'watch', 'these', 'videos', ',', 'they', 'are', 'essential', 'for', 'understanding', 'of', 'what', 'ad', '##hd', 'people', 'need', 'from', 'education', '*', '*', ':', '*', '[', 'sir', 'ken', 'robinson', '-', 'school', 'kills', 'creativity', ']', '(', 'http', ':', '/', '/', 'www', '.', 'ted', '.', 'com', '/', 'talks', '/', 'ken', '_', 'robinson', '_', 'says', '_', 'schools', '_', 'kill', '_', 'creativity', '.', 'html', ')', '*', '[', 'sir', 'ken', 'robinson', '-', 'bring', 'on', 'the', 'revolution', ']', '(', 'http', ':', '/', '/', 'www', '.', 'ted', '.', 'com', '/', 'talks', '/', 'sir', '_', 'ken', '_', 'robinson', '_', 'bring', '_', 'on', '_', 'the', '_', 'revolution', '.', 'html', ')', '*', '[', 'sir', 'ken', 'robinson', '-', 'changing', 'education', 'paradigm', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'z', '##d', '##z', '##fc', '##d', '##gp', '##l', '##4', '##u', ')', '*', '[', 'the', 'young', 'turks', 'coverage', '-', 'finland', \"'\", 's', 'revolutionary', 'education', 'program', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##lo', '##f', '##z', '##l', '_', 'j', '##5', '##fo', '&', 'feature', '=', 'plc', '##p', '&', 'context', '=', 'c', '##4', '##ce', '##59', '##1', '##b', '##vd', '##v', '##j', '##v', '##qa', '##1', '##pp', '##c', '##fp', '##a', '##90', '##q', '##x', '##7', '##3', '##v', '##4', '##n', '##8', '##d', '_', 'y', '##x', '##fu', '##oh', '##ay', '##ph', '##w', '##jo', '##c', '##9', '##i', '##5', '##iy', '=', ')', '*', '[', 'more', 'about', 'finland', \"'\", 's', 'educational', 'system', '\\\\', '(', 'long', 'article', 'alert', '\\\\', ')', ']', '(', 'http', ':', '/', '/', 'www', '.', 'the', '##at', '##lan', '##tic', '.', 'com', '/', 'national', '/', 'archive', '/', '2011', '/', '12', '/', 'what', '-', 'americans', '-', 'keep', '-', 'ignoring', '-', 'about', '-', 'finland', '##s', '-', 'school', '-', 'success', '/', '250', '##56', '##4', '/', ')', 'if', 'those', 'ideas', 'got', 'implemented', ',', 'the', 'educational', 'system', 'and', 'future', 'life', 'would', 'improve', 'drastically', 'for', 'all', 'ad', '##hd', 'people', 'living', 'in', 'the', 'given', 'country', '.']\n",
      "INFO:__main__:Number of tokens: 386\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['important', ':', 'what', 'ad', '##hd', 'people', 'need', 'in', 'education', '.', '.', '.', 'the', 'thing', 'is', 'that', 'ad', '##hd', 'is', 'not', 'that', 'much', 'of', 'a', '*', 'serious', 'disorder', '*', ',', 'the', 'thing', 'is', 'that', 'it', 'is', 'manufactured', 'that', 'way', 'by', 'simple', 'fucked', 'up', '-', 'one', '-', 'way', 'system', 'focused', 'on', 'as', 'much', 'regular', '##ity', 'as', 'possible', ',', 'st', '##omp', '##ing', 'individual', '##ity', 'and', 'creativity', ',', 'to', 'better', 'understand', 'this', ',', '*', '*', 'i', 'highly', 'recommend', 'you', 'guys', 'to', 'go', 'watch', 'these', 'videos', ',', 'they', 'are', 'essential', 'for', 'understanding', 'of', 'what', 'ad', '##hd', 'people', 'need', 'from', 'education', '*', '*', ':', '*', '[', 'sir', 'ken', 'robinson', '-', 'school', 'kills', 'creativity', ']', '(', 'http', ':', '/', '/', 'www', '.', 'ted', '.', 'com', '/', 'talks', '/', 'ken', '_', 'robinson', '_', 'says', '_', 'schools', '_', 'kill', '_', 'creativity', '.', 'html', ')', '*', '[', 'sir', 'ken', 'robinson', '-', 'bring', 'on', 'the', 'revolution', ']', '(', 'http', ':', '/', '/', 'www', '.', 'ted', '.', 'com', '/', 'talks', '/', 'sir', '_', 'ken', '_', 'robinson', '_', 'bring', '_', 'on', '_', 'the', '_', 'revolution', '.', 'html', ')', '*', '[', 'sir', 'ken', 'robinson', '-', 'changing', 'education', 'paradigm', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'z', '##d', '##z', '##fc', '##d', '##gp', '##l', '##4', '##u', ')', '*', '[', 'the', 'young', 'turks', 'coverage', '-', 'finland', \"'\", 's', 'revolutionary', 'education', 'program', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##lo', '##f', '##z', '##l', '_', 'j', '##5', '##fo', '&', 'feature', '=', 'plc', '##p', '&', 'context', '=', 'c', '##4', '##ce', '##59', '##1', '##b', '##vd', '##v', '##j', '##v', '##qa', '##1', '##pp', '##c', '##fp', '##a', '##90', '##q', '##x', '##7', '##3', '##v', '##4', '##n', '##8', '##d', '_', 'y', '##x', '##fu', '##oh', '##ay', '##ph', '##w', '##jo', '##c', '##9', '##i', '##5', '##iy', '=', ')', '*', '[', 'more', 'about', 'finland', \"'\", 's', 'educational', 'system', '\\\\', '(', 'long', 'article', 'alert', '\\\\', ')', ']', '(', 'http', ':', '/', '/', 'www', '.', 'the', '##at', '##lan', '##tic', '.', 'com', '/', 'national', '/', 'archive', '/', '2011', '/', '12', '/', 'what', '-', 'americans', '-', 'keep', '-', 'ignoring', '-', 'about', '-', 'finland', '##s', '-', 'school', '-', 'success', '/', '250', '##56', '##4', '/', ')', 'if', 'those', 'ideas', 'got', 'implemented', ',', 'the', 'educational', 'system', 'and', 'future', 'life', 'would', 'improve', 'drastically', 'for', 'all', 'ad', '##hd', 'people', 'living', 'in', 'the', 'given', 'country', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'ad', '##hd', 'brain', ':', 'qui', '##ntes', '##sen', '##tial', 'super', '##com', '##put', '##er', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'ad', '##hd', 'brain', ':', 'qui', '##ntes', '##sen', '##tial', 'super', '##com', '##put', '##er', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'tips', 'for', 'ad', '##hd', 'and', 'calculus', '?', 'i', 'have', 'a', 'test', 'in', 'my', 'calculus', 'class', 'on', 'monday', 'and', 'i', \"'\", 'm', 'freaking', 'out', '.', 'in', 'the', 'first', 'two', 'tests', ',', 'i', 'did', 'pretty', 'well', 'in', 'relation', 'to', 'the', 'class', 'average', ',', 'although', 'not', 'as', 'well', 'as', 'i', 'had', 'hopped', ',', 'i', 'still', 'passed', 'and', 'couldn', \"'\", 't', 'complain', 'very', 'much', '.', 'on', 'my', '3rd', 'test', ',', 'i', 'mis', '##era', '##bly', 'failed', ',', 'admitted', '##ly', 'i', 'didn', \"'\", 't', 'study', 'as', 'many', 'hours', ',', 'but', 'i', 'still', 'felt', 'relatively', 'comfortable', 'with', 'the', 'material', '.', 'the', 'thing', 'is', ',', 'that', 'in', 'both', 'situations', 'my', 'mistakes', 'were', 'so', 'silly', 'it', 'easily', 'cost', 'me', 'a', 'letter', 'grade', 'or', 'even', 'two', '.', 'as', 'most', 'of', 'you', 'know', 'all', 'too', 'well', ',', 'this', 'is', 'incredibly', 'in', '##fur', '##iating', '.', 'i', 'see', 'tutor', '##s', ',', 'devote', 'hours', 'and', 'hours', 'to', 'studying', ',', 'and', 'i', 'feel', 'like', 'i', 'get', 'the', 'concepts', ',', 'but', 'when', 'the', 'test', 'comes', ',', 'i', 'just', 'can', \"'\", 't', 'seem', 'to', 'do', 'as', 'well', 'as', 'i', 'should', '.', 'it', \"'\", 's', 'worth', 'adding', 'that', 'i', 'haven', \"'\", 't', 'performed', 'well', 'in', 'math', 'since', 'middle', 'school', '.', 'i', 'forget', 'the', 'order', 'of', 'the', 'problems', ',', 'and', 'what', 'to', 'do', 'next', ',', 'then', 'i', 'start', 'questioning', 'myself', 'and', 'it', 'just', 'becomes', 'a', 'mess', 'from', 'there', '.', 'i', 'am', 'allowed', 'to', 'take', 'my', 'test', 'in', 'a', 'testing', 'center', 'and', 'have', 'extra', 'time', ',', 'i', 'also', 'wear', 'ear', 'plug', '##s', 'so', 'that', 'i', 'am', 'not', 'distracted', 'every', 'time', 'someone', 'moves', 'in', 'their', 'chair', '.', 'i', 'just', 'don', \"'\", 't', 'know', 'what', 'else', 'i', 'could', 'do', 'and', 'i', 'think', 'i', 'can', 'really', 'benefit', 'from', 'some', 'tips', ',', 'if', 'anyone', 'has', 'any', 'it', 'would', 'be', 'great', '!', 'thanks', 'in', 'advanced', '.', 't', '##ld', '##r', ';', 'need', 'some', 'help', 'in', 'preparing', 'for', 'and', 'taking', 'my', 'calculus', 'test', '.', 'please', 'share', 'some', 'tips', 'if', 'you', 'have', 'had', 'a', 'similar', 'problem', 'or', 'knows', 'some', 'good', 'techniques', '.']\n",
      "INFO:__main__:Number of tokens: 325\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'tips', 'for', 'ad', '##hd', 'and', 'calculus', '?', 'i', 'have', 'a', 'test', 'in', 'my', 'calculus', 'class', 'on', 'monday', 'and', 'i', \"'\", 'm', 'freaking', 'out', '.', 'in', 'the', 'first', 'two', 'tests', ',', 'i', 'did', 'pretty', 'well', 'in', 'relation', 'to', 'the', 'class', 'average', ',', 'although', 'not', 'as', 'well', 'as', 'i', 'had', 'hopped', ',', 'i', 'still', 'passed', 'and', 'couldn', \"'\", 't', 'complain', 'very', 'much', '.', 'on', 'my', '3rd', 'test', ',', 'i', 'mis', '##era', '##bly', 'failed', ',', 'admitted', '##ly', 'i', 'didn', \"'\", 't', 'study', 'as', 'many', 'hours', ',', 'but', 'i', 'still', 'felt', 'relatively', 'comfortable', 'with', 'the', 'material', '.', 'the', 'thing', 'is', ',', 'that', 'in', 'both', 'situations', 'my', 'mistakes', 'were', 'so', 'silly', 'it', 'easily', 'cost', 'me', 'a', 'letter', 'grade', 'or', 'even', 'two', '.', 'as', 'most', 'of', 'you', 'know', 'all', 'too', 'well', ',', 'this', 'is', 'incredibly', 'in', '##fur', '##iating', '.', 'i', 'see', 'tutor', '##s', ',', 'devote', 'hours', 'and', 'hours', 'to', 'studying', ',', 'and', 'i', 'feel', 'like', 'i', 'get', 'the', 'concepts', ',', 'but', 'when', 'the', 'test', 'comes', ',', 'i', 'just', 'can', \"'\", 't', 'seem', 'to', 'do', 'as', 'well', 'as', 'i', 'should', '.', 'it', \"'\", 's', 'worth', 'adding', 'that', 'i', 'haven', \"'\", 't', 'performed', 'well', 'in', 'math', 'since', 'middle', 'school', '.', 'i', 'forget', 'the', 'order', 'of', 'the', 'problems', ',', 'and', 'what', 'to', 'do', 'next', ',', 'then', 'i', 'start', 'questioning', 'myself', 'and', 'it', 'just', 'becomes', 'a', 'mess', 'from', 'there', '.', 'i', 'am', 'allowed', 'to', 'take', 'my', 'test', 'in', 'a', 'testing', 'center', 'and', 'have', 'extra', 'time', ',', 'i', 'also', 'wear', 'ear', 'plug', '##s', 'so', 'that', 'i', 'am', 'not', 'distracted', 'every', 'time', 'someone', 'moves', 'in', 'their', 'chair', '.', 'i', 'just', 'don', \"'\", 't', 'know', 'what', 'else', 'i', 'could', 'do', 'and', 'i', 'think', 'i', 'can', 'really', 'benefit', 'from', 'some', 'tips', ',', 'if', 'anyone', 'has', 'any', 'it', 'would', 'be', 'great', '!', 'thanks', 'in', 'advanced', '.', 't', '##ld', '##r', ';', 'need', 'some', 'help', 'in', 'preparing', 'for', 'and', 'taking', 'my', 'calculus', 'test', '.', 'please', 'share', 'some', 'tips', 'if', 'you', 'have', 'had', 'a', 'similar', 'problem', 'or', 'knows', 'some', 'good', 'techniques', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'had', 'any', 'experience', 'with', 'bio', '##fe', '##ed', '##back', '?', 'i', 'may', 'have', 'use', 'of', 'a', 'bio', '##fe', '##ed', '##back', 'lab', 'at', 'my', 'university', \"'\", 's', 'psychological', 'services', '.', 'i', 'don', \"'\", 't', 'actually', 'know', 'what', \"'\", 's', 'in', 'it', 'or', 'what', 'sorts', 'of', 'things', 'people', 'do', 'there', ',', 'but', 'i', 'am', 'willing', 'to', 'try', 'it', 'out', 'if', 'it', 'can', 'be', 'of', 'any', 'benefit', 'in', 'supplement', '##ing', 'or', 'su', '##pp', '##lan', '##ting', 'med', '##s', ',', 'which', 'i', 'don', \"'\", 't', 'like', 'because', 'of', 'the', 'come', 'down', ',', 'occasional', 'bouts', 'of', 'anxiety', ',', 'and', 'spaced', 'out', 'feeling', '.', 'has', 'anyone', 'in', '/', 'ad', '##hd', '(', 'or', '/', 'add', ')', 'ever', 'tried', 'such', 'a', 'thing', '?', 'did', 'it', 'help', 'at', 'all', '?', 'what', 'kinds', 'of', 'exercises', 'does', 'one', 'do', 'there', '?', 'does', 'it', 'compare', 'at', 'all', 'to', 'physical', 'activity', '?']\n",
      "INFO:__main__:Number of tokens: 139\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'had', 'any', 'experience', 'with', 'bio', '##fe', '##ed', '##back', '?', 'i', 'may', 'have', 'use', 'of', 'a', 'bio', '##fe', '##ed', '##back', 'lab', 'at', 'my', 'university', \"'\", 's', 'psychological', 'services', '.', 'i', 'don', \"'\", 't', 'actually', 'know', 'what', \"'\", 's', 'in', 'it', 'or', 'what', 'sorts', 'of', 'things', 'people', 'do', 'there', ',', 'but', 'i', 'am', 'willing', 'to', 'try', 'it', 'out', 'if', 'it', 'can', 'be', 'of', 'any', 'benefit', 'in', 'supplement', '##ing', 'or', 'su', '##pp', '##lan', '##ting', 'med', '##s', ',', 'which', 'i', 'don', \"'\", 't', 'like', 'because', 'of', 'the', 'come', 'down', ',', 'occasional', 'bouts', 'of', 'anxiety', ',', 'and', 'spaced', 'out', 'feeling', '.', 'has', 'anyone', 'in', '/', 'ad', '##hd', '(', 'or', '/', 'add', ')', 'ever', 'tried', 'such', 'a', 'thing', '?', 'did', 'it', 'help', 'at', 'all', '?', 'what', 'kinds', 'of', 'exercises', 'does', 'one', 'do', 'there', '?', 'does', 'it', 'compare', 'at', 'all', 'to', 'physical', 'activity', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'game', 'for', 'picking', 'up', 'cl', '##utter', 'this', 'sounds', 'simple', 'but', 'it', \"'\", 's', 'really', 'helped', 'me', '.', 'when', 'i', 'need', 'to', 'put', 'away', 'cl', '##utter', ',', 'i', 'play', 'a', 'game', 'called', '\"', 'put', 'ten', 'things', 'away', '\"', '.', 'i', 'try', 'to', 'put', '*', 'just', 'ten', 'things', '*', 'where', 'they', 'belong', '.', 'i', 'count', 'them', 'down', 'as', 'i', 'go', '.', 'the', 'goal', 'of', 'the', 'game', 'is', 'to', 'put', 'away', 'ten', 'and', 'only', 'ten', 'things', '.', 'it', 'helps', 'it', 'keep', '\"', 'cleaning', 'cl', '##utter', '\"', 'from', 'becoming', '\"', 'organize', 'the', 'books', '##hel', '##ves', '\"', 'or', '\"', 'cook', 'a', 'meal', '\"', '.', 'if', 'i', 'enjoyed', 'my', 'game', 'of', '\"', 'put', 'ten', 'things', 'away', '\"', ',', 'and', 'the', 'place', 'is', 'still', 'messy', ',', 'then', 'i', 'might', 'decide', 'to', 'go', 'for', 'a', 'second', 'round', '.', 'it', 'sounds', 'childish', 'but', 'it', 'works', 'well', 'for', 'me', '.']\n",
      "INFO:__main__:Number of tokens: 141\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'game', 'for', 'picking', 'up', 'cl', '##utter', 'this', 'sounds', 'simple', 'but', 'it', \"'\", 's', 'really', 'helped', 'me', '.', 'when', 'i', 'need', 'to', 'put', 'away', 'cl', '##utter', ',', 'i', 'play', 'a', 'game', 'called', '\"', 'put', 'ten', 'things', 'away', '\"', '.', 'i', 'try', 'to', 'put', '*', 'just', 'ten', 'things', '*', 'where', 'they', 'belong', '.', 'i', 'count', 'them', 'down', 'as', 'i', 'go', '.', 'the', 'goal', 'of', 'the', 'game', 'is', 'to', 'put', 'away', 'ten', 'and', 'only', 'ten', 'things', '.', 'it', 'helps', 'it', 'keep', '\"', 'cleaning', 'cl', '##utter', '\"', 'from', 'becoming', '\"', 'organize', 'the', 'books', '##hel', '##ves', '\"', 'or', '\"', 'cook', 'a', 'meal', '\"', '.', 'if', 'i', 'enjoyed', 'my', 'game', 'of', '\"', 'put', 'ten', 'things', 'away', '\"', ',', 'and', 'the', 'place', 'is', 'still', 'messy', ',', 'then', 'i', 'might', 'decide', 'to', 'go', 'for', 'a', 'second', 'round', '.', 'it', 'sounds', 'childish', 'but', 'it', 'works', 'well', 'for', 'me', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'sure', 'those', 'of', 'you', 'who', 'don', \"'\", 't', 'take', 'medication', 'on', 'the', 'weekends', 'can', 'relate']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'sure', 'those', 'of', 'you', 'who', 'don', \"'\", 't', 'take', 'medication', 'on', 'the', 'weekends', 'can', 'relate']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['total', 'loss', 'of', 'ability', 'to', 'sense', 'hunger', '/', 'full', '##ness', ',', 'help', '!']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['total', 'loss', 'of', 'ability', 'to', 'sense', 'hunger', '/', 'full', '##ness', ',', 'help', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'you', 'view', 'as', 'na', '##gging', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'you', 'view', 'as', 'na', '##gging', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'many', 'of', 'you', 'have', 'ad', '##hd', 'and', 'anxiety', ',', 'and', 'how', 'many', 'have', 'ad', '##hd', 'that', 'causes', 'anxiety', '?']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'many', 'of', 'you', 'have', 'ad', '##hd', 'and', 'anxiety', ',', 'and', 'how', 'many', 'have', 'ad', '##hd', 'that', 'causes', 'anxiety', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ir', '##rita', '##bility', 'and', 'med', '##s', '.', 'i', 'am', 'taking', 'add', '##eral', 'x', '##r', '.', 'my', 'psychiatrist', 'told', 'me', 'before', 'the', 'medication', 'that', 'i', 'would', 'have', 'some', 'ir', '##rita', '##bility', 'the', 'first', 'couple', 'of', 'weeks', '.', 'i', 'started', 'of', 'with', '10', 'mg', 'and', 'i', 'was', 'fine', '.', 'on', 'day', '7', 'i', 'switched', 'up', 'to', '20', 'mg', 'like', 'he', 'instructed', '.', 'the', 'day', 'before', 'i', 'felt', 'like', 'the', 'med', '##s', 'were', 'ineffective', 'for', 'the', 'most', 'part', '.', 'so', 'now', 'it', 'is', 'day', '9', 'and', 'everything', 'is', 'like', 'nails', 'on', 'a', 'chalk', '##board', 'to', 'me', '.', 'i', 'am', 'keeping', 'it', 'together', 'and', 'not', 'lash', '##ing', 'out', 'but', 'it', 'is', 'really', 'tough', '.', 'anyone', 'else', 'experience', 'this', 'and', 'did', 'it', 'go', 'away', 'for', 'you', '?']\n",
      "INFO:__main__:Number of tokens: 122\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ir', '##rita', '##bility', 'and', 'med', '##s', '.', 'i', 'am', 'taking', 'add', '##eral', 'x', '##r', '.', 'my', 'psychiatrist', 'told', 'me', 'before', 'the', 'medication', 'that', 'i', 'would', 'have', 'some', 'ir', '##rita', '##bility', 'the', 'first', 'couple', 'of', 'weeks', '.', 'i', 'started', 'of', 'with', '10', 'mg', 'and', 'i', 'was', 'fine', '.', 'on', 'day', '7', 'i', 'switched', 'up', 'to', '20', 'mg', 'like', 'he', 'instructed', '.', 'the', 'day', 'before', 'i', 'felt', 'like', 'the', 'med', '##s', 'were', 'ineffective', 'for', 'the', 'most', 'part', '.', 'so', 'now', 'it', 'is', 'day', '9', 'and', 'everything', 'is', 'like', 'nails', 'on', 'a', 'chalk', '##board', 'to', 'me', '.', 'i', 'am', 'keeping', 'it', 'together', 'and', 'not', 'lash', '##ing', 'out', 'but', 'it', 'is', 'really', 'tough', '.', 'anyone', 'else', 'experience', 'this', 'and', 'did', 'it', 'go', 'away', 'for', 'you', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'a', 'question', 'about', 'add', '##eral', '##l', 'x', '##r', 'so', 'the', 'first', 'medication', 'i', 'had', 'started', 'taking', 'was', 'just', '10', '##mg', 'of', 'add', '##eral', '##l', 'x', '##r', '.', 'i', 'had', 'discovered', 'that', 'it', 'wore', 'off', 'way', 'too', 'quickly', 'into', 'the', 'day', '.', 'i', 'told', 'my', 'doctor', 'about', 'it', 'and', 'she', 'said', 'it', 'is', 'known', 'to', 'wear', 'off', 'a', 'lot', '.', 'i', 'asked', 'her', 'if', 'a', 'higher', 'dose', 'like', '20', 'or', '30', 'mg', 'would', 'make', 'it', 'last', 'longer', '.', 'she', 'said', 'the', 'dos', '##age', 'size', 'doesn', \"'\", 't', 'effect', 'how', 'long', 'it', 'lasts', '.', 'is', 'that', 'actually', 'true', '?', 'i', 'mean', 'it', 'has', 'to', 'be', 'able', 'to', 'work', 'longer', 'if', 'it', 'has', 'more', 'to', 'release', 'into', 'my', 'body', 'but', 'it', 'might', 'just', 'be', 'me', 'being', 'stubborn', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'does', 'the', 'mill', '##ig', '##ram', 'size', 'in', 'add', '##eral', '##l', 'x', '##r', 'effect', 'the', 'duration', 'of', 'it', '?', '*', '*']\n",
      "INFO:__main__:Number of tokens: 154\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'a', 'question', 'about', 'add', '##eral', '##l', 'x', '##r', 'so', 'the', 'first', 'medication', 'i', 'had', 'started', 'taking', 'was', 'just', '10', '##mg', 'of', 'add', '##eral', '##l', 'x', '##r', '.', 'i', 'had', 'discovered', 'that', 'it', 'wore', 'off', 'way', 'too', 'quickly', 'into', 'the', 'day', '.', 'i', 'told', 'my', 'doctor', 'about', 'it', 'and', 'she', 'said', 'it', 'is', 'known', 'to', 'wear', 'off', 'a', 'lot', '.', 'i', 'asked', 'her', 'if', 'a', 'higher', 'dose', 'like', '20', 'or', '30', 'mg', 'would', 'make', 'it', 'last', 'longer', '.', 'she', 'said', 'the', 'dos', '##age', 'size', 'doesn', \"'\", 't', 'effect', 'how', 'long', 'it', 'lasts', '.', 'is', 'that', 'actually', 'true', '?', 'i', 'mean', 'it', 'has', 'to', 'be', 'able', 'to', 'work', 'longer', 'if', 'it', 'has', 'more', 'to', 'release', 'into', 'my', 'body', 'but', 'it', 'might', 'just', 'be', 'me', 'being', 'stubborn', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'does', 'the', 'mill', '##ig', '##ram', 'size', 'in', 'add', '##eral', '##l', 'x', '##r', 'effect', 'the', 'duration', 'of', 'it', '?', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['well', '##bu', '##tri', '##n', 'for', 'ad', '##hd', 'hello', '.', 'i', 'recently', 'started', 'on', 'well', '##bu', '##tri', '##n', '(', 'actually', 'the', 'generic', 'version', ')', 'after', 'recently', 'being', 'diagnosed', 'with', 'ad', '##hd', '.', 'the', 'reason', 'i', 'chose', 'it', 'was', 'because', 'i', 'wanted', 'to', 'us', 'a', 'non', '-', 'st', '##im', '##ula', '##nt', 'medication', ',', 'and', 'it', 'was', 'the', 'least', 'costly', '.', 'if', 'it', 'matters', ',', 'i', 'used', 'to', 'be', 'taking', 'pa', '##xi', '##l', 'for', 'depression', '.', 'i', 'would', 'like', 'to', 'know', 'what', 'others', 'experience', 'with', 'using', 'well', '##bu', '##tri', '##n', 'were', '.', 'anyone', 'else', 'use', 'it', '?', 'thanks', '.', 'edit', ':', 'thanks', 'for', 'the', 'replies', '.', 'i', 'plan', 'on', 'staying', 'on', 'well', '##bu', '##tri', '##n', ',', 'as', 'my', 'doc', 'recommends', 'for', 'now', 'and', 'just', 'track', 'my', 'progress', '.', 'if', 'i', 'don', \"'\", 't', 'see', 'any', 'difference', 'then', 'i', 'may', 'try', 'st', '##rate', '##rra', '.']\n",
      "INFO:__main__:Number of tokens: 141\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['well', '##bu', '##tri', '##n', 'for', 'ad', '##hd', 'hello', '.', 'i', 'recently', 'started', 'on', 'well', '##bu', '##tri', '##n', '(', 'actually', 'the', 'generic', 'version', ')', 'after', 'recently', 'being', 'diagnosed', 'with', 'ad', '##hd', '.', 'the', 'reason', 'i', 'chose', 'it', 'was', 'because', 'i', 'wanted', 'to', 'us', 'a', 'non', '-', 'st', '##im', '##ula', '##nt', 'medication', ',', 'and', 'it', 'was', 'the', 'least', 'costly', '.', 'if', 'it', 'matters', ',', 'i', 'used', 'to', 'be', 'taking', 'pa', '##xi', '##l', 'for', 'depression', '.', 'i', 'would', 'like', 'to', 'know', 'what', 'others', 'experience', 'with', 'using', 'well', '##bu', '##tri', '##n', 'were', '.', 'anyone', 'else', 'use', 'it', '?', 'thanks', '.', 'edit', ':', 'thanks', 'for', 'the', 'replies', '.', 'i', 'plan', 'on', 'staying', 'on', 'well', '##bu', '##tri', '##n', ',', 'as', 'my', 'doc', 'recommends', 'for', 'now', 'and', 'just', 'track', 'my', 'progress', '.', 'if', 'i', 'don', \"'\", 't', 'see', 'any', 'difference', 'then', 'i', 'may', 'try', 'st', '##rate', '##rra', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'to', 'communicate', 'better', 'with', 'my', 'doctor', '?', 'i', 'feel', 'like', 'i', 'am', 'in', 'a', 'total', 'bind', ',', 'unable', 'to', 'get', 'the', 'right', 'med', '##s', 'and', 'i', 'need', 'advice', '.', '.', '.', 'i', 'feel', 'like', 'i', 'am', 'in', 'a', 'bit', 'of', 'a', 'bind', 'with', 'my', 'medication', 'situation', 'right', 'now', 'partly', 'due', 'to', 'being', 'un', '##ski', '##lled', 'at', 'communicating', 'with', 'my', 'doctor', '.', 'i', 'haven', \"'\", 't', 'been', 'with', 'a', 'doctor', 'for', 'that', 'long', 'and', 'i', 'hope', 'that', 'some', 'wise', '##r', ',', 'more', 'experienced', 'ad', '##hd', '##ers', 'will', 'be', 'able', 'to', 'give', 'me', 'some', 'advice', '.', 'i', 'have', 'been', 'med', '##icated', 'for', 'a', 'little', 'over', 'a', 'year', '.', 'i', 'am', 'currently', 'on', 'short', 'release', 'rita', '##lin', '.', 'i', 'have', 'been', 'on', 'other', 'things', ',', 'including', 'concert', '##a', ',', 'with', 'varied', 'degrees', 'of', 'success', '.', 'concert', '##a', 'did', 'not', 'agree', 'with', 'me', '.', 'rita', '##lin', 'seemed', 'to', 'work', 'the', 'best', 'for', 'a', 'while', '-', 'approximately', 'since', 'the', 'end', 'of', 'last', 'fall', '.', 'lately', ',', 'i', \"'\", 've', 'started', 'dread', '##ing', 'the', 'days', 'that', 'i', 'have', 'to', 'take', 'it', '.', 'it', 'gives', 'me', 'very', 'little', 'focus', '.', 'it', 'makes', 'me', 'paranoid', '.', 'and', 'the', 'crashes', 'are', 'awful', 'ordeal', '##s', 'of', 'self', '-', 'lo', '##athing', ',', 'ji', '##tters', ',', 'and', 'sadness', 'every', 'time', '.', 'i', 'am', 'not', 'taking', 'a', 'lot', ';', 'maybe', 'fifteen', 'to', 'twenty', 'mg', 'if', 'i', \"'\", 'm', 'going', 'to', 'be', 'studying', 'for', 'a', 'full', 'day', ',', 'spaced', 'out', 'considerably', '.', 'usually', 'less', 'than', 'this', '.', 'if', 'i', 'don', \"'\", 't', 'take', 'it', ',', 'i', 'find', 'myself', 'completely', 'unable', 'to', 'focus', 'on', 'school', '##work', ',', 'and', 'so', 'dismay', '##ed', 'and', 'overwhelmed', 'that', 'i', 'just', 'sit', 'on', 'my', 'ass', 'wishing', 'i', 'was', 'somebody', 'else', '.', 'i', 'would', 'like', 'to', 'find', 'a', 'med', 'that', 'works', 'better', 'for', 'me', '.', 'i', 'recently', 'tried', 'v', '##y', '##van', '##se', ',', 'obtained', 'from', 'a', 'friend', '.', 'honestly', 'it', 'seems', 'like', 'a', 'miracle', 'drug', 'to', 'me', '.', 'whenever', 'i', \"'\", 've', 'taken', 'it', 'i', \"'\", 've', 'been', 'calm', ',', 'focused', ',', 'and', 'productive', 'all', 'day', 'long', '.', 'i', 'didn', \"'\", 't', 'abuse', 'it', ';', 'i', 'took', 'low', 'doses', '.', 'about', 'twenty', 'mg', '.', 'i', 'know', 'that', 'add', '##eral', '##l', 'and', 'v', '##y', '##van', '##se', 'work', 'well', 'for', 'me', 'but', 'i', \"'\", 'm', 'terrified', 'of', 'asking', 'my', 'doctor', 'to', 'put', 'me', 'on', 'either', 'of', 'these', ',', 'or', 'even', 'to', 'change', 'me', 'to', 'something', 'besides', 'rita', '##lin', '.', 'there', 'are', 'a', 'few', 'reasons', 'for', 'this', '.', '-', 'i', \"'\", 've', 'asked', 'to', 'be', 'switched', 'a', 'lot', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'come', 'off', 'as', 'someone', 'who', 'will', 'never', 'be', 'satisfied', 'and', 'will', 'just', 'waste', 'her', 'time', '.', 'i', 'don', \"'\", 't', 'know', 'if', 'i', \"'\", 'm', 'being', 'paranoid', 'but', 'i', 'get', 'the', 'impression', 'that', 'she', 'is', 'impatient', 'with', 'me', 'sometimes', '.', '-', 'she', 'knows', 'about', 'almost', 'all', 'of', 'my', 'illegal', '/', 'recreational', 'drug', 'use', '.', 'i', 'use', 'hall', '##uc', '##ino', '##gens', 'and', 'md', '##ma', ',', 'and', 'never', 'speed', ',', 'cocaine', ',', 'or', 'any', 'pharmaceuticals', 'of', 'any', 'type', 'for', 'fun', ',', 'but', 'i', 'don', \"'\", 't', 'think', 'that', 'matters', 'to', 'her', '.', 'a', 'drug', '##gy', 'is', 'a', 'drug', '##gy', ',', 'right', '?', 'i', 'tried', 'to', 'lie', 'for', 'a', 'while', 'and', 'say', 'i', 'stopped', 'using', 'drugs', 'but', 'i', 'really', 'hate', 'lying', 'and', 'i', \"'\", 'm', 'awful', 'at', 'maintaining', 'lies', 'anyway', 'because', 'i', 'can', \"'\", 't', 'keep', 'stories', 'straight', 'in', 'my', 'head', '.', 'she', 'recently', 'wanted', 'to', 'put', 'me', 'on', 'anti', '##de', '##press', '##ants', 'due', 'to', 'some', 'social', 'anxiety', 'that', 'i', 'have', ';', 'i', 'was', 'very', 'hesitant', 'but', 'said', 'i', \"'\", 'd', 'think', 'about', 'it', '.', 'when', 'she', 'grille', '##d', 'me', 'for', 'a', 'prolonged', 'period', 'of', 'time', 'i', 'just', 'ended', 'up', 'telling', 'her', 'that', 'not', 'being', 'able', 'to', 'use', 'psychedelic', '##s', 'this', 'summer', 'would', 'probably', 'bum', 'me', 'out', '.', 'in', 'reality', ',', 'this', 'is', 'true', 'but', 'i', 'also', 'just', 'don', \"'\", 't', 'think', 'i', 'need', 'anti', '##de', '##press', '##ants', 'and', 'that', 'is', 'what', 'i', 'should', 'have', 'said', '.', 'i', 'tend', 'to', 'be', 'stupid', 'under', 'pressure', '.', '-', 'i', 'am', 'not', 'good', 'at', 'expressing', 'my', 'feelings', '.', 'i', 'had', 'a', 'very', 'difficult', 'time', 'explaining', 'the', 'inconsistent', 'and', 'confusing', 'effects', 'of', 'concert', '##a', 'to', 'her', ',', 'and', 'for', 'this', 'reason', 'and', 'other', 'reasons', 'i', 'feel', 'like', 'she', 'doesn', \"'\", 't', 'really', 'take', 'me', 'seriously', '.', 'two', 'other', 'things', ';', '-', 'she', 'knows', 'that', 'i', 'have', 'had', 'add', '##eral', '##l', '.', 'when', 'i', 'talked', 'to', 'her', 'about', 'going', 'on', 'it', 'before', 'she', 'seemed', 'like', 'she', 'would', 'consider', 'it', 'but', 'would', 'prefer', 'not', 'to', ',', 'as', 'she', 'thinks', 'it', 'would', 'be', 'bad', 'for', 'my', 'anxiety', '.', 'since', 'she', 'seemed', 'uncomfortable', 'with', 'giving', 'me', 'add', '##eral', '##l', ',', 'that', 'was', 'when', 'i', 'asked', 'for', 'short', 'release', 'rita', '##lin', 'instead', '.', '-', 'i', \"'\", 'm', 'aware', 'that', 'v', '##y', '##van', '##se', 'may', 'have', 'the', 'same', 'problem', 'as', 'rita', '##lin', 'for', 'me', 'and', 'stop', 'working', '/', 'present', 'emotional', 'problems', 'after', 'a', 'while', 'but', 'i', 'think', 'it', \"'\", 's', 'time', 'to', 'try', 'it', 'anyway', '.', 'i', 'hate', 'rita', '##lin', ',', 'guys', '.', 'it', 'makes', 'me', 'into', 'a', 'worse', 'person', 'than', 'i', 'really', 'am', '.', 'but', 'i', \"'\", 'm', 'terrified', 'of', 'coming', 'off', 'as', 'a', 'speed', 'seeker', 'and', 'being', 'put', 'on', 'some', 'kind', 'of', 'drug', 'watch', '##list', 'if', 'i', 'ask', 'her', 'for', 'v', '##y', '##van', '##se', ',', 'or', 'even', 'if', 'i', 'just', 'ask', 'her', 'for', 'a', 'med', 'change', '.', 'am', 'i', 'as', 'helpless', 'as', 'i', 'feel', '?', 'can', 'i', 'do', 'anything', '?', 'please', 'help', 'me', '.', 't', '##ld', '##r', ':', 'i', 'am', 'worried', 'i', 'will', 'be', 'put', 'on', 'a', 'drug', 'watch', '##list', '/', 'dropped', 'by', 'my', 'doctor', '/', 'switched', 'to', 'something', 'less', 'effective', 'if', 'i', 'ask', 'to', 'be', 'changed', 'to', 'v', '##y', '##van', '##se', ',', 'which', 'works', ',', 'from', 'rita', '##lin', ',', 'which', 'has', 'started', 'giving', 'me', 'emotional', 'problems', 'and', 'barely', 'works', '.', 'i', 'am', 'not', 'experienced', 'in', 'these', 'things', '.', 'what', 'can', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 975\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['how', 'to', 'communicate', 'better', 'with', 'my', 'doctor', '?', 'i', 'feel', 'like', 'i', 'am', 'in', 'a', 'total', 'bind', ',', 'unable', 'to', 'get', 'the', 'right', 'med', '##s', 'and', 'i', 'need', 'advice', '.', '.', '.', 'i', 'feel', 'like', 'i', 'am', 'in', 'a', 'bit', 'of', 'a', 'bind', 'with', 'my', 'medication', 'situation', 'right', 'now', 'partly', 'due', 'to', 'being', 'un', '##ski', '##lled', 'at', 'communicating', 'with', 'my', 'doctor', '.', 'i', 'haven', \"'\", 't', 'been', 'with', 'a', 'doctor', 'for', 'that', 'long', 'and', 'i', 'hope', 'that', 'some', 'wise', '##r', ',', 'more', 'experienced', 'ad', '##hd', '##ers', 'will', 'be', 'able', 'to', 'give', 'me', 'some', 'advice', '.', 'i', 'have', 'been', 'med', '##icated', 'for', 'a', 'little', 'over', 'a', 'year', '.', 'i', 'am', 'currently', 'on', 'short', 'release', 'rita', '##lin', '.', 'i', 'have', 'been', 'on', 'other', 'things', ',', 'including', 'concert', '##a', ',', 'with', 'varied', 'degrees', 'of', 'success', '.', 'concert', '##a', 'did', 'not', 'agree', 'with', 'me', '.', 'rita', '##lin', 'seemed', 'to', 'work', 'the', 'best', 'for', 'a', 'while', '-', 'approximately', 'since', 'the', 'end', 'of', 'last', 'fall', '.', 'lately', ',', 'i', \"'\", 've', 'started', 'dread', '##ing', 'the', 'days', 'that', 'i', 'have', 'to', 'take', 'it', '.', 'it', 'gives', 'me', 'very', 'little', 'focus', '.', 'it', 'makes', 'me', 'paranoid', '.', 'and', 'the', 'crashes', 'are', 'awful', 'ordeal', '##s', 'of', 'self', '-', 'lo', '##athing', ',', 'ji', '##tters', ',', 'and', 'sadness', 'every', 'time', '.', 'i', 'am', 'not', 'taking', 'a', 'lot', ';', 'maybe', 'fifteen', 'to', 'twenty', 'mg', 'if', 'i', \"'\", 'm', 'going', 'to', 'be', 'studying', 'for', 'a', 'full', 'day', ',', 'spaced', 'out', 'considerably', '.', 'usually', 'less', 'than', 'this', '.', 'if', 'i', 'don', \"'\", 't', 'take', 'it', ',', 'i', 'find', 'myself', 'completely', 'unable', 'to', 'focus', 'on', 'school', '##work', ',', 'and', 'so', 'dismay', '##ed', 'and', 'overwhelmed', 'that', 'i', 'just', 'sit', 'on', 'my', 'ass', 'wishing', 'i', 'was', 'somebody', 'else', '.', 'i', 'would', 'like', 'to', 'find', 'a', 'med', 'that', 'works', 'better', 'for', 'me', '.', 'i', 'recently', 'tried', 'v', '##y', '##van', '##se', ',', 'obtained', 'from', 'a', 'friend', '.', 'honestly', 'it', 'seems', 'like', 'a', 'miracle', 'drug', 'to', 'me', '.', 'whenever', 'i', \"'\", 've', 'taken', 'it', 'i', \"'\", 've', 'been', 'calm', ',', 'focused', ',', 'and', 'productive', 'all', 'day', 'long', '.', 'i', 'didn', \"'\", 't', 'abuse', 'it', ';', 'i', 'took', 'low', 'doses', '.', 'about', 'twenty', 'mg', '.', 'i', 'know', 'that', 'add', '##eral', '##l', 'and', 'v', '##y', '##van', '##se', 'work', 'well', 'for', 'me', 'but', 'i', \"'\", 'm', 'terrified', 'of', 'asking', 'my', 'doctor', 'to', 'put', 'me', 'on', 'either', 'of', 'these', ',', 'or', 'even', 'to', 'change', 'me', 'to', 'something', 'besides', 'rita', '##lin', '.', 'there', 'are', 'a', 'few', 'reasons', 'for', 'this', '.', '-', 'i', \"'\", 've', 'asked', 'to', 'be', 'switched', 'a', 'lot', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'come', 'off', 'as', 'someone', 'who', 'will', 'never', 'be', 'satisfied', 'and', 'will', 'just', 'waste', 'her', 'time', '.', 'i', 'don', \"'\", 't', 'know', 'if', 'i', \"'\", 'm', 'being', 'paranoid', 'but', 'i', 'get', 'the', 'impression', 'that', 'she', 'is', 'impatient', 'with', 'me', 'sometimes', '.', '-', 'she', 'knows', 'about', 'almost', 'all', 'of', 'my', 'illegal', '/', 'recreational', 'drug', 'use', '.', 'i', 'use', 'hall', '##uc', '##ino', '##gens', 'and', 'md', '##ma', ',', 'and', 'never', 'speed', ',', 'cocaine', ',', 'or', 'any', 'pharmaceuticals', 'of', 'any', 'type', 'for', 'fun', ',', 'but', 'i', 'don', \"'\", 't', 'think', 'that', 'matters', 'to', 'her', '.'], ['a', 'drug', '##gy', 'is', 'a', 'drug', '##gy', ',', 'right', '?', 'i', 'tried', 'to', 'lie', 'for', 'a', 'while', 'and', 'say', 'i', 'stopped', 'using', 'drugs', 'but', 'i', 'really', 'hate', 'lying', 'and', 'i', \"'\", 'm', 'awful', 'at', 'maintaining', 'lies', 'anyway', 'because', 'i', 'can', \"'\", 't', 'keep', 'stories', 'straight', 'in', 'my', 'head', '.', 'she', 'recently', 'wanted', 'to', 'put', 'me', 'on', 'anti', '##de', '##press', '##ants', 'due', 'to', 'some', 'social', 'anxiety', 'that', 'i', 'have', ';', 'i', 'was', 'very', 'hesitant', 'but', 'said', 'i', \"'\", 'd', 'think', 'about', 'it', '.', 'when', 'she', 'grille', '##d', 'me', 'for', 'a', 'prolonged', 'period', 'of', 'time', 'i', 'just', 'ended', 'up', 'telling', 'her', 'that', 'not', 'being', 'able', 'to', 'use', 'psychedelic', '##s', 'this', 'summer', 'would', 'probably', 'bum', 'me', 'out', '.', 'in', 'reality', ',', 'this', 'is', 'true', 'but', 'i', 'also', 'just', 'don', \"'\", 't', 'think', 'i', 'need', 'anti', '##de', '##press', '##ants', 'and', 'that', 'is', 'what', 'i', 'should', 'have', 'said', '.', 'i', 'tend', 'to', 'be', 'stupid', 'under', 'pressure', '.', '-', 'i', 'am', 'not', 'good', 'at', 'expressing', 'my', 'feelings', '.', 'i', 'had', 'a', 'very', 'difficult', 'time', 'explaining', 'the', 'inconsistent', 'and', 'confusing', 'effects', 'of', 'concert', '##a', 'to', 'her', ',', 'and', 'for', 'this', 'reason', 'and', 'other', 'reasons', 'i', 'feel', 'like', 'she', 'doesn', \"'\", 't', 'really', 'take', 'me', 'seriously', '.', 'two', 'other', 'things', ';', '-', 'she', 'knows', 'that', 'i', 'have', 'had', 'add', '##eral', '##l', '.', 'when', 'i', 'talked', 'to', 'her', 'about', 'going', 'on', 'it', 'before', 'she', 'seemed', 'like', 'she', 'would', 'consider', 'it', 'but', 'would', 'prefer', 'not', 'to', ',', 'as', 'she', 'thinks', 'it', 'would', 'be', 'bad', 'for', 'my', 'anxiety', '.', 'since', 'she', 'seemed', 'uncomfortable', 'with', 'giving', 'me', 'add', '##eral', '##l', ',', 'that', 'was', 'when', 'i', 'asked', 'for', 'short', 'release', 'rita', '##lin', 'instead', '.', '-', 'i', \"'\", 'm', 'aware', 'that', 'v', '##y', '##van', '##se', 'may', 'have', 'the', 'same', 'problem', 'as', 'rita', '##lin', 'for', 'me', 'and', 'stop', 'working', '/', 'present', 'emotional', 'problems', 'after', 'a', 'while', 'but', 'i', 'think', 'it', \"'\", 's', 'time', 'to', 'try', 'it', 'anyway', '.', 'i', 'hate', 'rita', '##lin', ',', 'guys', '.', 'it', 'makes', 'me', 'into', 'a', 'worse', 'person', 'than', 'i', 'really', 'am', '.', 'but', 'i', \"'\", 'm', 'terrified', 'of', 'coming', 'off', 'as', 'a', 'speed', 'seeker', 'and', 'being', 'put', 'on', 'some', 'kind', 'of', 'drug', 'watch', '##list', 'if', 'i', 'ask', 'her', 'for', 'v', '##y', '##van', '##se', ',', 'or', 'even', 'if', 'i', 'just', 'ask', 'her', 'for', 'a', 'med', 'change', '.', 'am', 'i', 'as', 'helpless', 'as', 'i', 'feel', '?', 'can', 'i', 'do', 'anything', '?', 'please', 'help', 'me', '.', 't', '##ld', '##r', ':', 'i', 'am', 'worried', 'i', 'will', 'be', 'put', 'on', 'a', 'drug', 'watch', '##list', '/', 'dropped', 'by', 'my', 'doctor', '/', 'switched', 'to', 'something', 'less', 'effective', 'if', 'i', 'ask', 'to', 'be', 'changed', 'to', 'v', '##y', '##van', '##se', ',', 'which', 'works', ',', 'from', 'rita', '##lin', ',', 'which', 'has', 'started', 'giving', 'me', 'emotional', 'problems', 'and', 'barely', 'works', '.', 'i', 'am', 'not', 'experienced', 'in', 'these', 'things', '.', 'what', 'can', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['high', 'blood', 'pressure', 'and', 'concert', '##a', '/', 'generic', '-', '-', 'alternate', 'medications', '?', 'i', \"'\", 've', 'been', 'diagnosed', 'ad', '##hd', 'since', 'i', 'was', 'in', 'elementary', 'school', '.', '(', 'i', \"'\", 'm', 'in', 'college', 'now', '.', ')', 'i', \"'\", 'm', 'currently', 'prescribed', '36', '##mg', 'of', 'concert', '##a', 'daily', '.', 'recently', ',', 'i', \"'\", 've', 'had', 'to', 'have', 'a', 'physical', 'with', 'health', 'services', 'to', 'get', 'medical', '##ly', 'cleared', 'for', 'athletics', '.', 'but', ',', 'since', 'i', \"'\", 've', 'been', 'in', 'college', 'and', 'on', 'the', 'concert', '##a', ',', 'my', 'blood', 'pressure', 'has', 'been', 'on', 'the', 'high', 'end', 'of', 'normal', '.', 'i', \"'\", 've', 'never', 'had', 'blood', 'pressure', 'problems', 'before', 'and', 'when', 'i', 'checked', 'it', 'at', 'home', 'over', 'break', ',', 'i', 'was', 'at', 'low', 'normal', '.', 'are', 'there', 'any', 'other', 'medications', 'out', 'there', 'that', 'might', 'help', '?', 'and', 'how', 'would', 'these', 'vary', 'from', 'the', 'concert', '##a', '/', 'rita', '##lin', 'routine', '?']\n",
      "INFO:__main__:Number of tokens: 146\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['high', 'blood', 'pressure', 'and', 'concert', '##a', '/', 'generic', '-', '-', 'alternate', 'medications', '?', 'i', \"'\", 've', 'been', 'diagnosed', 'ad', '##hd', 'since', 'i', 'was', 'in', 'elementary', 'school', '.', '(', 'i', \"'\", 'm', 'in', 'college', 'now', '.', ')', 'i', \"'\", 'm', 'currently', 'prescribed', '36', '##mg', 'of', 'concert', '##a', 'daily', '.', 'recently', ',', 'i', \"'\", 've', 'had', 'to', 'have', 'a', 'physical', 'with', 'health', 'services', 'to', 'get', 'medical', '##ly', 'cleared', 'for', 'athletics', '.', 'but', ',', 'since', 'i', \"'\", 've', 'been', 'in', 'college', 'and', 'on', 'the', 'concert', '##a', ',', 'my', 'blood', 'pressure', 'has', 'been', 'on', 'the', 'high', 'end', 'of', 'normal', '.', 'i', \"'\", 've', 'never', 'had', 'blood', 'pressure', 'problems', 'before', 'and', 'when', 'i', 'checked', 'it', 'at', 'home', 'over', 'break', ',', 'i', 'was', 'at', 'low', 'normal', '.', 'are', 'there', 'any', 'other', 'medications', 'out', 'there', 'that', 'might', 'help', '?', 'and', 'how', 'would', 'these', 'vary', 'from', 'the', 'concert', '##a', '/', 'rita', '##lin', 'routine', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'common', 'is', 'joint', 'and', 'muscle', 'pain', 'for', 'you', 'coming', 'down', 'off', 'your', 'med', '##s', '?', '*', '*', '*', '50', 'mg', 'v', '##y', '##van', '##se', '*', '*', 'once', 'daily', 'for', '*', '*', '1', 'week', '*', '*', 'now', '*', 'lower', '-', 'back', 'joint', 'pain', '(', '6', ')', ',', 'and', ';', '*', 'neck', 'muscle', 'pain', '(', '6', ')', 'coming', 'down', 'in', 'the', 'pm', '*', 'also', 'de', '##hy', '##dra', '##tion', 'and', 'loss', 'of', 'appetite', 'leading', 'to', 'dr', '##ows', '##iness', ',', 'headache', '##s', ',', 'anxiety', ',', 'teeth', 'grinding', '.', '.', '.', 'but', 'who', \"'\", 's', 'keeping', 'track', '*', 'joint', 'and', 'muscle', 'pain', '(', 'outside', 'of', 'upper', 'abdominal', 'pain', ')', 'is', 'not', 'listed', 'in', 'the', 'common', 'or', 'rare', 'side', '-', 'effects', '.', 'anyone', 'else', 'here', 'beg', 'to', 'differ', '?', 'edit', '-', 'mild', 'ep', '##ip', '##han', '##y', 'about', 'the', 'de', '##hy', '##dra', '##tion', '-', 'muscle', 'ache', 'connection']\n",
      "INFO:__main__:Number of tokens: 141\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'common', 'is', 'joint', 'and', 'muscle', 'pain', 'for', 'you', 'coming', 'down', 'off', 'your', 'med', '##s', '?', '*', '*', '*', '50', 'mg', 'v', '##y', '##van', '##se', '*', '*', 'once', 'daily', 'for', '*', '*', '1', 'week', '*', '*', 'now', '*', 'lower', '-', 'back', 'joint', 'pain', '(', '6', ')', ',', 'and', ';', '*', 'neck', 'muscle', 'pain', '(', '6', ')', 'coming', 'down', 'in', 'the', 'pm', '*', 'also', 'de', '##hy', '##dra', '##tion', 'and', 'loss', 'of', 'appetite', 'leading', 'to', 'dr', '##ows', '##iness', ',', 'headache', '##s', ',', 'anxiety', ',', 'teeth', 'grinding', '.', '.', '.', 'but', 'who', \"'\", 's', 'keeping', 'track', '*', 'joint', 'and', 'muscle', 'pain', '(', 'outside', 'of', 'upper', 'abdominal', 'pain', ')', 'is', 'not', 'listed', 'in', 'the', 'common', 'or', 'rare', 'side', '-', 'effects', '.', 'anyone', 'else', 'here', 'beg', 'to', 'differ', '?', 'edit', '-', 'mild', 'ep', '##ip', '##han', '##y', 'about', 'the', 'de', '##hy', '##dra', '##tion', '-', 'muscle', 'ache', 'connection']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'in', 'the', 'states', 'know', 'if', 'help', 'for', 'adult', 'ad', '##hd', 'is', 'covered', 'through', 'med', '##ica', '##id', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'in', 'the', 'states', 'know', 'if', 'help', 'for', 'adult', 'ad', '##hd', 'is', 'covered', 'through', 'med', '##ica', '##id', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hey', ',', 'i', 'don', \"'\", 't', 'understand', 'my', 'ad', '##hd', 'and', 'it', \"'\", 's', 'most', 'scary', '.']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hey', ',', 'i', 'don', \"'\", 't', 'understand', 'my', 'ad', '##hd', 'and', 'it', \"'\", 's', 'most', 'scary', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['long', '-', 'term', 'use', 'of', 'add', '##eral', '##l', '.', '(', 'possible', 'side', 'effect', ')', 'a', 'friend', 'of', 'mine', 'has', 'knows', 'i', 'was', 'diagnosed', 'with', 'add', 'when', 'i', 'was', 'young', 'and', 'have', 'been', 'taking', '30', '##mg', 'of', 'add', '##eral', '##l', 'the', 'entire', 'time', '.', 'recently', 'he', \"'\", 's', 'been', 'telling', 'me', 'to', 'stop', 'taking', 'the', 'medication', ',', 'because', 'i', \"'\", 'm', 'addicted', '/', 'dependent', 'on', 'it', 'to', 'function', 'in', 'school', '.', 'i', \"'\", 'm', 'currently', 'in', 'my', 'second', 'year', 'of', 'col', '##lage', ',', 'i', 'never', 'take', 'more', 'than', 'my', 'prescribed', 'amount', ',', 'never', 'abuse', 'add', '##eral', '##l', ',', 'and', 'don', \"'\", 't', 'use', 'it', 'outside', 'of', 'school', '.', 'weekends', ',', 'holidays', ',', 'and', 'summer', 'i', \"'\", 'm', 'completely', 'off', 'my', 'medication', '.', 'i', \"'\", 've', 'never', 'thought', 'of', 'myself', 'as', 'dependent', 'on', 'add', '##eral', '##l', ',', 'but', 'my', 'friend', 'is', 'convinced', 'that', 'i', \"'\", 've', 'taken', 'add', '##eral', '##l', 'for', 'too', 'long', '(', '30', '##mg', '5', 'days', 'a', 'week', 'during', ',', 'only', 'during', 'school', ',', 'for', 'the', 'past', '9', 'years', ')', 'and', 'that', 'my', 'mental', 'health', 'is', 'at', 'risk', '.', 'is', 'there', 'any', 'merit', 'to', 'his', 'claims', '.']\n",
      "INFO:__main__:Number of tokens: 187\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['long', '-', 'term', 'use', 'of', 'add', '##eral', '##l', '.', '(', 'possible', 'side', 'effect', ')', 'a', 'friend', 'of', 'mine', 'has', 'knows', 'i', 'was', 'diagnosed', 'with', 'add', 'when', 'i', 'was', 'young', 'and', 'have', 'been', 'taking', '30', '##mg', 'of', 'add', '##eral', '##l', 'the', 'entire', 'time', '.', 'recently', 'he', \"'\", 's', 'been', 'telling', 'me', 'to', 'stop', 'taking', 'the', 'medication', ',', 'because', 'i', \"'\", 'm', 'addicted', '/', 'dependent', 'on', 'it', 'to', 'function', 'in', 'school', '.', 'i', \"'\", 'm', 'currently', 'in', 'my', 'second', 'year', 'of', 'col', '##lage', ',', 'i', 'never', 'take', 'more', 'than', 'my', 'prescribed', 'amount', ',', 'never', 'abuse', 'add', '##eral', '##l', ',', 'and', 'don', \"'\", 't', 'use', 'it', 'outside', 'of', 'school', '.', 'weekends', ',', 'holidays', ',', 'and', 'summer', 'i', \"'\", 'm', 'completely', 'off', 'my', 'medication', '.', 'i', \"'\", 've', 'never', 'thought', 'of', 'myself', 'as', 'dependent', 'on', 'add', '##eral', '##l', ',', 'but', 'my', 'friend', 'is', 'convinced', 'that', 'i', \"'\", 've', 'taken', 'add', '##eral', '##l', 'for', 'too', 'long', '(', '30', '##mg', '5', 'days', 'a', 'week', 'during', ',', 'only', 'during', 'school', ',', 'for', 'the', 'past', '9', 'years', ')', 'and', 'that', 'my', 'mental', 'health', 'is', 'at', 'risk', '.', 'is', 'there', 'any', 'merit', 'to', 'his', 'claims', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'i', 'find', 'the', 'right', 'medication', '?', 'hi', 'everyone', ',', 'so', 'right', 'now', ',', 'i', \"'\", 'm', 'taking', '54', 'mg', 'of', 'concert', '##a', '(', 'started', 'on', '18', ',', 'then', '36', ')', '.', 'i', \"'\", 've', 'been', 'taking', 'it', 'for', 'about', 'three', 'weeks', ',', 'and', 'i', 'have', 'an', 'appointment', 'this', 'wednesday', '.', 'i', 'feel', 'a', 'little', 'bit', 'of', 'a', 'change', ',', 'but', 'honestly', 'not', 'enough', 'that', 'it', \"'\", 's', 'really', 'helping', 'my', 'situation', '(', 'studying', ')', '.', 'he', 'told', 'me', 'to', 'tell', 'him', 'if', 'i', 'don', \"'\", 't', 'feel', 'like', 'it', \"'\", 's', 'doing', 'it', 'for', 'me', ',', 'and', 'we', \"'\", 'll', 'try', 'another', 'medication', '.', 'so', 'i', \"'\", 'd', 'like', 'to', 'do', 'that', '.', 'i', \"'\", 'm', 'hoping', 'that', 'a', 'different', 'medication', 'will', 'help', 'me', 'more', 'than', 'this', 'one', '.', 'but', 'what', 'if', 'it', 'doesn', \"'\", 't', '?', 'i', 'mean', ',', 'i', \"'\", 'd', 'rather', 'have', 'concert', '##a', 'than', 'nothing', ',', 'it', \"'\", 's', 'helping', 'me', 'a', '*', 'little', 'bit', '*', '.', 'is', 'that', 'little', 'bit', 'of', 'assistance', 'as', 'much', 'as', 'i', 'should', 'be', 'expecting', 'from', 'medication', '?', 'should', 'i', 'ask', 'to', 'change', 'medications', 'but', 'risk', 'being', 'on', 'something', 'worse', '?', 'i', 'guess', 'my', 'biggest', 'fear', 'is', 'changing', 'to', 'something', 'i', 'hate', ',', 'and', 'then', 'not', 'being', 'able', 'to', 'get', 'the', 'concert', '##a', 'back', ',', 'and', 'being', 'stuck', 'with', 'nothing', '.', 'thanks', 'guys', ',', 'i', \"'\", 'm', 'very', 'new', 'to', 'this', '.']\n",
      "INFO:__main__:Number of tokens: 233\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'i', 'find', 'the', 'right', 'medication', '?', 'hi', 'everyone', ',', 'so', 'right', 'now', ',', 'i', \"'\", 'm', 'taking', '54', 'mg', 'of', 'concert', '##a', '(', 'started', 'on', '18', ',', 'then', '36', ')', '.', 'i', \"'\", 've', 'been', 'taking', 'it', 'for', 'about', 'three', 'weeks', ',', 'and', 'i', 'have', 'an', 'appointment', 'this', 'wednesday', '.', 'i', 'feel', 'a', 'little', 'bit', 'of', 'a', 'change', ',', 'but', 'honestly', 'not', 'enough', 'that', 'it', \"'\", 's', 'really', 'helping', 'my', 'situation', '(', 'studying', ')', '.', 'he', 'told', 'me', 'to', 'tell', 'him', 'if', 'i', 'don', \"'\", 't', 'feel', 'like', 'it', \"'\", 's', 'doing', 'it', 'for', 'me', ',', 'and', 'we', \"'\", 'll', 'try', 'another', 'medication', '.', 'so', 'i', \"'\", 'd', 'like', 'to', 'do', 'that', '.', 'i', \"'\", 'm', 'hoping', 'that', 'a', 'different', 'medication', 'will', 'help', 'me', 'more', 'than', 'this', 'one', '.', 'but', 'what', 'if', 'it', 'doesn', \"'\", 't', '?', 'i', 'mean', ',', 'i', \"'\", 'd', 'rather', 'have', 'concert', '##a', 'than', 'nothing', ',', 'it', \"'\", 's', 'helping', 'me', 'a', '*', 'little', 'bit', '*', '.', 'is', 'that', 'little', 'bit', 'of', 'assistance', 'as', 'much', 'as', 'i', 'should', 'be', 'expecting', 'from', 'medication', '?', 'should', 'i', 'ask', 'to', 'change', 'medications', 'but', 'risk', 'being', 'on', 'something', 'worse', '?', 'i', 'guess', 'my', 'biggest', 'fear', 'is', 'changing', 'to', 'something', 'i', 'hate', ',', 'and', 'then', 'not', 'being', 'able', 'to', 'get', 'the', 'concert', '##a', 'back', ',', 'and', 'being', 'stuck', 'with', 'nothing', '.', 'thanks', 'guys', ',', 'i', \"'\", 'm', 'very', 'new', 'to', 'this', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['om', '##itt', '##ing', 'of', 'prior', 'grades']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['om', '##itt', '##ing', 'of', 'prior', 'grades']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'tips', 'for', 'studying', 'with', 'no', 'med', '##s', '?', 'the', 'ab', '##ridge', '##d', 'version', 'of', 'this', 'is', 'i', 'can', \"'\", 't', 'focus', 'to', 'save', 'my', 'life', 'and', 'have', 'no', 'access', 'to', 'any', 'sort', 'of', 'medication', 'and', 'am', 'trying', 'to', 'learn', 'the', 'tri', '##g', 'identities', 'with', 'absolutely', 'no', 'luck', '.']\n",
      "INFO:__main__:Number of tokens: 49\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'tips', 'for', 'studying', 'with', 'no', 'med', '##s', '?', 'the', 'ab', '##ridge', '##d', 'version', 'of', 'this', 'is', 'i', 'can', \"'\", 't', 'focus', 'to', 'save', 'my', 'life', 'and', 'have', 'no', 'access', 'to', 'any', 'sort', 'of', 'medication', 'and', 'am', 'trying', 'to', 'learn', 'the', 'tri', '##g', 'identities', 'with', 'absolutely', 'no', 'luck', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['re', '##thi', '##nk', 'add', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['re', '##thi', '##nk', 'add', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['looking', 'for', 'the', 'title', 'of', 'a', 'certain', 'ad', '##hd', 'book', ',', 'flipped', 'through', 'it', 'and', 'now', 'i', 'can', \"'\", 't', 'recall', '.', '.', '.', 'details', 'inside', '.']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['looking', 'for', 'the', 'title', 'of', 'a', 'certain', 'ad', '##hd', 'book', ',', 'flipped', 'through', 'it', 'and', 'now', 'i', 'can', \"'\", 't', 'recall', '.', '.', '.', 'details', 'inside', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['almost', '32', '.', 'was', 'diagnosed', 'add', 'when', 'i', 'was', '12', 'or', 'so', '.', 'a', 'couple', 'questions', '.']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['almost', '32', '.', 'was', 'diagnosed', 'add', 'when', 'i', 'was', '12', 'or', 'so', '.', 'a', 'couple', 'questions', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'ad', '##hd', 'child', 'now', 'on', 'night', 'time', 'med', '##s', '.', 'anyone', 'else', 'have', 'to', 'use', 'day', 'and', 'night', 'time', 'med', '##s', 'to', 'manage', 'with', 'your', 'children', '?']\n",
      "INFO:__main__:Number of tokens: 28\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'ad', '##hd', 'child', 'now', 'on', 'night', 'time', 'med', '##s', '.', 'anyone', 'else', 'have', 'to', 'use', 'day', 'and', 'night', 'time', 'med', '##s', 'to', 'manage', 'with', 'your', 'children', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['oh', 'yeah', 'this', 'is', 'hot', ',', 'oh', '.', '.', 'wait', ',', 'i', 'wonder', 'if', 'an', 'abstract', 'class', 'can', 'contain', 'a', 'method', '.', '.', '.', '?', 'so', 'there', 'i', 'am', ',', 'making', 'out', 'with', 'this', 'cute', 'girl', 'in', 'my', 'car', 'before', 'she', 'heads', 'in', 'and', 'part', 'way', 'into', 'it', ',', 'my', 'brain', 'shifts', 'to', 'coding', 'in', 'java', ',', 'which', 'in', 'my', 'head', 'i', 'am', 'thinking', ',', '\"', 'w', '##tf', '?', 'really', '?', 'right', 'now', 'you', 'need', 'to', 'think', 'of', '*', 'this', '*', '?', '\"', 'shit', 'like', 'this', 'happens', 'all', 'the', 'time', ',', 'it', \"'\", 's', 'soo', '##o', 'annoying', '.', 'anyone', 'else', 'have', 'this', 'wonderful', 'joy', '?']\n",
      "INFO:__main__:Number of tokens: 105\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['oh', 'yeah', 'this', 'is', 'hot', ',', 'oh', '.', '.', 'wait', ',', 'i', 'wonder', 'if', 'an', 'abstract', 'class', 'can', 'contain', 'a', 'method', '.', '.', '.', '?', 'so', 'there', 'i', 'am', ',', 'making', 'out', 'with', 'this', 'cute', 'girl', 'in', 'my', 'car', 'before', 'she', 'heads', 'in', 'and', 'part', 'way', 'into', 'it', ',', 'my', 'brain', 'shifts', 'to', 'coding', 'in', 'java', ',', 'which', 'in', 'my', 'head', 'i', 'am', 'thinking', ',', '\"', 'w', '##tf', '?', 'really', '?', 'right', 'now', 'you', 'need', 'to', 'think', 'of', '*', 'this', '*', '?', '\"', 'shit', 'like', 'this', 'happens', 'all', 'the', 'time', ',', 'it', \"'\", 's', 'soo', '##o', 'annoying', '.', 'anyone', 'else', 'have', 'this', 'wonderful', 'joy', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hi', 'so', 'i', 'just', 'found', 'this', 'red', '##dit', ',', 'watched', 'some', 'of', 'the', 'videos', 'by', 'dr', '.', 'russell', 'bark', '##ley', ',', 'and', 'i', 'have', 'some', 'questions', '.', 'hey', 'so', 'i', \"'\", 've', 'struggled', 'most', 'of', 'my', 'life', 'with', 'this', 'mysterious', 'inability', 'to', 'get', 'myself', 'to', 'do', 'the', '\"', 'right', '\"', 'things', '.', 'heck', ',', 'i', \"'\", 'm', 'not', 'even', 'doing', 'the', '\"', 'right', '\"', 'thing', 'right', 'now', '(', 'english', 'paper', 'to', 'write', ')', ',', 'but', 'if', 'this', 'turns', 'up', 'anything', 'for', 'me', ',', 'i', 'figure', 'it', 'could', 'help', 'me', 'out', 'in', 'the', 'long', 'run', '.', 'in', 'any', 'case', ',', 'i', 'and', 'my', 'brother', 'were', 'diagnosed', 'with', 'ad', '##hd', 'last', 'year', '.', 'we', 'both', 'went', 'on', 'med', '##s', ',', 'but', 'i', 'suffered', 'from', 'serious', 'depression', ',', 'and', 'as', 'a', 'result', ',', 'haven', \"'\", 't', 'been', 'interested', 'in', 'trying', 'something', 'different', '.', 'plus', 'i', 'don', \"'\", 't', 'even', 'have', 'the', 'motivation', 'to', 'go', 'do', 'it', ',', 'which', 'brings', 'me', 'to', 'my', 'concern', '.', 'i', 'watched', 'the', 'videos', 'by', 'dr', '.', 'bark', '##ley', 'that', 'were', 'posted', 'on', 'a', 'thread', 'here', ',', 'and', 'as', 'much', 'truth', 'as', 'he', 'speaks', ',', 'it', 'de', '##press', '##es', 'me', '.', 'i', 'believed', 'that', 'there', 'was', 'something', 'i', 'could', 'do', 'about', 'my', 'ad', '##hd', ',', 'my', 'lack', 'of', 'motivation', ',', 'and', 'my', 'nearly', 'complete', 'inability', 'to', 'manage', 'my', 'time', 'wise', '##ly', ',', 'but', 'what', 'he', 'tells', 'me', 'says', 'otherwise', '.', 'especially', 'this', 'video', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##3', '##vu', '##v', '##5', '##j', '##va', '##z', '##s', 'i', 'guess', 'i', \"'\", 'm', 'just', 'looking', 'for', 'other', \"'\", 's', 'response', 'to', 'his', 'videos', 'and', '/', 'or', 'some', 'advice', '/', 'help', 'on', 'what', 'to', 'do', 'next', '.', 'i', 'want', 'to', 'be', 'motivated', '(', 'i', 'want', 'to', 'be', 'a', 'talented', 'and', 'great', 'designer', 'one', 'day', ')', ',', 'but', 'now', 'i', 'fear', 'that', 'my', 'ad', '##hd', 'will', 'keep', 'me', 'from', 'doing', 'so', '.', 'actually', ',', 'i', \"'\", 've', 'always', 'feared', '/', 'known', 'that', ',', 'but', 'now', 'it', \"'\", 's', 'been', 'articulated', 'for', 'me', '.', 'so', '.', '.', 'advice', '?', 'help', '?', 'stories', 'of', 'awesome', '##ness', '?', 'links', 'to', 'a', 'thread', 'where', 'this', 'is', 'already', 'been', 'done', '?', 'thanks', '.', ':', ')']\n",
      "INFO:__main__:Number of tokens: 368\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hi', 'so', 'i', 'just', 'found', 'this', 'red', '##dit', ',', 'watched', 'some', 'of', 'the', 'videos', 'by', 'dr', '.', 'russell', 'bark', '##ley', ',', 'and', 'i', 'have', 'some', 'questions', '.', 'hey', 'so', 'i', \"'\", 've', 'struggled', 'most', 'of', 'my', 'life', 'with', 'this', 'mysterious', 'inability', 'to', 'get', 'myself', 'to', 'do', 'the', '\"', 'right', '\"', 'things', '.', 'heck', ',', 'i', \"'\", 'm', 'not', 'even', 'doing', 'the', '\"', 'right', '\"', 'thing', 'right', 'now', '(', 'english', 'paper', 'to', 'write', ')', ',', 'but', 'if', 'this', 'turns', 'up', 'anything', 'for', 'me', ',', 'i', 'figure', 'it', 'could', 'help', 'me', 'out', 'in', 'the', 'long', 'run', '.', 'in', 'any', 'case', ',', 'i', 'and', 'my', 'brother', 'were', 'diagnosed', 'with', 'ad', '##hd', 'last', 'year', '.', 'we', 'both', 'went', 'on', 'med', '##s', ',', 'but', 'i', 'suffered', 'from', 'serious', 'depression', ',', 'and', 'as', 'a', 'result', ',', 'haven', \"'\", 't', 'been', 'interested', 'in', 'trying', 'something', 'different', '.', 'plus', 'i', 'don', \"'\", 't', 'even', 'have', 'the', 'motivation', 'to', 'go', 'do', 'it', ',', 'which', 'brings', 'me', 'to', 'my', 'concern', '.', 'i', 'watched', 'the', 'videos', 'by', 'dr', '.', 'bark', '##ley', 'that', 'were', 'posted', 'on', 'a', 'thread', 'here', ',', 'and', 'as', 'much', 'truth', 'as', 'he', 'speaks', ',', 'it', 'de', '##press', '##es', 'me', '.', 'i', 'believed', 'that', 'there', 'was', 'something', 'i', 'could', 'do', 'about', 'my', 'ad', '##hd', ',', 'my', 'lack', 'of', 'motivation', ',', 'and', 'my', 'nearly', 'complete', 'inability', 'to', 'manage', 'my', 'time', 'wise', '##ly', ',', 'but', 'what', 'he', 'tells', 'me', 'says', 'otherwise', '.', 'especially', 'this', 'video', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'q', '##3', '##vu', '##v', '##5', '##j', '##va', '##z', '##s', 'i', 'guess', 'i', \"'\", 'm', 'just', 'looking', 'for', 'other', \"'\", 's', 'response', 'to', 'his', 'videos', 'and', '/', 'or', 'some', 'advice', '/', 'help', 'on', 'what', 'to', 'do', 'next', '.', 'i', 'want', 'to', 'be', 'motivated', '(', 'i', 'want', 'to', 'be', 'a', 'talented', 'and', 'great', 'designer', 'one', 'day', ')', ',', 'but', 'now', 'i', 'fear', 'that', 'my', 'ad', '##hd', 'will', 'keep', 'me', 'from', 'doing', 'so', '.', 'actually', ',', 'i', \"'\", 've', 'always', 'feared', '/', 'known', 'that', ',', 'but', 'now', 'it', \"'\", 's', 'been', 'articulated', 'for', 'me', '.', 'so', '.', '.', 'advice', '?', 'help', '?', 'stories', 'of', 'awesome', '##ness', '?', 'links', 'to', 'a', 'thread', 'where', 'this', 'is', 'already', 'been', 'done', '?', 'thanks', '.', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'hate', 'football', 'and', 'sports', '?', 'ok', ',', 'not', 'trying', 'to', 'insult', 'anyone', 'who', 'does', 'like', 'sports', 'or', 'anything', '.', '.', 'but', ',', 'being', 'attention', 'deficit', 'i', 'absolutely', 'hate', 'football', 'and', 'wonder', 'if', 'maybe', 'others', 'who', 'have', 'short', 'attention', 'spans', 'dislike', 'watching', 'other', 'people', 'play', 'sports', 'as', 'well', '?']\n",
      "INFO:__main__:Number of tokens: 50\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'hate', 'football', 'and', 'sports', '?', 'ok', ',', 'not', 'trying', 'to', 'insult', 'anyone', 'who', 'does', 'like', 'sports', 'or', 'anything', '.', '.', 'but', ',', 'being', 'attention', 'deficit', 'i', 'absolutely', 'hate', 'football', 'and', 'wonder', 'if', 'maybe', 'others', 'who', 'have', 'short', 'attention', 'spans', 'dislike', 'watching', 'other', 'people', 'play', 'sports', 'as', 'well', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tips', 'for', 'side', 'effect', '(', 'chewing', ')', '?', 'just', 'found', 'this', 'sub', '##red', '##dit', 'and', 'hoping', 'to', 'get', 'advice', 'from', 'anyone', 'who', 'might', 'know', 'better', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'in', 'september', 'after', 'years', 'of', 'mis', '##dia', '##gno', '##sis', '(', 'everything', 'from', 'bipolar', 'to', 'sc', '##hi', '##zo', '##af', '##fect', '##ive', ')', '.', 'unfortunately', ',', 'in', 'early', 'november', ',', 'i', 'got', 'a', 'serious', 'case', 'of', 'the', 'flu', 'that', 'lasted', 'until', 'january', ',', 'so', 'i', 'blew', 'my', 'medical', 'budget', 'and', 'had', 'to', 'stop', 'the', 'medication', '(', 'though', 'it', 'was', 'tremendous', '##ly', 'helpful', ')', '.', 'after', 'i', 'got', 'better', ',', 'i', 'felt', 'so', 'much', 'improved', 'compared', 'to', 'being', 'laid', 'up', 'in', 'bed', 'pu', '##king', ',', 'that', 'i', 'didn', \"'\", 't', 'think', 'to', 'go', 'back', 'and', 'get', 'my', 'medication', 'ref', '##ille', '##d', '.', 'now', 'that', 'finals', 'are', 'looming', 'and', 'i', \"'\", 've', 'got', 'nothing', 'done', 'and', 'can', \"'\", 't', 'even', 'read', 'a', 'sentence', 'without', 'losing', 'focus', ',', 'i', \"'\", 'm', 'freaking', 'out', 'and', 'have', 'to', 'start', 'the', 'med', '##s', 'up', 'again', 'if', 'i', 'want', 'any', 'chance', 'of', 'passing', '.', 'my', 'only', 'apprehension', 'is', 'a', 'side', 'effect', 'that', 'drives', 'me', 'crazy', ':', 'i', 'chew', 'the', 'hell', 'out', 'of', 'my', 'lips', ',', 'cheeks', ',', 'and', 'tongue', 'when', 'i', 'take', 'the', 'medication', '(', 'concert', '##a', '54', '##mg', ')', '.', 'my', 'whole', 'mouth', 'gets', 'visibly', 'swollen', 'and', 'is', 'really', 'uncomfortable', '.', 'i', 'try', 'chewing', 'gum', ',', 'but', 'if', 'i', 'don', \"'\", 't', 'have', 'any', 'on', 'me', ',', 'i', \"'\", 'll', 'still', 'just', 'do', 'it', 'without', 'thinking', 'about', 'it', '.', 'does', 'anyone', 'else', 'deal', 'with', 'similar', 'side', 'effects', '?', 'is', 'there', 'any', 'good', 'way', 'to', 'avoid', 'it', ',', 'or', 'at', 'least', 'a', 'good', 'method', 'of', 'mit', '##iga', '##ting', 'the', 'effect', '?']\n",
      "INFO:__main__:Number of tokens: 284\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tips', 'for', 'side', 'effect', '(', 'chewing', ')', '?', 'just', 'found', 'this', 'sub', '##red', '##dit', 'and', 'hoping', 'to', 'get', 'advice', 'from', 'anyone', 'who', 'might', 'know', 'better', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'in', 'september', 'after', 'years', 'of', 'mis', '##dia', '##gno', '##sis', '(', 'everything', 'from', 'bipolar', 'to', 'sc', '##hi', '##zo', '##af', '##fect', '##ive', ')', '.', 'unfortunately', ',', 'in', 'early', 'november', ',', 'i', 'got', 'a', 'serious', 'case', 'of', 'the', 'flu', 'that', 'lasted', 'until', 'january', ',', 'so', 'i', 'blew', 'my', 'medical', 'budget', 'and', 'had', 'to', 'stop', 'the', 'medication', '(', 'though', 'it', 'was', 'tremendous', '##ly', 'helpful', ')', '.', 'after', 'i', 'got', 'better', ',', 'i', 'felt', 'so', 'much', 'improved', 'compared', 'to', 'being', 'laid', 'up', 'in', 'bed', 'pu', '##king', ',', 'that', 'i', 'didn', \"'\", 't', 'think', 'to', 'go', 'back', 'and', 'get', 'my', 'medication', 'ref', '##ille', '##d', '.', 'now', 'that', 'finals', 'are', 'looming', 'and', 'i', \"'\", 've', 'got', 'nothing', 'done', 'and', 'can', \"'\", 't', 'even', 'read', 'a', 'sentence', 'without', 'losing', 'focus', ',', 'i', \"'\", 'm', 'freaking', 'out', 'and', 'have', 'to', 'start', 'the', 'med', '##s', 'up', 'again', 'if', 'i', 'want', 'any', 'chance', 'of', 'passing', '.', 'my', 'only', 'apprehension', 'is', 'a', 'side', 'effect', 'that', 'drives', 'me', 'crazy', ':', 'i', 'chew', 'the', 'hell', 'out', 'of', 'my', 'lips', ',', 'cheeks', ',', 'and', 'tongue', 'when', 'i', 'take', 'the', 'medication', '(', 'concert', '##a', '54', '##mg', ')', '.', 'my', 'whole', 'mouth', 'gets', 'visibly', 'swollen', 'and', 'is', 'really', 'uncomfortable', '.', 'i', 'try', 'chewing', 'gum', ',', 'but', 'if', 'i', 'don', \"'\", 't', 'have', 'any', 'on', 'me', ',', 'i', \"'\", 'll', 'still', 'just', 'do', 'it', 'without', 'thinking', 'about', 'it', '.', 'does', 'anyone', 'else', 'deal', 'with', 'similar', 'side', 'effects', '?', 'is', 'there', 'any', 'good', 'way', 'to', 'avoid', 'it', ',', 'or', 'at', 'least', 'a', 'good', 'method', 'of', 'mit', '##iga', '##ting', 'the', 'effect', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['diagnosis', 'in', 'adulthood']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['diagnosis', 'in', 'adulthood']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['help', 'with', 'meditation', '?', 'what', 'have', 'you', 'found', 'is', 'helpful', 'in', 'med', '##itating', '?', 'because', 'i', 'can', 'not', 'shut', 'my', 'mind', 'up', '.', 'not', 'only', 'that', ',', 'i', \"'\", 'll', 'forget', 'that', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'med', '##itating', ',', 'and', 'catch', 'myself', '5', 'minutes', 'later', 'as', 'i', \"'\", 'm', 'running', 'hypothetical', 'situations', 'through', 'my', 'head', '.', 'so', 'what', 'works', 'for', 'you', '?', 'music', '?', 'bin', '##aur', '##al', 'beats', '?', 'cannabis', '?', 'add', '##eral', '?', 'staying', 'hydra', '##ted', '?', 'also', ',', 'what', 'have', 'you', 'found', 'makes', 'it', 'harder', 'for', 'you', 'to', 'med', '##itate', '?']\n",
      "INFO:__main__:Number of tokens: 95\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['help', 'with', 'meditation', '?', 'what', 'have', 'you', 'found', 'is', 'helpful', 'in', 'med', '##itating', '?', 'because', 'i', 'can', 'not', 'shut', 'my', 'mind', 'up', '.', 'not', 'only', 'that', ',', 'i', \"'\", 'll', 'forget', 'that', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'med', '##itating', ',', 'and', 'catch', 'myself', '5', 'minutes', 'later', 'as', 'i', \"'\", 'm', 'running', 'hypothetical', 'situations', 'through', 'my', 'head', '.', 'so', 'what', 'works', 'for', 'you', '?', 'music', '?', 'bin', '##aur', '##al', 'beats', '?', 'cannabis', '?', 'add', '##eral', '?', 'staying', 'hydra', '##ted', '?', 'also', ',', 'what', 'have', 'you', 'found', 'makes', 'it', 'harder', 'for', 'you', 'to', 'med', '##itate', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'other', 'v', '##y', '##vance', 'users', '?', 'hi', ',', 'my', 'name', 'is', 'taylor', 'and', 'i', 'am', '17', 'years', 'old', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'about', '3', 'years', 'ago', '.', 'i', 'started', 'taking', 'concert', '##a', ',', 'but', 'stopped', 'because', 'of', 'the', 'side', 'effects', '.', 'i', 'now', 'take', 'v', '##y', '##van', '##se', ',', '(', 'not', 'sure', 'if', 'spelled', 'with', 'an', '\"', 's', '\"', 'or', 'a', '\"', 'c', '\"', ')', 'and', 'i', 'absolutely', 'can', \"'\", 't', 'stand', 'the', 'side', 'effects', '.', 'my', 'main', 'side', 'effects', 'are', ':', 'random', 'fits', 'of', 'anger', ',', 'no', 'appetite', 'at', 'all', ',', 'mood', 'swings', ',', 'and', 'impaired', 'vision', '.', 'i', 'was', 'wondering', 'what', 'i', 'could', 'do', 'about', 'this', ',', 'and', 'what', 'side', 'effects', 'you', 'all', 'have', '?', 'is', 'there', 'a', 'better', 'medicine', 'i', 'should', 'be', 'taking', '?', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 133\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'other', 'v', '##y', '##vance', 'users', '?', 'hi', ',', 'my', 'name', 'is', 'taylor', 'and', 'i', 'am', '17', 'years', 'old', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'about', '3', 'years', 'ago', '.', 'i', 'started', 'taking', 'concert', '##a', ',', 'but', 'stopped', 'because', 'of', 'the', 'side', 'effects', '.', 'i', 'now', 'take', 'v', '##y', '##van', '##se', ',', '(', 'not', 'sure', 'if', 'spelled', 'with', 'an', '\"', 's', '\"', 'or', 'a', '\"', 'c', '\"', ')', 'and', 'i', 'absolutely', 'can', \"'\", 't', 'stand', 'the', 'side', 'effects', '.', 'my', 'main', 'side', 'effects', 'are', ':', 'random', 'fits', 'of', 'anger', ',', 'no', 'appetite', 'at', 'all', ',', 'mood', 'swings', ',', 'and', 'impaired', 'vision', '.', 'i', 'was', 'wondering', 'what', 'i', 'could', 'do', 'about', 'this', ',', 'and', 'what', 'side', 'effects', 'you', 'all', 'have', '?', 'is', 'there', 'a', 'better', 'medicine', 'i', 'should', 'be', 'taking', '?', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dex', '##ed', '##rine', 'doesn', \"'\", 't', 'work', 'very', 'well', ',', 'but', 'focal', '##in', 'makes', 'me', 'feel', 'like', 'shit']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dex', '##ed', '##rine', 'doesn', \"'\", 't', 'work', 'very', 'well', ',', 'but', 'focal', '##in', 'makes', 'me', 'feel', 'like', 'shit']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'have', 'experience', 'moving', 'from', 'st', '##im', '##ula', '##nts', 'to', 'st', '##rat', '##tera', '?', 'my', 'doctor', 'told', 'me', 'about', '[', 'st', '##rat', '##tera', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'st', '##rat', '##tera', ')', 'today', '.', 'i', 'guess', 'it', \"'\", 's', 'a', 'newer', 'drug', 'that', \"'\", 's', 'been', 'approved', 'for', 'ad', '##hd', ',', 'but', 'isn', \"'\", 't', 'a', 'st', '##im', '##ula', '##nt', 'or', 'controlled', 'substance', '.', 'has', 'anyone', 'else', 'made', 'a', 'transition', 'from', 'add', '##eral', '##l', '/', 'rita', '##lin', 'to', 'it', ',', 'and', 'if', 'so', ',', 'how', 'was', 'it', '?', 'i', \"'\", 'm', 'thinking', 'of', 'giving', 'it', 'a', 'try', '.']\n",
      "INFO:__main__:Number of tokens: 108\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'have', 'experience', 'moving', 'from', 'st', '##im', '##ula', '##nts', 'to', 'st', '##rat', '##tera', '?', 'my', 'doctor', 'told', 'me', 'about', '[', 'st', '##rat', '##tera', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'st', '##rat', '##tera', ')', 'today', '.', 'i', 'guess', 'it', \"'\", 's', 'a', 'newer', 'drug', 'that', \"'\", 's', 'been', 'approved', 'for', 'ad', '##hd', ',', 'but', 'isn', \"'\", 't', 'a', 'st', '##im', '##ula', '##nt', 'or', 'controlled', 'substance', '.', 'has', 'anyone', 'else', 'made', 'a', 'transition', 'from', 'add', '##eral', '##l', '/', 'rita', '##lin', 'to', 'it', ',', 'and', 'if', 'so', ',', 'how', 'was', 'it', '?', 'i', \"'\", 'm', 'thinking', 'of', 'giving', 'it', 'a', 'try', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['as', 'a', '17', 'year', 'old', 'with', 'ad', '##hd', ',', 'what', 'are', 'some', 'tips', 'you', 'guys', 'can', 'give', 'me', 'for', 'living', 'the', 'rest', 'of', 'my', 'life', 'with', 'this', 'gift', '/', 'curse', '.', 'pretty', 'much', 'what', 'it', 'says', 'in', 'the', 'title', ',', 'sometimes', 'i', 'get', 'depressed', 'or', 'worried', 'about', 'the', 'rest', 'of', 'my', 'life', 'because', 'my', 'grades', 'aren', \"'\", 't', 'great', '(', 'at', 'least', 'compared', 'to', 'the', 'rest', 'of', 'my', 'school', ')', '.', 'i', 'can', 'get', 'into', 'a', '4', 'year', 'college', 'but', 'the', 'school', 'environment', 'is', 'a', 'complete', 'hell', 'to', 'me', 'and', 'i', \"'\", 'm', 'afraid', 'the', 'office', 'environment', 'will', 'be', 'similar', '.', 'any', 'tips', 'you', 'can', 'think', 'of', ',', 'no', 'matter', 'how', 'big', 'or', 'small', ',', 'will', 'help']\n",
      "INFO:__main__:Number of tokens: 117\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['as', 'a', '17', 'year', 'old', 'with', 'ad', '##hd', ',', 'what', 'are', 'some', 'tips', 'you', 'guys', 'can', 'give', 'me', 'for', 'living', 'the', 'rest', 'of', 'my', 'life', 'with', 'this', 'gift', '/', 'curse', '.', 'pretty', 'much', 'what', 'it', 'says', 'in', 'the', 'title', ',', 'sometimes', 'i', 'get', 'depressed', 'or', 'worried', 'about', 'the', 'rest', 'of', 'my', 'life', 'because', 'my', 'grades', 'aren', \"'\", 't', 'great', '(', 'at', 'least', 'compared', 'to', 'the', 'rest', 'of', 'my', 'school', ')', '.', 'i', 'can', 'get', 'into', 'a', '4', 'year', 'college', 'but', 'the', 'school', 'environment', 'is', 'a', 'complete', 'hell', 'to', 'me', 'and', 'i', \"'\", 'm', 'afraid', 'the', 'office', 'environment', 'will', 'be', 'similar', '.', 'any', 'tips', 'you', 'can', 'think', 'of', ',', 'no', 'matter', 'how', 'big', 'or', 'small', ',', 'will', 'help']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'any', 'of', 'you', 'listen', 'to', 'jazz', 'while', 'studying', '?', 'john', 'colt', '##rane', 'seems', 'to', 'help', 'me', 'concentrate', '.']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'any', 'of', 'you', 'listen', 'to', 'jazz', 'while', 'studying', '?', 'john', 'colt', '##rane', 'seems', 'to', 'help', 'me', 'concentrate', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'many', 'adults', '(', 'over', '20', ')', 'with', 'ad', '##hd', 'are', 'chronic', 'marijuana', 'smoke', '##rs', '?', 'i', \"'\", 'm', '17', 'years', 'old', 'and', 'i', 'smoke', 'multiple', 'times', 'every', 'day', ',', 'and', 'it', \"'\", 's', 'possible', 'that', 'i', \"'\", 'll', 'continue', 'smoking', 'often', 'into', 'my', 'adult', 'life', '.', 'so', 'i', 'have', 'a', 'few', 'questions', 'for', 'the', 'adults', 'that', 'were', 'once', 'curious', '17', 'year', 'old', 'weed', '-', 'smoking', 'add', '##ers', ':', 'how', 'often', 'do', 'you', 'smoke', '?', 'do', 'you', 'think', 'weed', 'affects', 'your', 'life', '?', 'positively', 'or', 'negatively', '?', 'do', 'you', 'think', 'people', 'with', 'ad', '##hd', 'are', 'more', 'prone', 'to', 'smoke', 'daily', '?', 'thank', 'you', 'to', 'anyone', 'who', 'answers', '!']\n",
      "INFO:__main__:Number of tokens: 108\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'many', 'adults', '(', 'over', '20', ')', 'with', 'ad', '##hd', 'are', 'chronic', 'marijuana', 'smoke', '##rs', '?', 'i', \"'\", 'm', '17', 'years', 'old', 'and', 'i', 'smoke', 'multiple', 'times', 'every', 'day', ',', 'and', 'it', \"'\", 's', 'possible', 'that', 'i', \"'\", 'll', 'continue', 'smoking', 'often', 'into', 'my', 'adult', 'life', '.', 'so', 'i', 'have', 'a', 'few', 'questions', 'for', 'the', 'adults', 'that', 'were', 'once', 'curious', '17', 'year', 'old', 'weed', '-', 'smoking', 'add', '##ers', ':', 'how', 'often', 'do', 'you', 'smoke', '?', 'do', 'you', 'think', 'weed', 'affects', 'your', 'life', '?', 'positively', 'or', 'negatively', '?', 'do', 'you', 'think', 'people', 'with', 'ad', '##hd', 'are', 'more', 'prone', 'to', 'smoke', 'daily', '?', 'thank', 'you', 'to', 'anyone', 'who', 'answers', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['questions', 'for', 'those', 'diagnosed', 'as', 'an', 'adult', '.', 'did', 'any', 'of', 'you', 'find', 'that', 'you', 'used', 'barriers', 'such', 'as', 'cigarettes', '/', 'alcohol', 'excessive', '##ly', 'in', 'social', 'situations', 'to', 'calm', 'your', 'nerves', '.', 'i', 'feel', '(', 'for', 'many', 'reasons', ')', 'that', 'i', 'do', 'have', 'ad', '##hd', 'and', 'am', 'in', 'the', 'process', 'of', 'being', 'screened', '/', 'tested', '.', 'i', 'remember', 'when', 'i', 'was', 'a', 'kid', 'i', 'would', 'often', 'blu', '##t', 'things', 'out', 'and', '(', 'kids', 'will', 'be', 'kids', ')', 'often', 'found', 'myself', 'embarrassing', 'myself', 'especially', 'in', 'large', 'group', 'situations', '.', 'i', 'feel', 'that', 'this', 'caused', 'me', 'to', 'become', 'socially', 'rep', '##ressed', 'in', 'regards', 'to', 'large', 'groups', ',', 'and', 'while', 'i', 'do', 'have', 'very', 'close', 'friends', 'i', 'am', 'comfortable', 'with', ',', 'i', 'am', 'very', 'cautious', 'in', 'larger', 'social', 'situations', 'to', 'the', 'point', 'where', 'i', \"'\", 'm', 'often', 'just', 'sitting', 'on', 'the', 'side', 'line', 'lost', 'in', 'my', 'own', 'thoughts', '.', 't', '##l', ';', 'dr', 'i', 'feel', 'like', 'ad', '##hd', 'as', 'child', 'caused', 'social', 'anxiety', 'as', 'an', 'adult', '.']\n",
      "INFO:__main__:Number of tokens: 164\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['questions', 'for', 'those', 'diagnosed', 'as', 'an', 'adult', '.', 'did', 'any', 'of', 'you', 'find', 'that', 'you', 'used', 'barriers', 'such', 'as', 'cigarettes', '/', 'alcohol', 'excessive', '##ly', 'in', 'social', 'situations', 'to', 'calm', 'your', 'nerves', '.', 'i', 'feel', '(', 'for', 'many', 'reasons', ')', 'that', 'i', 'do', 'have', 'ad', '##hd', 'and', 'am', 'in', 'the', 'process', 'of', 'being', 'screened', '/', 'tested', '.', 'i', 'remember', 'when', 'i', 'was', 'a', 'kid', 'i', 'would', 'often', 'blu', '##t', 'things', 'out', 'and', '(', 'kids', 'will', 'be', 'kids', ')', 'often', 'found', 'myself', 'embarrassing', 'myself', 'especially', 'in', 'large', 'group', 'situations', '.', 'i', 'feel', 'that', 'this', 'caused', 'me', 'to', 'become', 'socially', 'rep', '##ressed', 'in', 'regards', 'to', 'large', 'groups', ',', 'and', 'while', 'i', 'do', 'have', 'very', 'close', 'friends', 'i', 'am', 'comfortable', 'with', ',', 'i', 'am', 'very', 'cautious', 'in', 'larger', 'social', 'situations', 'to', 'the', 'point', 'where', 'i', \"'\", 'm', 'often', 'just', 'sitting', 'on', 'the', 'side', 'line', 'lost', 'in', 'my', 'own', 'thoughts', '.', 't', '##l', ';', 'dr', 'i', 'feel', 'like', 'ad', '##hd', 'as', 'child', 'caused', 'social', 'anxiety', 'as', 'an', 'adult', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'coping', 'skills', '/', 'tricks', 'did', 'you', 'learn', 'to', 'compensate', 'for', 'your', 'ad', '##hd', 'before', 'you', 'found', 'out', '?', 'this', 'may', 'apply', 'more', 'to', 'those', 'who', 'were', 'diagnosed', 'as', 'adults', '.', 'i', 'did', 'most', 'of', 'my', 'homework', 'while', 'the', 'teacher', 'was', 'teaching', 'for', 'that', 'homework', '.', 'i', 'had', 'to', 'strike', 'while', 'the', 'motivation', 'was', 'high', '.', 'for', 'me', 'my', 'motivation', 'is', 'usually', 'very', 'high', 'when', 'i', 'first', 'start', 'something', '.', 'it', 'has', 'a', 'very', 'small', 'half', 'life', 'though', '.', 'it', 'didn', \"'\", 't', 'work', 'when', 'they', 'waited', 'to', 'hand', 'out', 'the', 'assignment', 'at', 'the', 'end', 'of', 'class', 'though', '.']\n",
      "INFO:__main__:Number of tokens: 99\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'coping', 'skills', '/', 'tricks', 'did', 'you', 'learn', 'to', 'compensate', 'for', 'your', 'ad', '##hd', 'before', 'you', 'found', 'out', '?', 'this', 'may', 'apply', 'more', 'to', 'those', 'who', 'were', 'diagnosed', 'as', 'adults', '.', 'i', 'did', 'most', 'of', 'my', 'homework', 'while', 'the', 'teacher', 'was', 'teaching', 'for', 'that', 'homework', '.', 'i', 'had', 'to', 'strike', 'while', 'the', 'motivation', 'was', 'high', '.', 'for', 'me', 'my', 'motivation', 'is', 'usually', 'very', 'high', 'when', 'i', 'first', 'start', 'something', '.', 'it', 'has', 'a', 'very', 'small', 'half', 'life', 'though', '.', 'it', 'didn', \"'\", 't', 'work', 'when', 'they', 'waited', 'to', 'hand', 'out', 'the', 'assignment', 'at', 'the', 'end', 'of', 'class', 'though', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'story', 'hi', '/', 'r', '/', 'ad', '##hd', ',', 'i', 'just', 'discovered', 'this', 'sub', '##red', '##dit', 'and', 'after', 'reading', 'a', 'lot', 'of', 'comments', 'that', 'i', 'could', 'relate', 'to', 'i', 'decided', 'to', 'just', 'sort', 'of', 'spill', 'my', 'story', 'and', 'heart', 'out', '.', 'please', 'excuse', 'me', 'if', 'the', 'writing', 'gets', 'sloppy', 'at', 'some', 'points', '.', 'it', \"'\", 's', 'late', 'at', 'night', 'and', 'i', \"'\", 've', 'been', 'drinking', 'a', 'few', 'beers', '.', '.', '.', 'so', 'i', \"'\", 'm', '21', 'years', 'old', 'and', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', 'in', '1st', 'grade', ',', 'which', 'i', 'believe', 'was', 'around', 'the', 'time', 'when', 'ad', '##hd', 'was', 'starting', 'to', 'become', 'more', 'well', 'known', 'and', 'understood', '.', 'i', 'can', \"'\", 't', 'recall', 'how', 'much', 'it', 'affected', 'my', 'childhood', 'and', 'it', \"'\", 's', 'not', 'really', 'important', 'to', 'this', 'conversation', 'so', 'i', 'won', \"'\", 't', 'really', 'go', 'into', 'it', '.', 'you', 'know', 'what', ',', 'let', 'me', 'get', 'to', 'my', 'main', 'points', 'because', 'i', \"'\", 'm', 'starting', 'to', 'feel', 'less', 'and', 'less', 'motivated', 'to', 'do', 'this', ',', 'so', 'let', 'me', 'get', 'this', 'out', 'quickly', 'before', 'i', 'just', 'abandon', 'it', 'altogether', '0', '_', 'o', '.', 'fuck', 'it', 'i', \"'\", 'm', 'going', 'james', 'joyce', \"'\", 'stream', 'of', 'consciousness', \"'\", '.', '.', '.', 'about', 'two', 'years', 'ago', '(', 'summer', 'before', 'junior', 'year', ')', 'i', 'started', 'to', 'feel', 'very', 'depressed', '.', 'during', 'that', 'summer', 'i', 'started', 'my', 'first', 'job', 'as', 'an', 'intern', 'and', 'hated', 'it', '.', 'i', 'hated', 'working', 'for', 'people', ',', 'i', 'hated', 'being', 'told', 'what', 'to', 'do', ',', 'i', 'hated', 'doing', 'bullshit', 'tasks', 'etc', '.', 'around', 'that', 'time', 'i', 'also', 'started', 'self', '-', 'studying', 'philosophy', ',', 'which', 'i', \"'\", 've', 'continued', 'to', 'do', 'since', 'then', '.', 'i', 'feel', 'i', \"'\", 've', 'become', 'a', 'lot', 'more', 'intelligent', 'since', 'i', \"'\", 've', 'started', 'studying', 'philosophy', ',', 'but', 'at', 'the', 'same', 'time', 'i', \"'\", 've', 'felt', 'more', 'depressed', 'by', 'doing', 'it', '.', 'ec', '##cles', '##ias', '##tes', '1', ':', '17', '-', '18', 'really', 'speaks', 'to', 'me', 'there', '(', 'i', \"'\", 'm', 'atheist', 'bt', '##w', ')', '.', 'also', 'i', \"'\", 'm', 'a', 'virgin', '.', 'not', 'exactly', 'an', 'easy', 'thing', 'to', 'say', ',', 'even', 'anonymous', '##ly', 'on', 'the', 'internet', '.', 'never', 'had', 'a', 'girlfriend', ',', 'but', 'i', 'don', \"'\", 't', 'think', 'i', \"'\", 'm', 'ugly', 'and', 'i', 'can', 'generally', 'talk', 'to', 'girls', 'pretty', 'well', '.', 'i', 'think', 'i', 'have', 'an', 'appearance', 'issue', 'though', '.', 'i', 'always', 'have', 'to', 'see', 'how', 'i', 'look', 'in', 'a', 'mirror', 'and', 'i', \"'\", 'm', 'generally', 'uncomfortable', 'with', 'how', 'i', 'look', ',', 'so', 'i', 'always', 'feel', 'that', 'girls', 'will', 'just', 'find', 'me', 'rep', '##ulsive', 'for', 'whatever', 'reason', ',', 'despite', 'deep', 'down', 'i', 'know', 'i', \"'\", 'm', 'rather', 'attractive', '.', 'i', 'guess', 'i', 'always', 'i', 'always', 'have', 'these', 'fear', 'that', 'i', \"'\", 'm', 'being', 'judged', 'around', 'strangers', 'or', 'acquaintances', 'as', 'well', '.', 'i', 'think', 'i', \"'\", 'd', 'be', 'a', 'pretty', 'ball', '##er', 'boyfriend', ',', 'i', 'would', 'just', 'love', 'nothing', 'better', 'to', 'cu', '##ddle', 'the', 'shit', 'out', 'of', 'whatever', 'girl', 'i', \"'\", 'd', 'be', 'dating', '.', 'i', 'really', 'can', \"'\", 't', 'think', 'of', 'anything', 'better', 'than', 'to', 'get', 'under', 'the', 'covers', ',', 'spoon', 'like', 'a', 'boss', ',', 'and', 'listen', 'to', 'that', 'rainy', '##mo', '##od', '.', 'com', 'site', 'for', 'hours', '.', 'this', 'all', 'has', 'been', 'a', 'large', 'influence', 'on', 'my', 'depression', 'obviously', '.', 'i', 'have', 'almost', 'no', 'motivation', 'to', 'do', 'anything', '.', 'i', 'have', 'a', 'bad', 'habit', 'of', 'not', 'doing', 'my', 'dishes', 'and', 'when', 'my', 'roommate', '##s', 'asks', 'me', 'to', 'do', 'them', 'i', 'get', 'really', 'pissed', 'off', ',', 'despite', 'the', 'fact', 'that', 'i', 'know', 'i', 'should', 'really', 'do', 'my', 'dishes', '.', 'i', 'guess', 'i', 'have', 'a', 'really', 'hard', 'time', 'being', 'told', 'what', 'to', 'do', '.', 'my', 'major', 'is', 'marketing', ',', 'but', 'i', 'fucking', 'hate', 'it', '.', 'i', 'feel', 'like', 'college', 'has', 'been', 'a', 'general', 'waste', '.', 'i', 'really', 'really', 'hate', 'being', 'taught', '.', 'i', 'have', 'a', 'test', 'tomorrow', 'and', 'i', \"'\", 'm', 'looking', 'at', 'the', 'book', 'and', 'the', 'chapters', 'that', 'are', 'supposed', 'to', 'be', 'on', 'the', 'test', 'and', 'i', 'just', 'couldn', \"'\", 't', 'care', 'less', '.', 'despite', 'this', 'i', 'still', 'love', 'to', 'learn', ',', 'as', 'i', 'mentioned', 'before', ',', 'i', 'like', 'to', 'read', 'a', 'lot', 'of', 'philosophy', 'books', 'and', 'i', 'also', 'like', 'studying', 'science', ',', 'and', 'economics', ',', 'history', ',', 'politics', ',', 'psychology', 'etc', '.', 'but', 'only', 'if', 'i', 'choose', 'to', 'on', 'my', 'own', '.', 'i', 'took', 'a', 'philosophy', 'class', 'last', 'semester', 'and', 'dropped', 'it', 'after', 'three', 'days', 'because', 'i', 'was', 'starting', 'to', 'lose', 'interest', 'in', 'it', 'just', 'because', 'someone', 'was', 'teaching', 'it', 'to', 'me', '.', 'what', 'the', 'fuck', '?', 'does', 'anyone', 'else', 'feel', 'that', 'way', 'too', '?', 'i', 'have', 'this', 'really', 'strong', 'desire', 'to', 'write', 'a', 'screenplay', ',', 'write', 'poetry', ',', 'write', 'song', 'lyrics', 'etc', '.', 'but', 'it', \"'\", 's', 'hard', 'to', 'bring', 'myself', 'to', 'do', 'any', 'of', 'it', ',', 'and', 'when', 'i', 'do', 'i', 'cross', 'it', 'out', 'thinking', 'it', \"'\", 's', 'just', 'not', 'good', 'enough', '.', 'so', 'i', \"'\", 'm', 'about', 'to', 'graduate', 'college', 'soon', 'and', 'i', 'am', 'trying', 'to', 'find', 'a', 'job', '.', 'and', 'by', 'trying', 'i', 'mean', 'i', \"'\", 've', 'done', 'very', 'little', 'about', 'it', '.', 'mostly', 'it', \"'\", 's', 'my', 'father', 'who', 'has', 'been', 'pushing', 'me', 'to', 'send', 'my', 'resume', 'out', ',', 'make', 'contacts', 'etc', '.', 'but', 'i', 'feel', 'that', 'if', 'i', 'get', 'out', 'of', 'college', 'and', 'get', 'a', 'job', 'in', 'marketing', ',', 'my', 'brain', 'will', 'just', 'collapse', 'in', 'on', 'itself', '.', 'i', 'wouldn', \"'\", 't', 'be', 'able', 'to', 'handle', 'it', '.', 'for', 'the', 'last', 'few', 'months', 'i', \"'\", 've', 'been', 'ob', '##ses', '##sing', 'over', 'just', 'taking', 'some', 'men', '##ial', 'job', '.', 'i', 'was', 'looking', 'at', 'some', 'of', 'the', 'top', 'thread', 'in', 'this', 'sub', '##red', '##dit', ',', 'and', 'i', 'noticed', 'some', 'comments', 'in', 'which', 'the', 'red', '##dit', '##ors', 'stated', 'that', 'they', 'would', 'like', 'to', 'just', 'do', 'men', '##ial', 'tasks', 'like', 'gardening', 'or', 'painting', '.', 'i', 'really', 'wouldn', \"'\", 't', 'mind', 'working', 'at', 'barnes', 'and', 'noble', 'or', 'even', 'fucking', 'mcdonald', '##s', '.', 'but', 'what', 'the', 'hell', 'am', 'i', 'supposed', 'to', 'do', 'with', 'the', 'rest', 'of', 'my', 'life', '?', 'i', 'still', 'what', 'to', 'live', 'normally', '(', 'whatever', 'the', 'hell', 'that', 'means', ')', '.', 'i', 'take', 'medicine', 'for', 'my', 'ad', '##hd', ',', 'and', 'it', 'helps', 'me', 'to', 'think', 'clearly', ',', 'but', 'i', 'can', \"'\", 't', 'use', 'it', 'forever', '.', 'i', 'can', \"'\", 't', 'really', 'talk', 'to', 'anyone', 'when', 'i', \"'\", 'm', 'on', 'it', ',', 'so', 'it', 'wouldn', \"'\", 't', 'be', 'practical', 'to', 'take', 'at', 'work', 'or', 'whatever', '.', 'so', 'it', \"'\", 's', 'like', 'a', 'lose', '-', 'lose', 'situation', '.', 'if', 'i', 'take', 'it', 'at', 'a', 'job', 'i', \"'\", 'm', 'as', '##oc', '##ial', '.', 'if', 'i', 'don', \"'\", 't', 'take', 'it', 'i', 'can', \"'\", 't', 'think', 'clearly', '.', 'i', 'was', 'supposed', 'to', 'call', 'my', 'dad', 'on', 'sunday', 'about', 'email', '##ing', 'a', 'friend', 'of', 'his', 'regarding', 'job', 'contacts', '.', 'still', 'haven', \"'\", 't', 'done', 'it', 'yet', '.', 'i', 'think', 'i', \"'\", 'm', 'just', 'going', 'to', 'call', 'my', 'parents', 'tomorrow', 'and', 'tell', 'them', 'there', \"'\", 's', 'no', 'way', 'in', 'hell', 'i', 'can', 'get', 'a', '\"', 'real', '\"', 'job', 'right', 'now', '.', 'i', 'just', 'can', \"'\", 't', 'do', 'it', '.', 'what', 'the', 'hell', 'should', 'i', 'do', '?', 'i', \"'\", 'd', 'really', 'just', 'rather', 'sit', 'on', 'my', 'ass', 'all', 'summer', 'and', 'read', ',', 'sleep', ',', 'relax', 'etc', 'maybe', 'work', 'at', 'barnes', 'and', 'noble', '.', 'but', 'i', 'really', 'can', \"'\", 't', 'get', 'by', 'in', 'life', 'with', 'this', 'sort', 'of', 'overall', 'minds', '##et', 'going', 'on', '.', 'i', 'want', 'a', 'normal', 'life', '(', 'again', ',', 'not', 'entirely', 'sure', 'what', 'that', 'means', ')', 'but', 'i', 'don', \"'\", 't', 'see', 'how', 'it', \"'\", 's', 'possible', 'for', 'me', 'to', 'get', 'one', '.', 'i', 'feel', 'like', 'i', 'could', 'do', 'a', 'lot', 'of', 'great', 'things', 'with', 'my', 'life', 'if', 'all', 'these', 'issues', 'were', 'con', '##stra', '##ining', 'me', '.']\n",
      "INFO:__main__:Number of tokens: 1271\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['my', 'story', 'hi', '/', 'r', '/', 'ad', '##hd', ',', 'i', 'just', 'discovered', 'this', 'sub', '##red', '##dit', 'and', 'after', 'reading', 'a', 'lot', 'of', 'comments', 'that', 'i', 'could', 'relate', 'to', 'i', 'decided', 'to', 'just', 'sort', 'of', 'spill', 'my', 'story', 'and', 'heart', 'out', '.', 'please', 'excuse', 'me', 'if', 'the', 'writing', 'gets', 'sloppy', 'at', 'some', 'points', '.', 'it', \"'\", 's', 'late', 'at', 'night', 'and', 'i', \"'\", 've', 'been', 'drinking', 'a', 'few', 'beers', '.', '.', '.', 'so', 'i', \"'\", 'm', '21', 'years', 'old', 'and', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', 'in', '1st', 'grade', ',', 'which', 'i', 'believe', 'was', 'around', 'the', 'time', 'when', 'ad', '##hd', 'was', 'starting', 'to', 'become', 'more', 'well', 'known', 'and', 'understood', '.', 'i', 'can', \"'\", 't', 'recall', 'how', 'much', 'it', 'affected', 'my', 'childhood', 'and', 'it', \"'\", 's', 'not', 'really', 'important', 'to', 'this', 'conversation', 'so', 'i', 'won', \"'\", 't', 'really', 'go', 'into', 'it', '.', 'you', 'know', 'what', ',', 'let', 'me', 'get', 'to', 'my', 'main', 'points', 'because', 'i', \"'\", 'm', 'starting', 'to', 'feel', 'less', 'and', 'less', 'motivated', 'to', 'do', 'this', ',', 'so', 'let', 'me', 'get', 'this', 'out', 'quickly', 'before', 'i', 'just', 'abandon', 'it', 'altogether', '0', '_', 'o', '.', 'fuck', 'it', 'i', \"'\", 'm', 'going', 'james', 'joyce', \"'\", 'stream', 'of', 'consciousness', \"'\", '.', '.', '.', 'about', 'two', 'years', 'ago', '(', 'summer', 'before', 'junior', 'year', ')', 'i', 'started', 'to', 'feel', 'very', 'depressed', '.', 'during', 'that', 'summer', 'i', 'started', 'my', 'first', 'job', 'as', 'an', 'intern', 'and', 'hated', 'it', '.', 'i', 'hated', 'working', 'for', 'people', ',', 'i', 'hated', 'being', 'told', 'what', 'to', 'do', ',', 'i', 'hated', 'doing', 'bullshit', 'tasks', 'etc', '.', 'around', 'that', 'time', 'i', 'also', 'started', 'self', '-', 'studying', 'philosophy', ',', 'which', 'i', \"'\", 've', 'continued', 'to', 'do', 'since', 'then', '.', 'i', 'feel', 'i', \"'\", 've', 'become', 'a', 'lot', 'more', 'intelligent', 'since', 'i', \"'\", 've', 'started', 'studying', 'philosophy', ',', 'but', 'at', 'the', 'same', 'time', 'i', \"'\", 've', 'felt', 'more', 'depressed', 'by', 'doing', 'it', '.', 'ec', '##cles', '##ias', '##tes', '1', ':', '17', '-', '18', 'really', 'speaks', 'to', 'me', 'there', '(', 'i', \"'\", 'm', 'atheist', 'bt', '##w', ')', '.', 'also', 'i', \"'\", 'm', 'a', 'virgin', '.', 'not', 'exactly', 'an', 'easy', 'thing', 'to', 'say', ',', 'even', 'anonymous', '##ly', 'on', 'the', 'internet', '.', 'never', 'had', 'a', 'girlfriend', ',', 'but', 'i', 'don', \"'\", 't', 'think', 'i', \"'\", 'm', 'ugly', 'and', 'i', 'can', 'generally', 'talk', 'to', 'girls', 'pretty', 'well', '.', 'i', 'think', 'i', 'have', 'an', 'appearance', 'issue', 'though', '.', 'i', 'always', 'have', 'to', 'see', 'how', 'i', 'look', 'in', 'a', 'mirror', 'and', 'i', \"'\", 'm', 'generally', 'uncomfortable', 'with', 'how', 'i', 'look', ',', 'so', 'i', 'always', 'feel', 'that', 'girls', 'will', 'just', 'find', 'me', 'rep', '##ulsive', 'for', 'whatever', 'reason', ',', 'despite', 'deep', 'down', 'i', 'know', 'i', \"'\", 'm', 'rather', 'attractive', '.', 'i', 'guess', 'i', 'always', 'i', 'always', 'have', 'these', 'fear', 'that', 'i', \"'\", 'm', 'being', 'judged', 'around', 'strangers', 'or', 'acquaintances', 'as', 'well', '.', 'i', 'think', 'i', \"'\", 'd', 'be', 'a', 'pretty', 'ball', '##er', 'boyfriend', ',', 'i', 'would', 'just', 'love', 'nothing', 'better', 'to', 'cu', '##ddle', 'the', 'shit', 'out', 'of', 'whatever', 'girl', 'i', \"'\", 'd', 'be', 'dating', '.', 'i', 'really', 'can', \"'\", 't', 'think', 'of', 'anything', 'better', 'than', 'to', 'get', 'under', 'the', 'covers', ',', 'spoon', 'like', 'a', 'boss', ','], ['and', 'listen', 'to', 'that', 'rainy', '##mo', '##od', '.', 'com', 'site', 'for', 'hours', '.', 'this', 'all', 'has', 'been', 'a', 'large', 'influence', 'on', 'my', 'depression', 'obviously', '.', 'i', 'have', 'almost', 'no', 'motivation', 'to', 'do', 'anything', '.', 'i', 'have', 'a', 'bad', 'habit', 'of', 'not', 'doing', 'my', 'dishes', 'and', 'when', 'my', 'roommate', '##s', 'asks', 'me', 'to', 'do', 'them', 'i', 'get', 'really', 'pissed', 'off', ',', 'despite', 'the', 'fact', 'that', 'i', 'know', 'i', 'should', 'really', 'do', 'my', 'dishes', '.', 'i', 'guess', 'i', 'have', 'a', 'really', 'hard', 'time', 'being', 'told', 'what', 'to', 'do', '.', 'my', 'major', 'is', 'marketing', ',', 'but', 'i', 'fucking', 'hate', 'it', '.', 'i', 'feel', 'like', 'college', 'has', 'been', 'a', 'general', 'waste', '.', 'i', 'really', 'really', 'hate', 'being', 'taught', '.', 'i', 'have', 'a', 'test', 'tomorrow', 'and', 'i', \"'\", 'm', 'looking', 'at', 'the', 'book', 'and', 'the', 'chapters', 'that', 'are', 'supposed', 'to', 'be', 'on', 'the', 'test', 'and', 'i', 'just', 'couldn', \"'\", 't', 'care', 'less', '.', 'despite', 'this', 'i', 'still', 'love', 'to', 'learn', ',', 'as', 'i', 'mentioned', 'before', ',', 'i', 'like', 'to', 'read', 'a', 'lot', 'of', 'philosophy', 'books', 'and', 'i', 'also', 'like', 'studying', 'science', ',', 'and', 'economics', ',', 'history', ',', 'politics', ',', 'psychology', 'etc', '.', 'but', 'only', 'if', 'i', 'choose', 'to', 'on', 'my', 'own', '.', 'i', 'took', 'a', 'philosophy', 'class', 'last', 'semester', 'and', 'dropped', 'it', 'after', 'three', 'days', 'because', 'i', 'was', 'starting', 'to', 'lose', 'interest', 'in', 'it', 'just', 'because', 'someone', 'was', 'teaching', 'it', 'to', 'me', '.', 'what', 'the', 'fuck', '?', 'does', 'anyone', 'else', 'feel', 'that', 'way', 'too', '?', 'i', 'have', 'this', 'really', 'strong', 'desire', 'to', 'write', 'a', 'screenplay', ',', 'write', 'poetry', ',', 'write', 'song', 'lyrics', 'etc', '.', 'but', 'it', \"'\", 's', 'hard', 'to', 'bring', 'myself', 'to', 'do', 'any', 'of', 'it', ',', 'and', 'when', 'i', 'do', 'i', 'cross', 'it', 'out', 'thinking', 'it', \"'\", 's', 'just', 'not', 'good', 'enough', '.', 'so', 'i', \"'\", 'm', 'about', 'to', 'graduate', 'college', 'soon', 'and', 'i', 'am', 'trying', 'to', 'find', 'a', 'job', '.', 'and', 'by', 'trying', 'i', 'mean', 'i', \"'\", 've', 'done', 'very', 'little', 'about', 'it', '.', 'mostly', 'it', \"'\", 's', 'my', 'father', 'who', 'has', 'been', 'pushing', 'me', 'to', 'send', 'my', 'resume', 'out', ',', 'make', 'contacts', 'etc', '.', 'but', 'i', 'feel', 'that', 'if', 'i', 'get', 'out', 'of', 'college', 'and', 'get', 'a', 'job', 'in', 'marketing', ',', 'my', 'brain', 'will', 'just', 'collapse', 'in', 'on', 'itself', '.', 'i', 'wouldn', \"'\", 't', 'be', 'able', 'to', 'handle', 'it', '.', 'for', 'the', 'last', 'few', 'months', 'i', \"'\", 've', 'been', 'ob', '##ses', '##sing', 'over', 'just', 'taking', 'some', 'men', '##ial', 'job', '.', 'i', 'was', 'looking', 'at', 'some', 'of', 'the', 'top', 'thread', 'in', 'this', 'sub', '##red', '##dit', ',', 'and', 'i', 'noticed', 'some', 'comments', 'in', 'which', 'the', 'red', '##dit', '##ors', 'stated', 'that', 'they', 'would', 'like', 'to', 'just', 'do', 'men', '##ial', 'tasks', 'like', 'gardening', 'or', 'painting', '.', 'i', 'really', 'wouldn', \"'\", 't', 'mind', 'working', 'at', 'barnes', 'and', 'noble', 'or', 'even', 'fucking', 'mcdonald', '##s', '.', 'but', 'what', 'the', 'hell', 'am', 'i', 'supposed', 'to', 'do', 'with', 'the', 'rest', 'of', 'my', 'life', '?', 'i', 'still', 'what', 'to', 'live', 'normally', '(', 'whatever', 'the', 'hell', 'that', 'means', ')', '.', 'i', 'take', 'medicine', 'for', 'my', 'ad', '##hd', ',', 'and', 'it', 'helps', 'me', 'to', 'think', 'clearly', ',', 'but', 'i', 'can', \"'\", 't', 'use', 'it', 'forever'], ['.', 'i', 'can', \"'\", 't', 'really', 'talk', 'to', 'anyone', 'when', 'i', \"'\", 'm', 'on', 'it', ',', 'so', 'it', 'wouldn', \"'\", 't', 'be', 'practical', 'to', 'take', 'at', 'work', 'or', 'whatever', '.', 'so', 'it', \"'\", 's', 'like', 'a', 'lose', '-', 'lose', 'situation', '.', 'if', 'i', 'take', 'it', 'at', 'a', 'job', 'i', \"'\", 'm', 'as', '##oc', '##ial', '.', 'if', 'i', 'don', \"'\", 't', 'take', 'it', 'i', 'can', \"'\", 't', 'think', 'clearly', '.', 'i', 'was', 'supposed', 'to', 'call', 'my', 'dad', 'on', 'sunday', 'about', 'email', '##ing', 'a', 'friend', 'of', 'his', 'regarding', 'job', 'contacts', '.', 'still', 'haven', \"'\", 't', 'done', 'it', 'yet', '.', 'i', 'think', 'i', \"'\", 'm', 'just', 'going', 'to', 'call', 'my', 'parents', 'tomorrow', 'and', 'tell', 'them', 'there', \"'\", 's', 'no', 'way', 'in', 'hell', 'i', 'can', 'get', 'a', '\"', 'real', '\"', 'job', 'right', 'now', '.', 'i', 'just', 'can', \"'\", 't', 'do', 'it', '.', 'what', 'the', 'hell', 'should', 'i', 'do', '?', 'i', \"'\", 'd', 'really', 'just', 'rather', 'sit', 'on', 'my', 'ass', 'all', 'summer', 'and', 'read', ',', 'sleep', ',', 'relax', 'etc', 'maybe', 'work', 'at', 'barnes', 'and', 'noble', '.', 'but', 'i', 'really', 'can', \"'\", 't', 'get', 'by', 'in', 'life', 'with', 'this', 'sort', 'of', 'overall', 'minds', '##et', 'going', 'on', '.', 'i', 'want', 'a', 'normal', 'life', '(', 'again', ',', 'not', 'entirely', 'sure', 'what', 'that', 'means', ')', 'but', 'i', 'don', \"'\", 't', 'see', 'how', 'it', \"'\", 's', 'possible', 'for', 'me', 'to', 'get', 'one', '.', 'i', 'feel', 'like', 'i', 'could', 'do', 'a', 'lot', 'of', 'great', 'things', 'with', 'my', 'life', 'if', 'all', 'these', 'issues', 'were', 'con', '##stra', '##ining', 'me', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['weight', 'gain', 'on', 'add', '##eral', '##l', '.', 'help', '?', 'i', \"'\", 've', 'been', 'taking', '30', '##mg', 'twice', 'daily', 'for', 'several', 'years', '.', 'i', 'gained', 'a', 'lot', 'of', 'weight', 'a', 'year', 'ago', 'so', 'i', 'stopped', 'for', 'a', 'few', 'months', ',', 'continued', ',', 'and', 'easily', 'lost', 'the', 'weight', 'plus', 'more', '.', 'since', 'then', 'i', \"'\", 've', 'gained', 'about', '40', '##lb', '##s', '.', 'my', 'birth', 'control', 'may', 'slightly', 'contribute', 'to', 'this', ',', 'but', 'it', \"'\", 's', 'mostly', 'the', 'add', '##eral', '##l', '.', 'i', \"'\", 'm', 'finishing', 'up', 'a', 'degree', 'in', 'school', 'so', 'i', 'need', 'the', 'med', '##s', ',', 'but', 'this', 'is', 'really', 'becoming', 'an', 'issue', '.', 'i', \"'\", 've', 'grown', 'to', 'hate', 'myself', 'because', 'of', 'this', 'and', 'now', 'just', 'feel', 'lost', '.', 'add', '##eral', '##l', 'is', 'the', 'best', 'and', 'worst', 'thing', 'to', 'happen', 'to', 'me', '.', 'i', 'love', 'being', 'able', 'to', 'focus', 'and', 'do', 'well', 'in', 'college', 'but', 'i', 'can', 'not', 'stand', 'the', 'weight', 'gain', '.', 'anyone', 'have', 'similar', 'issues', 'or', 'advice', '?', 'it', \"'\", 'd', 'be', 'greatly', 'appreciated', '.', 'thanks', '!', 'also', ',', 'this', 'is', 'a', 'throw', '##away', 'acc', '##t', ',', 'obviously', '.']\n",
      "INFO:__main__:Number of tokens: 181\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['weight', 'gain', 'on', 'add', '##eral', '##l', '.', 'help', '?', 'i', \"'\", 've', 'been', 'taking', '30', '##mg', 'twice', 'daily', 'for', 'several', 'years', '.', 'i', 'gained', 'a', 'lot', 'of', 'weight', 'a', 'year', 'ago', 'so', 'i', 'stopped', 'for', 'a', 'few', 'months', ',', 'continued', ',', 'and', 'easily', 'lost', 'the', 'weight', 'plus', 'more', '.', 'since', 'then', 'i', \"'\", 've', 'gained', 'about', '40', '##lb', '##s', '.', 'my', 'birth', 'control', 'may', 'slightly', 'contribute', 'to', 'this', ',', 'but', 'it', \"'\", 's', 'mostly', 'the', 'add', '##eral', '##l', '.', 'i', \"'\", 'm', 'finishing', 'up', 'a', 'degree', 'in', 'school', 'so', 'i', 'need', 'the', 'med', '##s', ',', 'but', 'this', 'is', 'really', 'becoming', 'an', 'issue', '.', 'i', \"'\", 've', 'grown', 'to', 'hate', 'myself', 'because', 'of', 'this', 'and', 'now', 'just', 'feel', 'lost', '.', 'add', '##eral', '##l', 'is', 'the', 'best', 'and', 'worst', 'thing', 'to', 'happen', 'to', 'me', '.', 'i', 'love', 'being', 'able', 'to', 'focus', 'and', 'do', 'well', 'in', 'college', 'but', 'i', 'can', 'not', 'stand', 'the', 'weight', 'gain', '.', 'anyone', 'have', 'similar', 'issues', 'or', 'advice', '?', 'it', \"'\", 'd', 'be', 'greatly', 'appreciated', '.', 'thanks', '!', 'also', ',', 'this', 'is', 'a', 'throw', '##away', 'acc', '##t', ',', 'obviously', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['online', 'cognitive', 'behavioral', 'therapy', '(', 'cb', '##t', ')', 'for', 'adults', 'with', 'ad', '##hd', '?', 'i', 'read', 'somewhere', 'recently', 'that', 'cb', '##t', 'was', 'shown', 'to', 'be', 'effective', 'in', 'treating', 'ad', '##hd', '.', 'does', 'anyone', 'know', 'of', 'a', 'website', 'that', 'offers', 'online', 'cb', '##t', 'for', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 46\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['online', 'cognitive', 'behavioral', 'therapy', '(', 'cb', '##t', ')', 'for', 'adults', 'with', 'ad', '##hd', '?', 'i', 'read', 'somewhere', 'recently', 'that', 'cb', '##t', 'was', 'shown', 'to', 'be', 'effective', 'in', 'treating', 'ad', '##hd', '.', 'does', 'anyone', 'know', 'of', 'a', 'website', 'that', 'offers', 'online', 'cb', '##t', 'for', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['there', 'is', 'a', 'question', 'about', 'ad', '##hd', 'in', 'r', '/', 'asks', '##cie', '##nce', '.', 'most', 'of', 'the', 'answers', 'are', 'speculation', ',', 'could', 'someone', 'knowledge', '##able', 'give', 'a', 'good', 'answer', '?']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['there', 'is', 'a', 'question', 'about', 'ad', '##hd', 'in', 'r', '/', 'asks', '##cie', '##nce', '.', 'most', 'of', 'the', 'answers', 'are', 'speculation', ',', 'could', 'someone', 'knowledge', '##able', 'give', 'a', 'good', 'answer', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'to', 'get', 'dia', '##gno', '##se', 'as', 'an', 'adult', '?', 'i', 'live', 'in', 'colombia', ',', 'i', 'was', 'not', 'dia', '##gno', '##se', 'as', 'a', 'child', 'with', 'ad', '##hd', ',', 'but', 'i', '’', 'm', 'pretty', 'sure', 'i', 'was', '.', 'that', 'has', 'been', 'the', 'main', 'problem', ',', 'but', 'in', 'college', 'it', 'started', 'to', 'show', 'much', 'more', 'and', 'when', 'i', 'tried', 'to', 'get', 'dia', '##gno', '##se', 'by', 'some', 'different', 'doctors', 'they', 'dismiss', 'me', '\"', 'because', 'i', 'did', 'not', 'had', 'ad', '##hd', 'as', 'a', 'child', '\"', 'and', 'i', 'don', '##t', 'think', 'that', 'there', 'are', '\"', 'psychiatrist', 'specializing', 'in', 'ad', '##hd', 'in', 'adults', '\"', 'here', 'in', 'my', 'city', '.', 'so', 'that', '’', 's', 'my', 'problem', '.', 'anyone', 'know', 'what', 'should', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 117\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'to', 'get', 'dia', '##gno', '##se', 'as', 'an', 'adult', '?', 'i', 'live', 'in', 'colombia', ',', 'i', 'was', 'not', 'dia', '##gno', '##se', 'as', 'a', 'child', 'with', 'ad', '##hd', ',', 'but', 'i', '’', 'm', 'pretty', 'sure', 'i', 'was', '.', 'that', 'has', 'been', 'the', 'main', 'problem', ',', 'but', 'in', 'college', 'it', 'started', 'to', 'show', 'much', 'more', 'and', 'when', 'i', 'tried', 'to', 'get', 'dia', '##gno', '##se', 'by', 'some', 'different', 'doctors', 'they', 'dismiss', 'me', '\"', 'because', 'i', 'did', 'not', 'had', 'ad', '##hd', 'as', 'a', 'child', '\"', 'and', 'i', 'don', '##t', 'think', 'that', 'there', 'are', '\"', 'psychiatrist', 'specializing', 'in', 'ad', '##hd', 'in', 'adults', '\"', 'here', 'in', 'my', 'city', '.', 'so', 'that', '’', 's', 'my', 'problem', '.', 'anyone', 'know', 'what', 'should', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['coping', 'with', 'restless', 'leg', 'syndrome', 'or', 'constant', 'fi', '##dget', '##ing', '?', 'does', 'anyone', 'else', 'have', 'either', 'of', 'these', '.', 'i', 'just', 'can', \"'\", 't', 'stop', 'either', 'of', 'these', '.', 'if', 'i', 'stop', 'my', 'leg', ',', 'i', 'start', 'fi', '##dget', '##ing', 'with', 'a', 'pen', 'or', 'touch', 'my', 'chin', 'or', 'something', 'else', 'with', 'my', 'hands', '.', 'i', \"'\", 'm', 'not', 'yet', 'officially', 'diagnosed', '.', 'but', 'how', 'do', 'you', 'guys', 'cope', 'with', 'it', '?', 'if', 'i', 'try', 'to', 'sit', 'still', 'for', 'like', '1', 'min', ',', 'i', 'start', 'to', 'feel', 'like', 'i', \"'\", 'm', 'in', 'hell', '.', 'it', \"'\", 's', 'seriously', 'ag', '##gra', '##vating', '.', 'for', 'those', 'that', 'are', 'diagnosed', 'did', 'the', 'medication', 'help', 'with', 'that', '?']\n",
      "INFO:__main__:Number of tokens: 113\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['coping', 'with', 'restless', 'leg', 'syndrome', 'or', 'constant', 'fi', '##dget', '##ing', '?', 'does', 'anyone', 'else', 'have', 'either', 'of', 'these', '.', 'i', 'just', 'can', \"'\", 't', 'stop', 'either', 'of', 'these', '.', 'if', 'i', 'stop', 'my', 'leg', ',', 'i', 'start', 'fi', '##dget', '##ing', 'with', 'a', 'pen', 'or', 'touch', 'my', 'chin', 'or', 'something', 'else', 'with', 'my', 'hands', '.', 'i', \"'\", 'm', 'not', 'yet', 'officially', 'diagnosed', '.', 'but', 'how', 'do', 'you', 'guys', 'cope', 'with', 'it', '?', 'if', 'i', 'try', 'to', 'sit', 'still', 'for', 'like', '1', 'min', ',', 'i', 'start', 'to', 'feel', 'like', 'i', \"'\", 'm', 'in', 'hell', '.', 'it', \"'\", 's', 'seriously', 'ag', '##gra', '##vating', '.', 'for', 'those', 'that', 'are', 'diagnosed', 'did', 'the', 'medication', 'help', 'with', 'that', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', '*', 'really', '*', 'have', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', '*', 'really', '*', 'have', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'foods', 'should', 'i', 'eat', 'or', 'avoid', 'while', 'taking', 'my', 'medicine', '?', 'all', 'i', 'know', 'is', 'that', 'i', \"'\", 'm', 'not', 'supposed', 'to', 'eat', 'acidic', 'foods', 'before', 'or', 'during', 'my', 'med', '##icated', 'period', ',', 'and', 'no', 'energy', 'drinks', '.', 'i', 'was', 'wondering', 'if', 'there', 'was', 'any', 'other', 'information', 'about', 'how', 'to', 'increase', 'the', 'effectiveness', 'of', 'my', 'med', '##s', 'or', 'what', 'to', 'avoid', '.', 'if', 'it', 'helps', ',', 'i', \"'\", 'm', 'taking', 'low', 'dose', 'methyl', '##ph', '##eni', '##date', '(', '10', '##mg', ')']\n",
      "INFO:__main__:Number of tokens: 81\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'foods', 'should', 'i', 'eat', 'or', 'avoid', 'while', 'taking', 'my', 'medicine', '?', 'all', 'i', 'know', 'is', 'that', 'i', \"'\", 'm', 'not', 'supposed', 'to', 'eat', 'acidic', 'foods', 'before', 'or', 'during', 'my', 'med', '##icated', 'period', ',', 'and', 'no', 'energy', 'drinks', '.', 'i', 'was', 'wondering', 'if', 'there', 'was', 'any', 'other', 'information', 'about', 'how', 'to', 'increase', 'the', 'effectiveness', 'of', 'my', 'med', '##s', 'or', 'what', 'to', 'avoid', '.', 'if', 'it', 'helps', ',', 'i', \"'\", 'm', 'taking', 'low', 'dose', 'methyl', '##ph', '##eni', '##date', '(', '10', '##mg', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'to', 'accomplish', 'tuesday', ']', 'hey', '/', 'r', '/', 'ad', '##hd', '.', '.', '.', 'share', 'one', 'thing', 'you', 'would', 'like', 'to', 'accomplish', 'this', 'week', '.', 'we', 'will', 'check', '-', 'in', 'and', 'help', 'support', 'you', '!', 'welcome', 'to', 'part', 'iii', 'of', 'to', 'accomplish', 'tuesday', '!', 'we', 'made', 'the', 'jump', 'from', '14', 'participants', 'the', 'first', 'week', 'to', '*', '*', '28', 'participants', 'and', '>', '100', 'comments', '*', '*', '!', '(', '[', 'check', 'out', 'last', 'weeks', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', ')', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '*', 'suggested', 'rules', '*', '[UNK]', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to', 'do', 'that', 'too', '!', ')', '[UNK]', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'commit', '##ing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '[UNK]', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '[UNK]', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', '*', '*', 'tips', '*', '*', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', 'one', 'hour', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '.', '*', '*', '*', '[UNK]', '*', '*', 'examples', 'from', 'previous', 'weeks', ':', '*', '*', '[UNK]', '[UNK]', 'un', '##load', 'the', 'dish', '##wash', '##er', '[UNK]', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '[UNK]', 'make', 'an', 'appointment', 'with', 'doctor', '[UNK]', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '[UNK]', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', 'i', 'made', 'it', 'easier', 'this', 'week', 'to', 'respond', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '(', '*', 'i', 'fixed', 'it', '!', 'copy', 'the', 'code', 'below', 'and', 'you', 'will', 'automatically', 'have', 'bullets', 'on', 'each', 'line', '(', 'and', 'bold', ')', '*', ')', '*', '*', 'edit', ':', 'added', 'reward', '!', '(', 'doesn', \"'\", 't', 'have', 'to', 'cost', 'money', '!', ')', '*', '*', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?', 'when', '?', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-']\n",
      "INFO:__main__:Number of tokens: 747\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['[', 'to', 'accomplish', 'tuesday', ']', 'hey', '/', 'r', '/', 'ad', '##hd', '.', '.', '.', 'share', 'one', 'thing', 'you', 'would', 'like', 'to', 'accomplish', 'this', 'week', '.', 'we', 'will', 'check', '-', 'in', 'and', 'help', 'support', 'you', '!', 'welcome', 'to', 'part', 'iii', 'of', 'to', 'accomplish', 'tuesday', '!', 'we', 'made', 'the', 'jump', 'from', '14', 'participants', 'the', 'first', 'week', 'to', '*', '*', '28', 'participants', 'and', '>', '100', 'comments', '*', '*', '!', '(', '[', 'check', 'out', 'last', 'weeks', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', ')', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '*', 'suggested', 'rules', '*', '[UNK]', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to', 'do', 'that', 'too', '!', ')', '[UNK]', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'commit', '##ing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '[UNK]', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '[UNK]', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', '*', '*', 'tips', '*', '*', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start'], ['small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', 'one', 'hour', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '.', '*', '*', '*', '[UNK]', '*', '*', 'examples', 'from', 'previous', 'weeks', ':', '*', '*', '[UNK]', '[UNK]', 'un', '##load', 'the', 'dish', '##wash', '##er', '[UNK]', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '[UNK]', 'make', 'an', 'appointment', 'with', 'doctor', '[UNK]', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '[UNK]', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', 'i', 'made', 'it', 'easier', 'this', 'week', 'to', 'respond', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '(', '*', 'i', 'fixed', 'it', '!', 'copy', 'the', 'code', 'below', 'and', 'you', 'will', 'automatically', 'have', 'bullets', 'on', 'each', 'line', '(', 'and', 'bold', ')', '*', ')', '*', '*', 'edit', ':', 'added', 'reward', '!', '(', 'doesn', \"'\", 't', 'have', 'to', 'cost', 'money', '!', ')', '*', '*', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?', 'when', '?', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['21', 'tips', 'for', 'raising', 'kids', 'with', 'ad', '##hd', 'when', 'you', 'have', 'ad', '##hd', 'too']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['21', 'tips', 'for', 'raising', 'kids', 'with', 'ad', '##hd', 'when', 'you', 'have', 'ad', '##hd', 'too']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'to', 'add', '##eral', '##l', 'x', '##r', 'since', 'last', 'week', '.', 'why', 'don', \"'\", 't', 'i', 'feel', 'the', 'same', 'focused', 'energetic', 'drive', 'as', 'i', 'did', 'the', 'first', 'two', 'days', '?', 'started', 'medication', 'last', 'thursday', '.', 'wrote', 'a', 'decent', '15', 'page', 'paper', 'between', 'thursday', 'and', 'friday', ',', 'thanks', 'to', 'the', 'focus', '(', 'and', 'energy', ',', 'and', 'control', 'of', 'distraction', '##s', ')', 'that', 'the', 'add', '##eral', '##l', 'provided', '.', 'i', 'didn', \"'\", 't', 'take', 'it', 'over', 'the', 'weekend', ',', 'as', 'i', 'wasn', \"'\", 't', 'working', '.', 'but', 'yesterday', 'and', 'today', ',', 'i', \"'\", 've', 'noticed', 'that', 'i', 'can', 'distract', 'myself', 'as', 'easily', 'as', 'if', 'i', 'wasn', \"'\", 't', 'on', 'med', '##s', '.', 'i', \"'\", 've', 'avoided', 'the', 'difficult', 'homework', 'i', 'need', 'to', 'do', ',', 'and', 'am', 'instead', 'grading', 'assignments', '(', 'i', \"'\", 'm', 'a', 'gr', '##ad', 'student', 'and', 'teaching', 'assistant', ')', '.', 'have', 'i', 'developed', 'a', 'tolerance', 'so', 'rapidly', 'that', 'i', 'don', \"'\", 't', 'get', 'the', '\"', 'rush', '\"', '(', 'not', 'sure', 'if', 'that', \"'\", 's', 'the', 'right', 'word', ',', 'but', 'is', 'how', 'it', 'felt', 'last', 'week', ')', 'anymore', '?']\n",
      "INFO:__main__:Number of tokens: 178\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'to', 'add', '##eral', '##l', 'x', '##r', 'since', 'last', 'week', '.', 'why', 'don', \"'\", 't', 'i', 'feel', 'the', 'same', 'focused', 'energetic', 'drive', 'as', 'i', 'did', 'the', 'first', 'two', 'days', '?', 'started', 'medication', 'last', 'thursday', '.', 'wrote', 'a', 'decent', '15', 'page', 'paper', 'between', 'thursday', 'and', 'friday', ',', 'thanks', 'to', 'the', 'focus', '(', 'and', 'energy', ',', 'and', 'control', 'of', 'distraction', '##s', ')', 'that', 'the', 'add', '##eral', '##l', 'provided', '.', 'i', 'didn', \"'\", 't', 'take', 'it', 'over', 'the', 'weekend', ',', 'as', 'i', 'wasn', \"'\", 't', 'working', '.', 'but', 'yesterday', 'and', 'today', ',', 'i', \"'\", 've', 'noticed', 'that', 'i', 'can', 'distract', 'myself', 'as', 'easily', 'as', 'if', 'i', 'wasn', \"'\", 't', 'on', 'med', '##s', '.', 'i', \"'\", 've', 'avoided', 'the', 'difficult', 'homework', 'i', 'need', 'to', 'do', ',', 'and', 'am', 'instead', 'grading', 'assignments', '(', 'i', \"'\", 'm', 'a', 'gr', '##ad', 'student', 'and', 'teaching', 'assistant', ')', '.', 'have', 'i', 'developed', 'a', 'tolerance', 'so', 'rapidly', 'that', 'i', 'don', \"'\", 't', 'get', 'the', '\"', 'rush', '\"', '(', 'not', 'sure', 'if', 'that', \"'\", 's', 'the', 'right', 'word', ',', 'but', 'is', 'how', 'it', 'felt', 'last', 'week', ')', 'anymore', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['after', 'about', 'a', 'month', 'on', 'medication', 'i', 'feel', ',', 'for', 'the', 'first', 'time', ',', 'that', 'the', 'effort', 'i', \"'\", 'm', 'putting', 'into', 'school', 'is', 'worth', 'it', '.', 'i', 'just', 'got', 'back', 'a', 'paper', 'that', 'i', 'worked', 'really', 'hard', 'on', 'and', 'even', 'started', 'before', 'the', 'last', 'minute', '.', 'i', 'got', 'a', '98', 'and', 'my', 'professor', 'even', 'announced', 'to', 'the', 'class', 'what', 'a', 'fantastic', 'paper', 'it', 'was', '.', 'in', 'the', 'past', ',', 'anytime', 'i', 'put', 'a', 'lot', 'of', 'effort', 'into', 'anything', 'school', 'related', 'i', 'was', 'always', 'met', ',', 'to', 'my', 'confusion', ',', 'with', 'negative', 'results', '.', 'i', 'am', 'so', 'happy', 'right', 'now', ',', 'i', 'just', 'needed', 'to', 'tell', 'someone', 'without', 'sounding', 'like', 'i', \"'\", 'm', 'bragg', '##ing', '.', 'this', 'is', 'the', 'first', 'paper', 'i', \"'\", 've', 'written', 'that', 'actually', 'makes', 'linear', 'sense', 'instead', 'of', 'just', 'being', 'a', 'ju', '##mble', 'of', 'interesting', 'ideas', 'that', 'i', 'expect', 'the', 'reader', 'to', 'connect', 'by', 'themselves', 'the', 'way', 'my', 'mind', 'does', '.', 'normally', ',', 'if', 'i', 'tried', 'writing', 'a', 'paper', 'before', 'the', 'day', '/', 'night', 'before', 'it', 'was', 'due', 'i', 'would', 'spend', 'up', 'to', '5', 'hours', 'in', 'the', 'library', 'with', 'no', 'more', 'than', 'an', 'introduction', '.', 'finally', 'i', 'don', \"'\", 't', 'feel', 'like', 'school', 'is', 'a', 'waste', 'of', 'time', 'filled', 'with', 'idiots', 'and', 'i', 'don', \"'\", 't', 'feel', 'like', 'a', 'pe', '##ssi', '##mist', '##ic', 'burn', 'out', 'pot', 'head', 'spiral', '##ing', 'downhill', '.']\n",
      "INFO:__main__:Number of tokens: 225\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['after', 'about', 'a', 'month', 'on', 'medication', 'i', 'feel', ',', 'for', 'the', 'first', 'time', ',', 'that', 'the', 'effort', 'i', \"'\", 'm', 'putting', 'into', 'school', 'is', 'worth', 'it', '.', 'i', 'just', 'got', 'back', 'a', 'paper', 'that', 'i', 'worked', 'really', 'hard', 'on', 'and', 'even', 'started', 'before', 'the', 'last', 'minute', '.', 'i', 'got', 'a', '98', 'and', 'my', 'professor', 'even', 'announced', 'to', 'the', 'class', 'what', 'a', 'fantastic', 'paper', 'it', 'was', '.', 'in', 'the', 'past', ',', 'anytime', 'i', 'put', 'a', 'lot', 'of', 'effort', 'into', 'anything', 'school', 'related', 'i', 'was', 'always', 'met', ',', 'to', 'my', 'confusion', ',', 'with', 'negative', 'results', '.', 'i', 'am', 'so', 'happy', 'right', 'now', ',', 'i', 'just', 'needed', 'to', 'tell', 'someone', 'without', 'sounding', 'like', 'i', \"'\", 'm', 'bragg', '##ing', '.', 'this', 'is', 'the', 'first', 'paper', 'i', \"'\", 've', 'written', 'that', 'actually', 'makes', 'linear', 'sense', 'instead', 'of', 'just', 'being', 'a', 'ju', '##mble', 'of', 'interesting', 'ideas', 'that', 'i', 'expect', 'the', 'reader', 'to', 'connect', 'by', 'themselves', 'the', 'way', 'my', 'mind', 'does', '.', 'normally', ',', 'if', 'i', 'tried', 'writing', 'a', 'paper', 'before', 'the', 'day', '/', 'night', 'before', 'it', 'was', 'due', 'i', 'would', 'spend', 'up', 'to', '5', 'hours', 'in', 'the', 'library', 'with', 'no', 'more', 'than', 'an', 'introduction', '.', 'finally', 'i', 'don', \"'\", 't', 'feel', 'like', 'school', 'is', 'a', 'waste', 'of', 'time', 'filled', 'with', 'idiots', 'and', 'i', 'don', \"'\", 't', 'feel', 'like', 'a', 'pe', '##ssi', '##mist', '##ic', 'burn', 'out', 'pot', 'head', 'spiral', '##ing', 'downhill', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'prescribed', 'v', '##y', '##van', '##se', 'and', 'have', 'a', 'drug', 'test', 'coming', 'up', '.', 'what', 'should', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'prescribed', 'v', '##y', '##van', '##se', 'and', 'have', 'a', 'drug', 'test', 'coming', 'up', '.', 'what', 'should', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', \"'\", 's', 'it', 'supposed', 'to', 'feel', 'like', '?', 'i', \"'\", 've', 'been', 'on', 'rita', '##lin', 'a', 'few', 'days', 'now', 'and', 'have', 'not', 'seen', 'very', 'noticeable', 'effects', '.', 'i', \"'\", 've', 'found', 'it', 'a', 'little', 'easier', 'to', 'concentrate', 'in', 'class', ',', 'a', 'little', 'easier', 'to', 'get', 'things', 'done', 'without', 'pro', '##cr', '##ast', '##inating', ',', 'but', 'that', \"'\", 's', 'about', 'it', '.', 'i', \"'\", 've', 'read', 'stories', 'where', 'people', 'had', 'very', 'dramatic', 'effects', 'the', 'first', 'day', '.', 'were', 'my', 'expectations', 'too', 'high', ',', 'or', 'is', 'the', 'rita', '##lin', 'not', 'working', 'the', 'way', 'it', \"'\", 's', 'supposed', 'to', '?', 'what', 'is', 'it', 'supposed', 'to', 'feel', 'like', '?']\n",
      "INFO:__main__:Number of tokens: 105\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', \"'\", 's', 'it', 'supposed', 'to', 'feel', 'like', '?', 'i', \"'\", 've', 'been', 'on', 'rita', '##lin', 'a', 'few', 'days', 'now', 'and', 'have', 'not', 'seen', 'very', 'noticeable', 'effects', '.', 'i', \"'\", 've', 'found', 'it', 'a', 'little', 'easier', 'to', 'concentrate', 'in', 'class', ',', 'a', 'little', 'easier', 'to', 'get', 'things', 'done', 'without', 'pro', '##cr', '##ast', '##inating', ',', 'but', 'that', \"'\", 's', 'about', 'it', '.', 'i', \"'\", 've', 'read', 'stories', 'where', 'people', 'had', 'very', 'dramatic', 'effects', 'the', 'first', 'day', '.', 'were', 'my', 'expectations', 'too', 'high', ',', 'or', 'is', 'the', 'rita', '##lin', 'not', 'working', 'the', 'way', 'it', \"'\", 's', 'supposed', 'to', '?', 'what', 'is', 'it', 'supposed', 'to', 'feel', 'like', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['experiencing', 'major', 'add', '/', 'ad', '##hd', '-', 'like', 'side', 'effects', 'from', 'necessary', 'medicine', '.', 'help', '?']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['experiencing', 'major', 'add', '/', 'ad', '##hd', '-', 'like', 'side', 'effects', 'from', 'necessary', 'medicine', '.', 'help', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'advice', 'would', 'be', 'appreciated', '.', '.', '.', 'boyfriend', 'has', 'ad', '##hd', '.', 'got', 'fired', 'today', '-', '-', 'and', 'is', 'beating', 'himself', 'up', 'about', 'it', '.', 'he', \"'\", 's', 'a', 'graphic', 'designer', ',', 'and', 'his', 'job', 'didn', \"'\", 't', 'have', 'a', 'lot', 'of', 'structure', '.', 'which', 'lent', 'to', 'him', 'occasionally', 'checking', 'f', '##b', '/', 'craig', '##sl', '##ist', '/', 'red', '##dit', '/', 'sending', 'a', 'text', '.', 'we', \"'\", 're', 'both', 'pretty', 'convinced', 'they', \"'\", 've', 'been', 'looking', 'for', 'a', 'reason', 'to', 'fire', 'him', 'for', 'awhile', '.', 'so', 'he', 'checked', 'his', 'computer', 'when', 'he', 'was', 'not', 'te', '##c', '##ni', '##qual', '##ly', 'on', 'his', 'lunch', '##break', '.', 'asshole', '##s', '.', 'anyway', ',', 'he', 'hated', 'that', 'job', '.', 'it', 'drained', 'him', ',', 'and', 'waste', '##s', 'his', 'passion', 'and', 'talent', '.', '.', 'but', 'he', 'said', 'that', 'he', 'feels', 'like', 'a', 'failure', 'because', 'he', 'can', \"'\", 't', 'seem', 'to', \"'\", 'hold', 'down', 'a', 'job', 'like', 'everyone', 'else', \"'\", '.', 'i', \"'\", 've', 'been', 'reading', 'some', 'literature', 'about', 'living', 'with', '/', 'dating', 'someone', 'with', 'ad', '##hd', ',', 'so', 'in', 'that', 'realm', 'i', \"'\", 'm', 'well', 'verse', '##d', '.', 'i', 'realize', 'that', 'much', 'of', 'that', 'info', 'translates', 'over', '.', 'but', 'i', \"'\", 'm', 'trying', 'to', 'think', 'of', 'some', 'things', 'to', 'say', 'to', 'him', 'that', 'make', 'him', 'feel', 'better', ',', 'but', 'aren', \"'\", 't', 'patron', '##izing', '.', 'u', '##gh', '.', 'any', 'suggestions', '?']\n",
      "INFO:__main__:Number of tokens: 224\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'advice', 'would', 'be', 'appreciated', '.', '.', '.', 'boyfriend', 'has', 'ad', '##hd', '.', 'got', 'fired', 'today', '-', '-', 'and', 'is', 'beating', 'himself', 'up', 'about', 'it', '.', 'he', \"'\", 's', 'a', 'graphic', 'designer', ',', 'and', 'his', 'job', 'didn', \"'\", 't', 'have', 'a', 'lot', 'of', 'structure', '.', 'which', 'lent', 'to', 'him', 'occasionally', 'checking', 'f', '##b', '/', 'craig', '##sl', '##ist', '/', 'red', '##dit', '/', 'sending', 'a', 'text', '.', 'we', \"'\", 're', 'both', 'pretty', 'convinced', 'they', \"'\", 've', 'been', 'looking', 'for', 'a', 'reason', 'to', 'fire', 'him', 'for', 'awhile', '.', 'so', 'he', 'checked', 'his', 'computer', 'when', 'he', 'was', 'not', 'te', '##c', '##ni', '##qual', '##ly', 'on', 'his', 'lunch', '##break', '.', 'asshole', '##s', '.', 'anyway', ',', 'he', 'hated', 'that', 'job', '.', 'it', 'drained', 'him', ',', 'and', 'waste', '##s', 'his', 'passion', 'and', 'talent', '.', '.', 'but', 'he', 'said', 'that', 'he', 'feels', 'like', 'a', 'failure', 'because', 'he', 'can', \"'\", 't', 'seem', 'to', \"'\", 'hold', 'down', 'a', 'job', 'like', 'everyone', 'else', \"'\", '.', 'i', \"'\", 've', 'been', 'reading', 'some', 'literature', 'about', 'living', 'with', '/', 'dating', 'someone', 'with', 'ad', '##hd', ',', 'so', 'in', 'that', 'realm', 'i', \"'\", 'm', 'well', 'verse', '##d', '.', 'i', 'realize', 'that', 'much', 'of', 'that', 'info', 'translates', 'over', '.', 'but', 'i', \"'\", 'm', 'trying', 'to', 'think', 'of', 'some', 'things', 'to', 'say', 'to', 'him', 'that', 'make', 'him', 'feel', 'better', ',', 'but', 'aren', \"'\", 't', 'patron', '##izing', '.', 'u', '##gh', '.', 'any', 'suggestions', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mood', 'swings', '.', '.', '.', 'looking', 'for', 'support', ',', 'glad', 'i', 'found', 'this', 'sub', '##red', '##dit', '.', 'well', 'hi', 'there', 'r', '/', 'ad', '##hd', '.', 'i', 'had', 'a', 'feeling', 'there', 'would', 'be', 'a', 'group', 'of', 'us', 'on', 'red', '##dit', 'but', 'hadn', \"'\", 't', 'found', 'it', 'due', 'to', 'extreme', 'pro', '##cr', '##ast', '##ination', 'of', 'course', '.', 'i', 'was', 'diagnosed', 'young', ',', 'and', 'am', 'a', 'female', 'with', 'hyper', '##active', 'type', 'ad', '##hd', '.', 'i', \"'\", 'm', 'honestly', 'lucky', 'i', 'had', 'the', 'hyper', '##act', '##ivity', 'since', 'it', 'got', 'me', 'diagnosed', '.', 'but', 'as', 'many', 'of', 'us', 'know', ',', 'mood', 'swings', 'are', 'also', 'a', 'part', 'of', 'the', 'package', 'and', 'they', 'tend', 'to', 'lead', 'to', 'my', 'moods', '.', '.', '.', 'not', 'being', 'moderate', '.', 'i', \"'\", 'm', 'sporadic', '##ally', 'taking', 'my', 'add', '##eral', '##l', 'at', 'the', 'time', 'since', 'i', 'am', 'temporarily', 'un', '##ins', '##ured', '.', 'i', 'work', 'nights', 'in', 'a', 'field', 'i', 'love', 'and', 'while', 'i', \"'\", 'm', 'glad', 'about', 'that', '.', '.', '.', 'i', 'need', 'to', 'move', 'up', '.', 'i', \"'\", 'm', 'not', 'making', 'enough', 'money', 'and', 'i', \"'\", 'm', 'not', 'challenged', '.', 'however', ',', 'a', 'position', 'working', 'days', 'as', 'a', 'case', 'manager', 'has', 'opened', '.', 'i', 'want', 'this', 'job', '.', 'i', 'am', 'fully', 'capable', 'of', 'doing', 'it', '.', 'and', 'i', 'need', 'the', 'damn', 'money', '.', 'i', 'applied', 'and', 'now', 'im', 'waiting', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'in', 'a', 'ru', '##t', '-', 'bad', 'money', 'management', 'due', 'to', 'my', 'impulse', 'control', 'has', 'led', 'me', 'to', 'be', 'stressed', 'the', 'hell', 'out', 'over', 'money', 'and', 'i', \"'\", 'm', 'just', 'so', 'depressed', 'an', '##s', 'anxious', 'and', 'having', 'nightmares', '.', 'support', '.', 'please', '.', 'any', 'encouragement', 'you', 'can', 'give', 'me', 'or', 'behavioral', 'suggestions', '.', '.', '.', 'this', 'is', 'a', 'hard', 'time', 'for', 'me', '.', 'sorry', 'my', 'post', 'seems', 'scattered', 'or', 'bad', 'format', '##ting', '.', 'i', \"'\", 'm', 'on', 'my', 'phone', '.']\n",
      "INFO:__main__:Number of tokens: 304\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mood', 'swings', '.', '.', '.', 'looking', 'for', 'support', ',', 'glad', 'i', 'found', 'this', 'sub', '##red', '##dit', '.', 'well', 'hi', 'there', 'r', '/', 'ad', '##hd', '.', 'i', 'had', 'a', 'feeling', 'there', 'would', 'be', 'a', 'group', 'of', 'us', 'on', 'red', '##dit', 'but', 'hadn', \"'\", 't', 'found', 'it', 'due', 'to', 'extreme', 'pro', '##cr', '##ast', '##ination', 'of', 'course', '.', 'i', 'was', 'diagnosed', 'young', ',', 'and', 'am', 'a', 'female', 'with', 'hyper', '##active', 'type', 'ad', '##hd', '.', 'i', \"'\", 'm', 'honestly', 'lucky', 'i', 'had', 'the', 'hyper', '##act', '##ivity', 'since', 'it', 'got', 'me', 'diagnosed', '.', 'but', 'as', 'many', 'of', 'us', 'know', ',', 'mood', 'swings', 'are', 'also', 'a', 'part', 'of', 'the', 'package', 'and', 'they', 'tend', 'to', 'lead', 'to', 'my', 'moods', '.', '.', '.', 'not', 'being', 'moderate', '.', 'i', \"'\", 'm', 'sporadic', '##ally', 'taking', 'my', 'add', '##eral', '##l', 'at', 'the', 'time', 'since', 'i', 'am', 'temporarily', 'un', '##ins', '##ured', '.', 'i', 'work', 'nights', 'in', 'a', 'field', 'i', 'love', 'and', 'while', 'i', \"'\", 'm', 'glad', 'about', 'that', '.', '.', '.', 'i', 'need', 'to', 'move', 'up', '.', 'i', \"'\", 'm', 'not', 'making', 'enough', 'money', 'and', 'i', \"'\", 'm', 'not', 'challenged', '.', 'however', ',', 'a', 'position', 'working', 'days', 'as', 'a', 'case', 'manager', 'has', 'opened', '.', 'i', 'want', 'this', 'job', '.', 'i', 'am', 'fully', 'capable', 'of', 'doing', 'it', '.', 'and', 'i', 'need', 'the', 'damn', 'money', '.', 'i', 'applied', 'and', 'now', 'im', 'waiting', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'in', 'a', 'ru', '##t', '-', 'bad', 'money', 'management', 'due', 'to', 'my', 'impulse', 'control', 'has', 'led', 'me', 'to', 'be', 'stressed', 'the', 'hell', 'out', 'over', 'money', 'and', 'i', \"'\", 'm', 'just', 'so', 'depressed', 'an', '##s', 'anxious', 'and', 'having', 'nightmares', '.', 'support', '.', 'please', '.', 'any', 'encouragement', 'you', 'can', 'give', 'me', 'or', 'behavioral', 'suggestions', '.', '.', '.', 'this', 'is', 'a', 'hard', 'time', 'for', 'me', '.', 'sorry', 'my', 'post', 'seems', 'scattered', 'or', 'bad', 'format', '##ting', '.', 'i', \"'\", 'm', 'on', 'my', 'phone', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['paralyzed', ',', 'helpless', 'feelings', 'when', 'faced', 'with', 'homework', ':', 'do', 'med', '##s', 'help', '?', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'ad', '##hd', 'since', 'i', 'was', 'in', 'middle', 'school', '.', 'i', \"'\", 've', 'tried', 'several', 'different', 'medications', ',', 'but', 'a', 'few', '(', 'add', '##eral', '##l', ',', 'rita', '##lin', ')', 'led', 'to', 'extreme', '(', 'suicidal', ')', 'mood', 'swings', ',', 'and', 'i', 'finally', 'settled', 'on', 'concert', '##a', '.', 'stopped', 'taking', 'in', 'beginning', 'of', 'college', ',', 'and', 'i', 'didn', \"'\", 't', 'notice', 'much', 'difference', '.', 'i', \"'\", 've', 'been', 'doing', 'okay', 'throughout', ',', 'around', '2', '.', '6', 'gp', '##a', ',', 'although', 'i', \"'\", 'm', 'capable', 'of', 'much', 'more', '.', 'my', 'semester', '##s', 'are', 'always', 'the', 'same', '.', 'start', 'out', ',', 'attend', 'all', 'classes', ',', 'do', 'all', 'homework', 'with', 'high', 'a', 'grades', ',', 'to', 'the', 'end', ',', 'making', 'low', 'c', \"'\", 's', 'and', 'd', \"'\", 's', 'and', 'skipping', 'most', 'classes', '.', 'this', 'semester', '(', 'my', 'last', ')', 'has', 'been', 'especially', 'bad', 'to', 'the', 'point', 'i', 'might', 'not', 'graduate', '.', 'my', 'problem', 'is', 'whenever', 'i', 'think', 'about', 'how', 'far', 'i', \"'\", 've', 'fallen', 'behind', 'or', 'how', 'much', 'work', 'i', 'have', 'to', 'do', ',', 'my', 'brain', 'para', '##ly', '##zes', 'and', 'i', 'have', 'to', 'think', 'about', 'something', 'else', 'or', 'play', 'a', 'video', 'game', '.', 'i', 'do', 'this', 'every', 'day', 'and', 'fall', 'further', 'behind', ',', 'therefore', 'becoming', 'even', 'less', 'able', 'to', 'help', 'myself', '.', 'end', 'of', 'question', 'is', ':', 'will', 'med', '##s', 'help', 'with', 'this', '?', 'i', 'find', 'my', 'ad', '##hd', 'really', 'easy', 'to', 'control', 'when', 'i', \"'\", 'm', 'on', 'top', 'of', 'all', 'my', 'work', ',', 'but', 'things', 'always', ',', 'always', 'fall', 'apart', ',', 'and', 'it', \"'\", 's', 'a', 'vicious', 'cycle', '.']\n",
      "INFO:__main__:Number of tokens: 273\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['paralyzed', ',', 'helpless', 'feelings', 'when', 'faced', 'with', 'homework', ':', 'do', 'med', '##s', 'help', '?', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'ad', '##hd', 'since', 'i', 'was', 'in', 'middle', 'school', '.', 'i', \"'\", 've', 'tried', 'several', 'different', 'medications', ',', 'but', 'a', 'few', '(', 'add', '##eral', '##l', ',', 'rita', '##lin', ')', 'led', 'to', 'extreme', '(', 'suicidal', ')', 'mood', 'swings', ',', 'and', 'i', 'finally', 'settled', 'on', 'concert', '##a', '.', 'stopped', 'taking', 'in', 'beginning', 'of', 'college', ',', 'and', 'i', 'didn', \"'\", 't', 'notice', 'much', 'difference', '.', 'i', \"'\", 've', 'been', 'doing', 'okay', 'throughout', ',', 'around', '2', '.', '6', 'gp', '##a', ',', 'although', 'i', \"'\", 'm', 'capable', 'of', 'much', 'more', '.', 'my', 'semester', '##s', 'are', 'always', 'the', 'same', '.', 'start', 'out', ',', 'attend', 'all', 'classes', ',', 'do', 'all', 'homework', 'with', 'high', 'a', 'grades', ',', 'to', 'the', 'end', ',', 'making', 'low', 'c', \"'\", 's', 'and', 'd', \"'\", 's', 'and', 'skipping', 'most', 'classes', '.', 'this', 'semester', '(', 'my', 'last', ')', 'has', 'been', 'especially', 'bad', 'to', 'the', 'point', 'i', 'might', 'not', 'graduate', '.', 'my', 'problem', 'is', 'whenever', 'i', 'think', 'about', 'how', 'far', 'i', \"'\", 've', 'fallen', 'behind', 'or', 'how', 'much', 'work', 'i', 'have', 'to', 'do', ',', 'my', 'brain', 'para', '##ly', '##zes', 'and', 'i', 'have', 'to', 'think', 'about', 'something', 'else', 'or', 'play', 'a', 'video', 'game', '.', 'i', 'do', 'this', 'every', 'day', 'and', 'fall', 'further', 'behind', ',', 'therefore', 'becoming', 'even', 'less', 'able', 'to', 'help', 'myself', '.', 'end', 'of', 'question', 'is', ':', 'will', 'med', '##s', 'help', 'with', 'this', '?', 'i', 'find', 'my', 'ad', '##hd', 'really', 'easy', 'to', 'control', 'when', 'i', \"'\", 'm', 'on', 'top', 'of', 'all', 'my', 'work', ',', 'but', 'things', 'always', ',', 'always', 'fall', 'apart', ',', 'and', 'it', \"'\", 's', 'a', 'vicious', 'cycle', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['woo', '##ho', '##o', 'r', '/', 'ad', '##hd', '!', 'finally', 'worked', 'and', 'saw', 'results', '!', 'ah', '!', 'i', 'am', 'so', 'happy', '.', 'i', 'finally', 'worked', 'as', 'hard', 'as', 'i', 'could', ',', 'harder', 'than', 'i', 'ever', 'have', 'in', 'my', 'life', 'and', 'at', 'the', 'time', 'i', 'still', 'felt', 'as', 'though', 'i', 'achieved', 'nothing', '.', 'but', 'i', 'just', 'got', 'my', 'results', 'and', 'it', 'shows', '!', 'all', 'that', 'work', 'i', 'put', 'in', 'was', 'worth', 'it', ',', 'a', 'major', 'problem', 'of', 'mine', 'is', 'seeing', 'the', 'end', 'goal', 'that', 'i', 'am', 'working', 'for', ',', 'but', 'this', 'feeling', 'of', 'happiness', 'at', 'achieving', 'something', '?', 'its', 'worth', 'it', '!', 'so', 'push', 'on', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'and', 'do', 'your', 'very', 'best', ',', 'because', 'if', 'you', 'honestly', 'work', 'hard', 'despite', 'your', 'ad', '##hd', 'we', 'can', 'do', 'well', '!', 'i', 'just', 'needed', 'to', 'share', 'this', 'with', 'someone', ':', ')', 't', '##l', ';', 'dr', 'finally', 'worked', 'hard', 'for', 'something', '.', 'at', 'time', 'felt', 'like', 'a', 'waste', '.', 'totally', 'worth', 'it', '.', 'you', 'can', 'do', 'ee', '##t', '!', '(', 'say', 'that', 'like', 'rob', 'schneider', ')']\n",
      "INFO:__main__:Number of tokens: 182\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['woo', '##ho', '##o', 'r', '/', 'ad', '##hd', '!', 'finally', 'worked', 'and', 'saw', 'results', '!', 'ah', '!', 'i', 'am', 'so', 'happy', '.', 'i', 'finally', 'worked', 'as', 'hard', 'as', 'i', 'could', ',', 'harder', 'than', 'i', 'ever', 'have', 'in', 'my', 'life', 'and', 'at', 'the', 'time', 'i', 'still', 'felt', 'as', 'though', 'i', 'achieved', 'nothing', '.', 'but', 'i', 'just', 'got', 'my', 'results', 'and', 'it', 'shows', '!', 'all', 'that', 'work', 'i', 'put', 'in', 'was', 'worth', 'it', ',', 'a', 'major', 'problem', 'of', 'mine', 'is', 'seeing', 'the', 'end', 'goal', 'that', 'i', 'am', 'working', 'for', ',', 'but', 'this', 'feeling', 'of', 'happiness', 'at', 'achieving', 'something', '?', 'its', 'worth', 'it', '!', 'so', 'push', 'on', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'and', 'do', 'your', 'very', 'best', ',', 'because', 'if', 'you', 'honestly', 'work', 'hard', 'despite', 'your', 'ad', '##hd', 'we', 'can', 'do', 'well', '!', 'i', 'just', 'needed', 'to', 'share', 'this', 'with', 'someone', ':', ')', 't', '##l', ';', 'dr', 'finally', 'worked', 'hard', 'for', 'something', '.', 'at', 'time', 'felt', 'like', 'a', 'waste', '.', 'totally', 'worth', 'it', '.', 'you', 'can', 'do', 'ee', '##t', '!', '(', 'say', 'that', 'like', 'rob', 'schneider', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['life', 'without', 'a', 'degree', 'so', 'assuming', 'i', 'don', \"'\", 't', 'get', 'a', 'college', 'degree', '.', '.', 'what', 'is', 'my', 'job', 'outlook', '?', 'i', 'really', 'want', 'to', 'be', 'a', 'programmer', 'but', ',', 'that', 'just', 'isn', \"'\", 't', 'happening', '.', '.', '5', 'years', 'in', 'school', 'and', 'i', 'still', 'don', \"'\", 't', 'have', 'a', '2', 'year', 'degree', '.', '.', '(', 'i', 'started', 'off', 'at', 'a', '4', 'y', '##r', 'institution', ',', 'and', 'after', '4', 'years', 'i', 'decided', 'to', 'cut', 'my', 'losses', 'and', 'just', 'get', 'a', '2', 'year', 'degree', 'and', 'now', ',', 'i', 'can', \"'\", 't', 'even', 'focus', 'enough', 'to', 'get', 'that', '.', '.', ')', 'i', 'am', 'probably', 'not', 'living', 'in', 'reality', ',', 'but', 'at', 'this', 'point', 'med', '##s', 'aren', \"'\", 't', 'much', 'of', 'an', 'option', 'and', 'that', 'kinda', 'cuts', 'my', 'chances', 'of', 'ever', 'getting', 'out', 'of', 'school', 'to', '0', '.', 'anyone', 'out', 'there', 'manage', 'to', 'do', 'well', 'for', 'yourself', 'without', 'a', 'degree', '?', 'any', 'advice', 'to', 'be', 'had', '?']\n",
      "INFO:__main__:Number of tokens: 155\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['life', 'without', 'a', 'degree', 'so', 'assuming', 'i', 'don', \"'\", 't', 'get', 'a', 'college', 'degree', '.', '.', 'what', 'is', 'my', 'job', 'outlook', '?', 'i', 'really', 'want', 'to', 'be', 'a', 'programmer', 'but', ',', 'that', 'just', 'isn', \"'\", 't', 'happening', '.', '.', '5', 'years', 'in', 'school', 'and', 'i', 'still', 'don', \"'\", 't', 'have', 'a', '2', 'year', 'degree', '.', '.', '(', 'i', 'started', 'off', 'at', 'a', '4', 'y', '##r', 'institution', ',', 'and', 'after', '4', 'years', 'i', 'decided', 'to', 'cut', 'my', 'losses', 'and', 'just', 'get', 'a', '2', 'year', 'degree', 'and', 'now', ',', 'i', 'can', \"'\", 't', 'even', 'focus', 'enough', 'to', 'get', 'that', '.', '.', ')', 'i', 'am', 'probably', 'not', 'living', 'in', 'reality', ',', 'but', 'at', 'this', 'point', 'med', '##s', 'aren', \"'\", 't', 'much', 'of', 'an', 'option', 'and', 'that', 'kinda', 'cuts', 'my', 'chances', 'of', 'ever', 'getting', 'out', 'of', 'school', 'to', '0', '.', 'anyone', 'out', 'there', 'manage', 'to', 'do', 'well', 'for', 'yourself', 'without', 'a', 'degree', '?', 'any', 'advice', 'to', 'be', 'had', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['this', 'blows', '!', '(', 'vent', '##ing', ')', 'i', 'just', 'finished', 'writing', 'a', '10', 'page', 'research', 'paper', 'for', 'one', 'of', 'my', 'classes', '(', 'i', 'am', 'a', 'senior', 'in', 'engineering', ')', '.', 'i', 'spent', 'the', 'last', '12', 'hours', 'and', 'managed', 'to', 'write', '5', 'pages', '(', 'including', 'a', 'table', 'that', 'takes', 'up', '3', '/', '4', 'of', 'a', 'page', ')', '.', 'while', 'i', 'expected', 'to', 'spend', 'this', 'long', 'writing', 'it', ',', 'i', 'am', 'just', 'so', 'annoyed', 'at', 'the', 'quality', 'of', 'my', 'work', '.', 'i', 'can', 'objective', '##ly', 'say', 'this', 'is', 'one', 'of', 'my', 'worst', 'papers', 'in', 'all', 'of', 'college', '.', 'i', 'am', 'glad', 'that', 'my', 'med', '##s', 'gave', 'me', 'the', 'focus', 'to', 'finish', ',', 'but', 'i', 'just', 'wish', 'i', 'could', 'finish', 'all', 'my', 'homework', 'in', 'a', 'reasonable', 'time', 'frame', 'so', 'that', 'i', 'didn', \"'\", 't', 'have', 'to', 'push', 'my', 'big', 'projects', 'back', 'to', 'the', 'last', 'minute', '.', 'i', 'very', 'rarely', 'get', 'frustrated', 'by', 'my', 'ad', '##hd', '(', 'after', '15', '+', 'years', 'i', 'have', 'grown', 'to', 'accept', 'and', 'embrace', 'it', ')', ',', 'but', 'i', 'really', 'do', 'not', 'want', 'to', 'turn', 'in', 'this', 'paper', 'because', 'it', 'is', 'so', 'terrible', '.', 'i', 'can', \"'\", 't', 'think', 'of', 'anything', 'else', 'to', 'say', ',', 'i', 'just', 'needed', 'to', 'tell', 'someone', 'how', 'i', 'feel', 'and', 'everyone', 'i', 'know', 'ir', '##l', 'is', 'long', 'asleep', 'right', 'now', '(', 'its', '1', ':', '30', '##am', 'local', 'time', ')', '.', 'see', 'you', 'all', 'in', '6', 'hours', '.']\n",
      "INFO:__main__:Number of tokens: 232\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['this', 'blows', '!', '(', 'vent', '##ing', ')', 'i', 'just', 'finished', 'writing', 'a', '10', 'page', 'research', 'paper', 'for', 'one', 'of', 'my', 'classes', '(', 'i', 'am', 'a', 'senior', 'in', 'engineering', ')', '.', 'i', 'spent', 'the', 'last', '12', 'hours', 'and', 'managed', 'to', 'write', '5', 'pages', '(', 'including', 'a', 'table', 'that', 'takes', 'up', '3', '/', '4', 'of', 'a', 'page', ')', '.', 'while', 'i', 'expected', 'to', 'spend', 'this', 'long', 'writing', 'it', ',', 'i', 'am', 'just', 'so', 'annoyed', 'at', 'the', 'quality', 'of', 'my', 'work', '.', 'i', 'can', 'objective', '##ly', 'say', 'this', 'is', 'one', 'of', 'my', 'worst', 'papers', 'in', 'all', 'of', 'college', '.', 'i', 'am', 'glad', 'that', 'my', 'med', '##s', 'gave', 'me', 'the', 'focus', 'to', 'finish', ',', 'but', 'i', 'just', 'wish', 'i', 'could', 'finish', 'all', 'my', 'homework', 'in', 'a', 'reasonable', 'time', 'frame', 'so', 'that', 'i', 'didn', \"'\", 't', 'have', 'to', 'push', 'my', 'big', 'projects', 'back', 'to', 'the', 'last', 'minute', '.', 'i', 'very', 'rarely', 'get', 'frustrated', 'by', 'my', 'ad', '##hd', '(', 'after', '15', '+', 'years', 'i', 'have', 'grown', 'to', 'accept', 'and', 'embrace', 'it', ')', ',', 'but', 'i', 'really', 'do', 'not', 'want', 'to', 'turn', 'in', 'this', 'paper', 'because', 'it', 'is', 'so', 'terrible', '.', 'i', 'can', \"'\", 't', 'think', 'of', 'anything', 'else', 'to', 'say', ',', 'i', 'just', 'needed', 'to', 'tell', 'someone', 'how', 'i', 'feel', 'and', 'everyone', 'i', 'know', 'ir', '##l', 'is', 'long', 'asleep', 'right', 'now', '(', 'its', '1', ':', '30', '##am', 'local', 'time', ')', '.', 'see', 'you', 'all', 'in', '6', 'hours', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['oh', 'that', \"'\", 's', 'me', 'alright', '.']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['oh', 'that', \"'\", 's', 'me', 'alright', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'what', 'is', 'one', 'thing', 'you', 'can', 'be', 'proud', 'about', '?', '[', 'week', '4', ']', 'even', 'though', 'wednesday', 'is', '(', 'almost', ')', 'over', '.', '.', '.', 'i', 'invite', 'you', 'to', 'still', 'participate', 'and', 'let', 'us', 'know', 'what', 'awesome', 'things', 'you', 'have', 'done', 'lately', '!', '*', '*', '*', '*', '*', 'welcome', 'to', 'the', '4th', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', 'it', \"'\", 's', 'the', 'moment', 'that', '~', '~', '3', '~', '~', '*', '*', '24', '*', '*', 'of', 'you', 'have', 'been', 'waiting', 'for', '!', 'we', 'had', '12', 'participants', 'the', 'first', 'week', ',', '24', 'last', 'week', '.', '.', '.', '(', 'and', 'down', 'to', '15', 'last', 'week', '*', 'sad', 'face', '*', ')', '*', '*', 'if', 'you', 'can', \"'\", 't', 'think', 'of', 'anything', 'put', 'something', 'you', 'are', 'grateful', 'for', '.', 'we', 'all', 'can', 'express', 'some', 'gratitude', '.', '*', '*', '?', '*', '*', '*', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', 'so', 'here', 'if', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', '*', 'some', 'examples', 'from', 'past', 'weeks', '*', 'started', 'taking', 'ad', '##hd', 'medication', 'x', '##2', '*', 'called', 'doctor', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '*', 'working', 'out', 'and', 'eating', 'a', 'healthy', 'diet', '*', 'got', 'to', 'work', '.', '.', '.', 'on', 'time', '!', '*', 'started', 'going', 'to', 'bed', 'an', 'hour', 'earlier', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*']\n",
      "INFO:__main__:Number of tokens: 395\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'what', 'is', 'one', 'thing', 'you', 'can', 'be', 'proud', 'about', '?', '[', 'week', '4', ']', 'even', 'though', 'wednesday', 'is', '(', 'almost', ')', 'over', '.', '.', '.', 'i', 'invite', 'you', 'to', 'still', 'participate', 'and', 'let', 'us', 'know', 'what', 'awesome', 'things', 'you', 'have', 'done', 'lately', '!', '*', '*', '*', '*', '*', 'welcome', 'to', 'the', '4th', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', 'it', \"'\", 's', 'the', 'moment', 'that', '~', '~', '3', '~', '~', '*', '*', '24', '*', '*', 'of', 'you', 'have', 'been', 'waiting', 'for', '!', 'we', 'had', '12', 'participants', 'the', 'first', 'week', ',', '24', 'last', 'week', '.', '.', '.', '(', 'and', 'down', 'to', '15', 'last', 'week', '*', 'sad', 'face', '*', ')', '*', '*', 'if', 'you', 'can', \"'\", 't', 'think', 'of', 'anything', 'put', 'something', 'you', 'are', 'grateful', 'for', '.', 'we', 'all', 'can', 'express', 'some', 'gratitude', '.', '*', '*', '?', '*', '*', '*', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', 'so', 'here', 'if', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', '*', 'some', 'examples', 'from', 'past', 'weeks', '*', 'started', 'taking', 'ad', '##hd', 'medication', 'x', '##2', '*', 'called', 'doctor', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '*', 'working', 'out', 'and', 'eating', 'a', 'healthy', 'diet', '*', 'got', 'to', 'work', '.', '.', '.', 'on', 'time', '!', '*', 'started', 'going', 'to', 'bed', 'an', 'hour', 'earlier', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'benefits', 'do', 'i', 'receive', 'as', 'an', 'diagnosed', 'ad', '##hd', 'patient', '?']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'benefits', 'do', 'i', 'receive', 'as', 'an', 'diagnosed', 'ad', '##hd', 'patient', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'thinking', 'about', 'taking', 'some', 'of', 'my', 'friend', \"'\", 's', 'add', '##eral', 'i', \"'\", 've', 'tried', 'resources', 'from', 'my', 'school', '.', 'the', 'diagnostic', 'center', 'here', 'is', 'booked', 'until', 'the', 'fall', ',', 'and', 'with', 'only', 'a', 'month', 'left', 'in', 'the', 'semester', 'i', 'feel', 'like', 'i', 'don', \"'\", 't', 'have', 'time', 'to', 'deal', 'with', 'a', 'doctor', '.', 'is', 'this', 'a', 'terrible', ',', 'terrible', 'idea', '?']\n",
      "INFO:__main__:Number of tokens: 65\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'thinking', 'about', 'taking', 'some', 'of', 'my', 'friend', \"'\", 's', 'add', '##eral', 'i', \"'\", 've', 'tried', 'resources', 'from', 'my', 'school', '.', 'the', 'diagnostic', 'center', 'here', 'is', 'booked', 'until', 'the', 'fall', ',', 'and', 'with', 'only', 'a', 'month', 'left', 'in', 'the', 'semester', 'i', 'feel', 'like', 'i', 'don', \"'\", 't', 'have', 'time', 'to', 'deal', 'with', 'a', 'doctor', '.', 'is', 'this', 'a', 'terrible', ',', 'terrible', 'idea', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'no', 'idea', 'if', 'i', 'have', 'ad', '##hd', ',', 'and', 'it', 'feels', 'like', 'my', 'options', 'are', 'limited', '.', 'any', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'no', 'idea', 'if', 'i', 'have', 'ad', '##hd', ',', 'and', 'it', 'feels', 'like', 'my', 'options', 'are', 'limited', '.', 'any', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'week', 'on', 'dex', '##ed', '##rine', ':', 'problem', 'with', 'my', 'eyes']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', 'week', 'on', 'dex', '##ed', '##rine', ':', 'problem', 'with', 'my', 'eyes']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'deal', 'with', 'having', 'a', 'short', 'temper', '?', 'tell', 'me', 'your', 'coping', 'strategies', 'if', 'you', 'have', 'them', '!', 'i', 'don', \"'\", 't', 'know', 'about', 'you', ',', 'but', 'i', 'have', 'a', 'crap', 'temper', '.', 'i', 'lose', 'my', 'shit', 'over', 'the', 'most', 'trivial', 'of', 'things', '.', 'i', \"'\", 'm', 'bad', 'when', 'it', 'comes', 'to', 'being', 'interrupted', ',', 'especially', 'if', 'more', 'than', 'one', 'thing', 'is', 'overwhelming', 'me', 'atm', '.', 'god', 'forbid', 'a', 'perfectly', 'innocent', 'person', 'interrupt', '##s', 'me', 'or', 'reminds', 'me', 'to', 'do', 'something', 'i', \"'\", 've', 'been', 'putting', 'off', 'd', ':', 'over', 'the', 'years', 'i', \"'\", 've', 'learned', 'that', '*', '*', 'exercise', '*', '*', 'and', '*', '*', 'deep', 'breathing', '*', '*', 'helps', '.', 'as', 'well', 'as', 'add', '##eral', '##l', '.', 'one', 'thing', 'this', 'article', 'brings', 'up', 'is', 'the', 'idea', 'of', '*', 'allowing', '*', 'yourself', 'to', 'be', 'angry', 'for', ',', 'like', ',', '5', 'minutes', '.', 'that', \"'\", 's', 'novel', 'for', 'me', ',', 'having', 'been', 'taught', 'that', '*', '*', 'anger', '=', 'bad', '*', '*', 'and', 'i', 'must', 'sur', '##press', 'it', '.', 'i', \"'\", 'm', 'lucky', 'in', 'that', 'i', 'live', 'with', 'someone', 'who', 'is', 'ad', '##hd', 'also', ',', 'and', 'therefore', 'knows', 'where', 'i', \"'\", 'm', 'coming', 'from', '.', 'and', 'that', 'i', 'have', 'an', 'office', 'at', 'work', 'where', 'i', 'can', 'quietly', 'rage', 'and', 'no', 'one', 'will', 'see', 'me', 'lo', '##l', 'd', ':', 't', '##l', ';', 'dr', ':', 'i', 'have', 'a', 'short', 'fuse', ',', 'how', 'about', 'you', '?', 'and', 'how', 'do', 'you', 'cope', 'with', 'it', '?', ':', 'd', '?', 'edit', ':', '[', 'this', ']', '(', 'http', ':', '/', '/', 'www', '.', 'add', '##itude', '##ma', '##g', '.', 'com', '/', 'ad', '##hd', '/', 'article', '/', '52', '##35', '.', 'html', ')', 'was', 'the', 'article', 'i', 'was', 'referring', 'to', '.', 'sorry', '!', 'edit', '2', ':', 'wow', ',', 'thanks', 'everyone', 'for', 'your', 'responses', '.', 'i', 'appreciate', 'it', ':', 'd']\n",
      "INFO:__main__:Number of tokens: 299\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'deal', 'with', 'having', 'a', 'short', 'temper', '?', 'tell', 'me', 'your', 'coping', 'strategies', 'if', 'you', 'have', 'them', '!', 'i', 'don', \"'\", 't', 'know', 'about', 'you', ',', 'but', 'i', 'have', 'a', 'crap', 'temper', '.', 'i', 'lose', 'my', 'shit', 'over', 'the', 'most', 'trivial', 'of', 'things', '.', 'i', \"'\", 'm', 'bad', 'when', 'it', 'comes', 'to', 'being', 'interrupted', ',', 'especially', 'if', 'more', 'than', 'one', 'thing', 'is', 'overwhelming', 'me', 'atm', '.', 'god', 'forbid', 'a', 'perfectly', 'innocent', 'person', 'interrupt', '##s', 'me', 'or', 'reminds', 'me', 'to', 'do', 'something', 'i', \"'\", 've', 'been', 'putting', 'off', 'd', ':', 'over', 'the', 'years', 'i', \"'\", 've', 'learned', 'that', '*', '*', 'exercise', '*', '*', 'and', '*', '*', 'deep', 'breathing', '*', '*', 'helps', '.', 'as', 'well', 'as', 'add', '##eral', '##l', '.', 'one', 'thing', 'this', 'article', 'brings', 'up', 'is', 'the', 'idea', 'of', '*', 'allowing', '*', 'yourself', 'to', 'be', 'angry', 'for', ',', 'like', ',', '5', 'minutes', '.', 'that', \"'\", 's', 'novel', 'for', 'me', ',', 'having', 'been', 'taught', 'that', '*', '*', 'anger', '=', 'bad', '*', '*', 'and', 'i', 'must', 'sur', '##press', 'it', '.', 'i', \"'\", 'm', 'lucky', 'in', 'that', 'i', 'live', 'with', 'someone', 'who', 'is', 'ad', '##hd', 'also', ',', 'and', 'therefore', 'knows', 'where', 'i', \"'\", 'm', 'coming', 'from', '.', 'and', 'that', 'i', 'have', 'an', 'office', 'at', 'work', 'where', 'i', 'can', 'quietly', 'rage', 'and', 'no', 'one', 'will', 'see', 'me', 'lo', '##l', 'd', ':', 't', '##l', ';', 'dr', ':', 'i', 'have', 'a', 'short', 'fuse', ',', 'how', 'about', 'you', '?', 'and', 'how', 'do', 'you', 'cope', 'with', 'it', '?', ':', 'd', '?', 'edit', ':', '[', 'this', ']', '(', 'http', ':', '/', '/', 'www', '.', 'add', '##itude', '##ma', '##g', '.', 'com', '/', 'ad', '##hd', '/', 'article', '/', '52', '##35', '.', 'html', ')', 'was', 'the', 'article', 'i', 'was', 'referring', 'to', '.', 'sorry', '!', 'edit', '2', ':', 'wow', ',', 'thanks', 'everyone', 'for', 'your', 'responses', '.', 'i', 'appreciate', 'it', ':', 'd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', 'and', 'ad', '##hd', 'help']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', 'and', 'ad', '##hd', 'help']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['damn', 'you', 'appetite', '-', 'suppress', '##ing', 'rita', '##lin', '!', 'now', 'my', 'once', 'delicious', '-', 'looking', 'lunch', 'is', 'decidedly', 'me', '##h', '.', 'took', 'my', 'rita', '##lin', 'before', 'starting', 'on', 'a', 'paper', '(', 'and', 'of', 'course', 'i', 'end', 'up', 'on', 'red', '##dit', ')', '.', 'went', 'to', 'grab', 'a', 'hamburger', 'to', 'bring', 'home', 'and', 'by', 'the', 'time', 'i', 'get', 'home', 'the', 'rita', '##lin', 'has', 'kicked', 'in', 'and', 'my', 'once', 'hungry', 'stomach', 'now', 'wants', 'nothing', 'to', 'do', 'with', 'that', 'burger', '.', 'real', 'tired', 'of', 'this', ',', 'but', ',', 'what', 'are', 'you', 'gonna', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 90\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['damn', 'you', 'appetite', '-', 'suppress', '##ing', 'rita', '##lin', '!', 'now', 'my', 'once', 'delicious', '-', 'looking', 'lunch', 'is', 'decidedly', 'me', '##h', '.', 'took', 'my', 'rita', '##lin', 'before', 'starting', 'on', 'a', 'paper', '(', 'and', 'of', 'course', 'i', 'end', 'up', 'on', 'red', '##dit', ')', '.', 'went', 'to', 'grab', 'a', 'hamburger', 'to', 'bring', 'home', 'and', 'by', 'the', 'time', 'i', 'get', 'home', 'the', 'rita', '##lin', 'has', 'kicked', 'in', 'and', 'my', 'once', 'hungry', 'stomach', 'now', 'wants', 'nothing', 'to', 'do', 'with', 'that', 'burger', '.', 'real', 'tired', 'of', 'this', ',', 'but', ',', 'what', 'are', 'you', 'gonna', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['no', 'one', 'will', 'freaking', 'give', 'me', 'my', 'medicine', '!', 'i', \"'\", 'm', 'trying', 'to', 'get', 'my', 'medicine', 'from', 'my', 'university', 'health', 'center', ',', 'but', 'they', 'say', 'that', 'my', 'evaluation', 'from', 'middle', 'school', 'is', 'too', 'old', 'and', 'i', 'need', 'to', 'get', 'another', 'full', 'eva', '##l', 'before', 'i', 'can', 'get', 'my', 'prescription', ',', 'as', 'if', 'it', 'just', 'goes', 'away', 'after', 'a', 'few', 'years', ',', 'that', 'makes', 'sense', '!', 'my', 'old', 'hospital', 'won', \"'\", 't', 'ref', '##ill', 'my', 'prescription', 'because', 'it', \"'\", 's', 'been', 'more', 'than', 'six', 'months', 'since', 'i', 'last', 'asked', 'them', 'to', 'ref', '##ill', '.', 'i', 'have', 'no', 'insurance', 'and', 'my', 'parents', 'can', \"'\", 't', 'afford', 'to', 'give', 'me', 'the', '1', ',', '200', 'it', 'takes', 'to', 'get', 'another', 'eva', '##l', ',', 'and', 'to', 'hell', 'with', 'being', 'able', 'to', 'afford', 'it', 'myself', '!', 'what', 'the', 'hell', 'am', 'i', 'supposed', 'to', 'do', '?', 'i', \"'\", 'm', 'freaking', 'out', 'here', '.']\n",
      "INFO:__main__:Number of tokens: 147\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['no', 'one', 'will', 'freaking', 'give', 'me', 'my', 'medicine', '!', 'i', \"'\", 'm', 'trying', 'to', 'get', 'my', 'medicine', 'from', 'my', 'university', 'health', 'center', ',', 'but', 'they', 'say', 'that', 'my', 'evaluation', 'from', 'middle', 'school', 'is', 'too', 'old', 'and', 'i', 'need', 'to', 'get', 'another', 'full', 'eva', '##l', 'before', 'i', 'can', 'get', 'my', 'prescription', ',', 'as', 'if', 'it', 'just', 'goes', 'away', 'after', 'a', 'few', 'years', ',', 'that', 'makes', 'sense', '!', 'my', 'old', 'hospital', 'won', \"'\", 't', 'ref', '##ill', 'my', 'prescription', 'because', 'it', \"'\", 's', 'been', 'more', 'than', 'six', 'months', 'since', 'i', 'last', 'asked', 'them', 'to', 'ref', '##ill', '.', 'i', 'have', 'no', 'insurance', 'and', 'my', 'parents', 'can', \"'\", 't', 'afford', 'to', 'give', 'me', 'the', '1', ',', '200', 'it', 'takes', 'to', 'get', 'another', 'eva', '##l', ',', 'and', 'to', 'hell', 'with', 'being', 'able', 'to', 'afford', 'it', 'myself', '!', 'what', 'the', 'hell', 'am', 'i', 'supposed', 'to', 'do', '?', 'i', \"'\", 'm', 'freaking', 'out', 'here', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'know', 'i', \"'\", 'm', 'preaching', 'to', 'the', 'choir', '.', '.', 'if', 'you', 'repeatedly', 'try', 'something', 'and', 'fail', '.', 'it', 'is', 'not', 'the', 'moral', 'short', '##coming', 'of', '\"', 'la', '##zine', '##ss', '\"', ',', 'it', 'is', 'by', 'definition', 'persistence', '.']\n",
      "INFO:__main__:Number of tokens: 39\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'know', 'i', \"'\", 'm', 'preaching', 'to', 'the', 'choir', '.', '.', 'if', 'you', 'repeatedly', 'try', 'something', 'and', 'fail', '.', 'it', 'is', 'not', 'the', 'moral', 'short', '##coming', 'of', '\"', 'la', '##zine', '##ss', '\"', ',', 'it', 'is', 'by', 'definition', 'persistence', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'have', \"'\", 'pang', '##s', \"'\", 'or', 'semi', '-', 'voluntary', \"'\", 'twitch', '##es', \"'\", '(', 'when', 'no', 'one', \"'\", 's', 'looking', ')', '?', 'i', 'think', 'these', 'may', 'be', 'left', 'over', 'from', 'when', 'i', 'had', 'temper', 'tan', '##trum', '##s', 'as', 'a', 'kid', '.', 'it', \"'\", 's', 'almost', 'like', 'physically', 'mel', '##od', '##rama', '##ti', '##zing', 'the', 'emotional', 'effect', 'things', 'have', 'on', 'me', '(', 'think', 'a', 'sentence', 'of', 'hamlet', \"'\", 's', 'mono', '##logue', 'translated', 'into', 'a', 'physical', 'reaction', ')', '.', 'if', 'i', \"'\", 'm', 'alone', ',', 'and', 'something', \"'\", 'hits', 'me', \"'\", ',', 'even', 'if', 'it', \"'\", 's', 'something', 'minor', ',', 'i', 'might', \"'\", 'spa', '##sm', \"'\", 'for', 'a', 'second', 'and', 'grab', 'onto', 'my', 'heart', '.', 'sometimes', 'i', 'even', 'punch', 'the', 'skin', 'above', 'my', '~', '~', 'heat', '~', '~', 'heart', 'to', 'keep', 'down', 'a', \"'\", 'pang', \"'\", 'of', 'whatever', 'kind', '.', 'i', 'often', 'get', 'these', 'pang', '##s', 'late', 'at', 'night', 'when', 'i', \"'\", 'm', 'alone', ',', 'and', 'they', 'are', 'accompanied', 'by', 'a', 'strange', 'kind', 'of', 'sadness', 'or', 'existent', '##ial', 'worrying', ',', 'including', 'uncomfortable', 'intro', '##sp', '##ection', 'and', 'sensitivity', 'to', 'my', 'own', 'emotions', '.', 'can', 'anyone', 'else', 'relate', '?', 'i', 'feel', 'like', 'this', 'is', 'related', 'to', 'ad', '##hd', 'in', 'that', 'it', 'involves', 'the', 'inability', 'to', 'physically', 'sur', '##press', 'emotion', ',', 'impulse', '##s', ',', 'motor', 'functions', ',', 'etc', '.', 'edit', ':', 'heart', ',', 'not', 'heat', '.', 'i', 'hope', 'that', 'didn', \"'\", 't', 'make', 'my', 'post', 'inadvertently', 'obscene', '.']\n",
      "INFO:__main__:Number of tokens: 234\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'have', \"'\", 'pang', '##s', \"'\", 'or', 'semi', '-', 'voluntary', \"'\", 'twitch', '##es', \"'\", '(', 'when', 'no', 'one', \"'\", 's', 'looking', ')', '?', 'i', 'think', 'these', 'may', 'be', 'left', 'over', 'from', 'when', 'i', 'had', 'temper', 'tan', '##trum', '##s', 'as', 'a', 'kid', '.', 'it', \"'\", 's', 'almost', 'like', 'physically', 'mel', '##od', '##rama', '##ti', '##zing', 'the', 'emotional', 'effect', 'things', 'have', 'on', 'me', '(', 'think', 'a', 'sentence', 'of', 'hamlet', \"'\", 's', 'mono', '##logue', 'translated', 'into', 'a', 'physical', 'reaction', ')', '.', 'if', 'i', \"'\", 'm', 'alone', ',', 'and', 'something', \"'\", 'hits', 'me', \"'\", ',', 'even', 'if', 'it', \"'\", 's', 'something', 'minor', ',', 'i', 'might', \"'\", 'spa', '##sm', \"'\", 'for', 'a', 'second', 'and', 'grab', 'onto', 'my', 'heart', '.', 'sometimes', 'i', 'even', 'punch', 'the', 'skin', 'above', 'my', '~', '~', 'heat', '~', '~', 'heart', 'to', 'keep', 'down', 'a', \"'\", 'pang', \"'\", 'of', 'whatever', 'kind', '.', 'i', 'often', 'get', 'these', 'pang', '##s', 'late', 'at', 'night', 'when', 'i', \"'\", 'm', 'alone', ',', 'and', 'they', 'are', 'accompanied', 'by', 'a', 'strange', 'kind', 'of', 'sadness', 'or', 'existent', '##ial', 'worrying', ',', 'including', 'uncomfortable', 'intro', '##sp', '##ection', 'and', 'sensitivity', 'to', 'my', 'own', 'emotions', '.', 'can', 'anyone', 'else', 'relate', '?', 'i', 'feel', 'like', 'this', 'is', 'related', 'to', 'ad', '##hd', 'in', 'that', 'it', 'involves', 'the', 'inability', 'to', 'physically', 'sur', '##press', 'emotion', ',', 'impulse', '##s', ',', 'motor', 'functions', ',', 'etc', '.', 'edit', ':', 'heart', ',', 'not', 'heat', '.', 'i', 'hope', 'that', 'didn', \"'\", 't', 'make', 'my', 'post', 'inadvertently', 'obscene', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['finally', 'back', 'on', 'my', 'med', '##s', '.', '.', '.', 'i', 'am', 'back', 'on', 'track', '.', '.', '.', 'got', 'an', 'earlier', 'appointment', 'out', 'of', 'the', 'blue', 'since', 'someone', 'cancelled', 'and', 'it', 'was', 'last', 'minute', 'so', 'i', 'took', 'it', ',', 'got', 'my', 'assessment', 'done', 'and', 'bam', '.', 'that', 'sense', 'of', 'normal', '##cy', 'is', 'kicking', 'in', 'again', '.', 'it', \"'\", 's', 'not', 'quite', 'where', 'i', 'would', 'like', 'it', 'yet', ',', 'but', 'i', 'do', 'feel', 'my', 'medication', 'kicking', 'in', 'again', 'and', 'it', \"'\", 's', 'so', 'wonderful', 'to', 'get', 'shit', 'done', 'in', 'a', 'reasonable', 'amount', 'of', 'time', 'today', '!', 'what', 'i', 'am', 'a', 'little', 'amazed', 'at', 'is', 'how', 'little', 'i', 'was', 'drilled', 'about', 'it', '.', 'i', 'was', 'asked', 'a', 'lot', 'of', 'questions', ',', 'but', 'i', 'thought', 'i', 'might', 'have', 'been', 'pro', '##dded', 'a', 'lot', 'more', '.', 'i', 'did', 'manage', 'to', 'give', 'a', 'pretty', 'detailed', 'history', 'fairly', 'quickly', 'so', 'that', 'might', 'have', 'cut', 'off', 'a', 'lot', 'of', 'other', 'questions', '.', 'also', 'now', 'that', 'i', 'have', 'my', 'assessment', ',', 'my', 'pc', '##p', 'is', 'willing', 'to', 'pre', '##scribe', 'so', 'i', 'don', \"'\", 't', 'have', 'to', 'go', 'back', 'so', 'much', 'to', 'the', 'psychiatrist', ',', 'which', 'can', 'be', 'ridiculous', '##ly', 'more', 'expensive', 'than', 'a', 'standard', 'pc', '##p', 'visit', '.', 'i', 'apologize', 'if', 'i', 'sound', 'over', '-', 'excited', ',', 'but', 'i', \"'\", 've', 'been', 'waiting', 'nearly', 'four', 'months', 'now', 'to', 'get', 'an', 'assessment', 'done', 'at', 'an', 'office', 'my', 'insurance', 'covers', 'and', 'it', \"'\", 's', 'been', 'hell', 'as', 'a', 'result', '.', 'it', \"'\", 's', 'great', 'to', 'finally', 'get', 'some', 'relief', 'and', 'get', 'back', 'on', 'track', '.', ':', '-', ')']\n",
      "INFO:__main__:Number of tokens: 257\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['finally', 'back', 'on', 'my', 'med', '##s', '.', '.', '.', 'i', 'am', 'back', 'on', 'track', '.', '.', '.', 'got', 'an', 'earlier', 'appointment', 'out', 'of', 'the', 'blue', 'since', 'someone', 'cancelled', 'and', 'it', 'was', 'last', 'minute', 'so', 'i', 'took', 'it', ',', 'got', 'my', 'assessment', 'done', 'and', 'bam', '.', 'that', 'sense', 'of', 'normal', '##cy', 'is', 'kicking', 'in', 'again', '.', 'it', \"'\", 's', 'not', 'quite', 'where', 'i', 'would', 'like', 'it', 'yet', ',', 'but', 'i', 'do', 'feel', 'my', 'medication', 'kicking', 'in', 'again', 'and', 'it', \"'\", 's', 'so', 'wonderful', 'to', 'get', 'shit', 'done', 'in', 'a', 'reasonable', 'amount', 'of', 'time', 'today', '!', 'what', 'i', 'am', 'a', 'little', 'amazed', 'at', 'is', 'how', 'little', 'i', 'was', 'drilled', 'about', 'it', '.', 'i', 'was', 'asked', 'a', 'lot', 'of', 'questions', ',', 'but', 'i', 'thought', 'i', 'might', 'have', 'been', 'pro', '##dded', 'a', 'lot', 'more', '.', 'i', 'did', 'manage', 'to', 'give', 'a', 'pretty', 'detailed', 'history', 'fairly', 'quickly', 'so', 'that', 'might', 'have', 'cut', 'off', 'a', 'lot', 'of', 'other', 'questions', '.', 'also', 'now', 'that', 'i', 'have', 'my', 'assessment', ',', 'my', 'pc', '##p', 'is', 'willing', 'to', 'pre', '##scribe', 'so', 'i', 'don', \"'\", 't', 'have', 'to', 'go', 'back', 'so', 'much', 'to', 'the', 'psychiatrist', ',', 'which', 'can', 'be', 'ridiculous', '##ly', 'more', 'expensive', 'than', 'a', 'standard', 'pc', '##p', 'visit', '.', 'i', 'apologize', 'if', 'i', 'sound', 'over', '-', 'excited', ',', 'but', 'i', \"'\", 've', 'been', 'waiting', 'nearly', 'four', 'months', 'now', 'to', 'get', 'an', 'assessment', 'done', 'at', 'an', 'office', 'my', 'insurance', 'covers', 'and', 'it', \"'\", 's', 'been', 'hell', 'as', 'a', 'result', '.', 'it', \"'\", 's', 'great', 'to', 'finally', 'get', 'some', 'relief', 'and', 'get', 'back', 'on', 'track', '.', ':', '-', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'keep', 'your', 'mind', 'positive', '?', 'i', \"'\", 'm', 'med', '##icated', 'on', 'add', '##eral', '##l', ',', 'been', 'diagnosed', 'since', 'this', 'summer', ',', 'but', 'i', \"'\", 've', 'never', 'been', 'referred', 'to', 'a', 'full', 'psychiatrist', '(', 'my', 'family', 'dr', 'diagnosed', 'me', ',', 'this', 'is', 'canada', ')', '.', 'i', 'notice', 'add', '##eral', '##l', 'makes', 'me', 'at', 'times', 'excited', 'for', 'the', 'future', ',', 'and', 'then', 'sometimes', 'down', '##right', 'depressed', 'and', 'sad', '.', 'how', 'do', 'you', 'cope', 'with', 'this', '?', 'i', \"'\", 've', 'had', 'to', 'use', 'my', 'girlfriend', 'as', 'an', 'emotional', 'cr', '##ut', '##ch', ',', 'and', 'i', 'don', \"'\", 't', 'want', 'to', 'keep', 'doing', 'this', ',', 'i', 'can', \"'\", 't', 'expect', 'its', 'easy', 'on', 'her', 'at', 'all', '.', 'how', 'do', 'the', 'rest', 'of', 'you', 'cope', 'with', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 125\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'keep', 'your', 'mind', 'positive', '?', 'i', \"'\", 'm', 'med', '##icated', 'on', 'add', '##eral', '##l', ',', 'been', 'diagnosed', 'since', 'this', 'summer', ',', 'but', 'i', \"'\", 've', 'never', 'been', 'referred', 'to', 'a', 'full', 'psychiatrist', '(', 'my', 'family', 'dr', 'diagnosed', 'me', ',', 'this', 'is', 'canada', ')', '.', 'i', 'notice', 'add', '##eral', '##l', 'makes', 'me', 'at', 'times', 'excited', 'for', 'the', 'future', ',', 'and', 'then', 'sometimes', 'down', '##right', 'depressed', 'and', 'sad', '.', 'how', 'do', 'you', 'cope', 'with', 'this', '?', 'i', \"'\", 've', 'had', 'to', 'use', 'my', 'girlfriend', 'as', 'an', 'emotional', 'cr', '##ut', '##ch', ',', 'and', 'i', 'don', \"'\", 't', 'want', 'to', 'keep', 'doing', 'this', ',', 'i', 'can', \"'\", 't', 'expect', 'its', 'easy', 'on', 'her', 'at', 'all', '.', 'how', 'do', 'the', 'rest', 'of', 'you', 'cope', 'with', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'wife', 'texted', 'this', 'picture', 'to', 'me', 'today', '.', 'fits', 'me', 'like', 'a', 'glove', '.']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'wife', 'texted', 'this', 'picture', 'to', 'me', 'today', '.', 'fits', 'me', 'like', 'a', 'glove', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'lived', 'in', 'denial', 'about', 'your', 'ad', '##hd', ',', 'even', 'when', 'diagnosed', ',', 'until', 'having', 'a', 'moment', 'of', 'clarity', 'where', 'you', 'thought', ':', 'it', \"'\", 's', 'impossible', 'i', 'don', \"'\", 't', 'have', 'it', 'i', 'have', 'been', 'diagnosed', 'since', '2009', 'but', 'only', 'after', 'one', 'accounting', 'exam', 'where', 'i', 'couldn', \"'\", 't', 'focus', 'at', 'all', ',', 'it', 'finally', 'hit', 'me', ':', 'dan', '##g', ',', 'this', 'thing', 'is', 'real', 'and', 'i', 'gotta', 'live', 'with', 'it', 'for', 'the', 'rest', 'of', 'my', 'life', '.']\n",
      "INFO:__main__:Number of tokens: 80\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'lived', 'in', 'denial', 'about', 'your', 'ad', '##hd', ',', 'even', 'when', 'diagnosed', ',', 'until', 'having', 'a', 'moment', 'of', 'clarity', 'where', 'you', 'thought', ':', 'it', \"'\", 's', 'impossible', 'i', 'don', \"'\", 't', 'have', 'it', 'i', 'have', 'been', 'diagnosed', 'since', '2009', 'but', 'only', 'after', 'one', 'accounting', 'exam', 'where', 'i', 'couldn', \"'\", 't', 'focus', 'at', 'all', ',', 'it', 'finally', 'hit', 'me', ':', 'dan', '##g', ',', 'this', 'thing', 'is', 'real', 'and', 'i', 'gotta', 'live', 'with', 'it', 'for', 'the', 'rest', 'of', 'my', 'life', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'use', 'the', 'television', 'analogy', ',', 'how', 'my', 'brain', 'feels']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'use', 'the', 'television', 'analogy', ',', 'how', 'my', 'brain', 'feels']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['doing', 'a', 'task', 'better', 'without', 'actually', 'thinking', 'about', 'it', '?', 'hey', 'guys', ',', 'is', 'anyone', 'else', 'able', 'to', 'do', 'a', 'physical', 'task', 'better', 'without', 'actually', 'thinking', 'about', 'doing', 'it', '?', 'it', \"'\", 's', 'hard', 'to', 'explain', 'but', 'for', 'example', 'i', \"'\", 'm', 'doing', 'a', 'typing', 'test', 'i', 'find', 'that', 'i', 'type', 'better', 'if', 'i', 'focus', 'on', 'something', 'else', 'instead', 'of', 'the', 'typing', '.', 'not', 'sure', 'if', 'this', 'is', 'an', 'ad', '##hd', 'thing', ',', 'it', \"'\", 's', 'just', 'something', 'that', 'i', 'noticed', '.', 'thanks', 'edit', ':', 'if', 'i', 'actually', 'think', 'about', 'typing', 'i', 'end', 'up', 'doing', 'the', 'test', 'horribly', '.']\n",
      "INFO:__main__:Number of tokens: 99\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['doing', 'a', 'task', 'better', 'without', 'actually', 'thinking', 'about', 'it', '?', 'hey', 'guys', ',', 'is', 'anyone', 'else', 'able', 'to', 'do', 'a', 'physical', 'task', 'better', 'without', 'actually', 'thinking', 'about', 'doing', 'it', '?', 'it', \"'\", 's', 'hard', 'to', 'explain', 'but', 'for', 'example', 'i', \"'\", 'm', 'doing', 'a', 'typing', 'test', 'i', 'find', 'that', 'i', 'type', 'better', 'if', 'i', 'focus', 'on', 'something', 'else', 'instead', 'of', 'the', 'typing', '.', 'not', 'sure', 'if', 'this', 'is', 'an', 'ad', '##hd', 'thing', ',', 'it', \"'\", 's', 'just', 'something', 'that', 'i', 'noticed', '.', 'thanks', 'edit', ':', 'if', 'i', 'actually', 'think', 'about', 'typing', 'i', 'end', 'up', 'doing', 'the', 'test', 'horribly', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['r', '/', 'ad', '##hd', 'lets', 'have', 'a', 'ran', '##t', 'thread', '!', 'whoever', 'wants', 'to', 'come', 'here', 'to', 'vent', 'their', 'anger', 'and', 'frustration', ',', 'release', 'some', 'pressure', 'or', 'relieve', 'some', 'stress', 'is', 'welcome', '!', 'as', 'it', 'goes', 'with', 'ad', '##hd', ',', 'most', 'of', 'us', 'are', 'very', 'imp', '##ulsive', 'and', 'have', 'a', 'short', 'temper', 'that', 'we', 'have', 'to', 'deal', 'with', 'every', 'day', '.', 'we', 'rep', '##ress', 'our', 'anger', 'and', 'bottle', 'up', 'our', 'rage', 'every', 'day', ',', 'but', 'now', 'it', \"'\", 's', 'time', 'to', 'release', 'some', 'of', 'that', 'pressure', 'for', 'the', 'sake', 'of', 'our', 'minds', \"'\", 'healthy', 'function', '!', 'let', 'me', 'begin', 'with', 'an', 'opening', 'mono', '##logue', '.', '.', '.', 'fuck', 'ignorant', 'people', ',', 'fuck', 'slow', 'moving', ',', 'fuck', 'long', 'waiting', ',', 'fuck', 'boring', 'classes', ',', 'fuck', '\"', 'socially', 'convenient', '\"', 'day', '>', 'night', 'working', 'regime', ',', 'fuck', 'stupid', 'simple', '-', 'minded', 'education', 'systems', ',', 'fuck', 'ce', '##nsor', '##ing', 'of', 'freedom', 'of', 'speech', '!', 'and', 'fuck', 'much', 'more', '!', 'i', 'will', 'be', 'sure', 'to', 'join', 'further', 'conversation', '!', 'ran', '##t', 'on', '!', 'edit', ':', 'tagged', 'as', 'ns', '##f', '##w', 'as', 'i', 'should', 'have', 'done', 'immediately', '.', 'sorry', 'for', 'not', 'having', 'done', 'that', ',', 'didn', \"'\", 't', 'know', 'how', 'to', 'tag', 'while', 'making', 'the', 'post', 'and', 'forgot', 'to', 'tag', 'immediately', 'after', ',', 'memory', '-', 'amir', '##ite', '?', '!', 'edit', ':', '[', 'important', 'link', 'for', 'everyone', 'that', 'came', 'across', 'this', 'post', 'who', 'dislike', '##s', 'swearing', '!', ']', '(', 'http', ':', '/', '/', '2', '.', 'bp', '.', 'blogs', '##pot', '.', 'com', '/', '_', '4', '##iq', '##am', '##wa', '##gn', '##1', '##w', '/', 'tea', '##l', '_', 'bane', '##ni', '/', 'aaa', '##aa', '##aa', '##ax', '##hm', '/', 'l', '##j', '-', 'mr', '##av', '##dl', '##wk', '/', 's', '##400', '/', 'nothing', '_', 'to', '_', 'see', '_', 'here', '.', 'jp', '##g', ')']\n",
      "INFO:__main__:Number of tokens: 289\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['r', '/', 'ad', '##hd', 'lets', 'have', 'a', 'ran', '##t', 'thread', '!', 'whoever', 'wants', 'to', 'come', 'here', 'to', 'vent', 'their', 'anger', 'and', 'frustration', ',', 'release', 'some', 'pressure', 'or', 'relieve', 'some', 'stress', 'is', 'welcome', '!', 'as', 'it', 'goes', 'with', 'ad', '##hd', ',', 'most', 'of', 'us', 'are', 'very', 'imp', '##ulsive', 'and', 'have', 'a', 'short', 'temper', 'that', 'we', 'have', 'to', 'deal', 'with', 'every', 'day', '.', 'we', 'rep', '##ress', 'our', 'anger', 'and', 'bottle', 'up', 'our', 'rage', 'every', 'day', ',', 'but', 'now', 'it', \"'\", 's', 'time', 'to', 'release', 'some', 'of', 'that', 'pressure', 'for', 'the', 'sake', 'of', 'our', 'minds', \"'\", 'healthy', 'function', '!', 'let', 'me', 'begin', 'with', 'an', 'opening', 'mono', '##logue', '.', '.', '.', 'fuck', 'ignorant', 'people', ',', 'fuck', 'slow', 'moving', ',', 'fuck', 'long', 'waiting', ',', 'fuck', 'boring', 'classes', ',', 'fuck', '\"', 'socially', 'convenient', '\"', 'day', '>', 'night', 'working', 'regime', ',', 'fuck', 'stupid', 'simple', '-', 'minded', 'education', 'systems', ',', 'fuck', 'ce', '##nsor', '##ing', 'of', 'freedom', 'of', 'speech', '!', 'and', 'fuck', 'much', 'more', '!', 'i', 'will', 'be', 'sure', 'to', 'join', 'further', 'conversation', '!', 'ran', '##t', 'on', '!', 'edit', ':', 'tagged', 'as', 'ns', '##f', '##w', 'as', 'i', 'should', 'have', 'done', 'immediately', '.', 'sorry', 'for', 'not', 'having', 'done', 'that', ',', 'didn', \"'\", 't', 'know', 'how', 'to', 'tag', 'while', 'making', 'the', 'post', 'and', 'forgot', 'to', 'tag', 'immediately', 'after', ',', 'memory', '-', 'amir', '##ite', '?', '!', 'edit', ':', '[', 'important', 'link', 'for', 'everyone', 'that', 'came', 'across', 'this', 'post', 'who', 'dislike', '##s', 'swearing', '!', ']', '(', 'http', ':', '/', '/', '2', '.', 'bp', '.', 'blogs', '##pot', '.', 'com', '/', '_', '4', '##iq', '##am', '##wa', '##gn', '##1', '##w', '/', 'tea', '##l', '_', 'bane', '##ni', '/', 'aaa', '##aa', '##aa', '##ax', '##hm', '/', 'l', '##j', '-', 'mr', '##av', '##dl', '##wk', '/', 's', '##400', '/', 'nothing', '_', 'to', '_', 'see', '_', 'here', '.', 'jp', '##g', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['concert', '##a', 'to', 'v', '##y', '##vance', 'was', 'on', 'concert', '##a', 'for', 'a', 'month', ',', 'now', 'switched', 'to', 'v', '##y', '##vance', 'because', 'of', 'the', 'headache', '##s', 'caused', 'by', 'concert', '##a', '.', 'anyone', 'else', 'made', 'the', 'same', 'switch', '?', 'what', 'should', 'i', 'suspect', '?']\n",
      "INFO:__main__:Number of tokens: 42\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['concert', '##a', 'to', 'v', '##y', '##vance', 'was', 'on', 'concert', '##a', 'for', 'a', 'month', ',', 'now', 'switched', 'to', 'v', '##y', '##vance', 'because', 'of', 'the', 'headache', '##s', 'caused', 'by', 'concert', '##a', '.', 'anyone', 'else', 'made', 'the', 'same', 'switch', '?', 'what', 'should', 'i', 'suspect', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['have', 'you', 'told', 'your', 'employer', 'about', 'your', 'attention', 'disorder', '?', 'why', 'or', 'why', 'not', '?', 'how', 'did', 'it', 'effect', 'your', 'daily', 'work', '-', 'life', '?', 'i', \"'\", 've', 'gotten', 'to', 'the', 'point', 'where', 'i', 'haven', \"'\", 't', 'felt', 'the', 'need', 'to', 'tell', 'my', 'employer', 'because', 'i', \"'\", 'm', 'able', 'to', 'keep', 'my', 'add', \"'\", 's', 'impact', 'on', 'my', 'work', '-', 'life', 'to', 'a', 'minimum', '.', 'this', 'has', 'been', 'through', 'a', 'mixture', 'of', 'practiced', 'self', '-', 'discipline', 'and', 'positive', 'guilt', '-', '/', 'fear', '-', 'based', 'motivation', '(', 'st', '##ri', '##ving', 'to', 'succeed', ')', ',', 'and', 'it', \"'\", 's', 'only', 'in', 'the', 'last', 'year', 'or', 'so', '(', 'i', 'had', 'new', 'jobs', 'in', 'june', ',', 'december', 'and', 'now', 'february', ')', 'that', 'i', \"'\", 've', 'stopped', 'mentioning', 'it', 'to', 'my', 'supervisors', '.']\n",
      "INFO:__main__:Number of tokens: 128\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['have', 'you', 'told', 'your', 'employer', 'about', 'your', 'attention', 'disorder', '?', 'why', 'or', 'why', 'not', '?', 'how', 'did', 'it', 'effect', 'your', 'daily', 'work', '-', 'life', '?', 'i', \"'\", 've', 'gotten', 'to', 'the', 'point', 'where', 'i', 'haven', \"'\", 't', 'felt', 'the', 'need', 'to', 'tell', 'my', 'employer', 'because', 'i', \"'\", 'm', 'able', 'to', 'keep', 'my', 'add', \"'\", 's', 'impact', 'on', 'my', 'work', '-', 'life', 'to', 'a', 'minimum', '.', 'this', 'has', 'been', 'through', 'a', 'mixture', 'of', 'practiced', 'self', '-', 'discipline', 'and', 'positive', 'guilt', '-', '/', 'fear', '-', 'based', 'motivation', '(', 'st', '##ri', '##ving', 'to', 'succeed', ')', ',', 'and', 'it', \"'\", 's', 'only', 'in', 'the', 'last', 'year', 'or', 'so', '(', 'i', 'had', 'new', 'jobs', 'in', 'june', ',', 'december', 'and', 'now', 'february', ')', 'that', 'i', \"'\", 've', 'stopped', 'mentioning', 'it', 'to', 'my', 'supervisors', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['fe', '##mini', '##nity', ',', 'women', ',', 'and', 'ad', '##hd', ':', 'too', 'often', 'when', 'i', 'seek', 'out', 'resources', 'on', 'attention', 'deficit', 'hyper', '##act', '##ivity', 'disorder', '(', 'ad', '##hd', ')', ',', 'i', 'come', 'across', 'stories', 'of', 'children', ',', 'adolescents', ',', 'and', 'grown', 'men', 'who', 'have', 'struggled', 'with', 'the', 'symptoms', 'of', 'ad', '##hd', '.', 'but', 'what', 'of', 'we', 'women', '?']\n",
      "INFO:__main__:Number of tokens: 57\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['fe', '##mini', '##nity', ',', 'women', ',', 'and', 'ad', '##hd', ':', 'too', 'often', 'when', 'i', 'seek', 'out', 'resources', 'on', 'attention', 'deficit', 'hyper', '##act', '##ivity', 'disorder', '(', 'ad', '##hd', ')', ',', 'i', 'come', 'across', 'stories', 'of', 'children', ',', 'adolescents', ',', 'and', 'grown', 'men', 'who', 'have', 'struggled', 'with', 'the', 'symptoms', 'of', 'ad', '##hd', '.', 'but', 'what', 'of', 'we', 'women', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['diagnosed', ',', 'then', 'un', '-', 'diagnosed', ':', 'should', 'i', 'get', 'a', 'third', 'opinion', 'about', 'my', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['diagnosed', ',', 'then', 'un', '-', 'diagnosed', ':', 'should', 'i', 'get', 'a', 'third', 'opinion', 'about', 'my', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dealing', 'with', 'anger', 'i', 'was', 'curious', 'as', 'to', 'how', 'others', 'here', 'dealt', 'with', 'anger', '.', 'i', \"'\", 'm', 'not', 'typically', 'an', 'angry', 'person', ',', 'but', 'every', 'once', 'in', 'a', 'while', ',', 'i', 'just', 'get', 'overwhelmed', 'with', 'this', 'anger', 'that', 'i', 'like', 'to', 'call', '\"', 'quiet', 'anger', '\"', '.', 'i', \"'\", 'm', 'not', 'outward', '##ly', 'angry', ',', 'but', 'i', 'feel', 'like', 'i', \"'\", 'm', 'going', 'to', 'snap', 'and', 'all', 'i', 'want', 'to', 'do', 'is', 'go', 'sit', 'in', 'a', 'quiet', 'room', 'by', 'myself', 'because', 'i', 'have', 'become', 'fed', 'up', 'with', 'people', '.', 'if', 'that', 'isn', \"'\", 't', 'possible', ',', 'i', 'get', 'the', 'overwhelming', 'urge', 'to', 'hit', 'things', '(', 'which', 'i', 'have', 'avoided', 'up', 'to', 'this', 'point', ')', '.', 'am', 'i', 'the', 'only', 'one', 'who', 'gets', 'angry', 'like', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 127\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dealing', 'with', 'anger', 'i', 'was', 'curious', 'as', 'to', 'how', 'others', 'here', 'dealt', 'with', 'anger', '.', 'i', \"'\", 'm', 'not', 'typically', 'an', 'angry', 'person', ',', 'but', 'every', 'once', 'in', 'a', 'while', ',', 'i', 'just', 'get', 'overwhelmed', 'with', 'this', 'anger', 'that', 'i', 'like', 'to', 'call', '\"', 'quiet', 'anger', '\"', '.', 'i', \"'\", 'm', 'not', 'outward', '##ly', 'angry', ',', 'but', 'i', 'feel', 'like', 'i', \"'\", 'm', 'going', 'to', 'snap', 'and', 'all', 'i', 'want', 'to', 'do', 'is', 'go', 'sit', 'in', 'a', 'quiet', 'room', 'by', 'myself', 'because', 'i', 'have', 'become', 'fed', 'up', 'with', 'people', '.', 'if', 'that', 'isn', \"'\", 't', 'possible', ',', 'i', 'get', 'the', 'overwhelming', 'urge', 'to', 'hit', 'things', '(', 'which', 'i', 'have', 'avoided', 'up', 'to', 'this', 'point', ')', '.', 'am', 'i', 'the', 'only', 'one', 'who', 'gets', 'angry', 'like', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'found', 'this', 'song', '.', 'seemed', 'like', 'something', 'you', 'guys', 'might', 'enjoy', '.']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'found', 'this', 'song', '.', 'seemed', 'like', 'something', 'you', 'guys', 'might', 'enjoy', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'good', 'are', 'you', 'guys', 'at', 'keeping', 'interested', 'in', 'lt', '##r', \"'\", 's', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'good', 'are', 'you', 'guys', 'at', 'keeping', 'interested', 'in', 'lt', '##r', \"'\", 's', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'am', '*', 'this', '##cl', '##ose', '*', 'to', 'being', 'fired', 'from', 'my', 'job', 'for', 'arriving', 'late', 'too', 'often', '.', '.', '.', 'i', 'don', \"'\", 't', 'have', 'many', 'more', 'chances', 'left', 'and', 'i', \"'\", 'm', 'terrified', 'because', 'it', 'seems', 'like', 'i', 'am', 'simply', 'incapable', 'of', 'being', 'on', 'time', '.', 'i', 'desperately', 'need', 'ad', '##hd', '-', 'friendly', 'tips', 'on', 'how', 'to', 'become', 'a', 'pun', '##ct', '##ual', 'person', '.', '.', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'the', 'pun', '##ct', '##ual', '##ity', 'gods', 'are', 'against', 'me', '.', 'i', 'need', 'help', 'app', '##ea', '##sing', 'them', 'so', 'i', 'can', 'get', 'to', 'work', 'on', 'time', 'and', 'remain', 'employed', '.', '*', '*', '*', 'whoa', '!', 'hey', 'there', ',', 'novel', '-', 'length', 'wall', 'of', 'text', '!', 'sorry', 'about', 'that', ',', 'you', 'guys', '.', 'i', 'got', 'a', 'bit', 'carried', 'away', 'but', 'vent', '##ing', 'about', 'it', 'felt', 'good', '.', 'while', 'i', 'tried', 'to', 'make', 'it', 'a', 'mildly', '-', 'entertaining', 'read', ',', 'don', \"'\", 't', 'worry', '-', 'i', 'won', \"'\", 't', 'force', 'you', 'to', 'read', 'it', 'all', '.', 'the', 't', '##l', ';', 'dr', 'gives', 'you', 'enough', 'of', 'a', 'gi', '##st', 'of', 'what', 'i', \"'\", 'm', 'looking', 'for', '.', 'the', 'rest', 'is', 'just', 'the', 'details', '(', 'though', 'i', 'imagine', 'many', 'of', 'you', 'can', 'relate', '.', '.', '.', ')', '.', '*', 'i', 'hate', '\"', 'time', '\"', '.', 'it', 'always', 'seems', 'to', 'fuck', 'me', 'up', 'and', 'i', 'feel', 'like', 'i', \"'\", 'm', 'constantly', 'running', 'after', 'it', ',', 'just', 'trying', 'to', 'keep', 'up', '(', 'and', 'usually', 'failing', 'at', 'doing', 'so', ')', '.', 'i', 'just', 'don', \"'\", 't', 'get', 'it', '.', 'i', 'don', \"'\", 't', 'understand', 'why', 'the', 'hell', 'it', 'is', 'so', 'fucking', 'hard', 'for', 'me', 'to', 'just', 'arrive', 'somewhere', 'on', 'time', '.', 'there', 'always', 'seems', 'to', 'be', 'something', 'holding', 'me', 'back', 'or', 'getting', 'in', 'the', 'way', ',', 'whether', 'it', \"'\", 's', 'my', 'own', 'in', '##ept', 'brain', 'or', 'external', 'factors', 'beyond', 'my', 'control', '.', 'so', ',', 'this', 'de', '##bility', 'of', 'mine', 'naturally', 'affects', 'my', 'work', 'life', 'as', 'well', ',', 'and', 'my', 'boss', 'is', 'fed', 'up', '.', 'now', ',', 'it', \"'\", 's', 'not', 'like', 'i', \"'\", 'm', 'regularly', 'arriving', '30', 'minute', 'to', 'an', 'hour', 'late', '.', 'i', \"'\", 'd', 'say', 'my', 'average', 'is', '6', '-', '7', 'minutes', 'late', ',', 'but', 'nevertheless', ',', 'this', 'is', 'not', 'okay', '.', 'i', 'need', 'to', 'arrive', '0', 'minutes', 'late', '.', 'my', 'god', ',', 'i', 'hate', 'minutes', '.', 'and', 'time', '.', 'now', ',', 'once', 'i', \"'\", 'm', 'at', 'work', ',', 'i', \"'\", 've', 'often', 'been', 'told', 'by', 'numerous', 'people', '(', 'including', 'my', 'boss', ')', 'that', 'my', 'performance', 'is', 'stellar', 'and', 'that', 'i', 'go', 'above', 'and', 'beyond', 'my', 'duties', '.', 'i', 'am', 'trained', 'in', 'a', 'wide', 'variety', 'of', 'tasks', 'around', 'the', 'office', ',', 'and', 'therefore', 'i', 'am', 'the', 'go', '-', 'to', 'substitute', 'for', 'a', 'number', 'of', 'positions', 'whenever', 'that', 'assigned', 'person', 'is', 'out', 'of', 'the', 'office', 'for', 'whatever', 'reason', '.', 'i', 'am', 'constantly', 'cleaning', 'up', 'and', 'fixing', 'other', 'people', \"'\", 's', 'mistakes', '.', 'i', 'train', 'new', 'hires', 'because', 'i', \"'\", 'm', 'good', 'at', 'training', 'and', 'teaching', 'people', '.', 'this', 'is', 'why', 'they', \"'\", 've', 'kept', 'me', 'around', 'for', 'so', 'long', 'despite', 'my', 'tar', '##dine', '##ss', '.', 'but', 'for', 'some', 'reason', ',', 'all', 'of', 'that', 'matters', 'less', 'than', 'my', 'arrival', 'time', '.', 'and', 'i', 'can', 'assure', 'you', 'that', 'my', 'job', 'is', 'in', 'no', 'way', '\"', 'time', '-', 'sensitive', '\"', '.', 'it', \"'\", 's', 'a', 'lot', 'of', 'standing', 'around', 'and', 'waiting', 'around', 'for', 'someone', 'to', 'give', 'me', 'something', 'to', 'do', '.', 'i', 'wish', 'my', 'boss', 'wasn', \"'\", 't', 'such', 'an', 'enthusiast', 'of', 'pun', '##ct', '##ual', '##ity', ',', 'but', 'he', 'is', '.', 'i', 'haven', \"'\", 't', 'told', 'him', 'about', 'my', 'ad', '##hd', 'because', 'he', \"'\", 's', 'one', 'of', 'those', 'people', 'who', 'wouldn', \"'\", 't', 'understand', 'or', 'sy', '##mp', '##athi', '##ze', '.', 'he', 'thinks', 'i', \"'\", 'm', 'just', 'not', 'trying', 'hard', 'enough', 'and', 'that', 'i', 'don', '##t', 'care', ',', 'but', 'the', 'reality', 'of', 'it', 'is', 'that', 'i', 'damn', 'near', 'have', 'a', 'panic', 'attack', 'every', 'morning', 'in', 'the', '30', '-', '45', 'minutes', 'before', 'i', 'have', 'to', 'leave', 'for', 'work', 'because', 'i', \"'\", 'm', 'freaking', 'out', 'about', 'being', 'late', '.', 'it', 'literally', 'gives', 'me', 'the', 'shit', '##s', ',', 'for', 'fuck', '##s', 'sake', '(', 'which', 'is', 'often', 'a', 'reason', 'for', 'my', 'late', '##ness', ')', '.', 'i', 'feel', 'like', 'his', 'focusing', 'on', 'my', 'being', 'on', 'time', 'and', 'hound', '##ing', 'me', 'about', 'it', 'only', 'makes', 'it', 'worse', '.', 'i', \"'\", 'll', 'arrive', 'to', 'work', 'on', 'time', 'for', 'a', 'few', 'days', 'after', 'having', 'a', 'serious', 'talk', 'with', 'my', 'boss', ',', 'but', 'i', 'inevitably', 'slip', 'for', 'some', 'reason', 'and', 'find', 'myself', 'right', 'back', 'to', 'where', 'i', 'was', '-', '7', 'minutes', 'late', '.', 'and', 'now', 'my', 'boss', 'is', 'fed', 'up', '.', 'i', 'can', \"'\", 't', 'be', 'late', 'again', '.', 'i', 'need', 'help', '.', '*', '*', 'what', 'the', 'fuck', 'do', 'i', 'do', ',', 'my', 'fellow', 'ad', '##hd', 'comrades', '?', 'some', 'of', 'the', 'strategies', 'i', \"'\", 've', 'tried', 'so', 'far', 'include', 'the', 'following', ':', '*', '*', '-', 'i', 'set', 'alarms', 'that', 'go', 'off', 'every', '15', 'minutes', '(', 'and', 'every', 'five', 'minutes', 'the', 'last', 'quarter', '-', 'hour', ')', 'leading', 'up', 'to', 'the', 'time', 'i', 'need', 'to', 'get', 'out', 'the', 'door', ',', 'but', 'i', 'always', 'end', 'up', 'having', 'to', 'do', '/', 'find', '/', 'get', 'just', 'one', 'more', 'thing', '.', '-', 'i', 'try', 'to', 'gather', 'all', 'my', 'shit', 'the', 'night', 'before', 'so', 'it', \"'\", 's', 'all', 'ready', 'to', 'go', 'but', 'that', 'works', 'for', ',', 'like', ',', 'a', 'day', 'before', 'i', 'forget', 'and', 'mind', '##lessly', '(', 'and', 'non', '##sen', '##sic', '##ally', ')', 'spread', 'around', 'my', 'shit', 'like', 'butter', 'on', 'toast', '.', 'or', 'i', \"'\", 'll', 'need', 'something', 'in', 'my', 'neatly', 'organized', 'to', '-', 'go', 'pile', 'then', 'forget', 'to', 'put', 'it', 'back', 'and', 'by', 'the', 'next', 'day', 'when', 'it', \"'\", 's', 'time', 'to', 'go', ',', 'i', 'forgot', 'what', 'i', 'did', 'with', 'it', 'because', 'odds', 'are', 'i', 'strolled', 'around', 'the', 'house', 'with', 'it', 'in', 'hand', 'en', 'route', 'to', 'god', '-', 'knows', '-', 'where', ',', 'then', 'set', 'it', 'down', 'in', 'the', 'least', 'logical', 'place', 'possible', '.', '-', 'my', 'clocks', 'are', 'set', '5', '-', '20', 'minutes', 'early', ',', 'but', 'i', \"'\", 'm', 'no', 'fool', 'and', 'i', 'can', 'easily', 'check', 'my', 'phone', 'to', 'see', 'the', 'accurate', 'time', '.', '-', 'i', 'think', 'about', 'possible', 'consequences', 'for', 'arriving', 'late', 'to', 'work', 'but', 'that', 'just', 'gets', 'me', 'all', 'hot', 'and', 'bothered', '(', 'and', 'not', 'in', 'a', 'good', 'way', ')', 'and', 'stresses', 'me', 'the', 'fuck', 'out', 'which', ',', 'in', 'turn', ',', 'triggers', 'this', 'subconscious', '\"', 'avoidance', '\"', 'minds', '##et', 'and', 'awake', '##ns', 'my', 'bow', '##els', 'at', 'the', 'worst', 'time', 'possible', '.', '-', 'if', 'i', 'do', 'manage', 'to', 'leave', 'on', 'time', ',', 'then', 'odds', 'are', 'good', 'that', 'i', \"'\", 'll', 'run', 'into', 'a', 'funeral', 'procession', 'or', 'an', 'accident', ',', 'or', 'i', \"'\", 'll', 'get', 'stuck', 'behind', 'a', 'vehicle', 'going', '10', 'mph', 'on', 'the', 'single', '-', 'lane', '-', 'each', '-', 'way', 'road', 'to', 'work', ',', 'or', 'the', 'railroad', 'track', 'barriers', 'will', 'be', 'stuck', 'in', 'the', 'down', 'position', 'so', 'no', 'one', 'can', 'proceed', '(', 'that', \"'\", 's', 'happened', 'twice', 'already', ')', ',', 'or', 'the', 'road', 'will', 'be', 'closed', 'due', 'to', 'a', 'swat', 'team', 'raid', 'so', 'i', 'have', 'to', 'turn', 'around', 'and', 'take', 'the', 'longer', 'alternate', 'route', '(', 'it', \"'\", 's', 'happened', 'as', 'well', ')', '.']\n",
      "INFO:__main__:Number of tokens: 1182\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['i', 'am', '*', 'this', '##cl', '##ose', '*', 'to', 'being', 'fired', 'from', 'my', 'job', 'for', 'arriving', 'late', 'too', 'often', '.', '.', '.', 'i', 'don', \"'\", 't', 'have', 'many', 'more', 'chances', 'left', 'and', 'i', \"'\", 'm', 'terrified', 'because', 'it', 'seems', 'like', 'i', 'am', 'simply', 'incapable', 'of', 'being', 'on', 'time', '.', 'i', 'desperately', 'need', 'ad', '##hd', '-', 'friendly', 'tips', 'on', 'how', 'to', 'become', 'a', 'pun', '##ct', '##ual', 'person', '.', '.', '.', '*', '*', 't', '##l', ';', 'dr', ':', 'the', 'pun', '##ct', '##ual', '##ity', 'gods', 'are', 'against', 'me', '.', 'i', 'need', 'help', 'app', '##ea', '##sing', 'them', 'so', 'i', 'can', 'get', 'to', 'work', 'on', 'time', 'and', 'remain', 'employed', '.', '*', '*', '*', 'whoa', '!', 'hey', 'there', ',', 'novel', '-', 'length', 'wall', 'of', 'text', '!', 'sorry', 'about', 'that', ',', 'you', 'guys', '.', 'i', 'got', 'a', 'bit', 'carried', 'away', 'but', 'vent', '##ing', 'about', 'it', 'felt', 'good', '.', 'while', 'i', 'tried', 'to', 'make', 'it', 'a', 'mildly', '-', 'entertaining', 'read', ',', 'don', \"'\", 't', 'worry', '-', 'i', 'won', \"'\", 't', 'force', 'you', 'to', 'read', 'it', 'all', '.', 'the', 't', '##l', ';', 'dr', 'gives', 'you', 'enough', 'of', 'a', 'gi', '##st', 'of', 'what', 'i', \"'\", 'm', 'looking', 'for', '.', 'the', 'rest', 'is', 'just', 'the', 'details', '(', 'though', 'i', 'imagine', 'many', 'of', 'you', 'can', 'relate', '.', '.', '.', ')', '.', '*', 'i', 'hate', '\"', 'time', '\"', '.', 'it', 'always', 'seems', 'to', 'fuck', 'me', 'up', 'and', 'i', 'feel', 'like', 'i', \"'\", 'm', 'constantly', 'running', 'after', 'it', ',', 'just', 'trying', 'to', 'keep', 'up', '(', 'and', 'usually', 'failing', 'at', 'doing', 'so', ')', '.', 'i', 'just', 'don', \"'\", 't', 'get', 'it', '.', 'i', 'don', \"'\", 't', 'understand', 'why', 'the', 'hell', 'it', 'is', 'so', 'fucking', 'hard', 'for', 'me', 'to', 'just', 'arrive', 'somewhere', 'on', 'time', '.', 'there', 'always', 'seems', 'to', 'be', 'something', 'holding', 'me', 'back', 'or', 'getting', 'in', 'the', 'way', ',', 'whether', 'it', \"'\", 's', 'my', 'own', 'in', '##ept', 'brain', 'or', 'external', 'factors', 'beyond', 'my', 'control', '.', 'so', ',', 'this', 'de', '##bility', 'of', 'mine', 'naturally', 'affects', 'my', 'work', 'life', 'as', 'well', ',', 'and', 'my', 'boss', 'is', 'fed', 'up', '.', 'now', ',', 'it', \"'\", 's', 'not', 'like', 'i', \"'\", 'm', 'regularly', 'arriving', '30', 'minute', 'to', 'an', 'hour', 'late', '.', 'i', \"'\", 'd', 'say', 'my', 'average', 'is', '6', '-', '7', 'minutes', 'late', ',', 'but', 'nevertheless', ',', 'this', 'is', 'not', 'okay', '.', 'i', 'need', 'to', 'arrive', '0', 'minutes', 'late', '.', 'my', 'god', ',', 'i', 'hate', 'minutes', '.', 'and', 'time', '.', 'now', ',', 'once', 'i', \"'\", 'm', 'at', 'work', ',', 'i', \"'\", 've', 'often', 'been', 'told', 'by', 'numerous', 'people', '(', 'including', 'my', 'boss', ')', 'that', 'my', 'performance', 'is', 'stellar', 'and', 'that', 'i', 'go', 'above', 'and', 'beyond', 'my', 'duties', '.', 'i', 'am', 'trained', 'in', 'a', 'wide', 'variety', 'of', 'tasks', 'around', 'the', 'office', ',', 'and', 'therefore', 'i', 'am', 'the', 'go', '-', 'to', 'substitute', 'for', 'a', 'number', 'of', 'positions', 'whenever', 'that', 'assigned', 'person', 'is', 'out', 'of', 'the', 'office', 'for', 'whatever', 'reason', '.', 'i', 'am', 'constantly', 'cleaning', 'up', 'and', 'fixing', 'other', 'people', \"'\", 's', 'mistakes', '.', 'i', 'train', 'new', 'hires', 'because', 'i', \"'\", 'm', 'good', 'at', 'training', 'and', 'teaching', 'people', '.', 'this', 'is', 'why', 'they', \"'\", 've', 'kept', 'me', 'around', 'for', 'so', 'long', 'despite', 'my', 'tar', '##dine', '##ss'], ['.', 'but', 'for', 'some', 'reason', ',', 'all', 'of', 'that', 'matters', 'less', 'than', 'my', 'arrival', 'time', '.', 'and', 'i', 'can', 'assure', 'you', 'that', 'my', 'job', 'is', 'in', 'no', 'way', '\"', 'time', '-', 'sensitive', '\"', '.', 'it', \"'\", 's', 'a', 'lot', 'of', 'standing', 'around', 'and', 'waiting', 'around', 'for', 'someone', 'to', 'give', 'me', 'something', 'to', 'do', '.', 'i', 'wish', 'my', 'boss', 'wasn', \"'\", 't', 'such', 'an', 'enthusiast', 'of', 'pun', '##ct', '##ual', '##ity', ',', 'but', 'he', 'is', '.', 'i', 'haven', \"'\", 't', 'told', 'him', 'about', 'my', 'ad', '##hd', 'because', 'he', \"'\", 's', 'one', 'of', 'those', 'people', 'who', 'wouldn', \"'\", 't', 'understand', 'or', 'sy', '##mp', '##athi', '##ze', '.', 'he', 'thinks', 'i', \"'\", 'm', 'just', 'not', 'trying', 'hard', 'enough', 'and', 'that', 'i', 'don', '##t', 'care', ',', 'but', 'the', 'reality', 'of', 'it', 'is', 'that', 'i', 'damn', 'near', 'have', 'a', 'panic', 'attack', 'every', 'morning', 'in', 'the', '30', '-', '45', 'minutes', 'before', 'i', 'have', 'to', 'leave', 'for', 'work', 'because', 'i', \"'\", 'm', 'freaking', 'out', 'about', 'being', 'late', '.', 'it', 'literally', 'gives', 'me', 'the', 'shit', '##s', ',', 'for', 'fuck', '##s', 'sake', '(', 'which', 'is', 'often', 'a', 'reason', 'for', 'my', 'late', '##ness', ')', '.', 'i', 'feel', 'like', 'his', 'focusing', 'on', 'my', 'being', 'on', 'time', 'and', 'hound', '##ing', 'me', 'about', 'it', 'only', 'makes', 'it', 'worse', '.', 'i', \"'\", 'll', 'arrive', 'to', 'work', 'on', 'time', 'for', 'a', 'few', 'days', 'after', 'having', 'a', 'serious', 'talk', 'with', 'my', 'boss', ',', 'but', 'i', 'inevitably', 'slip', 'for', 'some', 'reason', 'and', 'find', 'myself', 'right', 'back', 'to', 'where', 'i', 'was', '-', '7', 'minutes', 'late', '.', 'and', 'now', 'my', 'boss', 'is', 'fed', 'up', '.', 'i', 'can', \"'\", 't', 'be', 'late', 'again', '.', 'i', 'need', 'help', '.', '*', '*', 'what', 'the', 'fuck', 'do', 'i', 'do', ',', 'my', 'fellow', 'ad', '##hd', 'comrades', '?', 'some', 'of', 'the', 'strategies', 'i', \"'\", 've', 'tried', 'so', 'far', 'include', 'the', 'following', ':', '*', '*', '-', 'i', 'set', 'alarms', 'that', 'go', 'off', 'every', '15', 'minutes', '(', 'and', 'every', 'five', 'minutes', 'the', 'last', 'quarter', '-', 'hour', ')', 'leading', 'up', 'to', 'the', 'time', 'i', 'need', 'to', 'get', 'out', 'the', 'door', ',', 'but', 'i', 'always', 'end', 'up', 'having', 'to', 'do', '/', 'find', '/', 'get', 'just', 'one', 'more', 'thing', '.', '-', 'i', 'try', 'to', 'gather', 'all', 'my', 'shit', 'the', 'night', 'before', 'so', 'it', \"'\", 's', 'all', 'ready', 'to', 'go', 'but', 'that', 'works', 'for', ',', 'like', ',', 'a', 'day', 'before', 'i', 'forget', 'and', 'mind', '##lessly', '(', 'and', 'non', '##sen', '##sic', '##ally', ')', 'spread', 'around', 'my', 'shit', 'like', 'butter', 'on', 'toast', '.', 'or', 'i', \"'\", 'll', 'need', 'something', 'in', 'my', 'neatly', 'organized', 'to', '-', 'go', 'pile', 'then', 'forget', 'to', 'put', 'it', 'back', 'and', 'by', 'the', 'next', 'day', 'when', 'it', \"'\", 's', 'time', 'to', 'go', ',', 'i', 'forgot', 'what', 'i', 'did', 'with', 'it', 'because', 'odds', 'are', 'i', 'strolled', 'around', 'the', 'house', 'with', 'it', 'in', 'hand', 'en', 'route', 'to', 'god', '-', 'knows', '-', 'where', ',', 'then', 'set', 'it', 'down', 'in', 'the', 'least', 'logical', 'place', 'possible', '.', '-', 'my', 'clocks', 'are', 'set', '5', '-', '20', 'minutes', 'early', ',', 'but', 'i', \"'\", 'm', 'no', 'fool', 'and', 'i', 'can', 'easily', 'check', 'my', 'phone', 'to', 'see', 'the', 'accurate', 'time', '.', '-', 'i', 'think', 'about', 'possible', 'consequences', 'for', 'arriving', 'late', 'to', 'work', 'but'], ['that', 'just', 'gets', 'me', 'all', 'hot', 'and', 'bothered', '(', 'and', 'not', 'in', 'a', 'good', 'way', ')', 'and', 'stresses', 'me', 'the', 'fuck', 'out', 'which', ',', 'in', 'turn', ',', 'triggers', 'this', 'subconscious', '\"', 'avoidance', '\"', 'minds', '##et', 'and', 'awake', '##ns', 'my', 'bow', '##els', 'at', 'the', 'worst', 'time', 'possible', '.', '-', 'if', 'i', 'do', 'manage', 'to', 'leave', 'on', 'time', ',', 'then', 'odds', 'are', 'good', 'that', 'i', \"'\", 'll', 'run', 'into', 'a', 'funeral', 'procession', 'or', 'an', 'accident', ',', 'or', 'i', \"'\", 'll', 'get', 'stuck', 'behind', 'a', 'vehicle', 'going', '10', 'mph', 'on', 'the', 'single', '-', 'lane', '-', 'each', '-', 'way', 'road', 'to', 'work', ',', 'or', 'the', 'railroad', 'track', 'barriers', 'will', 'be', 'stuck', 'in', 'the', 'down', 'position', 'so', 'no', 'one', 'can', 'proceed', '(', 'that', \"'\", 's', 'happened', 'twice', 'already', ')', ',', 'or', 'the', 'road', 'will', 'be', 'closed', 'due', 'to', 'a', 'swat', 'team', 'raid', 'so', 'i', 'have', 'to', 'turn', 'around', 'and', 'take', 'the', 'longer', 'alternate', 'route', '(', 'it', \"'\", 's', 'happened', 'as', 'well', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'to', 'get', 'going', 'on', 'a', 'task', ':', 'ten', 'tips', 'for', 'success', '!']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'to', 'get', 'going', 'on', 'a', 'task', ':', 'ten', 'tips', 'for', 'success', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['unlock', 'the', 'secrets', 'to', 'your', 'entrepreneur', '##ial', 'brain', 'style']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['unlock', 'the', 'secrets', 'to', 'your', 'entrepreneur', '##ial', 'brain', 'style']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['bipolar', 'ii', 'girl', 'likes', 'add', 'guy', ',', 'advice', 'needed', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['bipolar', 'ii', 'girl', 'likes', 'add', 'guy', ',', 'advice', 'needed', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['did', 'you', 'try', 'meditation', '?', 'did', 'it', 'work', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['did', 'you', 'try', 'meditation', '?', 'did', 'it', 'work', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['huge', 'difficulty', 'studying']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['huge', 'difficulty', 'studying']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'make', 'up', 'a', 'lot', 'of', 'useless', 'rules', 'for', 'myself', '.', 'do', 'you', 'do', 'this', 'too', '?', 'tell', 'me', 'your', 'stories', '!', 'i', 'have', 'add', 'and', 'take', 'medication', 'on', 'school', '##day', '##s', '.', 'as', 'the', 'title', 'tells', ',', 'i', 'make', 'complicated', 'rules', 'up', '.', 'for', 'example', ',', 'if', 'i', 'leave', 'the', 'basement', ',', 'i', 'open', 'wide', 'the', 'door', 'and', 'run', 'up', 'the', 'stairs', '.', 'because', 'it', 'has', 'a', 'door', '-', 'close', '-', 'thing', '##y', ',', 'it', 'closes', 'automatically', ',', 'so', 'i', 'run', 'up', 'the', 'stairs', 'as', 'fast', 'as', 'i', 'can', ',', 'whereby', 'i', 'have', 'to', 'reach', 'at', 'least', 'the', 'middle', 'of', 'the', 'stair', '.', 'or', 'only', 'prime', '##s', 'and', '/', 'or', 'fi', '##bbon', '##ac', '##ci', 'numbers', 'for', 'sound', 'volume', '.', '.', '.', '.', 'etc', '.', 'do', 'you', 'do', 'this', 'too', '?', 'i', 'asked', 'my', 'friends', ',', 'but', 'they', 'say', ',', 'they', 'don', \"'\", 't', 'do', 'this', 'a', 'lot', '.']\n",
      "INFO:__main__:Number of tokens: 149\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'make', 'up', 'a', 'lot', 'of', 'useless', 'rules', 'for', 'myself', '.', 'do', 'you', 'do', 'this', 'too', '?', 'tell', 'me', 'your', 'stories', '!', 'i', 'have', 'add', 'and', 'take', 'medication', 'on', 'school', '##day', '##s', '.', 'as', 'the', 'title', 'tells', ',', 'i', 'make', 'complicated', 'rules', 'up', '.', 'for', 'example', ',', 'if', 'i', 'leave', 'the', 'basement', ',', 'i', 'open', 'wide', 'the', 'door', 'and', 'run', 'up', 'the', 'stairs', '.', 'because', 'it', 'has', 'a', 'door', '-', 'close', '-', 'thing', '##y', ',', 'it', 'closes', 'automatically', ',', 'so', 'i', 'run', 'up', 'the', 'stairs', 'as', 'fast', 'as', 'i', 'can', ',', 'whereby', 'i', 'have', 'to', 'reach', 'at', 'least', 'the', 'middle', 'of', 'the', 'stair', '.', 'or', 'only', 'prime', '##s', 'and', '/', 'or', 'fi', '##bbon', '##ac', '##ci', 'numbers', 'for', 'sound', 'volume', '.', '.', '.', '.', 'etc', '.', 'do', 'you', 'do', 'this', 'too', '?', 'i', 'asked', 'my', 'friends', ',', 'but', 'they', 'say', ',', 'they', 'don', \"'\", 't', 'do', 'this', 'a', 'lot', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'just', 'opened', 'up', 'my', 'report', 'card', ',', 'realized', 'how', 'much', 'better', 'my', 'life', 'has', 'gotten', 'since', 'i', \"'\", 've', 'started', 'taking', 'my', 'medication', 'and', 'almost', 'started', 'crying', '.']\n",
      "INFO:__main__:Number of tokens: 29\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'just', 'opened', 'up', 'my', 'report', 'card', ',', 'realized', 'how', 'much', 'better', 'my', 'life', 'has', 'gotten', 'since', 'i', \"'\", 've', 'started', 'taking', 'my', 'medication', 'and', 'almost', 'started', 'crying', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['internal', 'dial', '##og', 'doesn', \"'\", 't', 'quit', '.', 'any', 'tips', 'for', 'thinking', 'less', '?', 'i', 'seem', 'to', 'have', 'an', 'endless', 'stream', 'of', 'internal', 'dial', '##og', '.', '.', 'most', 'irrelevant', 'and', 'quickly', 'forgotten', 'but', ',', 'still', 'i', 'find', 'that', 'i', 'can', 'only', 'live', 'in', 'the', 'moment', 'and', 'sometimes', 'it', 'feels', 'like', 'i', \"'\", 'm', 'not', 'even', 'living', 'there', 'because', 'there', 'is', 'so', 'much', 'internal', 'dial', '##og', '.', '.']\n",
      "INFO:__main__:Number of tokens: 67\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['internal', 'dial', '##og', 'doesn', \"'\", 't', 'quit', '.', 'any', 'tips', 'for', 'thinking', 'less', '?', 'i', 'seem', 'to', 'have', 'an', 'endless', 'stream', 'of', 'internal', 'dial', '##og', '.', '.', 'most', 'irrelevant', 'and', 'quickly', 'forgotten', 'but', ',', 'still', 'i', 'find', 'that', 'i', 'can', 'only', 'live', 'in', 'the', 'moment', 'and', 'sometimes', 'it', 'feels', 'like', 'i', \"'\", 'm', 'not', 'even', 'living', 'there', 'because', 'there', 'is', 'so', 'much', 'internal', 'dial', '##og', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['federal', 'taxes', 'are', 'due', 'in', 'two', 'days', '-', '-', 'have', 'you', 'filed', 'yours', '?', '(', 'or', 'your', 'extension', '?', ')']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['federal', 'taxes', 'are', 'due', 'in', 'two', 'days', '-', '-', 'have', 'you', 'filed', 'yours', '?', '(', 'or', 'your', 'extension', '?', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['\"', 'going', 'through', 'a', 'rough', 'patch', '?', 'challenge', 'yourself', 'to', 'post', 'a', 'new', 'reason', 'every', 'day', '\"', 'in', '/', 'r', '/', 'reasons', '##to', '##li', '##ve', '.']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['\"', 'going', 'through', 'a', 'rough', 'patch', '?', 'challenge', 'yourself', 'to', 'post', 'a', 'new', 'reason', 'every', 'day', '\"', 'in', '/', 'r', '/', 'reasons', '##to', '##li', '##ve', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'having', 'ad', '##hd', 'a', 'gift', '?', 'do', 'you', 'agree', 'with', 'this', 'man', '?', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '4', '##x', '##pe', '##be', '##9', '##vd', '##w', '##w', '&', 'feature', '=', 'results', '_', 'main', '&', 'play', '##ne', '##xt', '=', '1', '&', 'list', '=', 'pl', '##3', '##cc', '##f', '##16', '##a', '##0', '##5', '##ba', '##6', '##7', '##6', '##7', '##7']\n",
      "INFO:__main__:Number of tokens: 65\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'having', 'ad', '##hd', 'a', 'gift', '?', 'do', 'you', 'agree', 'with', 'this', 'man', '?', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', '4', '##x', '##pe', '##be', '##9', '##vd', '##w', '##w', '&', 'feature', '=', 'results', '_', 'main', '&', 'play', '##ne', '##xt', '=', '1', '&', 'list', '=', 'pl', '##3', '##cc', '##f', '##16', '##a', '##0', '##5', '##ba', '##6', '##7', '##6', '##7', '##7']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', 'think', 'medical', 'marijuana', 'would', 'be', 'an', 'effective', 'treatment', 'for', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', 'think', 'medical', 'marijuana', 'would', 'be', 'an', 'effective', 'treatment', 'for', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['medicine', 'effects', 'my', 'doctor', 'has', 'me', 'trying', 'out', 'methyl', '##in', '.', 'i', 'tried', 'out', '5', '##mg', ',', '10', ',', '15', ',', '20', ',', '25', ',', 'and', '30', '.', 'i', 'haven', \"'\", 't', 'noticed', 'any', 'improvement', 'in', 'my', 'focus', ',', 'but', 'i', 'seem', 'to', 'have', 'more', 'energy', '.', 'in', 'fact', ',', '20', ',', '25', ',', 'and', '30', 'are', 'the', 'only', 'ones', 'to', 'give', 'me', 'more', 'energy', ',', 'and', 'the', '30', '##mg', 'feels', 'like', 'i', 'have', 'had', 'one', 'too', 'many', 'energy', 'drinks', '.', 'i', 'feel', 'like', 'there', 'should', 'be', 'more', 'happening', '.', 'am', 'i', 'at', 'the', 'wrong', 'dos', '##age', ',', 'or', 'the', 'wrong', 'drug', '?']\n",
      "INFO:__main__:Number of tokens: 103\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['medicine', 'effects', 'my', 'doctor', 'has', 'me', 'trying', 'out', 'methyl', '##in', '.', 'i', 'tried', 'out', '5', '##mg', ',', '10', ',', '15', ',', '20', ',', '25', ',', 'and', '30', '.', 'i', 'haven', \"'\", 't', 'noticed', 'any', 'improvement', 'in', 'my', 'focus', ',', 'but', 'i', 'seem', 'to', 'have', 'more', 'energy', '.', 'in', 'fact', ',', '20', ',', '25', ',', 'and', '30', 'are', 'the', 'only', 'ones', 'to', 'give', 'me', 'more', 'energy', ',', 'and', 'the', '30', '##mg', 'feels', 'like', 'i', 'have', 'had', 'one', 'too', 'many', 'energy', 'drinks', '.', 'i', 'feel', 'like', 'there', 'should', 'be', 'more', 'happening', '.', 'am', 'i', 'at', 'the', 'wrong', 'dos', '##age', ',', 'or', 'the', 'wrong', 'drug', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'almost', 'all', 'the', 'symptoms', 'for', 'add', '/', 'ad', '##hd', '.', '.', '.', 'questions', 'enclosed', '!', '1', ')', 'i', 'live', 'in', 'canada', ',', 'would', 'it', 'cost', 'me', 'anything', 'to', 'see', 'some', 'specialists', '?', 'i', 'have', 'my', 'parents', 'insurance', 'to', 'help', 'pay', 'for', 'any', 'pills', 'i', 'may', 'receive', 'as', 'well', ',', 'anyone', 'with', 'experience', 'in', 'getting', 'ad', '##er', '##ral', 'or', 'rita', '##lin', '?', '2', ')', 'i', 'have', 'shown', 'signs', 'of', 'add', '/', 'ad', '##hd', '(', 'still', 'hard', 'for', 'me', 'to', 'pick', 'them', 'apart', ',', 'i', 'appear', 'to', 'have', 'qualities', 'of', 'both', ')', 'how', 'can', 'i', 'tell', 'them', 'apart', 'more', 'clearly', '?', 'i', 'am', 'not', 'crazy', 'hyper', 'or', 'anything', ',', 'but', 'show', 'signs', 'leaning', 'towards', 'ad', '##hd', '.', '3', ')', 'i', 'was', 'never', 'diagnosed', 'as', 'a', 'child', ',', 'but', 'always', 'had', 'learning', 'difficulties', 'since', 'i', 'can', 'remember', '.', 'will', 'the', 'doctor', 'ask', 'me', 'about', 'my', 'past', 'struggles', '?', '4', ')', 'i', 'have', 'literally', 'no', 'friends', 'and', 'can', \"'\", 't', 'social', '##ize', 'with', 'anyone', 'in', 'person', 'because', 'i', 'am', 'too', 'distracted', 'and', 'nervous', 'and', 'talk', 'about', 'irrelevant', 'topics', 'to', 'try', 'to', 'ease', 'my', 'mind', '.', 'is', 'this', 'a', 'characteristic', '?', 'thanks', 'to', 'anyone', 'who', 'replies', ',', 'i', 'just', 'want', 'some', 'more', 'detailed', 'information', 'from', 'people', 'who', 'have', 'this', 'condition', '.']\n",
      "INFO:__main__:Number of tokens: 208\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'almost', 'all', 'the', 'symptoms', 'for', 'add', '/', 'ad', '##hd', '.', '.', '.', 'questions', 'enclosed', '!', '1', ')', 'i', 'live', 'in', 'canada', ',', 'would', 'it', 'cost', 'me', 'anything', 'to', 'see', 'some', 'specialists', '?', 'i', 'have', 'my', 'parents', 'insurance', 'to', 'help', 'pay', 'for', 'any', 'pills', 'i', 'may', 'receive', 'as', 'well', ',', 'anyone', 'with', 'experience', 'in', 'getting', 'ad', '##er', '##ral', 'or', 'rita', '##lin', '?', '2', ')', 'i', 'have', 'shown', 'signs', 'of', 'add', '/', 'ad', '##hd', '(', 'still', 'hard', 'for', 'me', 'to', 'pick', 'them', 'apart', ',', 'i', 'appear', 'to', 'have', 'qualities', 'of', 'both', ')', 'how', 'can', 'i', 'tell', 'them', 'apart', 'more', 'clearly', '?', 'i', 'am', 'not', 'crazy', 'hyper', 'or', 'anything', ',', 'but', 'show', 'signs', 'leaning', 'towards', 'ad', '##hd', '.', '3', ')', 'i', 'was', 'never', 'diagnosed', 'as', 'a', 'child', ',', 'but', 'always', 'had', 'learning', 'difficulties', 'since', 'i', 'can', 'remember', '.', 'will', 'the', 'doctor', 'ask', 'me', 'about', 'my', 'past', 'struggles', '?', '4', ')', 'i', 'have', 'literally', 'no', 'friends', 'and', 'can', \"'\", 't', 'social', '##ize', 'with', 'anyone', 'in', 'person', 'because', 'i', 'am', 'too', 'distracted', 'and', 'nervous', 'and', 'talk', 'about', 'irrelevant', 'topics', 'to', 'try', 'to', 'ease', 'my', 'mind', '.', 'is', 'this', 'a', 'characteristic', '?', 'thanks', 'to', 'anyone', 'who', 'replies', ',', 'i', 'just', 'want', 'some', 'more', 'detailed', 'information', 'from', 'people', 'who', 'have', 'this', 'condition', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['self', 'med', '##icated', 'with', 'ep', '##hid', '##rine', 'and', 'caf', '##fe', '##ine', 'and', 'didn', \"'\", 't', 'even', 'realize', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['self', 'med', '##icated', 'with', 'ep', '##hid', '##rine', 'and', 'caf', '##fe', '##ine', 'and', 'didn', \"'\", 't', 'even', 'realize', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['when', 'i', 'drop', 'stuff', 'i', 'usually', 'catch', 'it', 'because', 'of', 'med', '##s', '.', 'i', 'also', 'destroy', 'people', 'in', 'ra', '##c', '##quet', '##ball', '.', \"'\", 'thank', 'you', \"'\", 'number', 'a', 'million', 'to', 'science', '.']\n",
      "INFO:__main__:Number of tokens: 33\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['when', 'i', 'drop', 'stuff', 'i', 'usually', 'catch', 'it', 'because', 'of', 'med', '##s', '.', 'i', 'also', 'destroy', 'people', 'in', 'ra', '##c', '##quet', '##ball', '.', \"'\", 'thank', 'you', \"'\", 'number', 'a', 'million', 'to', 'science', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', 'help', 'completing', 'my', 'final', 'years', 'of', 'high', 'school', '(', '11', '&', '12', ')', 'hey', 'guys', ',', 'i', 'am', 'currently', 'studying', 'in', 'year', '11', 'at', 'school', ',', 'and', 'am', 'taking', '24', '##mg', 'con', '##ser', '##ta', 'to', 'deal', 'with', 'my', 'wan', '##ing', 'concentration', 'my', 'main', 'problem', 'is', 'studying', 'and', 'motivation', 'unless', 'its', 'something', 'i', 'want', 'to', 'do', 'i', 'have', 'no', 'interest', 'and', 'motivation', '(', 'inc', '.', 'school', 'work', ')', 'so', 'i', 'am', 'asking', 'if', 'you', 'could', 'please', 'give', 'me', 'some', 'much', 'needed', 'tips', 'for', 'studying', 'and', 'remaining', 'motivated', '?', 'thank', 'you', 'for', 'reading', 'oh', 'and', 'one', 'more', 'thing', 'do', 'any', 'of', 'you', 'have', 'a', 'good', 'net', 'nanny', 'or', 'internet', 'filter', 'for', 'when', 'i', 'am', 'doing', 'homework', 'and', 'study', '?', 'thanks', 'again']\n",
      "INFO:__main__:Number of tokens: 120\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', 'help', 'completing', 'my', 'final', 'years', 'of', 'high', 'school', '(', '11', '&', '12', ')', 'hey', 'guys', ',', 'i', 'am', 'currently', 'studying', 'in', 'year', '11', 'at', 'school', ',', 'and', 'am', 'taking', '24', '##mg', 'con', '##ser', '##ta', 'to', 'deal', 'with', 'my', 'wan', '##ing', 'concentration', 'my', 'main', 'problem', 'is', 'studying', 'and', 'motivation', 'unless', 'its', 'something', 'i', 'want', 'to', 'do', 'i', 'have', 'no', 'interest', 'and', 'motivation', '(', 'inc', '.', 'school', 'work', ')', 'so', 'i', 'am', 'asking', 'if', 'you', 'could', 'please', 'give', 'me', 'some', 'much', 'needed', 'tips', 'for', 'studying', 'and', 'remaining', 'motivated', '?', 'thank', 'you', 'for', 'reading', 'oh', 'and', 'one', 'more', 'thing', 'do', 'any', 'of', 'you', 'have', 'a', 'good', 'net', 'nanny', 'or', 'internet', 'filter', 'for', 'when', 'i', 'am', 'doing', 'homework', 'and', 'study', '?', 'thanks', 'again']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'hit', 'and', 'miss', '?', '?', 'after', 'jumping', 'through', 'a', 'shit', '##load', 'of', 'hoop', '##s', ',', 'i', 'am', 'finally', 'able', 'to', 'take', 'add', '##eral', '##l', '.', 'i', 'am', 'on', '10', '##mg', '2', '##x', 'a', 'day', '.', 'i', 'have', 'noticed', 'a', 'huge', 'difference', 'in', 'my', 'ability', 'to', 'read', '/', 'remember', 'things', 'and', 'stay', 'focused', '.', 'but', 'some', 'days', 'it', 'seems', 'like', 'it', 'isn', \"'\", 't', 'working', 'at', 'all', '.', 'does', 'anybody', 'else', 'have', 'something', 'like', 'this', 'going', 'on', '?', 'thanks', '-', '-', 'rev']\n",
      "INFO:__main__:Number of tokens: 84\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'hit', 'and', 'miss', '?', '?', 'after', 'jumping', 'through', 'a', 'shit', '##load', 'of', 'hoop', '##s', ',', 'i', 'am', 'finally', 'able', 'to', 'take', 'add', '##eral', '##l', '.', 'i', 'am', 'on', '10', '##mg', '2', '##x', 'a', 'day', '.', 'i', 'have', 'noticed', 'a', 'huge', 'difference', 'in', 'my', 'ability', 'to', 'read', '/', 'remember', 'things', 'and', 'stay', 'focused', '.', 'but', 'some', 'days', 'it', 'seems', 'like', 'it', 'isn', \"'\", 't', 'working', 'at', 'all', '.', 'does', 'anybody', 'else', 'have', 'something', 'like', 'this', 'going', 'on', '?', 'thanks', '-', '-', 'rev']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'to', 'add', '##eral', '##l', 'x', '##r', '.', '.', '.', 'anybody', 'else', 'deal', 'with', 'increased', 'sexual', 'appetite', 'at', 'all', 'the', 'wrong', 'times', '?', 'i', 'take', '20', '##mg', '##s', 'every', 'morning', 'before', 'work', '.', 'by', 'the', 'time', 'i', 'get', 'to', 'work', ',', 'sex', 'is', 'the', 'only', 'thing', 'on', 'my', 'mind', ',', 'it', 'becomes', 'the', 'thing', 'i', 'focus', 'on', '.', 'i', 'have', 'a', 'lot', 'of', 'trouble', 'trying', 'to', 'turn', 'that', 'off', ',', 'and', 'switch', 'focus', 'instead', 'to', 'tasks', 'at', 'hand', ',', 'as', 'a', 'gr', '##ad', 'student', '.', 'bt', '##w', ',', 'i', \"'\", 'm', 'married', ',', 'and', 'my', 'wife', 'and', 'i', 'both', 'appreciate', 'the', 'extra', 'energy', '.', 'it', \"'\", 's', 'just', 'an', 'issue', 'for', 'me', 'during', 'the', 'day', 'when', 'she', \"'\", 's', 'not', 'around', ',', 'and', 'i', ',', 'of', 'course', ',', 'have', 'no', 'reasonable', 'alternative', 'outlet', 'while', 'at', 'work', '.']\n",
      "INFO:__main__:Number of tokens: 137\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'to', 'add', '##eral', '##l', 'x', '##r', '.', '.', '.', 'anybody', 'else', 'deal', 'with', 'increased', 'sexual', 'appetite', 'at', 'all', 'the', 'wrong', 'times', '?', 'i', 'take', '20', '##mg', '##s', 'every', 'morning', 'before', 'work', '.', 'by', 'the', 'time', 'i', 'get', 'to', 'work', ',', 'sex', 'is', 'the', 'only', 'thing', 'on', 'my', 'mind', ',', 'it', 'becomes', 'the', 'thing', 'i', 'focus', 'on', '.', 'i', 'have', 'a', 'lot', 'of', 'trouble', 'trying', 'to', 'turn', 'that', 'off', ',', 'and', 'switch', 'focus', 'instead', 'to', 'tasks', 'at', 'hand', ',', 'as', 'a', 'gr', '##ad', 'student', '.', 'bt', '##w', ',', 'i', \"'\", 'm', 'married', ',', 'and', 'my', 'wife', 'and', 'i', 'both', 'appreciate', 'the', 'extra', 'energy', '.', 'it', \"'\", 's', 'just', 'an', 'issue', 'for', 'me', 'during', 'the', 'day', 'when', 'she', \"'\", 's', 'not', 'around', ',', 'and', 'i', ',', 'of', 'course', ',', 'have', 'no', 'reasonable', 'alternative', 'outlet', 'while', 'at', 'work', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', \"'\", 't', 'seem', 'to', 'do', 'anything', 'unless', 'it', 'has', 'an', 'external', 'deadline', '-', 'got', 'any', 'tips', '?', 'lately', 'i', \"'\", 've', 'started', 'making', 'crude', 'weekly', 'calendar', '##s', 'to', 'set', 'next', 'to', 'my', 'computer', 'to', 'make', 'sure', 'i', 'get', 'everything', 'done', '.', 'problem', 'is', ',', 'i', \"'\", 've', 'only', 'been', 'getting', 'the', 'stuff', 'done', 'that', 'only', 'needs', 'to', 'get', 'done', ',', 'and', 'then', 'goo', '##fin', '##g', 'off', 'afterwards', '.', 'for', 'example', ',', 'last', 'week', 'i', 'had', 'a', 'charcoal', 'project', 'due', ',', 'and', 'an', 'english', 'essay', 'that', 'is', 'due', 'in', 'two', 'weeks', 'to', 'get', 'done', '.', 'i', 'did', 'the', 'charcoal', 'project', '(', 'and', 'managed', 'to', 'get', 'good', 'marks', 'on', 'it', ')', ',', 'but', 'not', 'the', 'english', 'essay', ',', 'though', 'it', 'would', 'really', 'have', 'helped', 'me', 'out', 'this', 'upcoming', 'week', 'to', 'get', 'it', 'done', 'last', 'thursday', 'when', 'i', 'had', 'the', 'time', '.', 'does', 'anyone', 'have', 'any', 'good', 'strategies', 'they', 'use', 'to', 'get', 'stuff', 'done', 'ahead', 'of', 'time', '?']\n",
      "INFO:__main__:Number of tokens: 156\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', \"'\", 't', 'seem', 'to', 'do', 'anything', 'unless', 'it', 'has', 'an', 'external', 'deadline', '-', 'got', 'any', 'tips', '?', 'lately', 'i', \"'\", 've', 'started', 'making', 'crude', 'weekly', 'calendar', '##s', 'to', 'set', 'next', 'to', 'my', 'computer', 'to', 'make', 'sure', 'i', 'get', 'everything', 'done', '.', 'problem', 'is', ',', 'i', \"'\", 've', 'only', 'been', 'getting', 'the', 'stuff', 'done', 'that', 'only', 'needs', 'to', 'get', 'done', ',', 'and', 'then', 'goo', '##fin', '##g', 'off', 'afterwards', '.', 'for', 'example', ',', 'last', 'week', 'i', 'had', 'a', 'charcoal', 'project', 'due', ',', 'and', 'an', 'english', 'essay', 'that', 'is', 'due', 'in', 'two', 'weeks', 'to', 'get', 'done', '.', 'i', 'did', 'the', 'charcoal', 'project', '(', 'and', 'managed', 'to', 'get', 'good', 'marks', 'on', 'it', ')', ',', 'but', 'not', 'the', 'english', 'essay', ',', 'though', 'it', 'would', 'really', 'have', 'helped', 'me', 'out', 'this', 'upcoming', 'week', 'to', 'get', 'it', 'done', 'last', 'thursday', 'when', 'i', 'had', 'the', 'time', '.', 'does', 'anyone', 'have', 'any', 'good', 'strategies', 'they', 'use', 'to', 'get', 'stuff', 'done', 'ahead', 'of', 'time', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'got', 'prescribed', '10', 'mg', 'add', '##eral', '##l', '2', '##x', 'daily', '.', '.', '.', 'crashing', 'questions', ',', 'etc', '.', 'i', 'got', 'a', 'sc', '##rip', 'on', 'friday', 'for', '10', 'mg', 'add', '##eral', '##l', '2', '##x', 'daily', '.', 'considering', 'my', 'weight', '(', '260', 'lbs', ')', ',', 'i', 'didn', \"'\", 't', 'think', 'it', 'would', 'do', 'much', ',', 'and', 'it', 'didn', \"'\", 't', '.', 'i', 'decided', 'to', 'take', 'another', 'pill', 'a', 'couple', 'hours', 'later', 'after', 'my', 'first', 'dose', ',', 'and', 'proceeded', 'to', 'play', 'the', 'best', 'jazz', 'gig', 'of', 'my', 'life', ',', 'went', 'to', 'a', 'club', ',', 'and', 'was', 'more', 'related', 'and', 'social', 'than', 'i', \"'\", 've', 'ever', 'been', '.', 'i', 'was', 'then', 'able', 'to', 'easily', 'go', 'to', 'sleep', 'as', 'i', 'came', 'off', 'it', '(', 'i', 'usually', 'have', 'a', 'really', 'hard', 'time', 'getting', 'off', 'the', 'computer', '/', 'piano', '/', 'xbox', 'and', 'slipping', 'into', 'bed', ')', '.', 'i', 'had', 'a', 'big', 'day', 'yesterday', 'and', 'started', 'off', 'with', '2', 'pills', '.', 'i', 'got', 'more', 'done', 'then', 'i', 'ever', 'have', 'on', 'a', 'saturday', 'morning', ',', 'all', 'topped', 'off', 'with', 'a', 'great', 'workout', 'with', 'my', 'physical', 'trainer', '.', 'afterwards', ',', 'though', ',', 'as', 'the', 'dose', 'wore', 'off', ',', 'i', 'felt', 'really', 'sleepy', '.', 'i', 'then', 'took', '1', 'pill', ',', 'thinking', 'that', 'i', 'shouldn', \"'\", 't', 'mess', 'with', 'the', 'prescribed', 'doses', ',', 'as', 'my', 'therapist', 'said', 'he', \"'\", 'd', 'have', 'me', 'on', 'this', 'for', '2', 'weeks', 'and', 'see', 'what', 'it', 'does', '.', 'my', 'focus', 'and', 'energy', 'was', 'able', 'to', 'return', ',', 'and', 'i', 'went', 'wine', 'tasting', '(', 'it', 'was', 'a', 'business', 'thing', ')', '.', 'i', 'had', 'a', 'huge', 'charitable', '/', 'social', 'event', 'last', 'night', ',', 'so', 'i', 'took', 'a', 'pill', '2', 'hours', 'after', 'my', 'second', 'dose', 'so', 'the', 'effects', 'wouldn', \"'\", 't', 'wear', 'off', 'until', 'late', 'at', 'night', ',', 'which', 'was', 'probably', 'a', 'mistake', 'in', 'judgement', '.', 'i', 'was', 'designated', 'driver', ',', 'so', 'i', 'stuck', 'with', 'the', '1', 'drink', 'every', '2', 'hours', 'rule', 'i', 'usually', 'use', '.', 'driving', 'wasn', \"'\", 't', 'an', 'issue', ',', 'but', 'the', 'effects', 'wore', 'off', 'at', 'around', '9', 'pm', ',', 'which', 'was', 'still', 'early', '.', 'at', 'around', '11', 'pm', ',', 'i', 'developed', 'a', 'huge', 'headache', ',', 'got', 'really', 'tired', ',', 'and', 'became', 'my', 'usual', 'anti', '##so', '##cial', 'self', 'in', 'the', 'club', '.', 'i', 'stuck', 'around', 'until', '2', 'am', 'so', 'i', 'could', 'drive', 'my', 'drunk', 'friends', 'home', ',', 'but', 'it', 'was', 'extremely', 'unpleasant', '.', 'slept', 'really', 'well', ',', 'and', 'today', 'took', 'only', 'the', 'prescribed', 'dos', '##age', '.', 'i', 'had', 'a', 'productive', 'day', ',', 'and', 'i', 'think', 'it', 'helped', ',', 'but', 'i', \"'\", 'm', 'pretty', 'sure', 'i', 'should', 'be', 'on', '20', 'or', '30', 'mg', '.', 'i', 'will', 'continue', 'to', 'take', 'only', 'the', 'prescribed', 'dos', '##age', 'until', 'i', 'talk', 'with', 'my', 'therapist', 'next', 'week', '.', 'any', 'thoughts', '?', 'will', 'x', '##r', 'help', 'regulate', 'the', 'crashes', '?', 'when', 'i', 'go', 'out', 'on', 'weekends', ',', 'typically', ',', 'i', 'end', 'up', 'staying', 'out', 'till', 'well', 'past', 'midnight', '.', 'not', 'sure', 'i', 'want', 'to', 'change', 'this', ',', 'as', 'all', 'my', 'friends', 'stay', 'out', '(', 'so', 'i', 'need', 'to', 'as', 'well', 'to', 'be', 'able', 'to', 'get', 'a', 'ride', 'home', ')', ',', 'and', 'ladies', 'typically', 'like', 'to', 'close', 'out', 'the', 'bar', '/', 'club', '.', 'on', 'these', 'occasions', ',', 'should', 'i', 'take', 'my', 'dose', 'later', 'in', 'the', 'day', '?', 'both', '10', 'and', '20', 'mg', \"'\", 's', 'seem', 'to', 'wear', 'off', 'at', 'around', '5', 'hours', 'on', 'the', 'dot', '.']\n",
      "INFO:__main__:Number of tokens: 552\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['just', 'got', 'prescribed', '10', 'mg', 'add', '##eral', '##l', '2', '##x', 'daily', '.', '.', '.', 'crashing', 'questions', ',', 'etc', '.', 'i', 'got', 'a', 'sc', '##rip', 'on', 'friday', 'for', '10', 'mg', 'add', '##eral', '##l', '2', '##x', 'daily', '.', 'considering', 'my', 'weight', '(', '260', 'lbs', ')', ',', 'i', 'didn', \"'\", 't', 'think', 'it', 'would', 'do', 'much', ',', 'and', 'it', 'didn', \"'\", 't', '.', 'i', 'decided', 'to', 'take', 'another', 'pill', 'a', 'couple', 'hours', 'later', 'after', 'my', 'first', 'dose', ',', 'and', 'proceeded', 'to', 'play', 'the', 'best', 'jazz', 'gig', 'of', 'my', 'life', ',', 'went', 'to', 'a', 'club', ',', 'and', 'was', 'more', 'related', 'and', 'social', 'than', 'i', \"'\", 've', 'ever', 'been', '.', 'i', 'was', 'then', 'able', 'to', 'easily', 'go', 'to', 'sleep', 'as', 'i', 'came', 'off', 'it', '(', 'i', 'usually', 'have', 'a', 'really', 'hard', 'time', 'getting', 'off', 'the', 'computer', '/', 'piano', '/', 'xbox', 'and', 'slipping', 'into', 'bed', ')', '.', 'i', 'had', 'a', 'big', 'day', 'yesterday', 'and', 'started', 'off', 'with', '2', 'pills', '.', 'i', 'got', 'more', 'done', 'then', 'i', 'ever', 'have', 'on', 'a', 'saturday', 'morning', ',', 'all', 'topped', 'off', 'with', 'a', 'great', 'workout', 'with', 'my', 'physical', 'trainer', '.', 'afterwards', ',', 'though', ',', 'as', 'the', 'dose', 'wore', 'off', ',', 'i', 'felt', 'really', 'sleepy', '.', 'i', 'then', 'took', '1', 'pill', ',', 'thinking', 'that', 'i', 'shouldn', \"'\", 't', 'mess', 'with', 'the', 'prescribed', 'doses', ',', 'as', 'my', 'therapist', 'said', 'he', \"'\", 'd', 'have', 'me', 'on', 'this', 'for', '2', 'weeks', 'and', 'see', 'what', 'it', 'does', '.', 'my', 'focus', 'and', 'energy', 'was', 'able', 'to', 'return', ',', 'and', 'i', 'went', 'wine', 'tasting', '(', 'it', 'was', 'a', 'business', 'thing', ')', '.', 'i', 'had', 'a', 'huge', 'charitable', '/', 'social', 'event', 'last', 'night', ',', 'so', 'i', 'took', 'a', 'pill', '2', 'hours', 'after', 'my', 'second', 'dose', 'so', 'the', 'effects', 'wouldn', \"'\", 't', 'wear', 'off', 'until', 'late', 'at', 'night', ',', 'which', 'was', 'probably', 'a', 'mistake', 'in', 'judgement', '.', 'i', 'was', 'designated', 'driver', ',', 'so', 'i', 'stuck', 'with', 'the', '1', 'drink', 'every', '2', 'hours', 'rule', 'i', 'usually', 'use', '.', 'driving', 'wasn', \"'\", 't', 'an', 'issue', ',', 'but', 'the', 'effects', 'wore', 'off', 'at', 'around', '9', 'pm', ',', 'which', 'was', 'still', 'early', '.', 'at', 'around', '11', 'pm', ',', 'i', 'developed', 'a', 'huge', 'headache', ',', 'got', 'really', 'tired', ',', 'and', 'became', 'my', 'usual', 'anti', '##so', '##cial', 'self', 'in', 'the', 'club', '.', 'i', 'stuck', 'around', 'until', '2', 'am', 'so', 'i', 'could', 'drive', 'my', 'drunk', 'friends', 'home', ',', 'but', 'it', 'was', 'extremely', 'unpleasant', '.', 'slept', 'really', 'well', ',', 'and', 'today', 'took', 'only', 'the', 'prescribed', 'dos', '##age', '.', 'i', 'had', 'a', 'productive', 'day', ',', 'and', 'i', 'think', 'it', 'helped', ',', 'but', 'i', \"'\", 'm', 'pretty', 'sure', 'i', 'should', 'be', 'on', '20', 'or', '30', 'mg', '.', 'i', 'will', 'continue', 'to', 'take', 'only', 'the', 'prescribed', 'dos', '##age', 'until', 'i', 'talk', 'with', 'my', 'therapist', 'next', 'week', '.', 'any', 'thoughts', '?', 'will', 'x', '##r', 'help', 'regulate', 'the', 'crashes', '?', 'when', 'i', 'go', 'out', 'on', 'weekends', ',', 'typically', ',', 'i', 'end', 'up', 'staying', 'out', 'till', 'well', 'past', 'midnight', '.', 'not', 'sure', 'i', 'want', 'to', 'change', 'this', ',', 'as', 'all', 'my', 'friends', 'stay', 'out', '(', 'so', 'i', 'need', 'to', 'as', 'well', 'to', 'be', 'able', 'to', 'get', 'a', 'ride', 'home', ')', ',', 'and', 'ladies', 'typically', 'like', 'to'], ['close', 'out', 'the', 'bar', '/', 'club', '.', 'on', 'these', 'occasions', ',', 'should', 'i', 'take', 'my', 'dose', 'later', 'in', 'the', 'day', '?', 'both', '10', 'and', '20', 'mg', \"'\", 's', 'seem', 'to', 'wear', 'off', 'at', 'around', '5', 'hours', 'on', 'the', 'dot', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'irony', 'of', 'ad', '##hd', 'is', 'that', 'i', 'can', \"'\", 't', 'seem', 'to', 'stay', 'focused', 'long', 'enough', 'to', 'find', 'a', 'way', 'to', 'help', 'myself', '.', 'maybe', 'that', 'isn', \"'\", 't', 'ironic', ',', 'sort', 'of', 'like', 'alan', '##is', 'was', 'never', 'ironic', '.', 'or', 'maybe', 'the', 'irony', 'lies', 'within', 'the', 'non', '-', 'irony', 'of', 'the', 'situation', ',', 'sort', 'of', 'like', 'alan', '##is', '.', 'anyway', ',', 'here', \"'\", 's', 'my', 't', '##l', ';', 'dr', ':', 'i', 'didn', \"'\", 't', 'realize', 'that', 'i', 'had', 'ad', '##hd', 'until', 'about', 'a', 'year', 'ago', 'and', 'i', 'am', 'now', 'in', 'my', '30', \"'\", 's', '.', 'started', 'taking', 'add', '##eral', '##l', '10', '##mg', 'x', '##r', 'and', 'it', 'helped', 'a', 'lot', 'with', 'my', 'last', 'semester', 'of', 'school', '(', 'which', 'took', '10', 'years', 'to', 'complete', ',', 'mind', 'you', ')', '.', 'i', 'have', 'not', 'up', '##ped', 'my', 'dos', '##age', 'during', 'that', 'time', 'and', 'i', \"'\", 'd', 'like', 'to', 'start', 'looking', 'into', 'some', 'behavioral', 'things', 'that', 'i', 'can', 'do', 'to', '\"', 'get', '\"', 'better', '.', 'every', 'time', 'i', 'look', 'into', 'it', ',', 'however', ',', 'i', 'get', 'distracted', 'by', 'something', 'else', '.', 'so', 'in', 'the', 'interest', 'of', 'a', 'fellow', 'space', 'cadet', ',', 'please', 'share', 'with', 'me', 'the', 'non', '-', 'ph', '##arm', '##a', 'tools', 'you', 'have', 'used', 'to', '1', ')', 'control', 'impulse', '##s', ',', '2', ')', 'learn', 'to', 'listen', 'to', 'people', '/', 'pay', 'attention', 'better', ',', '3', ')', 'not', 'fly', 'off', 'the', 'handle', 'at', 'the', 'smallest', 'things', ',', 'and', '4', ')', 'is', 'that', 'a', 'butterfly', '?', 'for', 'me', 'what', \"'\", 's', 'worked', 'so', 'far', 'is', 'the', 'medication', 'combined', 'with', 'a', 'forceful', 'focusing', 'on', 'the', 'situation', 'at', 'hand', ';', 'so', 'far', 'i', 'have', 'mixed', 'results', 'with', 'the', 'latter', '.', 'i', 'notice', 'that', 'sitting', 'on', 'my', 'hands', 'will', 'sometimes', 'prevent', 'me', 'from', 'b', '##lab', '##ber', '##ing', 'non', '-', 'stop', 'and', 'the', 'other', 'part', 'is', 'acknowledging', 'when', 'i', 'actually', 'don', \"'\", 't', 'know', 'something', ',', 'which', 'allows', 'me', 'to', 'be', 'genuinely', 'interested', 'in', 'the', 'topic', 'and', 'thereby', 'at', '##ten', '##tive', '.', 'tri', '##athlon', '##s', 'have', 'really', 'turned', 'things', 'around', 'for', 'me', 'as', 'well', ',', 'although', 'after', 'completing', 'an', 'iron', '##man', '2', 'years', 'ago', ',', 'i', 'had', 'a', 'huge', 'motivation', '##al', 'collapse', 'which', 'almost', 'made', 'things', 'worse', 'than', 'they', 'ever', 'were', '.', 'now', 'i', 'spend', 'most', 'of', 'my', 'time', 'running', ',', 'which', 'is', 'a', 'true', 'god', '-', '/', 'f', '##sm', '-', 'send', '.', 'so', 'help', '!', 'i', 'am', 'lucky', 'enough', 'not', 'to', 'suffer', 'from', 'a', 'lack', 'of', 'self', '-', 'worth', 'and', 'i', \"'\", 'm', 'fully', 'confident', 'in', 'my', 'abilities', 'to', 'do', 'whatever', 'the', 'heck', 'i', 'decide', 'to', 'finally', 'do', '.', '.', '.', 'i', 'just', 'need', 'to', 'know', 'how', 'to', 'actually', 'get', 'up', 'and', 'do', 'the', 'boring', ',', 'men', '##ial', 'tasks', 'without', 'getting', 'distracted', 'by', 'that', 'goddamn', 'butterfly', 'again', '.', '*', '*', 'eta', ':', '*', '*', 'that', 'is', 'the', 'single', 'worst', 't', '##l', ';', 'dr', 'ever', 'and', 'in', 'a', 'place', 'like', 'this', 'sub', '##red', '##dit', ',', 'i', 'should', 'be', 'shot', 'out', 'of', 'a', 'cannon', 'for', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 487\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'irony', 'of', 'ad', '##hd', 'is', 'that', 'i', 'can', \"'\", 't', 'seem', 'to', 'stay', 'focused', 'long', 'enough', 'to', 'find', 'a', 'way', 'to', 'help', 'myself', '.', 'maybe', 'that', 'isn', \"'\", 't', 'ironic', ',', 'sort', 'of', 'like', 'alan', '##is', 'was', 'never', 'ironic', '.', 'or', 'maybe', 'the', 'irony', 'lies', 'within', 'the', 'non', '-', 'irony', 'of', 'the', 'situation', ',', 'sort', 'of', 'like', 'alan', '##is', '.', 'anyway', ',', 'here', \"'\", 's', 'my', 't', '##l', ';', 'dr', ':', 'i', 'didn', \"'\", 't', 'realize', 'that', 'i', 'had', 'ad', '##hd', 'until', 'about', 'a', 'year', 'ago', 'and', 'i', 'am', 'now', 'in', 'my', '30', \"'\", 's', '.', 'started', 'taking', 'add', '##eral', '##l', '10', '##mg', 'x', '##r', 'and', 'it', 'helped', 'a', 'lot', 'with', 'my', 'last', 'semester', 'of', 'school', '(', 'which', 'took', '10', 'years', 'to', 'complete', ',', 'mind', 'you', ')', '.', 'i', 'have', 'not', 'up', '##ped', 'my', 'dos', '##age', 'during', 'that', 'time', 'and', 'i', \"'\", 'd', 'like', 'to', 'start', 'looking', 'into', 'some', 'behavioral', 'things', 'that', 'i', 'can', 'do', 'to', '\"', 'get', '\"', 'better', '.', 'every', 'time', 'i', 'look', 'into', 'it', ',', 'however', ',', 'i', 'get', 'distracted', 'by', 'something', 'else', '.', 'so', 'in', 'the', 'interest', 'of', 'a', 'fellow', 'space', 'cadet', ',', 'please', 'share', 'with', 'me', 'the', 'non', '-', 'ph', '##arm', '##a', 'tools', 'you', 'have', 'used', 'to', '1', ')', 'control', 'impulse', '##s', ',', '2', ')', 'learn', 'to', 'listen', 'to', 'people', '/', 'pay', 'attention', 'better', ',', '3', ')', 'not', 'fly', 'off', 'the', 'handle', 'at', 'the', 'smallest', 'things', ',', 'and', '4', ')', 'is', 'that', 'a', 'butterfly', '?', 'for', 'me', 'what', \"'\", 's', 'worked', 'so', 'far', 'is', 'the', 'medication', 'combined', 'with', 'a', 'forceful', 'focusing', 'on', 'the', 'situation', 'at', 'hand', ';', 'so', 'far', 'i', 'have', 'mixed', 'results', 'with', 'the', 'latter', '.', 'i', 'notice', 'that', 'sitting', 'on', 'my', 'hands', 'will', 'sometimes', 'prevent', 'me', 'from', 'b', '##lab', '##ber', '##ing', 'non', '-', 'stop', 'and', 'the', 'other', 'part', 'is', 'acknowledging', 'when', 'i', 'actually', 'don', \"'\", 't', 'know', 'something', ',', 'which', 'allows', 'me', 'to', 'be', 'genuinely', 'interested', 'in', 'the', 'topic', 'and', 'thereby', 'at', '##ten', '##tive', '.', 'tri', '##athlon', '##s', 'have', 'really', 'turned', 'things', 'around', 'for', 'me', 'as', 'well', ',', 'although', 'after', 'completing', 'an', 'iron', '##man', '2', 'years', 'ago', ',', 'i', 'had', 'a', 'huge', 'motivation', '##al', 'collapse', 'which', 'almost', 'made', 'things', 'worse', 'than', 'they', 'ever', 'were', '.', 'now', 'i', 'spend', 'most', 'of', 'my', 'time', 'running', ',', 'which', 'is', 'a', 'true', 'god', '-', '/', 'f', '##sm', '-', 'send', '.', 'so', 'help', '!', 'i', 'am', 'lucky', 'enough', 'not', 'to', 'suffer', 'from', 'a', 'lack', 'of', 'self', '-', 'worth', 'and', 'i', \"'\", 'm', 'fully', 'confident', 'in', 'my', 'abilities', 'to', 'do', 'whatever', 'the', 'heck', 'i', 'decide', 'to', 'finally', 'do', '.', '.', '.', 'i', 'just', 'need', 'to', 'know', 'how', 'to', 'actually', 'get', 'up', 'and', 'do', 'the', 'boring', ',', 'men', '##ial', 'tasks', 'without', 'getting', 'distracted', 'by', 'that', 'goddamn', 'butterfly', 'again', '.', '*', '*', 'eta', ':', '*', '*', 'that', 'is', 'the', 'single', 'worst', 't', '##l', ';', 'dr', 'ever', 'and', 'in', 'a', 'place', 'like', 'this', 'sub', '##red', '##dit', ',', 'i', 'should', 'be', 'shot', 'out', 'of', 'a', 'cannon', 'for', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'other', 'gamer', '##s', '?', 'with', 'mad', 'ad', '##hd', 'skills', '?', 'i', 'really', 'like', 'twitch', 'games', 'that', 'move', 'at', 'high', 'speeds', 'like', '[', 'tribes', ':', 'as', '##cend', '-', 'the', 'world', 'faster', 'shooter', '.', ']', '(', 'https', ':', '/', '/', 'account', '.', 'hire', '##z', '##st', '##udi', '##os', '.', 'com', '/', 'tribes', '##as', '##cend', '/', '?', 'refer', '##ral', '=', '142', '##59', '##17', '&', 'ut', '##m', '_', 'campaign', '=', 'email', ')', 'if', 'so', 'try', 'it', ',', 'it', 'is', 'free', '.', 'just', 'curious', 'who', 'out', 'there', 'also', 'games', 'and', 'are', 'you', 'any', 'good', 'and', 'what', 'type', 'of', 'games', '.']\n",
      "INFO:__main__:Number of tokens: 95\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'other', 'gamer', '##s', '?', 'with', 'mad', 'ad', '##hd', 'skills', '?', 'i', 'really', 'like', 'twitch', 'games', 'that', 'move', 'at', 'high', 'speeds', 'like', '[', 'tribes', ':', 'as', '##cend', '-', 'the', 'world', 'faster', 'shooter', '.', ']', '(', 'https', ':', '/', '/', 'account', '.', 'hire', '##z', '##st', '##udi', '##os', '.', 'com', '/', 'tribes', '##as', '##cend', '/', '?', 'refer', '##ral', '=', '142', '##59', '##17', '&', 'ut', '##m', '_', 'campaign', '=', 'email', ')', 'if', 'so', 'try', 'it', ',', 'it', 'is', 'free', '.', 'just', 'curious', 'who', 'out', 'there', 'also', 'games', 'and', 'are', 'you', 'any', 'good', 'and', 'what', 'type', 'of', 'games', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['yesterday', 'my', 'employer', 'described', 'one', 'of', 'my', 'co', '-', 'workers', 'as', '\"', 'a', 'bit', 'add', '\"']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['yesterday', 'my', 'employer', 'described', 'one', 'of', 'my', 'co', '-', 'workers', 'as', '\"', 'a', 'bit', 'add', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['their', 'approach', 'to', '\"', 'fixing', '\"', 'us', 'and', 'making', 'us', 'focus', 'vs', 'your', 'approach', '.', 'i', \"'\", 've', 'grown', 'up', 'with', 'add', 'and', 'slight', 'autism', '.', 'the', 'public', 'school', 'system', 'was', 'completely', 'useless', 'for', 'me', 'as', 'it', 'was', 'a', 'joke', 'that', 'expected', 'me', 'to', 'focus', 'on', 'things', 'i', 'never', 'cared', 'about', '.', 'their', 'approach', 'to', 'fixing', 'this', '?', 'trying', 'to', 'make', 'me', 'care', 'about', 'this', 'shit', '.', 'never', 'worked', ',', 'because', 'it', 'simply', 'won', \"'\", 't', '.', 'i', 'just', 'continued', 'to', 'stumble', 'across', 'an', 'academic', 'career', 'of', 'med', '##io', '##cr', '##ity', 'and', 'frustration', '.', 'until', 'i', 'found', 'a', 'love', 'for', 'writing', ',', 'for', 'myself', 'instead', 'of', 'for', 'some', 'stranger', \"'\", 's', 'object', '##ified', 'judgement', '.', 'all', 'of', 'a', 'sudden', ',', 'when', 'i', 'wrote', 'fiction', 'i', 'found', 'myself', 'completely', 'engulfed', 'in', 'the', 'same', 'line', 'of', 'thoughts', 'for', 'the', 'first', 'time', 'ever', '.', 'my', 'point', 'is', 'that', 'you', 'can', \"'\", 't', 'force', 'yourself', 'to', 'care', 'for', 'something', 'that', 'doesn', \"'\", 't', 'interest', 'you', '.', 'that', \"'\", 's', 'just', 'the', 'problem', 'with', 'people', 'these', 'days', '.', '.', '.', 'they', 'mu', '##tila', '##te', 'their', 'life', \"'\", 's', 'calling', 'so', 'they', 'can', 'chase', 'an', 'empty', 'goal', ',', 'only', 'because', 'it', \"'\", 's', 'what', 'everyone', 'else', 'does', '.', 'do', 'what', 'makes', 'you', 'happy', '.', 'your', 'mind', 'will', 'know', 'when', 'to', 'focus', 'and', 'if', 'you', \"'\", 've', 'found', 'the', 'right', 'life', ',', 'you', 'will', 'not', 'fail', '.', 'i', 'promise', '.']\n",
      "INFO:__main__:Number of tokens: 233\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['their', 'approach', 'to', '\"', 'fixing', '\"', 'us', 'and', 'making', 'us', 'focus', 'vs', 'your', 'approach', '.', 'i', \"'\", 've', 'grown', 'up', 'with', 'add', 'and', 'slight', 'autism', '.', 'the', 'public', 'school', 'system', 'was', 'completely', 'useless', 'for', 'me', 'as', 'it', 'was', 'a', 'joke', 'that', 'expected', 'me', 'to', 'focus', 'on', 'things', 'i', 'never', 'cared', 'about', '.', 'their', 'approach', 'to', 'fixing', 'this', '?', 'trying', 'to', 'make', 'me', 'care', 'about', 'this', 'shit', '.', 'never', 'worked', ',', 'because', 'it', 'simply', 'won', \"'\", 't', '.', 'i', 'just', 'continued', 'to', 'stumble', 'across', 'an', 'academic', 'career', 'of', 'med', '##io', '##cr', '##ity', 'and', 'frustration', '.', 'until', 'i', 'found', 'a', 'love', 'for', 'writing', ',', 'for', 'myself', 'instead', 'of', 'for', 'some', 'stranger', \"'\", 's', 'object', '##ified', 'judgement', '.', 'all', 'of', 'a', 'sudden', ',', 'when', 'i', 'wrote', 'fiction', 'i', 'found', 'myself', 'completely', 'engulfed', 'in', 'the', 'same', 'line', 'of', 'thoughts', 'for', 'the', 'first', 'time', 'ever', '.', 'my', 'point', 'is', 'that', 'you', 'can', \"'\", 't', 'force', 'yourself', 'to', 'care', 'for', 'something', 'that', 'doesn', \"'\", 't', 'interest', 'you', '.', 'that', \"'\", 's', 'just', 'the', 'problem', 'with', 'people', 'these', 'days', '.', '.', '.', 'they', 'mu', '##tila', '##te', 'their', 'life', \"'\", 's', 'calling', 'so', 'they', 'can', 'chase', 'an', 'empty', 'goal', ',', 'only', 'because', 'it', \"'\", 's', 'what', 'everyone', 'else', 'does', '.', 'do', 'what', 'makes', 'you', 'happy', '.', 'your', 'mind', 'will', 'know', 'when', 'to', 'focus', 'and', 'if', 'you', \"'\", 've', 'found', 'the', 'right', 'life', ',', 'you', 'will', 'not', 'fail', '.', 'i', 'promise', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'supplements']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'supplements']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'i', 'cope', '/', 'deal', 'with', 'the', 'social', 'scars', 'of', 'childhood', '/', 'adult', 'ad', '##hd', '?', 'over', 'the', 'last', 'few', 'years', 'i', \"'\", 've', 'finally', 'got', 'over', 'the', 'academic', 'aspects', 'of', 'ad', '##hd', ',', 'and', 'really', 'started', 'to', 'realize', 'that', 'i', 'am', 'in', 'fact', 'smart', 'and', 'do', 'very', 'well', 'in', 'school', '(', 'i', \"'\", 'm', '33', ')', '.', 'i', \"'\", 've', 'really', 'had', 'more', 'success', 'then', 'i', 'could', 'have', 'imagined', 'in', 'my', 'wilde', '##st', 'dreams', 'in', 'my', 'career', '/', 'academic', 'life', '.', 'so', ',', 'lately', 'i', \"'\", 've', 'really', 'been', 'trying', 'to', 'deal', 'with', 'what', 'i', 'thought', 'were', 'my', 'own', 'psychological', 'issues', 'around', 'being', 'social', '.', 'i', 'have', 'a', 'fairly', 'di', '##sf', '##un', '##ction', '##al', 'childhood', ',', 'with', 'a', 'verbal', '##ly', '/', 'mentally', 'abusive', 'alcoholic', 'parent', 'which', 'has', 'led', 'me', ',', 'in', 'my', 'opinion', ',', 'to', 'have', 'low', 'self', 'esteem', 'and', 'poor', 'inter', '##personal', 'skills', '.', 'i', 'went', 'to', 'a', 'psychologist', 'for', 'a', 'year', 'to', 'tackle', 'these', 'issues', 'and', 'feel', 'that', 'i', 'really', 'have', 'dealt', 'with', 'most', 'of', 'them', '.', 'unfortunately', 'some', 'of', 'my', 'issues', 'remain', '.', 'despite', 'being', 'fairly', 'attractive', ',', 'social', ',', 'intelligent', ',', 'a', 'leader', ',', 'and', 'popular', 'with', 'women', ',', 'i', \"'\", 've', 'somehow', 'remained', 'alone', 'and', 'with', 'a', 'small', 'group', 'of', 'friends', 'for', 'most', 'of', 'my', 'life', '.', 'i', 'spend', 'most', 'of', 'my', 'free', 'time', 'at', 'home', 'in', 'front', 'of', 'the', 'computer', '(', 'read', 'addicted', 'to', ':', 'wikipedia', '/', 'porn', '/', 'tv', 'shows', '/', 'gaming', ')', 'and', 'not', 'interacting', 'with', 'people', '.', 'deep', 'down', 'i', \"'\", 'm', 'afraid', 'of', 'peoples', 'interpretations', '(', 'rejection', 'of', 'me', ')', 'of', 'my', 'actions', 'due', 'to', 'my', 'self', 'per', '##cie', '##ved', '\"', 'social', 'inc', '##omp', '##ten', '##ce', '.', '\"', 'despite', 'the', 'fact', 'that', 'my', 'friends', 'say', 'that', 'i', \"'\", 'm', 'really', 'social', 'and', 'fun', 'to', 'be', 'around', 'and', 'most', 'people', 'tend', 'to', 'enjoy', 'my', 'company', ',', 'i', 'continually', 'isolate', 'myself', '.', 'quite', 'simply', 'i', \"'\", 've', 'always', 'been', 'a', 'lone', '##r', 'because', 'i', \"'\", 've', 'had', 'trouble', 'with', 'or', 'perceived', 'that', 'i', 'have', 'trouble', 'with', 'social', 'interactions', ',', 'and', 'am', 'sick', 'of', 'it', '.', 'i', 'want', 'to', 'be', 'happier', 'and', 'i', 'know', 'that', 'becoming', 'more', 'social', 'will', 'help', 'me', '.', '(', 'i', 'have', 'started', 'to', 'limit', 'my', 'computer', 'time', 'to', '3', 'hours', 'a', 'day', ',', 'and', 'forcing', 'myself', 'to', 'be', 'more', 'social', ',', 'and', 'it', 'seems', 'to', 'be', 'working', '.', ')', 'today', ',', 'i', 'was', 'sitting', 'in', 'my', 'physiology', 'class', 'when', ',', 'we', 'started', 'talking', 'about', 'issues', 'of', 'the', 'pre', '##front', '##al', 'cortex', ',', 'namely', ',', 'ad', '##hd', '.', 'we', 'then', 'started', 'to', 'talk', 'about', 'the', 'social', 'aspects', 'of', 'ad', '##hd', ',', 'and', 'something', 'clicked', 'in', 'my', 'mind', '.', 'i', 'suppose', 'i', 'finally', 'realized', 'that', 'the', 'above', 'social', 'anxiety', 'most', 'likely', 'stemmed', 'from', 'my', 'childhood', 'social', 'problems', 'due', 'to', 'super', 'high', 'doses', 'of', 'ri', '##tila', '##n', '(', '150', '-', '180', '##mg', '/', 'day', ')', 'combined', 'with', 'some', 'pretty', 'crazy', 'ad', '##hd', '.', 'i', 'am', 'actually', 'a', 'pretty', 'social', 'and', 'li', '##ka', '##ble', 'guy', 'though', 'i', 'do', 'have', 'a', 'strong', 'personality', 'that', 'some', 'people', 'have', 'difficulty', 'dealing', 'with', '(', 'especially', 'me', 'being', 'an', 'american', 'living', 'in', 'sweden', '.', ')', 'yet', ',', 'somehow', 'i', 'am', 'still', 'driven', 'to', 'isolate', 'myself', 'from', 'social', 'interaction', 'possibly', 'in', 'order', 'to', 'preserve', 'my', 'self', 'esteem', '(', 'what', 'i', 'did', 'as', 'a', 'child', ')', '.', 'does', 'anyone', 'else', 'suffer', 'the', 'consequences', 'of', 'this', 'sort', 'of', 'social', 'scar', 'from', 'a', 'childhood', 'with', 'ad', '##hd', '?', 'any', 'tips', 'on', 'getting', 'over', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 575\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['how', 'do', 'i', 'cope', '/', 'deal', 'with', 'the', 'social', 'scars', 'of', 'childhood', '/', 'adult', 'ad', '##hd', '?', 'over', 'the', 'last', 'few', 'years', 'i', \"'\", 've', 'finally', 'got', 'over', 'the', 'academic', 'aspects', 'of', 'ad', '##hd', ',', 'and', 'really', 'started', 'to', 'realize', 'that', 'i', 'am', 'in', 'fact', 'smart', 'and', 'do', 'very', 'well', 'in', 'school', '(', 'i', \"'\", 'm', '33', ')', '.', 'i', \"'\", 've', 'really', 'had', 'more', 'success', 'then', 'i', 'could', 'have', 'imagined', 'in', 'my', 'wilde', '##st', 'dreams', 'in', 'my', 'career', '/', 'academic', 'life', '.', 'so', ',', 'lately', 'i', \"'\", 've', 'really', 'been', 'trying', 'to', 'deal', 'with', 'what', 'i', 'thought', 'were', 'my', 'own', 'psychological', 'issues', 'around', 'being', 'social', '.', 'i', 'have', 'a', 'fairly', 'di', '##sf', '##un', '##ction', '##al', 'childhood', ',', 'with', 'a', 'verbal', '##ly', '/', 'mentally', 'abusive', 'alcoholic', 'parent', 'which', 'has', 'led', 'me', ',', 'in', 'my', 'opinion', ',', 'to', 'have', 'low', 'self', 'esteem', 'and', 'poor', 'inter', '##personal', 'skills', '.', 'i', 'went', 'to', 'a', 'psychologist', 'for', 'a', 'year', 'to', 'tackle', 'these', 'issues', 'and', 'feel', 'that', 'i', 'really', 'have', 'dealt', 'with', 'most', 'of', 'them', '.', 'unfortunately', 'some', 'of', 'my', 'issues', 'remain', '.', 'despite', 'being', 'fairly', 'attractive', ',', 'social', ',', 'intelligent', ',', 'a', 'leader', ',', 'and', 'popular', 'with', 'women', ',', 'i', \"'\", 've', 'somehow', 'remained', 'alone', 'and', 'with', 'a', 'small', 'group', 'of', 'friends', 'for', 'most', 'of', 'my', 'life', '.', 'i', 'spend', 'most', 'of', 'my', 'free', 'time', 'at', 'home', 'in', 'front', 'of', 'the', 'computer', '(', 'read', 'addicted', 'to', ':', 'wikipedia', '/', 'porn', '/', 'tv', 'shows', '/', 'gaming', ')', 'and', 'not', 'interacting', 'with', 'people', '.', 'deep', 'down', 'i', \"'\", 'm', 'afraid', 'of', 'peoples', 'interpretations', '(', 'rejection', 'of', 'me', ')', 'of', 'my', 'actions', 'due', 'to', 'my', 'self', 'per', '##cie', '##ved', '\"', 'social', 'inc', '##omp', '##ten', '##ce', '.', '\"', 'despite', 'the', 'fact', 'that', 'my', 'friends', 'say', 'that', 'i', \"'\", 'm', 'really', 'social', 'and', 'fun', 'to', 'be', 'around', 'and', 'most', 'people', 'tend', 'to', 'enjoy', 'my', 'company', ',', 'i', 'continually', 'isolate', 'myself', '.', 'quite', 'simply', 'i', \"'\", 've', 'always', 'been', 'a', 'lone', '##r', 'because', 'i', \"'\", 've', 'had', 'trouble', 'with', 'or', 'perceived', 'that', 'i', 'have', 'trouble', 'with', 'social', 'interactions', ',', 'and', 'am', 'sick', 'of', 'it', '.', 'i', 'want', 'to', 'be', 'happier', 'and', 'i', 'know', 'that', 'becoming', 'more', 'social', 'will', 'help', 'me', '.', '(', 'i', 'have', 'started', 'to', 'limit', 'my', 'computer', 'time', 'to', '3', 'hours', 'a', 'day', ',', 'and', 'forcing', 'myself', 'to', 'be', 'more', 'social', ',', 'and', 'it', 'seems', 'to', 'be', 'working', '.', ')', 'today', ',', 'i', 'was', 'sitting', 'in', 'my', 'physiology', 'class', 'when', ',', 'we', 'started', 'talking', 'about', 'issues', 'of', 'the', 'pre', '##front', '##al', 'cortex', ',', 'namely', ',', 'ad', '##hd', '.', 'we', 'then', 'started', 'to', 'talk', 'about', 'the', 'social', 'aspects', 'of', 'ad', '##hd', ',', 'and', 'something', 'clicked', 'in', 'my', 'mind', '.', 'i', 'suppose', 'i', 'finally', 'realized', 'that', 'the', 'above', 'social', 'anxiety', 'most', 'likely', 'stemmed', 'from', 'my', 'childhood', 'social', 'problems', 'due', 'to', 'super', 'high', 'doses', 'of', 'ri', '##tila', '##n', '(', '150', '-', '180', '##mg', '/', 'day', ')', 'combined', 'with', 'some', 'pretty', 'crazy', 'ad', '##hd', '.', 'i', 'am', 'actually', 'a', 'pretty', 'social', 'and', 'li', '##ka', '##ble', 'guy', 'though', 'i', 'do', 'have', 'a', 'strong', 'personality', 'that', 'some', 'people', 'have', 'difficulty', 'dealing', 'with', '(', 'especially', 'me', 'being'], ['an', 'american', 'living', 'in', 'sweden', '.', ')', 'yet', ',', 'somehow', 'i', 'am', 'still', 'driven', 'to', 'isolate', 'myself', 'from', 'social', 'interaction', 'possibly', 'in', 'order', 'to', 'preserve', 'my', 'self', 'esteem', '(', 'what', 'i', 'did', 'as', 'a', 'child', ')', '.', 'does', 'anyone', 'else', 'suffer', 'the', 'consequences', 'of', 'this', 'sort', 'of', 'social', 'scar', 'from', 'a', 'childhood', 'with', 'ad', '##hd', '?', 'any', 'tips', 'on', 'getting', 'over', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'va', 'and', 'ad', '##hd', 'i', \"'\", 'm', 'an', 'o', '##if', 'veteran', '.', 'after', 'i', 'left', 'the', 'service', 'i', 'never', 'bothered', 'seeking', 'any', 'sort', 'of', 'health', 'benefits', 'from', 'the', 'va', '.', 'in', 'the', 'years', 'between', 'then', 'and', 'now', 'i', \"'\", 've', 'made', 'significant', 'use', 'of', 'the', 'psychological', 'health', 'services', 'offered', 'on', 'campus', ',', 'but', 'this', 'will', 'be', 'ending', 'shortly', '.', 'was', 'wondering', 'if', 'anyone', 'else', 'has', 'gotten', 'involved', 'with', 'the', 'va', 'regarding', 'ad', '##hd', '-', 'like', 'symptoms', ',', 'or', 'what', 'the', 'experience', 'is', 'like', 'with', 'them', '.', '.', '.', 'i', \"'\", 've', 'heard', 'that', 'since', 'most', 'people', 'consider', 'ad', '##hd', 'to', 'be', 'something', 'that', 'arises', 'in', 'childhood', ',', 'that', 'b', '/', 'c', 'it', 'is', 'a', 'pre', '-', 'existing', 'condition', ',', 'they', 'won', \"'\", 't', 'help', '.', 'thanks']\n",
      "INFO:__main__:Number of tokens: 127\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'va', 'and', 'ad', '##hd', 'i', \"'\", 'm', 'an', 'o', '##if', 'veteran', '.', 'after', 'i', 'left', 'the', 'service', 'i', 'never', 'bothered', 'seeking', 'any', 'sort', 'of', 'health', 'benefits', 'from', 'the', 'va', '.', 'in', 'the', 'years', 'between', 'then', 'and', 'now', 'i', \"'\", 've', 'made', 'significant', 'use', 'of', 'the', 'psychological', 'health', 'services', 'offered', 'on', 'campus', ',', 'but', 'this', 'will', 'be', 'ending', 'shortly', '.', 'was', 'wondering', 'if', 'anyone', 'else', 'has', 'gotten', 'involved', 'with', 'the', 'va', 'regarding', 'ad', '##hd', '-', 'like', 'symptoms', ',', 'or', 'what', 'the', 'experience', 'is', 'like', 'with', 'them', '.', '.', '.', 'i', \"'\", 've', 'heard', 'that', 'since', 'most', 'people', 'consider', 'ad', '##hd', 'to', 'be', 'something', 'that', 'arises', 'in', 'childhood', ',', 'that', 'b', '/', 'c', 'it', 'is', 'a', 'pre', '-', 'existing', 'condition', ',', 'they', 'won', \"'\", 't', 'help', '.', 'thanks']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sometimes', 'i', 'think', 'normal', 'people', 'have', 'a', '\"', 'focus', '\"', 'super', '##power', 'that', 'allows', 'them', 'to', 'turn', 'off', 'the', 'noise', '.', 'and', 'i', 'was', 'never', 'given', 'it', '.', ':', '/', '.', 'this', 'is', 'one', 'of', 'the', 'sad', '##des', '##t', 'thoughts', 'i', \"'\", 've', 'come', 'up', 'with', '.', 'it', 'puts', 'me', 'in', 'a', 'pretty', 'rough', 'mood', 'when', 'i', 'look', 'back', 'at', 'all', 'the', 'time', 'spent', 'not', 'knowing', 'what', 'was', 'wrong', 'and', 'beating', 'myself', 'up', 'emotionally', 'thinking', 'i', 'was', 'just', 'a', 'failure', '.']\n",
      "INFO:__main__:Number of tokens: 81\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sometimes', 'i', 'think', 'normal', 'people', 'have', 'a', '\"', 'focus', '\"', 'super', '##power', 'that', 'allows', 'them', 'to', 'turn', 'off', 'the', 'noise', '.', 'and', 'i', 'was', 'never', 'given', 'it', '.', ':', '/', '.', 'this', 'is', 'one', 'of', 'the', 'sad', '##des', '##t', 'thoughts', 'i', \"'\", 've', 'come', 'up', 'with', '.', 'it', 'puts', 'me', 'in', 'a', 'pretty', 'rough', 'mood', 'when', 'i', 'look', 'back', 'at', 'all', 'the', 'time', 'spent', 'not', 'knowing', 'what', 'was', 'wrong', 'and', 'beating', 'myself', 'up', 'emotionally', 'thinking', 'i', 'was', 'just', 'a', 'failure', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['are', 'we', 'doing', 'to', 'accomplish', 'tuesday', '?', 'or', 'can', 'we', 'switch', 'it', 'to', 'sunday', 'nights', 'after', 'this', 'week', '?', 'setting', 'goals', 'and', 'being', 'held', 'accountable', '(', 'as', 'much', 'as', 'internet', 'strangers', 'can', 'accomplish', 'that', ')', 'is', 'something', 'that', 'really', 'helps', 'me', ',', 'so', 'i', \"'\", 'd', 'love', 'to', 'do', 'this', 'before', 'the', 'work', 'week', 'starts', '.', 'i', 'really', 'like', 'the', 'idea', 'of', 'kick', 'start', 'sunday', 'or', 'motivation', 'monday', ',', 'followed', 'by', 'a', 'mid', '-', 'week', 'check', '-', 'in', 'with', 'win', 'wednesday', '.', 'i', 'could', 'really', 'use', 'a', 'kick', 'start', 'right', 'now', '.', ':', '(']\n",
      "INFO:__main__:Number of tokens: 94\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['are', 'we', 'doing', 'to', 'accomplish', 'tuesday', '?', 'or', 'can', 'we', 'switch', 'it', 'to', 'sunday', 'nights', 'after', 'this', 'week', '?', 'setting', 'goals', 'and', 'being', 'held', 'accountable', '(', 'as', 'much', 'as', 'internet', 'strangers', 'can', 'accomplish', 'that', ')', 'is', 'something', 'that', 'really', 'helps', 'me', ',', 'so', 'i', \"'\", 'd', 'love', 'to', 'do', 'this', 'before', 'the', 'work', 'week', 'starts', '.', 'i', 'really', 'like', 'the', 'idea', 'of', 'kick', 'start', 'sunday', 'or', 'motivation', 'monday', ',', 'followed', 'by', 'a', 'mid', '-', 'week', 'check', '-', 'in', 'with', 'win', 'wednesday', '.', 'i', 'could', 'really', 'use', 'a', 'kick', 'start', 'right', 'now', '.', ':', '(']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['could', 'i', 'be', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['could', 'i', 'be', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['could', 'i', 'be', 'experiencing', 'adult', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['could', 'i', 'be', 'experiencing', 'adult', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['who', 'else', 'is', 'just', 'finishing', 'up', 'their', 'taxes', '?', 'finally', 'took', 'the', 'hour', 'or', 'so', 'to', 'do', 'my', 'taxes', 'and', 'fa', '##sf', '##a', 'and', 'boy', 'does', 'it', 'feel', 'good', 'to', 'have', 'those', 'tab', '##s', 'i', 'opened', 'a', 'month', 'ago', 'with', 'the', 'forms', 'finally', 'closed', '.', '.', 'edit', ':', 'added', 'bonus', 'of', 'reminding', 'all', 'my', 'fellow', 'ad', '##hd', 'red', '##dit', '##ors', 'to', 'do', 'theirs', 'ha', '##ha', '.', ':', 'd']\n",
      "INFO:__main__:Number of tokens: 68\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['who', 'else', 'is', 'just', 'finishing', 'up', 'their', 'taxes', '?', 'finally', 'took', 'the', 'hour', 'or', 'so', 'to', 'do', 'my', 'taxes', 'and', 'fa', '##sf', '##a', 'and', 'boy', 'does', 'it', 'feel', 'good', 'to', 'have', 'those', 'tab', '##s', 'i', 'opened', 'a', 'month', 'ago', 'with', 'the', 'forms', 'finally', 'closed', '.', '.', 'edit', ':', 'added', 'bonus', 'of', 'reminding', 'all', 'my', 'fellow', 'ad', '##hd', 'red', '##dit', '##ors', 'to', 'do', 'theirs', 'ha', '##ha', '.', ':', 'd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['losing', '(', 'important', ')', 'things', 'does', 'anybody', 'else', 'on', 'this', 'sub', '##red', '##dit', 'find', 'they', 'lose', 'important', 'things', '?', '*', 'really', 'important', '*', 'things', '?', 'i', \"'\", 'm', 'ripping', 'my', 'apartment', 'apart', 'trying', 'to', 'find', 'my', 'de', '##bit', 'card', '.', 'i', 'went', 'to', 'the', 'grocery', 'store', 'with', 'it', ',', 'and', 'i', 'assume', 'i', 'put', 'it', 'down', 'when', 'i', 'came', 'back', 'and', 'didn', \"'\", 't', 'bother', 'to', 'note', 'where', 'it', 'was', '.', 'i', 'say', 'assume', 'because', 'lately', 'i', \"'\", 've', 'had', 'this', 'paranoia', 'of', 'sorts', 'about', 'things', '(', 'like', 'my', 'de', '##bit', 'card', ',', 'for', 'example', ')', 'falling', 'out', 'of', 'my', 'pocket', 'without', 'me', 'noticing', '.', 'this', 'is', 'not', 'the', 'first', 'time', 'i', \"'\", 've', 'lost', 'the', 'damn', 'thing', ',', 'either', '.', 'i', \"'\", 've', 'also', 'mis', '##placed', 'my', 'wallet', 'several', 'dozen', 'times', '(', 'and', 'by', 'extension', ',', 'my', 'drivers', 'license', ')', '.', 'does', 'anyone', 'have', 'a', 'similar', 'problem', '?', 'add', '##end', '##um', ':', 'this', 'is', 'starting', 'to', 'get', 'ridiculous', '.', 'i', 'have', 'searched', 'my', 'tiny', 'apartment', 'from', 'top', 'to', 'bottom', 'and', 'it', 'has', 'not', 'shown', 'up', '.', 'i', 'would', 'honestly', 'be', 'sort', 'of', 'okay', 'with', 'my', 'card', 'being', 'used', 'by', 'some', 'th', '##ieving', 'bastards', 'right', 'now', ',', 'just', 'because', 'that', 'would', 'mean', 'i', 'could', 'account', 'for', 'where', 'my', 'card', 'is', 'and', 'who', 'has', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 215\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['losing', '(', 'important', ')', 'things', 'does', 'anybody', 'else', 'on', 'this', 'sub', '##red', '##dit', 'find', 'they', 'lose', 'important', 'things', '?', '*', 'really', 'important', '*', 'things', '?', 'i', \"'\", 'm', 'ripping', 'my', 'apartment', 'apart', 'trying', 'to', 'find', 'my', 'de', '##bit', 'card', '.', 'i', 'went', 'to', 'the', 'grocery', 'store', 'with', 'it', ',', 'and', 'i', 'assume', 'i', 'put', 'it', 'down', 'when', 'i', 'came', 'back', 'and', 'didn', \"'\", 't', 'bother', 'to', 'note', 'where', 'it', 'was', '.', 'i', 'say', 'assume', 'because', 'lately', 'i', \"'\", 've', 'had', 'this', 'paranoia', 'of', 'sorts', 'about', 'things', '(', 'like', 'my', 'de', '##bit', 'card', ',', 'for', 'example', ')', 'falling', 'out', 'of', 'my', 'pocket', 'without', 'me', 'noticing', '.', 'this', 'is', 'not', 'the', 'first', 'time', 'i', \"'\", 've', 'lost', 'the', 'damn', 'thing', ',', 'either', '.', 'i', \"'\", 've', 'also', 'mis', '##placed', 'my', 'wallet', 'several', 'dozen', 'times', '(', 'and', 'by', 'extension', ',', 'my', 'drivers', 'license', ')', '.', 'does', 'anyone', 'have', 'a', 'similar', 'problem', '?', 'add', '##end', '##um', ':', 'this', 'is', 'starting', 'to', 'get', 'ridiculous', '.', 'i', 'have', 'searched', 'my', 'tiny', 'apartment', 'from', 'top', 'to', 'bottom', 'and', 'it', 'has', 'not', 'shown', 'up', '.', 'i', 'would', 'honestly', 'be', 'sort', 'of', 'okay', 'with', 'my', 'card', 'being', 'used', 'by', 'some', 'th', '##ieving', 'bastards', 'right', 'now', ',', 'just', 'because', 'that', 'would', 'mean', 'i', 'could', 'account', 'for', 'where', 'my', 'card', 'is', 'and', 'who', 'has', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'things', 'you', 'like', 'about', 'yourself', ',', 'your', 'life', ',', 'and', 'your', 'ad', '##hd', '?', 'i', 'see', 'so', 'many', 'negative', 'posts', ',', 'it', 'actually', 'brings', 'me', 'down', '.', 'i', \"'\", 'm', 'trying', 'to', 'focus', 'on', 'the', 'positive', ',', 'but', 'i', 'can', 'think', 'of', 'anything', 'at', 'the', 'moment', ',', 'that', 'don', \"'\", 't', 'sound', 'like', 'flat', 'out', 'bragg', '##ing', '.', 'so', ',', 'i', 'want', 'to', 'know', '-', '-', 'what', 'are', 'things', 'you', 'do', '*', 'well', '*', ',', 'what', 'goes', '*', 'well', '*', 'in', 'your', 'life', ',', 'what', 'do', 'you', '*', 'like', '*', 'about', 'yourself', ',', 'and', 'how', 'do', 'you', 'think', 'ad', '##hd', '*', 'benefits', '*', 'you', '?']\n",
      "INFO:__main__:Number of tokens: 107\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'things', 'you', 'like', 'about', 'yourself', ',', 'your', 'life', ',', 'and', 'your', 'ad', '##hd', '?', 'i', 'see', 'so', 'many', 'negative', 'posts', ',', 'it', 'actually', 'brings', 'me', 'down', '.', 'i', \"'\", 'm', 'trying', 'to', 'focus', 'on', 'the', 'positive', ',', 'but', 'i', 'can', 'think', 'of', 'anything', 'at', 'the', 'moment', ',', 'that', 'don', \"'\", 't', 'sound', 'like', 'flat', 'out', 'bragg', '##ing', '.', 'so', ',', 'i', 'want', 'to', 'know', '-', '-', 'what', 'are', 'things', 'you', 'do', '*', 'well', '*', ',', 'what', 'goes', '*', 'well', '*', 'in', 'your', 'life', ',', 'what', 'do', 'you', '*', 'like', '*', 'about', 'yourself', ',', 'and', 'how', 'do', 'you', 'think', 'ad', '##hd', '*', 'benefits', '*', 'you', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['one', 'of', 'the', 'best', 'apps', 'to', 'help', 'focusing', 'with', 'add', '/', 'ad', '##hd', 'focus', '##bar', 'is', 'an', 'incredibly', 'annoying', 'app', 'for', 'os', 'x', 'that', 'always', 'displays', 'the', 'task', 'at', 'hand', 'when', 'you', 'navigate', 'away', 'from', 'what', 'you', \"'\", 're', 'supposed', 'to', 'be', 'doing', ',', 'example', '…', 'checking', 'red', '##dit', ',', 'etc', '.', 'people', 'without', 'add', '/', 'ad', '##hd', 'will', 'find', 'this', 'incredibly', 'annoying', ',', 'but', 'i', \"'\", 've', 'found', 'when', 'i', 'pro', '##active', '##ly', 'use', 'it', 'i', \"'\", 'm', 'able', 'to', 'get', 'work', 'done', '.', 'it', \"'\", 's', 'free', ',', 'so', 'i', 'suggest', 'checking', 'it', 'out', '!', 'http', ':', '/', '/', 'itunes', '.', 'apple', '.', 'com', '/', 'us', '/', 'app', '/', 'focus', '##bar', '/', 'id', '##44', '##34', '##39', '##12', '##7', '?', 'mt', '=', '12']\n",
      "INFO:__main__:Number of tokens: 124\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['one', 'of', 'the', 'best', 'apps', 'to', 'help', 'focusing', 'with', 'add', '/', 'ad', '##hd', 'focus', '##bar', 'is', 'an', 'incredibly', 'annoying', 'app', 'for', 'os', 'x', 'that', 'always', 'displays', 'the', 'task', 'at', 'hand', 'when', 'you', 'navigate', 'away', 'from', 'what', 'you', \"'\", 're', 'supposed', 'to', 'be', 'doing', ',', 'example', '…', 'checking', 'red', '##dit', ',', 'etc', '.', 'people', 'without', 'add', '/', 'ad', '##hd', 'will', 'find', 'this', 'incredibly', 'annoying', ',', 'but', 'i', \"'\", 've', 'found', 'when', 'i', 'pro', '##active', '##ly', 'use', 'it', 'i', \"'\", 'm', 'able', 'to', 'get', 'work', 'done', '.', 'it', \"'\", 's', 'free', ',', 'so', 'i', 'suggest', 'checking', 'it', 'out', '!', 'http', ':', '/', '/', 'itunes', '.', 'apple', '.', 'com', '/', 'us', '/', 'app', '/', 'focus', '##bar', '/', 'id', '##44', '##34', '##39', '##12', '##7', '?', 'mt', '=', '12']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['who', 'has', 'been', 'your', 'biggest', 'supporter', '/', 'help', '##er', 'in', 'your', 'struggle', 'with', 'ad', '##hd', '?', 'while', 'i', 'have', 'always', 'known', 'that', 'my', 'mom', \"'\", 's', 'support', 'is', 'the', 'main', 'reason', 'i', 'have', 'been', 'able', 'to', 'succeed', 'despite', 'my', 'ad', '##hd', ',', 'it', 'just', 'kinda', 'hit', 'me', 'how', 'much', 'she', 'has', 'done', 'for', 'me', 'when', 'i', 'was', 'discussing', 'a', 'recent', 'insurance', '/', 'med', '##s', 'issue', '.', 'over', 'the', 'last', '15', 'years', 'she', 'has', 'helped', 'me', 'earn', 'my', 'eagle', 'scout', ',', 'taught', 'me', 'great', 'grammar', '/', 'spelling', ',', 'dealt', 'with', 'countless', 'insurance', 'issue', ',', 'advocated', 'for', 'me', 'with', 'teachers', 'who', 'didn', \"'\", 't', '\"', 'believe', '\"', 'in', 'ad', '##hd', ',', 'and', 'at', 'least', 'a', 'dozen', 'other', 'things', 'she', '##s', 'done', 'that', 'i', 'can', \"'\", 't', 'remember', '.', 'i', 'know', 'that', 'she', 'was', 'able', 'to', 'handle', 'me', 'because', 'her', 'sister', 'had', 'und', '##ia', '##gno', '##sed', 'ad', '##hd', '(', 'she', '[', 'the', 'sister', ']', 'finally', 'got', 'diagnosed', 'a', 'few', 'years', 'ago', ')', ',', 'and', 'my', 'mom', 'learned', 'how', 'to', 'deal', 'with', 'it', 'from', 'a', 'young', 'age', '.', 'who', 'has', 'been', 'the', 'biggest', 'help', '/', 'inspiration', 'for', 'you', ',', 'whether', 'you', \"'\", 've', 'been', 'diagnosed', 'for', 'many', 'years', 'or', 'a', 'few', 'weeks', '?']\n",
      "INFO:__main__:Number of tokens: 199\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['who', 'has', 'been', 'your', 'biggest', 'supporter', '/', 'help', '##er', 'in', 'your', 'struggle', 'with', 'ad', '##hd', '?', 'while', 'i', 'have', 'always', 'known', 'that', 'my', 'mom', \"'\", 's', 'support', 'is', 'the', 'main', 'reason', 'i', 'have', 'been', 'able', 'to', 'succeed', 'despite', 'my', 'ad', '##hd', ',', 'it', 'just', 'kinda', 'hit', 'me', 'how', 'much', 'she', 'has', 'done', 'for', 'me', 'when', 'i', 'was', 'discussing', 'a', 'recent', 'insurance', '/', 'med', '##s', 'issue', '.', 'over', 'the', 'last', '15', 'years', 'she', 'has', 'helped', 'me', 'earn', 'my', 'eagle', 'scout', ',', 'taught', 'me', 'great', 'grammar', '/', 'spelling', ',', 'dealt', 'with', 'countless', 'insurance', 'issue', ',', 'advocated', 'for', 'me', 'with', 'teachers', 'who', 'didn', \"'\", 't', '\"', 'believe', '\"', 'in', 'ad', '##hd', ',', 'and', 'at', 'least', 'a', 'dozen', 'other', 'things', 'she', '##s', 'done', 'that', 'i', 'can', \"'\", 't', 'remember', '.', 'i', 'know', 'that', 'she', 'was', 'able', 'to', 'handle', 'me', 'because', 'her', 'sister', 'had', 'und', '##ia', '##gno', '##sed', 'ad', '##hd', '(', 'she', '[', 'the', 'sister', ']', 'finally', 'got', 'diagnosed', 'a', 'few', 'years', 'ago', ')', ',', 'and', 'my', 'mom', 'learned', 'how', 'to', 'deal', 'with', 'it', 'from', 'a', 'young', 'age', '.', 'who', 'has', 'been', 'the', 'biggest', 'help', '/', 'inspiration', 'for', 'you', ',', 'whether', 'you', \"'\", 've', 'been', 'diagnosed', 'for', 'many', 'years', 'or', 'a', 'few', 'weeks', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'advice', '.', 'i', 'think', 'i', \"'\", 'm', 'going', 'do', 'poorly', 'on', 'my', 'business', 'calculus', 'test', '6', 'hours', 'from', 'now', '.', 'like', 'the', 'last', 'test', ',', 'i', 'tried', 'to', 'start', 'an', 'entire', 'week', 'early', 'and', 'tried', 'to', 'spend', 'as', 'much', 'time', 'as', 'possible', 'at', 'my', 'schools', 'tutor', '##ing', 'centers', '.', '.', '.', 'but', 'i', 'just', 'couldn', \"'\", 't', 'force', 'myself', 'to', 'focus', 'and', 'absorb', 'the', 'material', '.', 'or', ',', 'while', 'we', \"'\", 're', 'on', 'the', 'subject', ',', 'do', 'nearly', 'any', 'of', 'the', 'tasks', 'i', 'laid', 'out', 'for', 'myself', 'this', 'semester', 'or', 'pay', 'attention', 'in', 'any', 'of', 'my', 'classes', ',', 'or', 'catch', 'up', 'on', 'my', 'emails', 'which', 'are', 'now', 'almost', 'an', 'entire', 'semester', 'behind', '.', 'this', 'will', 'probably', 'be', 'the', 'first', 'class', 'i', 'will', 'have', 'failed', 'since', '1st', 'grade', ',', 'after', 'i', 'was', 'diagnosed', '/', 'prescribed', 'for', 'ad', '##hd', '.', 'if', 'you', 'guys', 'think', 'this', 'is', 'all', 'reason', 'enough', 'for', 'getting', 'med', '##s', 'to', 'treat', 'my', 'add', ',', 'i', 'wanna', 'make', 'a', 'pact', 'with', 'you', 'all', '.', 'despite', 'me', 'having', 'to', 'go', 'through', 'the', 'bureau', '##cratic', 'processes', 'of', 'comparing', '/', 'switching', 'insurance', 'providers', ',', 'possible', 'switching', 'banks', ',', 'and', 'searching', 'for', 'the', 'cheap', '##est', 'doctor', 'in', 'the', 'area', ',', 'i', 'will', 'get', 'through', 'those', 'ridiculous', '##ly', 'stupid', 'obstacles', ',', 'get', 'a', 'prescription', 'and', 'go', 'to', 'get', 'it', 'asa', '##p', 'after', 'failing', 'this', 'test', '.', 'also', ',', 'any', 'information', 'on', 'how', 'to', 'approach', 'getting', 'a', 'doctor', '/', 'prescription', 'would', 'be', 'appreciated', '.', 'i', 'hate', 'dealing', 'medical', 'bureaucracy', ',', 'but', 'still', 'prefer', 'cheaper', 'options', '.', 't', '##l', ';', 'dr', 'my', 'lack', 'of', 'focus', 'is', 'having', 'negative', 'effects', 'on', 'my', 'life', 'and', 'i', \"'\", 'm', 'wondering', 'if', 'i', 'should', 'get', 'med', '##s', 'because', 'of', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 283\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'advice', '.', 'i', 'think', 'i', \"'\", 'm', 'going', 'do', 'poorly', 'on', 'my', 'business', 'calculus', 'test', '6', 'hours', 'from', 'now', '.', 'like', 'the', 'last', 'test', ',', 'i', 'tried', 'to', 'start', 'an', 'entire', 'week', 'early', 'and', 'tried', 'to', 'spend', 'as', 'much', 'time', 'as', 'possible', 'at', 'my', 'schools', 'tutor', '##ing', 'centers', '.', '.', '.', 'but', 'i', 'just', 'couldn', \"'\", 't', 'force', 'myself', 'to', 'focus', 'and', 'absorb', 'the', 'material', '.', 'or', ',', 'while', 'we', \"'\", 're', 'on', 'the', 'subject', ',', 'do', 'nearly', 'any', 'of', 'the', 'tasks', 'i', 'laid', 'out', 'for', 'myself', 'this', 'semester', 'or', 'pay', 'attention', 'in', 'any', 'of', 'my', 'classes', ',', 'or', 'catch', 'up', 'on', 'my', 'emails', 'which', 'are', 'now', 'almost', 'an', 'entire', 'semester', 'behind', '.', 'this', 'will', 'probably', 'be', 'the', 'first', 'class', 'i', 'will', 'have', 'failed', 'since', '1st', 'grade', ',', 'after', 'i', 'was', 'diagnosed', '/', 'prescribed', 'for', 'ad', '##hd', '.', 'if', 'you', 'guys', 'think', 'this', 'is', 'all', 'reason', 'enough', 'for', 'getting', 'med', '##s', 'to', 'treat', 'my', 'add', ',', 'i', 'wanna', 'make', 'a', 'pact', 'with', 'you', 'all', '.', 'despite', 'me', 'having', 'to', 'go', 'through', 'the', 'bureau', '##cratic', 'processes', 'of', 'comparing', '/', 'switching', 'insurance', 'providers', ',', 'possible', 'switching', 'banks', ',', 'and', 'searching', 'for', 'the', 'cheap', '##est', 'doctor', 'in', 'the', 'area', ',', 'i', 'will', 'get', 'through', 'those', 'ridiculous', '##ly', 'stupid', 'obstacles', ',', 'get', 'a', 'prescription', 'and', 'go', 'to', 'get', 'it', 'asa', '##p', 'after', 'failing', 'this', 'test', '.', 'also', ',', 'any', 'information', 'on', 'how', 'to', 'approach', 'getting', 'a', 'doctor', '/', 'prescription', 'would', 'be', 'appreciated', '.', 'i', 'hate', 'dealing', 'medical', 'bureaucracy', ',', 'but', 'still', 'prefer', 'cheaper', 'options', '.', 't', '##l', ';', 'dr', 'my', 'lack', 'of', 'focus', 'is', 'having', 'negative', 'effects', 'on', 'my', 'life', 'and', 'i', \"'\", 'm', 'wondering', 'if', 'i', 'should', 'get', 'med', '##s', 'because', 'of', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['now', 'what', '?']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['now', 'what', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'seven', 'deadly', 'sins', ':', 'lessons', 'learned', 'from', 'the', 'three', '-', 'toe', '##d', 'slot', '##h']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'seven', 'deadly', 'sins', ':', 'lessons', 'learned', 'from', 'the', 'three', '-', 'toe', '##d', 'slot', '##h']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'to', 'accomplish', 'tuesday', ']', 'share', 'one', 'thing', 'you', 'would', 'like', 'to', 'do', 'this', 'week', '.', '/', 'r', '/', 'ad', '##hd', 'will', 'keep', 'you', 'accountable', 'and', 'support', 'you', '!', '(', 'moving', 'to', 'sun', '.', 'next', 'week', ')', 'welcome', 'to', 'part', 'iv', '(', 'and', 'final', 'edition', ')', 'of', 'to', 'accomplish', 'tuesday', '!', 'lots', 'of', 'people', 'have', 'been', 'suggesting', 'i', 'move', 'this', 'to', 'monday', '(', 'as', 'originally', 'planned', ')', 'or', 'sunday', '.', 'so', 'starting', 'next', 'week', 'we', 'will', 'have', '*', '*', '[', 'kicks', '##tar', '##t', 'sunday', ']', '*', '*', '(', 'unless', 'you', 'suggest', 'a', 'better', 'name', ')', '.', 'share', 'one', '(', 'or', 'a', 'couple', ')', 'of', 'things', 'you', 'want', 'to', 'get', 'done', 'this', 'week', '.', 'i', 'will', 'give', 'you', 'a', 'suggested', 'format', 'at', 'the', 'end', '.', '*', '*', 'everyone', 'can', 'feel', 'free', 'to', 'check', '-', 'in', 'with', 'people', 'who', 'request', 'it', '.', 'i', 'was', 'too', 'busy', '/', 'distracted', 'last', 'week', 'so', 'i', 'would', 'appreciate', 'help', 'from', 'everyone', 'in', 'doing', 'follow', '##up', '!', '*', '*', '*', '*', '*', '[', 'check', 'out', 'last', 'weeks', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '2', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '1', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'r', '##g', '##1', '##q', '##f', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '*', 'suggested', 'rules', '*', '[UNK]', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to', 'do', 'that', 'too', '!', ')', '[UNK]', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'commit', '##ing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '[UNK]', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '[UNK]', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', '*', '*', 'tips', '*', '*', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', 'one', 'hour', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '.', '*', '*', '*', '[UNK]', '*', '*', 'examples', 'from', 'previous', 'weeks', ':', '*', '*', '[UNK]', '[UNK]', 'un', '##load', 'the', 'dish', '##wash', '##er', '[UNK]', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '[UNK]', 'make', 'an', 'appointment', 'with', 'doctor', '[UNK]', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '[UNK]', '*', '*', 'start', '*', '*', 'something', '.', 'spend', '5', 'minutes', 'on', 'writing', 'my', 'paper', '.', '[UNK]', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '*', 'i', 'made', 'it', 'easier', 'this', 'week', 'to', 'respond', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '*', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?', 'when', '?', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-', '*', '*', '*', 'hopefully', 'most', 'of', 'you', 'can', 'also', 'participate', 'in', 'win', 'wednesday', 'tomorrow', '.', 'we', 'had', 'a', 'drop', 'in', 'participation', 'in', 'both', 'these', 'threads', 'last', 'week', '.', 'if', 'you', 'have', 'any', 'other', 'suggestions', 'for', 'weekly', 'threads', 'or', 'improvements', 'to', 'these', 'ones', 'let', 'me', 'know', '!', '*', '*', '*', '*', '*', 'today', 'is', 'the', 'final', 'day', 'to', 'submit', 'taxes', 'or', 'file', 'for', 'an', 'extension', '.', 'if', 'you', 'have', 'not', 'filed', 'taxes', 'yet', 'be', 'sure', 'to', 'file', 'an', 'extension', '!', 'it', 'takes', 'about', '2', 'minutes', 'and', 'you', 'can', 'avoid', 'the', 'penalties', '!', '*', '*']\n",
      "INFO:__main__:Number of tokens: 1001\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['[', 'to', 'accomplish', 'tuesday', ']', 'share', 'one', 'thing', 'you', 'would', 'like', 'to', 'do', 'this', 'week', '.', '/', 'r', '/', 'ad', '##hd', 'will', 'keep', 'you', 'accountable', 'and', 'support', 'you', '!', '(', 'moving', 'to', 'sun', '.', 'next', 'week', ')', 'welcome', 'to', 'part', 'iv', '(', 'and', 'final', 'edition', ')', 'of', 'to', 'accomplish', 'tuesday', '!', 'lots', 'of', 'people', 'have', 'been', 'suggesting', 'i', 'move', 'this', 'to', 'monday', '(', 'as', 'originally', 'planned', ')', 'or', 'sunday', '.', 'so', 'starting', 'next', 'week', 'we', 'will', 'have', '*', '*', '[', 'kicks', '##tar', '##t', 'sunday', ']', '*', '*', '(', 'unless', 'you', 'suggest', 'a', 'better', 'name', ')', '.', 'share', 'one', '(', 'or', 'a', 'couple', ')', 'of', 'things', 'you', 'want', 'to', 'get', 'done', 'this', 'week', '.', 'i', 'will', 'give', 'you', 'a', 'suggested', 'format', 'at', 'the', 'end', '.', '*', '*', 'everyone', 'can', 'feel', 'free', 'to', 'check', '-', 'in', 'with', 'people', 'who', 'request', 'it', '.', 'i', 'was', 'too', 'busy', '/', 'distracted', 'last', 'week', 'so', 'i', 'would', 'appreciate', 'help', 'from', 'everyone', 'in', 'doing', 'follow', '##up', '!', '*', '*', '*', '*', '*', '[', 'check', 'out', 'last', 'weeks', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '2', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '1', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'r', '##g', '##1', '##q', '##f', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '*', 'suggested', 'rules', '*', '[UNK]', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to', 'do', 'that', 'too', '!', ')', '[UNK]', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'commit', '##ing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '[UNK]', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '('], ['and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '[UNK]', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', '*', '*', 'tips', '*', '*', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', 'one', 'hour', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '.', '*', '*', '*', '[UNK]', '*', '*', 'examples', 'from', 'previous', 'weeks', ':', '*', '*', '[UNK]', '[UNK]', 'un', '##load', 'the', 'dish', '##wash', '##er', '[UNK]', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '[UNK]', 'make', 'an', 'appointment', 'with', 'doctor', '[UNK]', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '[UNK]', '*', '*', 'start', '*', '*', 'something', '.', 'spend', '5', 'minutes', 'on', 'writing', 'my', 'paper', '.', '[UNK]', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '*', 'i', 'made', 'it', 'easier', 'this', 'week', 'to', 'respond', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '*', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?', 'when', '?', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-', '*', '*', '*', 'hopefully', 'most', 'of', 'you', 'can', 'also', 'participate', 'in', 'win', 'wednesday', 'tomorrow', '.', 'we', 'had', 'a', 'drop', 'in', 'participation', 'in', 'both', 'these', 'threads', 'last', 'week', '.', 'if', 'you', 'have', 'any', 'other', 'suggestions', 'for', 'weekly', 'threads', 'or', 'improvements', 'to', 'these', 'ones', 'let', 'me', 'know', '!', '*', '*', '*', '*', '*', 'today', 'is', 'the', 'final', 'day', 'to', 'submit', 'taxes', 'or', 'file', 'for', 'an', 'extension', '.', 'if', 'you', 'have', 'not', 'filed', 'taxes', 'yet', 'be', 'sure', 'to', 'file', 'an', 'extension', '!', 'it', 'takes', 'about', '2', 'minutes', 'and', 'you', 'can', 'avoid', 'the', 'penalties', '!', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'can', \"'\", 't', 'do', 'this', 'every', 'day', 'for', 'a', 'week', 'i', 'have', 'spent', '3', 'hours', 'trying', 'to', 'focus', 'on', 'finishing', 'my', 'essay', ',', 'one', 'which', 'has', 'been', 'late', 'since', 'the', 'third', 'week', 'of', 'the', 'semester', '.', 'in', 'all', 'i', 'have', 'accomplished', 'a', 'single', 'paragraph', '.', 'i', 'feel', 'like', 'a', 'failure', '.', 'for', 'the', 'first', 'time', 'i', 'have', 'stopped', 'digging', 'in', 'my', 'heels', 'against', 'admitting', 'that', 'my', 'ad', '##hd', 'is', 'a', 'severe', 'problem', 'or', 'being', 'down', '##tro', '##dden', 'and', 'negative', 'about', 'it', '.', 'i', 'am', 'ca', '##ving', 'today', '…', '.']\n",
      "INFO:__main__:Number of tokens: 90\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'can', \"'\", 't', 'do', 'this', 'every', 'day', 'for', 'a', 'week', 'i', 'have', 'spent', '3', 'hours', 'trying', 'to', 'focus', 'on', 'finishing', 'my', 'essay', ',', 'one', 'which', 'has', 'been', 'late', 'since', 'the', 'third', 'week', 'of', 'the', 'semester', '.', 'in', 'all', 'i', 'have', 'accomplished', 'a', 'single', 'paragraph', '.', 'i', 'feel', 'like', 'a', 'failure', '.', 'for', 'the', 'first', 'time', 'i', 'have', 'stopped', 'digging', 'in', 'my', 'heels', 'against', 'admitting', 'that', 'my', 'ad', '##hd', 'is', 'a', 'severe', 'problem', 'or', 'being', 'down', '##tro', '##dden', 'and', 'negative', 'about', 'it', '.', 'i', 'am', 'ca', '##ving', 'today', '…', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['should', 'i', 'see', 'a', 'specialist', 'regarding', 'ad', '##hd', '.', '.', '.', 'again', '?', 'two', 'years', 'ago', ',', 'after', 'finishing', 'college', 'with', 'ab', '##ism', '##al', 'grades', 'and', 'being', 'recommended', 'to', 'go', 'see', 'a', 'specialist', 'by', 'two', 'd', '##ys', '##le', '##xia', 'specialists', ',', 'i', 'decided', 'to', 'go', 'along', 'to', 'a', 'highly', 'respected', 'psychologist', 'to', 'discuss', 'the', 'possibility', 'of', 'having', 'ad', '##hd', '.', 'none', 'of', 'my', 'primary', 'school', 'teacher', 'reports', 'ind', '##icia', '##ted', 'any', 'form', 'of', 'hyper', '##act', '##ivity', 'or', 'lack', 'of', 'focus', ',', 'so', 'he', 'believed', 'it', 'would', 'be', 'wrong', 'to', 'dia', '##gno', '##se', 'me', 'with', 'ad', '##hd', '.', 'instead', ',', 'he', 'determined', 'that', 'due', 'to', 'my', 'shy', 'nature', 'i', 'probably', 'had', 'some', 'form', 'of', 'social', 'ph', '##ob', '##ia', ',', 'putting', 'me', 'on', 'medication', 'and', 'cognitive', 'behaviour', '##al', 'therapy', '.', 'since', 'then', ',', 'socially', 'things', 'got', 'a', 'lot', 'better', '.', 'so', 'he', 'was', 'very', 'right', 'in', 'picking', 'up', 'on', 'my', 'anxiety', ',', 'but', 'i', 'still', 'find', 'myself', 'struggling', 'to', 'focus', '.', 'has', 'anyone', 'else', 'here', 'been', 'diagnosed', 'with', 'something', 'else', ',', 'later', 'finding', 'out', 'they', 'had', 'ad', '##hd', 'after', 'all', '?', 'would', 'i', 'be', 'better', 'of', 'going', 'to', 'another', 'psychologist', 'or', 'the', 'same', '?', 'do', 'you', 'have', 'a', 'success', 'story', '?', 'what', 'was', 'your', 'life', 'like', 'before', 'and', 'after', 'diagnosis', '?']\n",
      "INFO:__main__:Number of tokens: 209\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['should', 'i', 'see', 'a', 'specialist', 'regarding', 'ad', '##hd', '.', '.', '.', 'again', '?', 'two', 'years', 'ago', ',', 'after', 'finishing', 'college', 'with', 'ab', '##ism', '##al', 'grades', 'and', 'being', 'recommended', 'to', 'go', 'see', 'a', 'specialist', 'by', 'two', 'd', '##ys', '##le', '##xia', 'specialists', ',', 'i', 'decided', 'to', 'go', 'along', 'to', 'a', 'highly', 'respected', 'psychologist', 'to', 'discuss', 'the', 'possibility', 'of', 'having', 'ad', '##hd', '.', 'none', 'of', 'my', 'primary', 'school', 'teacher', 'reports', 'ind', '##icia', '##ted', 'any', 'form', 'of', 'hyper', '##act', '##ivity', 'or', 'lack', 'of', 'focus', ',', 'so', 'he', 'believed', 'it', 'would', 'be', 'wrong', 'to', 'dia', '##gno', '##se', 'me', 'with', 'ad', '##hd', '.', 'instead', ',', 'he', 'determined', 'that', 'due', 'to', 'my', 'shy', 'nature', 'i', 'probably', 'had', 'some', 'form', 'of', 'social', 'ph', '##ob', '##ia', ',', 'putting', 'me', 'on', 'medication', 'and', 'cognitive', 'behaviour', '##al', 'therapy', '.', 'since', 'then', ',', 'socially', 'things', 'got', 'a', 'lot', 'better', '.', 'so', 'he', 'was', 'very', 'right', 'in', 'picking', 'up', 'on', 'my', 'anxiety', ',', 'but', 'i', 'still', 'find', 'myself', 'struggling', 'to', 'focus', '.', 'has', 'anyone', 'else', 'here', 'been', 'diagnosed', 'with', 'something', 'else', ',', 'later', 'finding', 'out', 'they', 'had', 'ad', '##hd', 'after', 'all', '?', 'would', 'i', 'be', 'better', 'of', 'going', 'to', 'another', 'psychologist', 'or', 'the', 'same', '?', 'do', 'you', 'have', 'a', 'success', 'story', '?', 'what', 'was', 'your', 'life', 'like', 'before', 'and', 'after', 'diagnosis', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', 'have', 'trouble', 'letting', 'go', 'of', 'negative', 'emotions', '?', 'old', 'anger', ',', 'sadness', ',', 'grief', ',', 'etc', '.', 'there', \"'\", 's', 'a', 'few', 'negative', 'events', 'in', 'my', 'life', 'that', 'seem', 'really', 'hard', 'to', 'move', 'on', 'from', '-', 'many', 'are', '5', '+', 'years', 'out', '.', 'as', 'much', 'as', 'i', \"'\", 'd', 'like', 'to', ',', 'i', 'just', 'can', \"'\", 't', 'seem', 'to', 'fully', 'let', 'them', 'go', 'and', 'move', 'on', '.', 'some', 'things', 'have', 'taken', 'as', 'long', 'as', '8', 'years', 'to', 'get', 'out', 'of', 'my', 'system', '(', 'ie', ',', 'get', 'to', 'the', 'point', 'where', 'i', \"'\", 'm', 'not', 'thinking', 'about', 'a', 'particular', 'event', 'at', 'least', 'once', 'a', 'day', ')', '.', 'it', \"'\", 's', 'not', 'helped', 'that', 'with', 'how', 'much', 'i', 'day', '##dre', '##am', ',', 'i', 'end', 'up', 'thinking', 'about', 'the', 'negative', 'stuff', '*', 'a', 'lot', '*', '.', 'and', 'it', 'seems', 'like', 're', '-', 'living', 'that', 'pain', 'isn', \"'\", 't', 'helping', 'to', 'get', 'rid', 'of', 'it', ',', 'especially', 'when', 'i', 'find', 'ways', 'to', 'add', 'more', 'to', 'the', 'pile', '.', ':', 'p']\n",
      "INFO:__main__:Number of tokens: 168\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', 'have', 'trouble', 'letting', 'go', 'of', 'negative', 'emotions', '?', 'old', 'anger', ',', 'sadness', ',', 'grief', ',', 'etc', '.', 'there', \"'\", 's', 'a', 'few', 'negative', 'events', 'in', 'my', 'life', 'that', 'seem', 'really', 'hard', 'to', 'move', 'on', 'from', '-', 'many', 'are', '5', '+', 'years', 'out', '.', 'as', 'much', 'as', 'i', \"'\", 'd', 'like', 'to', ',', 'i', 'just', 'can', \"'\", 't', 'seem', 'to', 'fully', 'let', 'them', 'go', 'and', 'move', 'on', '.', 'some', 'things', 'have', 'taken', 'as', 'long', 'as', '8', 'years', 'to', 'get', 'out', 'of', 'my', 'system', '(', 'ie', ',', 'get', 'to', 'the', 'point', 'where', 'i', \"'\", 'm', 'not', 'thinking', 'about', 'a', 'particular', 'event', 'at', 'least', 'once', 'a', 'day', ')', '.', 'it', \"'\", 's', 'not', 'helped', 'that', 'with', 'how', 'much', 'i', 'day', '##dre', '##am', ',', 'i', 'end', 'up', 'thinking', 'about', 'the', 'negative', 'stuff', '*', 'a', 'lot', '*', '.', 'and', 'it', 'seems', 'like', 're', '-', 'living', 'that', 'pain', 'isn', \"'\", 't', 'helping', 'to', 'get', 'rid', 'of', 'it', ',', 'especially', 'when', 'i', 'find', 'ways', 'to', 'add', 'more', 'to', 'the', 'pile', '.', ':', 'p']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['learning', 'disability', 'getting', 'out', 'of', 'hand', ',', 'can', \"'\", 't', 'cope', '.', 'hey', 'guys', '.', 'i', \"'\", 'm', 'a', 'fresh', '##er', 'at', 'university', '.', 'for', 'the', 'past', '19', 'years', ',', 'i', \"'\", 've', 'been', 'suffering', 'from', 'this', 'mysterious', 'learning', 'disability', ',', 'that', 'no', 'educational', 'psychologist', 'has', 'been', 'able', 'to', 'dia', '##gno', '##se', 'properly', '.', 'they', \"'\", 've', 'always', 'attributed', 'my', 'symptoms', 'with', 'd', '##ys', '##le', '##xia', '/', 'd', '##ys', '##cal', '##cu', '##lia', '/', 'd', '##ys', '##pr', '##ax', '##ia', ',', 'which', 'is', 'sort', 'of', 'true', ',', 'i', 'write', 'and', 'read', 'horribly', 'slow', '(', 'when', 'reading', 'to', 'myself', ')', ',', 'i', 'find', 'math', '##s', 'a', 'real', 'nightmare', 'and', 'my', 'coordination', 'is', 'pretty', 'lame', '.', 'i', 'suffer', 'from', 'a', 'really', 'bad', 'short', 'term', 'memory', '(', 'also', 'had', 'from', 'when', 'i', 'was', 'young', ')', ',', 'which', 'can', 'make', 'listening', 'and', 'carrying', 'out', 'instructions', 'particularly', 'difficult', ',', 'and', 'quite', 'frustrating', 'to', 'all', 'parties', 'involved', '.', 'for', 'the', 'past', 'three', 'years', 'though', ',', 'i', \"'\", 've', 'been', 'suspect', '##ing', 'i', 'maybe', 'suffering', 'from', 'ad', '##hd', '/', 'add', '.', 'i', 'find', 'that', 'my', 'brain', 'shut', '##s', 'down', 'when', 'it', 'is', 'presented', 'with', 'too', 'much', 'information', ',', 'even', 'stuff', 'i', \"'\", 'm', 'interested', 'in', '.', 'i', 'find', 'it', 'difficult', 'to', 'art', '##iculate', 'my', 'thoughts', 'into', 'a', 'coherent', 'thing', ',', 'because', 'there', \"'\", 's', 'too', 'much', 'stuff', 'floating', 'in', 'my', 'head', '.', 'i', 'find', 'it', 'annoying', 'that', 'i', 'can', \"'\", 't', 'keep', 'track', 'during', 'some', 'elaborate', 'conversations', ',', 'again', 'because', 'theirs', 'too', 'much', 'detail', 'for', 'me', 'to', 'focus', 'on', '.', 'it', \"'\", 's', 'like', 'being', 'a', 'kid', 'in', 'a', 'sweet', 'shop', ',', 'there', \"'\", 's', 'so', 'much', 'there', 'that', 'i', 'want', 'it', 'all', 'but', 'i', 'can', \"'\", 't', 'have', 'it', 'all', '.', 'this', 'is', 'also', 'starting', 'to', 'affect', 'on', 'my', 'emotions', 'too', ',', 'my', 'mood', 'is', 'constantly', 'changing', 'from', 'happy', 'to', 'sad', '-', 'one', 'minute', 'i', \"'\", 'll', 'be', 'fine', ',', 'next', 'i', \"'\", 'll', 'be', 'really', 'sad', '.', 'my', 'temper', 'changes', 'really', 'quick', 'too', ',', 'same', 'deal', 'i', \"'\", 'll', 'be', 'fine', 'one', 'minute', 'and', 'then', 'bam', ',', 'someone', 'will', 'do', 'something', '/', 'say', 'something', 'and', 'i', \"'\", 'll', 'just', 'be', 'full', 'of', 'pure', 'rage', '-', 'i', 'don', \"'\", 't', 'show', 'it', ',', 'i', 'keep', 'it', 'to', 'myself', 'and', 'carry', 'on', '.', 'it', 'could', 'be', 'something', 'stupid', 'like', 'someone', 'knocking', 'on', 'my', 'bedroom', 'door', 'while', 'i', \"'\", 'm', 'working', '.', 'i', 'really', 'want', 'to', 'be', 'fixed', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'go', 'about', 'it', '-', 'i', \"'\", 'm', 'at', 'an', 'excellent', 'university', ',', 'studying', 'an', 'interesting', 'subject', 'and', 'i', \"'\", 'm', 'surrounded', 'by', 'my', 'friends', ',', 'it', \"'\", 's', 'taken', 'me', 'ages', 'to', 'build', 'this', 'up', '-', 'but', 'i', 'could', 'lose', 'it', 'all', '.', ':', '(', 'thanks', 'for', 'reading', '.']\n",
      "INFO:__main__:Number of tokens: 456\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['learning', 'disability', 'getting', 'out', 'of', 'hand', ',', 'can', \"'\", 't', 'cope', '.', 'hey', 'guys', '.', 'i', \"'\", 'm', 'a', 'fresh', '##er', 'at', 'university', '.', 'for', 'the', 'past', '19', 'years', ',', 'i', \"'\", 've', 'been', 'suffering', 'from', 'this', 'mysterious', 'learning', 'disability', ',', 'that', 'no', 'educational', 'psychologist', 'has', 'been', 'able', 'to', 'dia', '##gno', '##se', 'properly', '.', 'they', \"'\", 've', 'always', 'attributed', 'my', 'symptoms', 'with', 'd', '##ys', '##le', '##xia', '/', 'd', '##ys', '##cal', '##cu', '##lia', '/', 'd', '##ys', '##pr', '##ax', '##ia', ',', 'which', 'is', 'sort', 'of', 'true', ',', 'i', 'write', 'and', 'read', 'horribly', 'slow', '(', 'when', 'reading', 'to', 'myself', ')', ',', 'i', 'find', 'math', '##s', 'a', 'real', 'nightmare', 'and', 'my', 'coordination', 'is', 'pretty', 'lame', '.', 'i', 'suffer', 'from', 'a', 'really', 'bad', 'short', 'term', 'memory', '(', 'also', 'had', 'from', 'when', 'i', 'was', 'young', ')', ',', 'which', 'can', 'make', 'listening', 'and', 'carrying', 'out', 'instructions', 'particularly', 'difficult', ',', 'and', 'quite', 'frustrating', 'to', 'all', 'parties', 'involved', '.', 'for', 'the', 'past', 'three', 'years', 'though', ',', 'i', \"'\", 've', 'been', 'suspect', '##ing', 'i', 'maybe', 'suffering', 'from', 'ad', '##hd', '/', 'add', '.', 'i', 'find', 'that', 'my', 'brain', 'shut', '##s', 'down', 'when', 'it', 'is', 'presented', 'with', 'too', 'much', 'information', ',', 'even', 'stuff', 'i', \"'\", 'm', 'interested', 'in', '.', 'i', 'find', 'it', 'difficult', 'to', 'art', '##iculate', 'my', 'thoughts', 'into', 'a', 'coherent', 'thing', ',', 'because', 'there', \"'\", 's', 'too', 'much', 'stuff', 'floating', 'in', 'my', 'head', '.', 'i', 'find', 'it', 'annoying', 'that', 'i', 'can', \"'\", 't', 'keep', 'track', 'during', 'some', 'elaborate', 'conversations', ',', 'again', 'because', 'theirs', 'too', 'much', 'detail', 'for', 'me', 'to', 'focus', 'on', '.', 'it', \"'\", 's', 'like', 'being', 'a', 'kid', 'in', 'a', 'sweet', 'shop', ',', 'there', \"'\", 's', 'so', 'much', 'there', 'that', 'i', 'want', 'it', 'all', 'but', 'i', 'can', \"'\", 't', 'have', 'it', 'all', '.', 'this', 'is', 'also', 'starting', 'to', 'affect', 'on', 'my', 'emotions', 'too', ',', 'my', 'mood', 'is', 'constantly', 'changing', 'from', 'happy', 'to', 'sad', '-', 'one', 'minute', 'i', \"'\", 'll', 'be', 'fine', ',', 'next', 'i', \"'\", 'll', 'be', 'really', 'sad', '.', 'my', 'temper', 'changes', 'really', 'quick', 'too', ',', 'same', 'deal', 'i', \"'\", 'll', 'be', 'fine', 'one', 'minute', 'and', 'then', 'bam', ',', 'someone', 'will', 'do', 'something', '/', 'say', 'something', 'and', 'i', \"'\", 'll', 'just', 'be', 'full', 'of', 'pure', 'rage', '-', 'i', 'don', \"'\", 't', 'show', 'it', ',', 'i', 'keep', 'it', 'to', 'myself', 'and', 'carry', 'on', '.', 'it', 'could', 'be', 'something', 'stupid', 'like', 'someone', 'knocking', 'on', 'my', 'bedroom', 'door', 'while', 'i', \"'\", 'm', 'working', '.', 'i', 'really', 'want', 'to', 'be', 'fixed', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'go', 'about', 'it', '-', 'i', \"'\", 'm', 'at', 'an', 'excellent', 'university', ',', 'studying', 'an', 'interesting', 'subject', 'and', 'i', \"'\", 'm', 'surrounded', 'by', 'my', 'friends', ',', 'it', \"'\", 's', 'taken', 'me', 'ages', 'to', 'build', 'this', 'up', '-', 'but', 'i', 'could', 'lose', 'it', 'all', '.', ':', '(', 'thanks', 'for', 'reading', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['need', 'some', 'help', 'with', 'the', 'time', 'disturbances', 'su', '##p', ',', '/', 'r', '/', 'ad', '##hd', 'so', ',', 'i', 'must', 'preface', 'this', 'by', 'saying', 'i', 'am', 'on', 'add', '##eral', '##l', 'ir', 'as', 'a', 'form', 'of', 'medication', '.', 'even', 'with', 'the', 'medication', 'i', 'find', 'studying', 'my', 'sense', 'of', 'time', 'is', 'completely', 'out', 'of', 'w', '##ha', '##ck', '.', '10', 'minutes', 'can', 'feel', 'like', 'hours', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'reading', 'at', 'a', 'snails', 'pace', '(', 'even', 'if', 'i', \"'\", 'm', 'understanding', 'most', 'of', 'what', 'i', \"'\", 'm', 'reading', 'for', 'once', ',', 'the', 'feeling', 'of', 'slowly', 'reading', 'is', 'frustrating', ')', '.', 'it', \"'\", 's', 'all', 'so', 'frustrating', '.', 'i', 'end', 'up', 'checking', 'the', 'clock', ',', 'and', 'then', 'getting', 'more', 'angry', 'at', 'myself', 'for', 'only', 'being', 'able', 'to', 'study', 'for', '10', 'minutes', '.', 'off', 'med', '##s', ',', 'i', 'can', \"'\", 't', 'even', 'bring', 'myself', 'to', 'open', 'the', 'books', 'usually', '.', 'if', 'i', 'do', ',', 'i', 'read', 'a', 'quick', 'section', '(', 'and', 'by', 'read', ',', 'i', 'mean', 'ski', '##m', 'it', ')', ',', 'and', 'call', 'it', 'a', 'day', 'and', 'close', 'the', 'book', 'without', 'even', 'thinking', 'about', 'it', '.', 'i', 'have', 'the', 'same', 'issue', 'with', 'video', 'games', '.', 'where', 'previously', 'i', 'could', 'game', 'for', 'a', 'decent', 'amount', 'of', 'time', '(', '2', '-', '3', 'hours', ')', 'off', 'med', '##s', ',', 'i', 'can', 'only', 'go', 'about', 'an', 'hour', 'off', 'med', '##s', '.', '[', 'this', 'is', 'actually', 'what', 'prompted', 'me', 'to', 'seek', 'help', '.', 'imagine', 'that', ',', 'video', 'games', '>', '.', '>', ']', 'with', 'med', '##s', ',', 'there', 'is', 'no', 'stopping', 'unless', 'i', 'actively', 'remind', 'myself', 'to', '.', 'so', 'i', 'figured', 'at', 'least', 'my', 'studying', 'would', 'be', 'better', '.', 'not', 'so', '.', 'i', 'need', 'some', 'tips', 'or', 'advice', 'from', 'anyone', 'who', 'has', 'the', 'same', 'problem', '.', 'i', \"'\", 've', 'been', 'doing', 'better', 'in', 'school', '(', 'mostly', 'b', \"'\", 's', 'with', 'a', 'few', 'a', \"'\", 's', 'and', 'possibly', '1', 'c', ')', ',', 'but', 'when', 'push', 'comes', 'to', 'shove', 'i', \"'\", 'm', 'still', 'very', 'hopeless', 'for', 'preparing', 'for', 'exams', '.', 'i', 'mean', ',', 'i', 'hate', 'multiple', 'choice', 'exams', 'and', 'i', 'do', 'not', 'think', 'they', 'are', 'a', 'correct', 'indicator', 'of', 'my', 'intelligence', '(', 'my', 'a', 'classes', 'are', 'those', 'that', 'have', 'essay', 'based', 'exams', ',', 'the', 'c', \"'\", 's', 'and', 'b', \"'\", 's', 'are', 'mc', 'exams', ')', ',', 'but', 'i', 'just', 'can', 'not', 'bring', 'myself', 'to', 'study', '.', 'well', ',', 'i', 'can', '.', 'it', \"'\", 's', 'just', '10', 'minutes', 'feels', 'like', 'hours', '.', 'also', 'if', 'i', 'do', 'manage', 'to', 'study', 'for', 'about', 'an', 'hour', ',', 'i', 'feel', 'like', 'i', 'haven', \"'\", 't', 'covered', 'as', 'much', 'as', 'i', 'could', 'have', 'or', 'should', 'have', 'in', 'that', 'time', 'period', ',', 'thus', 'essentially', '\"', 'wasting', '\"', 'an', 'hour', 'of', 'med', '##icated', 'time', '!', 'so', 'yeah', ',', 'again', ',', 'advice', '?', 'tips', '?', 'should', 'i', 'make', 'another', 'appointment', 'with', 'the', 'psychiatrist', 'to', 'discuss', 'this', '?', 'i', \"'\", 'm', 'hesitant', 'to', 'try', 'another', 'type', 'of', 'medication', 'so', 'quickly', '(', 'it', \"'\", 's', 'been', 'only', '3', 'months', 'since', 'my', 'dia', '##gno', '##ses', ')', 'without', 'trying', 'something', 'else', 'first', '.']\n",
      "INFO:__main__:Number of tokens: 499\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['need', 'some', 'help', 'with', 'the', 'time', 'disturbances', 'su', '##p', ',', '/', 'r', '/', 'ad', '##hd', 'so', ',', 'i', 'must', 'preface', 'this', 'by', 'saying', 'i', 'am', 'on', 'add', '##eral', '##l', 'ir', 'as', 'a', 'form', 'of', 'medication', '.', 'even', 'with', 'the', 'medication', 'i', 'find', 'studying', 'my', 'sense', 'of', 'time', 'is', 'completely', 'out', 'of', 'w', '##ha', '##ck', '.', '10', 'minutes', 'can', 'feel', 'like', 'hours', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'reading', 'at', 'a', 'snails', 'pace', '(', 'even', 'if', 'i', \"'\", 'm', 'understanding', 'most', 'of', 'what', 'i', \"'\", 'm', 'reading', 'for', 'once', ',', 'the', 'feeling', 'of', 'slowly', 'reading', 'is', 'frustrating', ')', '.', 'it', \"'\", 's', 'all', 'so', 'frustrating', '.', 'i', 'end', 'up', 'checking', 'the', 'clock', ',', 'and', 'then', 'getting', 'more', 'angry', 'at', 'myself', 'for', 'only', 'being', 'able', 'to', 'study', 'for', '10', 'minutes', '.', 'off', 'med', '##s', ',', 'i', 'can', \"'\", 't', 'even', 'bring', 'myself', 'to', 'open', 'the', 'books', 'usually', '.', 'if', 'i', 'do', ',', 'i', 'read', 'a', 'quick', 'section', '(', 'and', 'by', 'read', ',', 'i', 'mean', 'ski', '##m', 'it', ')', ',', 'and', 'call', 'it', 'a', 'day', 'and', 'close', 'the', 'book', 'without', 'even', 'thinking', 'about', 'it', '.', 'i', 'have', 'the', 'same', 'issue', 'with', 'video', 'games', '.', 'where', 'previously', 'i', 'could', 'game', 'for', 'a', 'decent', 'amount', 'of', 'time', '(', '2', '-', '3', 'hours', ')', 'off', 'med', '##s', ',', 'i', 'can', 'only', 'go', 'about', 'an', 'hour', 'off', 'med', '##s', '.', '[', 'this', 'is', 'actually', 'what', 'prompted', 'me', 'to', 'seek', 'help', '.', 'imagine', 'that', ',', 'video', 'games', '>', '.', '>', ']', 'with', 'med', '##s', ',', 'there', 'is', 'no', 'stopping', 'unless', 'i', 'actively', 'remind', 'myself', 'to', '.', 'so', 'i', 'figured', 'at', 'least', 'my', 'studying', 'would', 'be', 'better', '.', 'not', 'so', '.', 'i', 'need', 'some', 'tips', 'or', 'advice', 'from', 'anyone', 'who', 'has', 'the', 'same', 'problem', '.', 'i', \"'\", 've', 'been', 'doing', 'better', 'in', 'school', '(', 'mostly', 'b', \"'\", 's', 'with', 'a', 'few', 'a', \"'\", 's', 'and', 'possibly', '1', 'c', ')', ',', 'but', 'when', 'push', 'comes', 'to', 'shove', 'i', \"'\", 'm', 'still', 'very', 'hopeless', 'for', 'preparing', 'for', 'exams', '.', 'i', 'mean', ',', 'i', 'hate', 'multiple', 'choice', 'exams', 'and', 'i', 'do', 'not', 'think', 'they', 'are', 'a', 'correct', 'indicator', 'of', 'my', 'intelligence', '(', 'my', 'a', 'classes', 'are', 'those', 'that', 'have', 'essay', 'based', 'exams', ',', 'the', 'c', \"'\", 's', 'and', 'b', \"'\", 's', 'are', 'mc', 'exams', ')', ',', 'but', 'i', 'just', 'can', 'not', 'bring', 'myself', 'to', 'study', '.', 'well', ',', 'i', 'can', '.', 'it', \"'\", 's', 'just', '10', 'minutes', 'feels', 'like', 'hours', '.', 'also', 'if', 'i', 'do', 'manage', 'to', 'study', 'for', 'about', 'an', 'hour', ',', 'i', 'feel', 'like', 'i', 'haven', \"'\", 't', 'covered', 'as', 'much', 'as', 'i', 'could', 'have', 'or', 'should', 'have', 'in', 'that', 'time', 'period', ',', 'thus', 'essentially', '\"', 'wasting', '\"', 'an', 'hour', 'of', 'med', '##icated', 'time', '!', 'so', 'yeah', ',', 'again', ',', 'advice', '?', 'tips', '?', 'should', 'i', 'make', 'another', 'appointment', 'with', 'the', 'psychiatrist', 'to', 'discuss', 'this', '?', 'i', \"'\", 'm', 'hesitant', 'to', 'try', 'another', 'type', 'of', 'medication', 'so', 'quickly', '(', 'it', \"'\", 's', 'been', 'only', '3', 'months', 'since', 'my', 'dia', '##gno', '##ses', ')', 'without', 'trying', 'something', 'else', 'first', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['external', 'controllers', 'i', 'wanted', 'to', 'write', 'a', 'text', 'and', 'focus', 'on', 'one', 'or', 'two', 'of', 'the', 'highlights', 'of', 'my', 'otherwise', 'bleak', 'academic', 'career', 'and', 'mention', 'two', 'awesome', 'teachers', 'i', \"'\", 've', 'had', '.', 'the', 'first', 'would', 'be', 'my', 'first', 'grade', 'teacher', 'during', 'my', 'second', 'time', 'taking', 'first', 'grade', '.', 'she', 'seemed', 'to', 'catch', 'on', 'fairly', 'quickly', 'that', 'i', 'couldn', \"'\", 't', 'pay', 'attention', ',', 'so', 'she', 'had', 'no', 'problem', 'as', 'soon', 'as', 'she', 'saw', 'me', 'drift', 'off', 'calling', 'me', 'out', 'on', 'it', 'by', 'either', 'tapping', 'the', 'board', 'or', 'doing', 'something', 'that', 'was', 'in', 'some', 'way', 'attention', 'getting', 'without', 'being', 'totally', 'obvious', '.', 'if', 'i', 'stopped', 'writing', 'because', 'my', 'mind', 'wandered', 'off', 'she', 'once', 'tapped', 'right', 'on', 'my', 'desk', '.', 'she', 'was', 'an', 'amazing', 'teacher', 'and', 'one', 'of', 'the', 'few', 'times', 'i', 'ever', 'had', 'great', 'grades', 'even', 'in', 'grade', 'school', '.', 'the', 'other', 'teacher', 'was', 'an', 'english', '102', 'teacher', 'in', 'college', '.', 'i', 'had', 'already', 'taken', 'the', 'class', '3', 'times', 'and', 'my', 'first', 'paper', 'in', 'his', 'class', 'he', 'gave', 'me', 'a', 'flat', 'out', '0', '.', 'i', 'was', 'a', 'single', 'line', 'short', 'and', 'for', 'that', 'i', 'received', 'a', '0', 'and', 'i', 'could', 'res', '##ub', '##mit', 'it', 'for', '50', '%', 'off', 'if', 'i', 'fixed', 'it', '.', 'this', 'was', 'a', 'very', 'gr', '##uel', '##ing', 'semester', 'because', 'i', 'was', 'fighting', 'such', 'an', 'up', '##hill', 'battle', '.', 'looking', 'back', ',', 'i', 'now', 'realize', 'that', 'the', 'sheer', 'level', 'of', 'pressure', 'from', 'that', 'class', 'was', 'the', 'only', 'reason', 'i', 'passed', 'it', '.', 'it', 'had', 'such', 'high', 'standards', 'that', 'i', 'had', 'to', 'have', 'every', 'single', 'paper', 'ready', '.', 'every', 'hour', 'was', 'the', '11th', 'hour', 'for', 'that', 'class', '.', 'unfortunately', 'it', 'did', 'cost', 'me', 'in', 'other', 'classes', 'because', 'i', 'had', 'to', 'focus', 'on', 'one', 'class', 'so', 'much', '.', 'i', 'still', 'greatly', 'respect', 'the', 'teacher', 'himself', 'because', 'he', 'had', 'such', 'high', 'standards', '.', 'while', 'i', 'understand', 'every', 'class', 'can', \"'\", 't', 'be', 'like', 'this', '(', 'because', ',', 'it', 'would', 'more', 'or', 'less', 'cause', 'heart', 'attacks', 'and', 'premature', 'death', ')', '.', '.', 'i', 'am', 'grateful', 'to', 'the', 'man', 'who', 'required', 'so', 'much', 'from', 'me', '.', 'while', 'external', 'controllers', 'aren', \"'\", 't', 'a', 'cure', ',', 'they', 'vastly', 'improve', 'at', 'least', 'some', 'aspects', 'of', 'life', '.', 'do', 'you', 'have', 'any', 'people', 'you', 'have', 'a', 'respect', 'or', 'admiration', 'for', 'helping', '/', 'pushing', 'you', 'to', 'do', 'your', 'absolute', 'best', '?']\n",
      "INFO:__main__:Number of tokens: 384\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['external', 'controllers', 'i', 'wanted', 'to', 'write', 'a', 'text', 'and', 'focus', 'on', 'one', 'or', 'two', 'of', 'the', 'highlights', 'of', 'my', 'otherwise', 'bleak', 'academic', 'career', 'and', 'mention', 'two', 'awesome', 'teachers', 'i', \"'\", 've', 'had', '.', 'the', 'first', 'would', 'be', 'my', 'first', 'grade', 'teacher', 'during', 'my', 'second', 'time', 'taking', 'first', 'grade', '.', 'she', 'seemed', 'to', 'catch', 'on', 'fairly', 'quickly', 'that', 'i', 'couldn', \"'\", 't', 'pay', 'attention', ',', 'so', 'she', 'had', 'no', 'problem', 'as', 'soon', 'as', 'she', 'saw', 'me', 'drift', 'off', 'calling', 'me', 'out', 'on', 'it', 'by', 'either', 'tapping', 'the', 'board', 'or', 'doing', 'something', 'that', 'was', 'in', 'some', 'way', 'attention', 'getting', 'without', 'being', 'totally', 'obvious', '.', 'if', 'i', 'stopped', 'writing', 'because', 'my', 'mind', 'wandered', 'off', 'she', 'once', 'tapped', 'right', 'on', 'my', 'desk', '.', 'she', 'was', 'an', 'amazing', 'teacher', 'and', 'one', 'of', 'the', 'few', 'times', 'i', 'ever', 'had', 'great', 'grades', 'even', 'in', 'grade', 'school', '.', 'the', 'other', 'teacher', 'was', 'an', 'english', '102', 'teacher', 'in', 'college', '.', 'i', 'had', 'already', 'taken', 'the', 'class', '3', 'times', 'and', 'my', 'first', 'paper', 'in', 'his', 'class', 'he', 'gave', 'me', 'a', 'flat', 'out', '0', '.', 'i', 'was', 'a', 'single', 'line', 'short', 'and', 'for', 'that', 'i', 'received', 'a', '0', 'and', 'i', 'could', 'res', '##ub', '##mit', 'it', 'for', '50', '%', 'off', 'if', 'i', 'fixed', 'it', '.', 'this', 'was', 'a', 'very', 'gr', '##uel', '##ing', 'semester', 'because', 'i', 'was', 'fighting', 'such', 'an', 'up', '##hill', 'battle', '.', 'looking', 'back', ',', 'i', 'now', 'realize', 'that', 'the', 'sheer', 'level', 'of', 'pressure', 'from', 'that', 'class', 'was', 'the', 'only', 'reason', 'i', 'passed', 'it', '.', 'it', 'had', 'such', 'high', 'standards', 'that', 'i', 'had', 'to', 'have', 'every', 'single', 'paper', 'ready', '.', 'every', 'hour', 'was', 'the', '11th', 'hour', 'for', 'that', 'class', '.', 'unfortunately', 'it', 'did', 'cost', 'me', 'in', 'other', 'classes', 'because', 'i', 'had', 'to', 'focus', 'on', 'one', 'class', 'so', 'much', '.', 'i', 'still', 'greatly', 'respect', 'the', 'teacher', 'himself', 'because', 'he', 'had', 'such', 'high', 'standards', '.', 'while', 'i', 'understand', 'every', 'class', 'can', \"'\", 't', 'be', 'like', 'this', '(', 'because', ',', 'it', 'would', 'more', 'or', 'less', 'cause', 'heart', 'attacks', 'and', 'premature', 'death', ')', '.', '.', 'i', 'am', 'grateful', 'to', 'the', 'man', 'who', 'required', 'so', 'much', 'from', 'me', '.', 'while', 'external', 'controllers', 'aren', \"'\", 't', 'a', 'cure', ',', 'they', 'vastly', 'improve', 'at', 'least', 'some', 'aspects', 'of', 'life', '.', 'do', 'you', 'have', 'any', 'people', 'you', 'have', 'a', 'respect', 'or', 'admiration', 'for', 'helping', '/', 'pushing', 'you', 'to', 'do', 'your', 'absolute', 'best', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'the', 'best', 'morning', 'breakfast', 'for', 'add', '##eral', '##l', '20', '##mg', '?', 'please', '!', 'i', 'don', \"'\", 't', 'want', 'to', 'eat', 'dun', '##kin', 'don', '##uts', 'egg', 'sandwich', '7', 'days', 'a', 'week', 'just', 'because', 'protein', 'helps', 'fight', 'off', 'me', 'from', 'feeling', 'sick', '.', 'ga', '##h', '.', 'any', 'other', 'breakfast', 'solutions', 'then', 'an', 'om', '##ele', '##t', 'with', 'o', '##j', '?']\n",
      "INFO:__main__:Number of tokens: 59\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'the', 'best', 'morning', 'breakfast', 'for', 'add', '##eral', '##l', '20', '##mg', '?', 'please', '!', 'i', 'don', \"'\", 't', 'want', 'to', 'eat', 'dun', '##kin', 'don', '##uts', 'egg', 'sandwich', '7', 'days', 'a', 'week', 'just', 'because', 'protein', 'helps', 'fight', 'off', 'me', 'from', 'feeling', 'sick', '.', 'ga', '##h', '.', 'any', 'other', 'breakfast', 'solutions', 'then', 'an', 'om', '##ele', '##t', 'with', 'o', '##j', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['essay', 'due', ':', 'monday', '-', '12', ':', '30', 'pm', 'friday', ':', '-', 'get', 'out', 'of', 'school', ',', 'consider', 'doing', 'paper', '.', '-', 'don', \"'\", 't', 'do', 'paper', '.', 'saturday', ':', '-', 'wake', 'up', ',', 'have', 'a', 'bowl', 'of', 'cereal', '.', '-', 'open', 'up', 'word', 'document', 'for', 'paper', '.', '-', 'surf', 'red', '##dit', '.', 'sunday', ':', '-', 'wake', 'up', ',', 'look', 'at', 'blinking', 'cu', '##rso', '##r', 'on', 'empty', 'word', 'document', '.', '-', 'type', 'a', 'paragraph', '.', '-', 'surf', 'red', '##dit', '.', 'monday', ':', '-', 'wake', 'up', 'to', '8', 'am', 'alarm', '.', '-', 'take', 'an', 'extra', 'long', 'shower', '.', '-', 'look', 'at', 'the', 'clock', ',', 'class', 'is', 'in', 'two', 'hours', '.', '-', 'write', 'two', 'more', 'paragraph', '##s', '.', '-', 'look', 'at', 'the', 'clock', ',', 'class', 'is', 'in', '30', 'minutes', '.', '-', 'decide', 'to', 'ditch', 'class', '.', 'fuck', 'it', ',', 'turn', 'it', 'in', 'late', 'on', 'wednesday', '.', 'tuesday', ':', '-', '[', 'see', 'sunday', ']', '-', 'realize', 'you', \"'\", 've', 'only', 'done', '1', '/', '5', 'of', 'your', 'total', 'report', '.', '-', 'freak', 'out', '.', '-', 'surf', 'red', '##dit', '.', 'wednesday', ':', '-', 'wake', 'up', 'to', '6', 'am', 'alarm', '.', '-', 'eat', 'breakfast', ',', 'take', '(', 'extra', 'long', ')', 'shower', '.', '-', 'look', 'at', 'paper', '.', '-', 'panic', '.', 'start', 'looking', 'up', 'trade', 'schools', '.', '-', 'panic', 'further', '.', '(', '\"', 'i', 'wonder', 'how', 'much', 'a', 'one', '-', 'way', 'flight', 'to', 'new', 'zealand', 'would', 'be', '.', '.', '.', '\"', ')', '-', 'write', 'a', 'mani', '##c', 'post', 'on', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'detailing', 'your', 'anxiety', '(', 'oh', ',', 'hey', 'guys', ')', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'amir', '##ight', '?']\n",
      "INFO:__main__:Number of tokens: 344\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['essay', 'due', ':', 'monday', '-', '12', ':', '30', 'pm', 'friday', ':', '-', 'get', 'out', 'of', 'school', ',', 'consider', 'doing', 'paper', '.', '-', 'don', \"'\", 't', 'do', 'paper', '.', 'saturday', ':', '-', 'wake', 'up', ',', 'have', 'a', 'bowl', 'of', 'cereal', '.', '-', 'open', 'up', 'word', 'document', 'for', 'paper', '.', '-', 'surf', 'red', '##dit', '.', 'sunday', ':', '-', 'wake', 'up', ',', 'look', 'at', 'blinking', 'cu', '##rso', '##r', 'on', 'empty', 'word', 'document', '.', '-', 'type', 'a', 'paragraph', '.', '-', 'surf', 'red', '##dit', '.', 'monday', ':', '-', 'wake', 'up', 'to', '8', 'am', 'alarm', '.', '-', 'take', 'an', 'extra', 'long', 'shower', '.', '-', 'look', 'at', 'the', 'clock', ',', 'class', 'is', 'in', 'two', 'hours', '.', '-', 'write', 'two', 'more', 'paragraph', '##s', '.', '-', 'look', 'at', 'the', 'clock', ',', 'class', 'is', 'in', '30', 'minutes', '.', '-', 'decide', 'to', 'ditch', 'class', '.', 'fuck', 'it', ',', 'turn', 'it', 'in', 'late', 'on', 'wednesday', '.', 'tuesday', ':', '-', '[', 'see', 'sunday', ']', '-', 'realize', 'you', \"'\", 've', 'only', 'done', '1', '/', '5', 'of', 'your', 'total', 'report', '.', '-', 'freak', 'out', '.', '-', 'surf', 'red', '##dit', '.', 'wednesday', ':', '-', 'wake', 'up', 'to', '6', 'am', 'alarm', '.', '-', 'eat', 'breakfast', ',', 'take', '(', 'extra', 'long', ')', 'shower', '.', '-', 'look', 'at', 'paper', '.', '-', 'panic', '.', 'start', 'looking', 'up', 'trade', 'schools', '.', '-', 'panic', 'further', '.', '(', '\"', 'i', 'wonder', 'how', 'much', 'a', 'one', '-', 'way', 'flight', 'to', 'new', 'zealand', 'would', 'be', '.', '.', '.', '\"', ')', '-', 'write', 'a', 'mani', '##c', 'post', 'on', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'detailing', 'your', 'anxiety', '(', 'oh', ',', 'hey', 'guys', ')', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'amir', '##ight', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['music', 'is', ',', 'and', 'always', 'has', 'been', 'my', 'passion', '.', 'started', 'med', '##s', '2', 'weeks', 'ago', 'and', 'found', 'my', 'love', 'for', 'music', 'grows', 'dramatically', 'while', 'med', '##icated', '.', 'on', 'add', '##eral', '##l', 'bt', '##w', '.', 'any', 'similar', 'stories', 'with', 'ho', '##bbies', ',', 'people', ',', 'anything', '?', 'what', 'do', 'you', 'attribute', 'it', 'to', '?']\n",
      "INFO:__main__:Number of tokens: 53\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['music', 'is', ',', 'and', 'always', 'has', 'been', 'my', 'passion', '.', 'started', 'med', '##s', '2', 'weeks', 'ago', 'and', 'found', 'my', 'love', 'for', 'music', 'grows', 'dramatically', 'while', 'med', '##icated', '.', 'on', 'add', '##eral', '##l', 'bt', '##w', '.', 'any', 'similar', 'stories', 'with', 'ho', '##bbies', ',', 'people', ',', 'anything', '?', 'what', 'do', 'you', 'attribute', 'it', 'to', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['friendly', 'reminder', ':', 'make', 'sure', 'you', 'filed', 'your', 'taxes', 'it', 'almost', 'slipped', 'my', 'mind', '.', '.', '.', 'again', '.', '.', '.', 'so', 'i', 'figured', 'i', \"'\", 'd', 'make', 'sure', 'all', 'of', 'you', 'filed', 'your', 'taxes', 'too', '!']\n",
      "INFO:__main__:Number of tokens: 37\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['friendly', 'reminder', ':', 'make', 'sure', 'you', 'filed', 'your', 'taxes', 'it', 'almost', 'slipped', 'my', 'mind', '.', '.', '.', 'again', '.', '.', '.', 'so', 'i', 'figured', 'i', \"'\", 'd', 'make', 'sure', 'all', 'of', 'you', 'filed', 'your', 'taxes', 'too', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'sometimes', 'get', 'uncomfortable', 'physical', 'effects', 'when', 'st', '##im', '##ula', '##nts', '(', 'v', '##y', '##van', '##se', '/', 'add', '##eral', '##l', ')', 'wear', 'off', '-', 'anyone', 'else', 'have', 'this', '?', '*', '*', '(', 'background', '-', 'skip', 'this', 'for', 'the', 'actual', 'question', ')', '*', '*', '22', 'year', 'old', 'female', '-', 'a', 'little', 'over', 'a', 'month', 'ago', 'i', 'was', 'diagnosed', 'with', 'the', 'ina', '##tten', '##tive', 'type', 'and', 'prescribed', '20', 'mg', 'v', '##y', '##van', '##se', ',', 'instructed', 'to', 'increase', 'to', '40', 'mg', 'after', 'a', 'week', 'to', 'see', 'which', 'dose', 'was', 'better', '.', 'due', 'to', 'insurance', 'red', 'tape', ',', 'i', 'also', 'had', 'to', 'try', 'generic', 'add', '##eral', '##l', '(', '20', 'mg', 'x', '##r', ')', ',', 'but', 'i', 'found', 'v', '##y', '##van', '##se', 'to', 'be', 'a', 'bit', '\"', 'smooth', '##er', '\"', '(', 'for', 'lack', 'of', 'a', 'better', 'word', ')', 'so', 'i', \"'\", 'm', 'back', 'to', 'that', 'now', '.', 'so', 'to', 'sum', '##mar', '##ize', 'this', 'is', 'what', 'i', \"'\", 've', 'had', 'so', 'far', ':', '*', 'week', '1', ':', '20', 'mg', 'v', '##y', '##van', '##se', '*', 'week', '2', ':', '40', 'mg', 'v', '##y', '##van', '##se', '*', 'week', '3', ':', '20', 'mg', 'add', '##eral', '##l', 'x', '##r', '(', 'generic', ')', '*', 'week', '4', ':', '40', 'mg', 'v', '##y', '##van', '##se', '(', 'where', 'i', \"'\", 'm', 'at', 'now', ')', 'i', 'could', 'go', 'on', 'about', 'the', 'psychological', 'wonders', 'of', 'these', 'pills', 'forever', ';', 'for', 'the', 'first', 'time', 'in', 'my', 'life', ',', 'i', 'feel', 'like', 'i', 'have', 'total', 'control', 'over', 'my', 'brain', '.', 'even', 'though', 'i', \"'\", 've', 'got', 'a', 'decent', 'amount', 'of', 'work', '(', 'i', 'am', 'a', 'college', 'student', ')', ',', 'i', \"'\", 've', 'never', 'felt', 'calm', '##er', 'or', 'more', 'productive', ',', 'and', 'it', \"'\", 's', 'awesome', '!', 'i', \"'\", 'm', 'kicking', 'myself', 'for', 'not', 'getting', 'help', 'for', 'this', 'earlier', ',', 'but', 'the', 'future', 'certainly', 'looks', 'bright', '.', ':', ')', 'however', ',', 'i', \"'\", 'm', 'concerned', 'about', 'the', 'physical', 'effects', 'of', 'the', 'st', '##im', '##ula', '##nts', '.', 'for', 'the', 'first', '2', 'weeks', ',', 'appetite', 'loss', 'was', 'a', '*', 'huge', '*', 'issue', 'while', 'the', 'med', '##s', 'were', 'in', 'effect', 'and', 'i', \"'\", 've', 'lost', 'a', 'bit', 'of', 'weight', ',', 'but', 'that', \"'\", 's', 'gotten', 'slightly', 'better', ',', 'thanks', 'in', 'part', 'to', 'your', 'guys', \"'\", 'advice', '[', 'here', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rf', '##db', '##p', '/', 'v', '##y', '##van', '##se', '_', 'side', '_', 'effect', '_', 'my', '_', 'appetite', '_', 'is', '_', 'severely', '/', ')', '.', 'also', ',', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'describe', 'this', ',', 'but', 'i', 'can', 'sort', 'of', 'feel', 'the', 'drug', \"'\", 's', 'presence', '*', 'physically', '*', 'when', 'i', \"'\", 'm', 'on', 'it', ',', 'like', 'a', 'slight', 'increase', 'in', 'heart', 'rate', ',', 'and', 'that', 'freaked', 'me', 'out', 'a', 'bit', 'at', 'first', ',', 'but', 'then', 'i', 'had', 'to', 'remind', 'myself', 'that', '*', 'it', 'is', 'a', 'drug', '*', '(', 'and', 'a', 'st', '##im', '##ula', '##nt', 'at', 'that', ')', 'so', 'of', 'course', 'i', \"'\", 'm', 'going', 'to', 'feel', 'something', '.', 'nothing', 'painful', 'or', 'uncomfortable', 'though', ',', 'so', 'i', 'stopped', 'worrying', 'about', 'that', '.', '*', '*', 'actual', 'question', '*', '*', '*', '*', 'so', 'the', 'physical', 'side', 'effects', 'when', 'i', \"'\", 'm', '*', 'on', '*', 'the', 'med', '##s', 'are', 'pretty', 'mild', ',', 'but', 'in', 'the', 'middle', 'of', 'week', '3', '(', 'add', '##eral', '##l', ')', 'i', 'sometimes', 'started', 'feeling', 'stronger', 'side', 'effects', 'when', 'they', 'start', 'wearing', '*', 'off', '*', '.', 'the', 'first', 'time', ',', 'it', 'was', 'like', 'slight', 'ji', '##tter', '##y', 'feeling', 'with', 'a', 'mild', 'tight', '##ness', 'in', 'my', 'chest', 'and', 'i', 'felt', 'like', 'i', 'had', 'to', 'breathe', 'more', 'deeply', ',', 'although', 'there', 'wasn', \"'\", 't', 'actually', 'any', 'shaking', ',', 'pain', ',', 'or', 'hyper', '##vent', '##ila', '##ting', '-', '-', 'it', 'was', 'more', 'just', 'a', 'weird', 'feeling', 'in', 'the', 'chest', 'area', 'and', 'an', 'unpleasant', 'awareness', 'of', 'the', 'vital', 'processes', '.', '*', '*', 'i', \"'\", 'd', 'never', 'experienced', 'anything', 'like', 'that', 'pre', '-', 'medication', ',', 'so', 'the', 'first', 'time', 'it', 'happened', ',', 'i', 'made', 'the', 'mistake', 'of', 'consulting', 'dr', '.', 'google', 'and', 'silently', '*', 'freaked', 'the', 'fuck', 'out', '*', 'convinced', 'i', 'was', 'having', 'a', 'heart', 'attack', '.', 'this', 'led', 'to', 'a', 'brief', 'feedback', 'loop', 'where', 'the', 'panic', '##king', 'made', 'it', 'worse', ',', 'which', 'in', 'turn', 'made', 'me', 'think', 'the', 'symptoms', 'were', 'getting', 'worse', ',', 'etc', '-', '-', 'but', 'after', 'i', 'looked', 'it', 'up', 'more', 'and', 'realized', 'that', '1', '.', 'i', \"'\", 'm', 'young', ',', 'healthy', ',', 'and', 'have', 'no', 'pre', '##ex', '##ist', '##ing', 'medical', 'conditions', 'or', 'family', 'history', 'and', '2', '.', 'other', 'people', 'have', 'reported', '[', 'similar', 'issues', ']', '(', 'http', ':', '/', '/', 'www', '.', 'add', '##for', '##ums', '.', 'com', '/', 'forums', '/', 'show', '##th', '##rea', '##d', '.', 'php', '?', 't', '=', '50', '##6', '##8', '##3', ')', 'and', 'lived', 'to', 'tell', 'the', 'tale', ',', 'so', 'i', 'calmed', 'down', 'after', 'that', ',', 'but', 'the', '\"', 'baseline', '\"', 'discomfort', 'still', 'lasted', 'for', '60', '-', '90', 'minutes', 'or', 'so', '.', 'i', 'took', 'the', 'add', '##eral', '##l', 'for', 'a', 'few', 'more', 'days', 'and', 'experienced', 'a', 'similar', 'come', '##down', 'one', 'more', 'time', '.', 'switched', 'back', 'to', 'v', '##y', '##van', '##se', 'this', 'week', 'and', 'didn', \"'\", 't', 'have', 'that', 'problem', 'at', 'all', 'yesterday', ',', 'but', 'felt', 'it', 'again', 'today', '.', 'again', ',', 'it', 'seems', 'to', 'last', 'for', '60', '-', '90', 'minutes', 'after', 'the', 'drug', 'wears', 'off', '(', 'as', 'indicated', 'by', 'my', 'usual', 'brain', 'fog', 'slowly', 'rolling', 'in', 'again', ')', '.', 'now', 'that', 'i', 'think', 'about', 'it', ',', 'the', 'only', 'days', 'where', 'this', 'has', 'been', 'a', 'problem', 'were', 'days', 'spent', 'sl', '##avi', '##ng', 'over', 'excel', 'data', 'all', 'day', 'in', 'my', 'cheap', '-', 'ass', 'folding', 'chair', 'which', 'makes', 'my', 'shoulders', 'and', 'back', 'ache', 'like', 'crazy', ',', 'so', 'i', 'wonder', 'if', 'there', \"'\", 's', 'a', 'connection', 'between', 'that', 'and', 'the', 'chest', 'discomfort', '.', 'however', ',', 'i', \"'\", 've', 'occasionally', 'spent', 'long', 'hours', 'in', 'this', 'shitty', 'chair', 'before', 'starting', 'med', '##s', 'and', 'it', 'didn', \"'\", 't', 'feel', 'like', 'the', 'amp', '##het', '##amine', 'come', '##down', 'i', 'just', 'described', ',', 'so', 'maybe', 'it', \"'\", 's', 'a', 'weird', 'interactive', 'effect', '?', 'or', 'maybe', 'it', \"'\", 's', 'just', 'a', 'coincidence', '.', '.', 'i', 'don', \"'\", 't', 'know', '.', 'again', ',', 'this', 'come', '##down', 'is', 'not', 'painful', 'or', 'inca', '##pac', '##itating', 'in', 'any', 'way', ',', 'but', 'since', 'i', \"'\", 've', 'never', 'had', 'a', 'similar', 'feeling', 'without', 'the', 'med', '##s', ',', 'it', 'still', 'freaks', 'me', 'out', 'a', 'little', 'bit', 'and', 'makes', 'me', 'wonder', 'if', 'it', \"'\", 's', 'like', 'over', '##working', 'my', 'heart', 'or', 'something', '.', 'i', \"'\", 'm', 'a', 'cautious', 'and', 'paranoid', 'person', 'in', 'general', ',', 'so', 'i', 'had', 'a', 'huge', 'internal', 'conflict', 'over', 'whether', 'i', 'even', 'wanted', 'med', '##s', 'in', 'the', 'first', 'place', 'given', 'the', 'lack', 'of', 'solid', 'long', '-', 'term', 'studies', ',', 'but', 'i', 'finally', 'decided', 'that', 'life', 'is', 'inherently', 'risky', 'and', 'i', 'don', \"'\", 't', 'want', 'to', 'let', 'my', 'occasionally', 'irrational', 'fear', 'of', 'the', 'unknown', 'prevent', 'me', 'from', 'fully', 'living', 'my', 'life', '*', 'now', '*', '.', 'plus', ',', 'i', 'reasoned', 'that', 'if', '(', 'non', '-', 'abusive', 'use', 'of', ')', 'these', 'drugs', 'were', 'highly', 'dangerous', ',', 'they', 'wouldn', \"'\", 't', 'be', 'prescribed', 'to', 'little', 'kids', '.', 'but', 'the', 'same', 'time', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'naive', '##ly', 'ignore', 'a', 'side', 'effect', 'and', 'proceed', 'have', 'a', 'heart', 'attack', 'when', 'i', \"'\", 'm', '35', 'or', 'something', '.', '*', '*', 'of', 'course', 'i', \"'\", 'm', 'going', 'to', 'talk', 'to', 'my', 'doc', ',', 'but', 'has', 'anyone', 'else', 'experienced', 'similar', 'effects', 'when', 'your', 'medication', 'wore', 'off', '?', 'do', 'you', 'know', 'why', 'it', 'happens', 'some', 'days', 'but', 'not', 'others', '?', 'is', 'it', 'a', 'serious', 'problem', 'that', 'warrant', '##s', 'stopping', 'the', 'medication', ',', 'or', 'is', 'it', 'just', 'a', 'weird', 'qui', '##rk', 'that', 'you', 'have', 'to', 'learn', 'to', 'live', 'with', '?', 'any', 'insight', 'would', 'be', 'great', '.', 'thanks', 'for', 'reading', '!', '*', '*', 'edit', ':', 'made', 'the', 'format', '##ting', 'more', 'add', '-', 'friendly', ';', ')']\n",
      "INFO:__main__:Number of tokens: 1285\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['i', 'sometimes', 'get', 'uncomfortable', 'physical', 'effects', 'when', 'st', '##im', '##ula', '##nts', '(', 'v', '##y', '##van', '##se', '/', 'add', '##eral', '##l', ')', 'wear', 'off', '-', 'anyone', 'else', 'have', 'this', '?', '*', '*', '(', 'background', '-', 'skip', 'this', 'for', 'the', 'actual', 'question', ')', '*', '*', '22', 'year', 'old', 'female', '-', 'a', 'little', 'over', 'a', 'month', 'ago', 'i', 'was', 'diagnosed', 'with', 'the', 'ina', '##tten', '##tive', 'type', 'and', 'prescribed', '20', 'mg', 'v', '##y', '##van', '##se', ',', 'instructed', 'to', 'increase', 'to', '40', 'mg', 'after', 'a', 'week', 'to', 'see', 'which', 'dose', 'was', 'better', '.', 'due', 'to', 'insurance', 'red', 'tape', ',', 'i', 'also', 'had', 'to', 'try', 'generic', 'add', '##eral', '##l', '(', '20', 'mg', 'x', '##r', ')', ',', 'but', 'i', 'found', 'v', '##y', '##van', '##se', 'to', 'be', 'a', 'bit', '\"', 'smooth', '##er', '\"', '(', 'for', 'lack', 'of', 'a', 'better', 'word', ')', 'so', 'i', \"'\", 'm', 'back', 'to', 'that', 'now', '.', 'so', 'to', 'sum', '##mar', '##ize', 'this', 'is', 'what', 'i', \"'\", 've', 'had', 'so', 'far', ':', '*', 'week', '1', ':', '20', 'mg', 'v', '##y', '##van', '##se', '*', 'week', '2', ':', '40', 'mg', 'v', '##y', '##van', '##se', '*', 'week', '3', ':', '20', 'mg', 'add', '##eral', '##l', 'x', '##r', '(', 'generic', ')', '*', 'week', '4', ':', '40', 'mg', 'v', '##y', '##van', '##se', '(', 'where', 'i', \"'\", 'm', 'at', 'now', ')', 'i', 'could', 'go', 'on', 'about', 'the', 'psychological', 'wonders', 'of', 'these', 'pills', 'forever', ';', 'for', 'the', 'first', 'time', 'in', 'my', 'life', ',', 'i', 'feel', 'like', 'i', 'have', 'total', 'control', 'over', 'my', 'brain', '.', 'even', 'though', 'i', \"'\", 've', 'got', 'a', 'decent', 'amount', 'of', 'work', '(', 'i', 'am', 'a', 'college', 'student', ')', ',', 'i', \"'\", 've', 'never', 'felt', 'calm', '##er', 'or', 'more', 'productive', ',', 'and', 'it', \"'\", 's', 'awesome', '!', 'i', \"'\", 'm', 'kicking', 'myself', 'for', 'not', 'getting', 'help', 'for', 'this', 'earlier', ',', 'but', 'the', 'future', 'certainly', 'looks', 'bright', '.', ':', ')', 'however', ',', 'i', \"'\", 'm', 'concerned', 'about', 'the', 'physical', 'effects', 'of', 'the', 'st', '##im', '##ula', '##nts', '.', 'for', 'the', 'first', '2', 'weeks', ',', 'appetite', 'loss', 'was', 'a', '*', 'huge', '*', 'issue', 'while', 'the', 'med', '##s', 'were', 'in', 'effect', 'and', 'i', \"'\", 've', 'lost', 'a', 'bit', 'of', 'weight', ',', 'but', 'that', \"'\", 's', 'gotten', 'slightly', 'better', ',', 'thanks', 'in', 'part', 'to', 'your', 'guys', \"'\", 'advice', '[', 'here', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rf', '##db', '##p', '/', 'v', '##y', '##van', '##se', '_', 'side', '_', 'effect', '_', 'my', '_', 'appetite', '_', 'is', '_', 'severely', '/', ')', '.', 'also', ',', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'describe', 'this', ',', 'but', 'i', 'can', 'sort', 'of', 'feel', 'the', 'drug', \"'\", 's', 'presence', '*', 'physically', '*', 'when', 'i', \"'\", 'm', 'on', 'it', ',', 'like', 'a', 'slight', 'increase', 'in', 'heart', 'rate', ',', 'and', 'that', 'freaked', 'me', 'out', 'a', 'bit', 'at', 'first', ',', 'but', 'then', 'i', 'had', 'to', 'remind', 'myself', 'that', '*', 'it', 'is', 'a', 'drug', '*', '(', 'and', 'a', 'st', '##im', '##ula', '##nt', 'at', 'that', ')', 'so', 'of', 'course', 'i', \"'\", 'm', 'going', 'to', 'feel', 'something', '.', 'nothing', 'painful', 'or', 'uncomfortable', 'though', ',', 'so', 'i', 'stopped', 'worrying', 'about', 'that', '.', '*', '*', 'actual', 'question', '*'], ['*', '*', '*', 'so', 'the', 'physical', 'side', 'effects', 'when', 'i', \"'\", 'm', '*', 'on', '*', 'the', 'med', '##s', 'are', 'pretty', 'mild', ',', 'but', 'in', 'the', 'middle', 'of', 'week', '3', '(', 'add', '##eral', '##l', ')', 'i', 'sometimes', 'started', 'feeling', 'stronger', 'side', 'effects', 'when', 'they', 'start', 'wearing', '*', 'off', '*', '.', 'the', 'first', 'time', ',', 'it', 'was', 'like', 'slight', 'ji', '##tter', '##y', 'feeling', 'with', 'a', 'mild', 'tight', '##ness', 'in', 'my', 'chest', 'and', 'i', 'felt', 'like', 'i', 'had', 'to', 'breathe', 'more', 'deeply', ',', 'although', 'there', 'wasn', \"'\", 't', 'actually', 'any', 'shaking', ',', 'pain', ',', 'or', 'hyper', '##vent', '##ila', '##ting', '-', '-', 'it', 'was', 'more', 'just', 'a', 'weird', 'feeling', 'in', 'the', 'chest', 'area', 'and', 'an', 'unpleasant', 'awareness', 'of', 'the', 'vital', 'processes', '.', '*', '*', 'i', \"'\", 'd', 'never', 'experienced', 'anything', 'like', 'that', 'pre', '-', 'medication', ',', 'so', 'the', 'first', 'time', 'it', 'happened', ',', 'i', 'made', 'the', 'mistake', 'of', 'consulting', 'dr', '.', 'google', 'and', 'silently', '*', 'freaked', 'the', 'fuck', 'out', '*', 'convinced', 'i', 'was', 'having', 'a', 'heart', 'attack', '.', 'this', 'led', 'to', 'a', 'brief', 'feedback', 'loop', 'where', 'the', 'panic', '##king', 'made', 'it', 'worse', ',', 'which', 'in', 'turn', 'made', 'me', 'think', 'the', 'symptoms', 'were', 'getting', 'worse', ',', 'etc', '-', '-', 'but', 'after', 'i', 'looked', 'it', 'up', 'more', 'and', 'realized', 'that', '1', '.', 'i', \"'\", 'm', 'young', ',', 'healthy', ',', 'and', 'have', 'no', 'pre', '##ex', '##ist', '##ing', 'medical', 'conditions', 'or', 'family', 'history', 'and', '2', '.', 'other', 'people', 'have', 'reported', '[', 'similar', 'issues', ']', '(', 'http', ':', '/', '/', 'www', '.', 'add', '##for', '##ums', '.', 'com', '/', 'forums', '/', 'show', '##th', '##rea', '##d', '.', 'php', '?', 't', '=', '50', '##6', '##8', '##3', ')', 'and', 'lived', 'to', 'tell', 'the', 'tale', ',', 'so', 'i', 'calmed', 'down', 'after', 'that', ',', 'but', 'the', '\"', 'baseline', '\"', 'discomfort', 'still', 'lasted', 'for', '60', '-', '90', 'minutes', 'or', 'so', '.', 'i', 'took', 'the', 'add', '##eral', '##l', 'for', 'a', 'few', 'more', 'days', 'and', 'experienced', 'a', 'similar', 'come', '##down', 'one', 'more', 'time', '.', 'switched', 'back', 'to', 'v', '##y', '##van', '##se', 'this', 'week', 'and', 'didn', \"'\", 't', 'have', 'that', 'problem', 'at', 'all', 'yesterday', ',', 'but', 'felt', 'it', 'again', 'today', '.', 'again', ',', 'it', 'seems', 'to', 'last', 'for', '60', '-', '90', 'minutes', 'after', 'the', 'drug', 'wears', 'off', '(', 'as', 'indicated', 'by', 'my', 'usual', 'brain', 'fog', 'slowly', 'rolling', 'in', 'again', ')', '.', 'now', 'that', 'i', 'think', 'about', 'it', ',', 'the', 'only', 'days', 'where', 'this', 'has', 'been', 'a', 'problem', 'were', 'days', 'spent', 'sl', '##avi', '##ng', 'over', 'excel', 'data', 'all', 'day', 'in', 'my', 'cheap', '-', 'ass', 'folding', 'chair', 'which', 'makes', 'my', 'shoulders', 'and', 'back', 'ache', 'like', 'crazy', ',', 'so', 'i', 'wonder', 'if', 'there', \"'\", 's', 'a', 'connection', 'between', 'that', 'and', 'the', 'chest', 'discomfort', '.', 'however', ',', 'i', \"'\", 've', 'occasionally', 'spent', 'long', 'hours', 'in', 'this', 'shitty', 'chair', 'before', 'starting', 'med', '##s', 'and', 'it', 'didn', \"'\", 't', 'feel', 'like', 'the', 'amp', '##het', '##amine', 'come', '##down', 'i', 'just', 'described', ',', 'so', 'maybe', 'it', \"'\", 's', 'a', 'weird', 'interactive', 'effect', '?', 'or', 'maybe', 'it', \"'\", 's', 'just', 'a', 'coincidence', '.', '.', 'i', 'don', \"'\", 't', 'know', '.', 'again', ',', 'this', 'come', '##down', 'is', 'not', 'painful', 'or', 'inca', '##pac', '##itating', 'in', 'any', 'way', ',', 'but', 'since', 'i', \"'\"], ['ve', 'never', 'had', 'a', 'similar', 'feeling', 'without', 'the', 'med', '##s', ',', 'it', 'still', 'freaks', 'me', 'out', 'a', 'little', 'bit', 'and', 'makes', 'me', 'wonder', 'if', 'it', \"'\", 's', 'like', 'over', '##working', 'my', 'heart', 'or', 'something', '.', 'i', \"'\", 'm', 'a', 'cautious', 'and', 'paranoid', 'person', 'in', 'general', ',', 'so', 'i', 'had', 'a', 'huge', 'internal', 'conflict', 'over', 'whether', 'i', 'even', 'wanted', 'med', '##s', 'in', 'the', 'first', 'place', 'given', 'the', 'lack', 'of', 'solid', 'long', '-', 'term', 'studies', ',', 'but', 'i', 'finally', 'decided', 'that', 'life', 'is', 'inherently', 'risky', 'and', 'i', 'don', \"'\", 't', 'want', 'to', 'let', 'my', 'occasionally', 'irrational', 'fear', 'of', 'the', 'unknown', 'prevent', 'me', 'from', 'fully', 'living', 'my', 'life', '*', 'now', '*', '.', 'plus', ',', 'i', 'reasoned', 'that', 'if', '(', 'non', '-', 'abusive', 'use', 'of', ')', 'these', 'drugs', 'were', 'highly', 'dangerous', ',', 'they', 'wouldn', \"'\", 't', 'be', 'prescribed', 'to', 'little', 'kids', '.', 'but', 'the', 'same', 'time', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'naive', '##ly', 'ignore', 'a', 'side', 'effect', 'and', 'proceed', 'have', 'a', 'heart', 'attack', 'when', 'i', \"'\", 'm', '35', 'or', 'something', '.', '*', '*', 'of', 'course', 'i', \"'\", 'm', 'going', 'to', 'talk', 'to', 'my', 'doc', ',', 'but', 'has', 'anyone', 'else', 'experienced', 'similar', 'effects', 'when', 'your', 'medication', 'wore', 'off', '?', 'do', 'you', 'know', 'why', 'it', 'happens', 'some', 'days', 'but', 'not', 'others', '?', 'is', 'it', 'a', 'serious', 'problem', 'that', 'warrant', '##s', 'stopping', 'the', 'medication', ',', 'or', 'is', 'it', 'just', 'a', 'weird', 'qui', '##rk', 'that', 'you', 'have', 'to', 'learn', 'to', 'live', 'with', '?', 'any', 'insight', 'would', 'be', 'great', '.', 'thanks', 'for', 'reading', '!', '*', '*', 'edit', ':', 'made', 'the', 'format', '##ting', 'more', 'add', '-', 'friendly', ';', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'ad', '##hd', 'success', 'story', '.', '.', '.', 'so', 'i', \"'\", 'm', 'writing', 'this', 'for', 'the', 'second', 'time', '.', 'even', 'with', 'how', 'far', 'i', \"'\", 've', 'come', 'i', 'still', 'have', 'a', 'problem', 'with', 'putting', 'my', 'thoughts', 'into', 'and', 'easy', 'to', 'follow', 'story', '.', 'but', 'my', 'point', 'to', 'all', 'of', 'this', 'is', 'that', 'i', 'have', 'ad', '##hd', 'and', 'i', 'went', 'through', 'really', 'hard', 'times', 'yet', 'i', 'somehow', 'made', 'it', 'out', 'on', 'top', '.', 'so', 'up', 'until', 'college', 'life', 'was', 'pretty', 'simple', 'and', 'awesome', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', '5', 'and', 'ultimately', 'went', 'on', 'med', '##s', 'and', 'ended', 'up', 'graduating', 'in', 'the', 'top', '10', '%', 'of', 'my', 'class', 'and', 'got', 'a', 'really', 'good', 'score', 'on', 'my', 'sat', 'and', 'in', 'turn', 'got', 'into', 'a', 'really', 'good', 'college', '.', 'the', 'summer', 'before', 'my', 'freshman', 'year', 'my', 'doctor', 'said', 'that', 'he', 'would', 'no', 'longer', 'be', 'treating', 'me', 'anymore', 'and', 'i', 'was', 'basically', 'on', 'my', 'own', '.', 'by', 'that', 'point', 'i', 'figured', 'that', 'i', 'could', 'handle', 'the', 'ad', '##hd', 'on', 'my', 'own', 'so', 'i', 'didn', \"'\", 't', 'argue', 'it', '.', 'unfortunately', 'what', 'i', 'failed', 'to', 'realize', 'was', 'how', 'much', 'different', 'college', 'would', 'be', 'than', 'high', 'school', '.', 'it', 'turns', 'out', 'my', 'mother', 'had', 'a', 'huge', 'role', 'in', 'my', 'success', 'in', 'grade', 'school', '.', '.', '.', 'she', 'was', 'always', 'there', 'hound', '##ing', 'me', 'and', 'pushing', 'me', 'to', 'do', 'homework', 'and', 'wake', 'up', 'on', 'time', 'for', 'school', 'etc', 'etc', '.', 'well', 'college', 'started', 'and', 'i', 'no', 'longer', 'had', 'that', 'structure', '.', '.', '.', 'or', 'any', 'type', 'of', 'structure', 'to', 'be', 'honest', '.', 'i', 'started', 'overs', '##lee', '##ping', ',', 'not', 'completing', 'homework', 'assignments', ',', 'not', 'studying', 'and', 'ultimately', 'failing', 'exams', '.', 'i', 'kept', 'telling', 'myself', 'that', 'i', 'would', 'be', 'able', 'to', 'put', 'my', 'head', 'down', 'and', 'push', 'through', 'it', 'but', 'i', 'was', 'dead', 'wrong', '.', 'anxiety', 'and', 'depression', 'started', 'to', 'set', 'in', 'by', 'this', 'point', 'and', 'unfortunately', 'i', 'continued', 'to', 'fail', 'and', 'ultimately', 'flu', '##nk', '##ed', 'out', 'of', 'school', '.', 'this', 'was', 'my', 'first', 'taste', 'of', 'failure', 'and', 'i', 'had', 'never', 'been', 'more', 'devastated', '.', '.', 'and', 'neither', 'had', 'my', 'parents', '.', 'it', 'was', 'at', 'this', 'point', 'that', 'my', 'relationship', 'with', 'them', 'really', 'started', 'to', 'tank', '.', 'i', 'was', 'an', 'embarrassment', 'to', 'them', 'and', 'i', 'could', 'tell', '.', '.', '.', 'it', 'was', 'an', 'awful', 'feeling', 'and', 'i', 'started', 'to', 'slip', 'into', 'a', 'deep', 'depression', '.', 'i', 'was', 'constantly', 'being', 'threatened', 'with', 'being', 'kicked', 'out', 'of', 'my', 'house', 'and', 'my', 'parents', 'would', 'yell', 'at', 'me', 'and', 'just', 'flat', 'out', 'told', 'me', 'how', 'much', 'i', 'let', 'them', 'down', '.', 'it', 'was', 'incredibly', 'painful', '.', 'at', 'this', 'point', 'i', 'had', 'fallen', 'so', 'low', 'that', 'all', 'i', 'could', 'do', 'was', 'en', '##tro', '##ll', 'into', 'a', 'junior', 'college', 'and', 'try', 'to', 'get', 'my', 'crap', 'together', 'and', 'hopefully', 'transfer', 'into', 'a', '4', 'year', 'school', '.', 'that', 'first', 'semester', 'of', 'jr', 'college', 'was', '.', '.', '.', '.', 'awful', '.', '.', '.', 'there', '##s', 'just', 'no', 'other', 'way', 'to', 'say', 'it', '.', 'i', 'felt', 'like', 'i', 'was', 'surrounded', 'by', 'med', '##io', '##cr', '##ity', 'and', 'that', 'i', 'was', 'doomed', 'to', 'just', 'wait', 'tables', '.', 'on', 'top', 'of', 'that', '.', '.', '.', 'i', 'was', 'still', 'doing', 'bad', 'in', 'my', 'classes', '.', 'th', 'depression', '.', '.', '.', 'the', 'anxiety', '.', '.', '.', 'my', 'relationship', 'with', 'my', 'family', '.', '.', '.', '.', 'it', 'just', 'kept', 'getting', 'worse', '.', 'this', 'was', 'the', 'point', 'where', 'i', 'finally', 'decided', 'to', 'let', 'go', 'of', 'my', 'pride', 'and', 'asked', 'for', 'help', '.', 'i', 'called', 'around', 'and', 'i', 'found', 'a', 'psychologist', 'who', 'actually', 'specialized', 'in', 'young', 'adults', 'with', 'ad', '##hd', '.', 'i', 'told', 'him', 'my', 'situation', 'and', 'this', 'is', 'when', 'i', 'learned', 'that', 'he', 'too', 'had', 'originally', 'flu', '##nk', '##ed', 'out', 'of', 'school', 'and', 'worked', 'his', 'ass', 'off', 'and', 'became', 'a', 'doctor', 'and', 'that', 'he', 'wanted', 'to', 'work', 'with', 'me', 'especially', 'and', 'that', 'he', 'would', 'see', 'me', 'once', 'a', 'week', 'and', 'treat', 'me', 'and', 'all', 'of', 'this', 'for', 'free', '.', 'my', 'first', 'time', 'actually', 'visiting', 'him', 'and', 'talking', 'to', 'him', 'was', 'really', 'brutal', '.', '.', '.', 'it', 'was', 'the', 'first', 'time', 'i', 'actually', 'talked', 'to', 'someone', 'about', 'everything', 'i', 'was', 'going', 'through', 'and', 'i', 'broke', 'down', 'right', 'there', '.', 'i', 'ba', '##wled', 'like', 'a', 'little', 'girl', 'and', 'that', '##s', 'when', 'we', 'finally', 'got', 'started', '.', 'he', 'put', 'me', 'back', 'on', 'med', '##s', 'and', 'counsel', '##ed', 'me', 'every', 'week', 'like', 'he', 'said', 'he', 'would', '.', 'even', 'though', 'i', 'was', 'back', 'on', 'med', '##s', ',', 'it', 'was', 'a', 'slow', 'start', '.', 'i', 'still', 'did', 'bad', 'in', 'class', 'at', 'first', 'but', 'other', 'things', 'started', 'coming', 'together', '.', 'so', 'fast', 'forward', 'a', 'year', 'and', 'a', 'half', '.', '.', '.', '.', 'a', 'very', 'frustrating', 'year', 'and', 'a', 'half', '.', 'i', 'was', 'still', 'dealing', 'with', 'depression', 'and', 'anxiety', 'and', 'for', 'the', 'most', 'part', 'my', 'parents', 'had', 'no', 'faith', 'in', 'me', 'and', 'still', 'saw', 'me', 'as', 'a', 'disappointment', 'but', 'this', 'is', 'where', 'things', 'started', 'to', 'change', '.', 'every', 'year', 'since', 'i', 'flu', '##nk', '##ed', 'out', 'i', 'would', 'apply', 'to', 'multiple', 'schools', ',', 'hoping', 'that', 'one', 'of', 'them', 'would', 'give', 'me', 'a', 'chance', 'and', 'of', 'course', 'every', 'single', 'one', 'would', 'reject', 'me', '.', 'by', 'this', 'point', 'in', 'life', 'i', 'had', 'become', 'incredibly', 'used', 'to', 'rejection', 'and', 'even', 'though', 'it', 'hurt', 'every', 'time', 'i', 'never', 'quit', '.', 'but', 'this', 'time', 'it', 'was', 'different', '.', '.', '.', '.', 'every', 'single', 'school', 'i', 'applied', 'to', 'actually', 'accepted', 'me', '.', 'i', 'remember', 'the', 'day', 'i', 'got', 'all', 'of', 'those', 'acceptance', 'letters', 'i', 'sprinted', 'upstairs', 'to', 'my', 'mother', \"'\", 's', 'room', 'and', 'i', 'broke', 'down', 'in', 'front', 'of', 'her', 'over', 'finally', 'being', 'able', 'to', 'go', 'back', 'to', 'a', '4', 'year', 'school', '.', 'easily', 'one', 'of', 'the', 'ha', '##pp', '##iest', 'days', 'of', 'my', 'life', '.', 'so', 'after', 'a', 'year', 'and', 'a', 'half', 'with', 'my', 'doctor', 'i', 'finally', 'said', 'goodbye', 'but', 'we', 'agreed', 'that', 'i', 'would', 'call', 'him', 'at', 'least', 'once', 'a', 'month', 'and', 'he', 'would', 'still', 'write', 'me', 'scripts', 'for', 'med', '##s', '.', 'i', 'went', 'off', 'to', 'school', 'and', 'after', 'a', 'shaky', 'on', 'my', 'own', 'i', 'finally', 'found', 'my', 'groove', '.', 'at', 'first', 'i', 'was', 'really', 'embarrassed', 'around', 'everyone', 'because', 'at', 'the', 'end', 'of', 'the', 'day', 'i', 'still', 'felt', 'like', 'a', 'failure', '.', 'during', 'my', 'entire', 'time', 'in', 'school', 'i', 'had', 'become', 'so', 'self', 'conscious', 'over', 'the', 'fact', 'that', 'i', 'felt', 'like', 'i', 'was', 'behind', 'so', 'i', 'would', 'never', 'really', 'ever', 'open', 'up', 'to', 'anyone', '.', '.', '.', 'and', 'on', 'top', 'of', 'that', 'my', 'counselor', '##s', 'could', 'not', 'tell', 'me', 'what', 'my', 'predicted', 'graduation', 'date', 'would', 'be', '.', '.', '.', 'it', 'seemed', 'like', 'the', 'finish', 'line', 'was', 'still', 'out', 'of', 'sight', '.', 'well', 'after', 'getting', 'the', 'ball', 'rolling', '.', '.', '.', 'it', 'started', 'to', 'roll', 'fast', ',', '2', 'years', 'later', 'i', 'actually', 'graduated', 'with', 'two', 'degrees', 'from', 'a', 'top', 'name', 'school', '.', 'that', 'was', 'the', 'next', 'time', 'i', 'cried', 'ha', '##ha', '.', 'it', 'was', 'embarrassing', '.', '.', '.', '.', 'i', 'was', 'so', 'proud', 'though', '.', 'by', 'this', 'time', 'i', 'had', 'already', 'sent', 'out', 'hundreds', 'of', 'resume', '##s', 'and', 'was', 'just', 'hoping', 'for', 'a', 'call', 'back', '.', 'like', 'most', 'others', 'who', 'graduated', 'in', '2010', '.', '.', '.', 'finding', 'a', 'job', 'was', 'hard', 'but', '6', 'months', 'after', 'graduation', 'i', 'got', 'a', 'call', 'from', 'one', 'of', 'the', 'largest', 'banks', 'in', 'the', 'country', '.', 'they', 'actually', 'wanted', 'to', 'interview', 'me', '.', 'needles', '##s', 'to', 'say', ',', 'i', 'rocked', 'my', 'interview', 'and', 'somehow', 'got', 'a', 'job', '.', 'it', 'was', 'at', 'that', 'point', '.', '.', '.', 'that', 'definitive', 'point', 'that', 'i', 'knew', 'everything', 'was', 'going', 'to', 'be', 'ok', '.', 'turns', 'out', 'the', 'job', 'i', 'was', 'offered', 'was', 'one', 'of', 'those', '\"', 'if', 'you', 'don', '##t', 'take', 'this', 'job', 'you', 'are', 'a', 'fucking', 'idiot', '\"', 'opportunities', '.', 'i', 'am', 'now', 'a', 'financial', 'analyst', 'being', 'groom', '##ed', 'for', 'portfolio', 'management', 'and', 'i', 'happen', 'to', 'be', 'very', 'good', 'at', 'what', 'i', 'do', '.', 'the', 'real', 'interesting', 'part', 'was', 'that', '5', 'young', 'guys', 'were', 'hired', 'into', 'this', 'group', 'for', 'the', 'specific', 'reason', 'of', 'being', 'groom', '##ed', 'for', 'future', 'management', 'positions', 'and', 'every', 'single', 'one', 'of', 'us', 'has', 'add', '.', 'i', 'am', 'a', 'year', 'and', 'a', 'half', 'into', 'my', 'job', 'and', 'have', 'never', 'been', 'happier', '.', 'my', 'parents', 'are', 'incredibly', 'proud', 'of', 'me', ',', 'i', 'keep', 'hearing', 'about', 'how', 'they', 'constantly', 'bra', '##g', 'about', 'me', 'and', 'our', 'relationship', 'is', 'amazing', '.', 'i', 'have', 'a', 'career', 'now', 'that', 'i', 'can', 'really', 'be', 'proud', 'of', '.', 'i', 'make', 'really', 'good', 'money', ',', 'i', 'love', 'what', 'i', 'do', 'and', 'i', 'have', 'a', 'lot', 'of', 'opportunities', 'in', 'front', 'of', 'me', '.', 'i', 'can', '##t', 'even', 'express', 'how', 'happy', 'i', 'am', '.', 'no', 'more', 'anxiety', ',', 'no', 'more', 'depression', '.', '.', '.', 'just', 'lots', 'of', 'hard', 'work', 'ahead', 'me', 'doing', 'something', 'i', 'really', 'love', 'to', 'do', '.', 'i', 'know', 'this', 'is', 'really', 'long', 'wind', '##ed', 'and', 'i', \"'\", 'm', 'sure', 'i', 'left', 'some', 'stuff', 'out', '.', 'i', 'have', 'to', 'be', 'honest', '.', '.', '.', 'near', 'the', 'end', 'of', 'writing', 'this', 'my', 'add', 'started', 'to', 'kick', 'in', 'ha', '##ha', '##ha', '.', 'at', 'the', 'end', 'of', 'all', 'of', 'it', 'though', ',', 'what', 'it', 'ultimately', 'took', 'was', 'a', 'lot', 'of', 'hard', 'work', 'and', 'knowing', 'when', 'to', 'ask', 'for', 'help', '.', 'if', 'you', 'have', 'questions', 'feel', 'free', 'to', 'ask', '.', 't', '##l', ';', 'dr', 'my', 'ad', '##hd', 'got', 'the', 'better', 'of', 'me', 'in', 'college', ',', 'i', 'flu', '##nk', '##ed', 'out', 'and', 'worked', 'my', 'ass', 'off', 'to', 'get', 'back', 'in', '.', 'i', 'graduated', 'and', 'now', 'i', 'have', 'a', 'pretty', 'high', 'paying', 'job', 'that', 'i', 'love', '.']\n",
      "INFO:__main__:Number of tokens: 1559\n",
      "INFO:__main__:Number of chunks: 4\n",
      "INFO:__main__:Chunks: [['my', 'ad', '##hd', 'success', 'story', '.', '.', '.', 'so', 'i', \"'\", 'm', 'writing', 'this', 'for', 'the', 'second', 'time', '.', 'even', 'with', 'how', 'far', 'i', \"'\", 've', 'come', 'i', 'still', 'have', 'a', 'problem', 'with', 'putting', 'my', 'thoughts', 'into', 'and', 'easy', 'to', 'follow', 'story', '.', 'but', 'my', 'point', 'to', 'all', 'of', 'this', 'is', 'that', 'i', 'have', 'ad', '##hd', 'and', 'i', 'went', 'through', 'really', 'hard', 'times', 'yet', 'i', 'somehow', 'made', 'it', 'out', 'on', 'top', '.', 'so', 'up', 'until', 'college', 'life', 'was', 'pretty', 'simple', 'and', 'awesome', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'when', 'i', 'was', '5', 'and', 'ultimately', 'went', 'on', 'med', '##s', 'and', 'ended', 'up', 'graduating', 'in', 'the', 'top', '10', '%', 'of', 'my', 'class', 'and', 'got', 'a', 'really', 'good', 'score', 'on', 'my', 'sat', 'and', 'in', 'turn', 'got', 'into', 'a', 'really', 'good', 'college', '.', 'the', 'summer', 'before', 'my', 'freshman', 'year', 'my', 'doctor', 'said', 'that', 'he', 'would', 'no', 'longer', 'be', 'treating', 'me', 'anymore', 'and', 'i', 'was', 'basically', 'on', 'my', 'own', '.', 'by', 'that', 'point', 'i', 'figured', 'that', 'i', 'could', 'handle', 'the', 'ad', '##hd', 'on', 'my', 'own', 'so', 'i', 'didn', \"'\", 't', 'argue', 'it', '.', 'unfortunately', 'what', 'i', 'failed', 'to', 'realize', 'was', 'how', 'much', 'different', 'college', 'would', 'be', 'than', 'high', 'school', '.', 'it', 'turns', 'out', 'my', 'mother', 'had', 'a', 'huge', 'role', 'in', 'my', 'success', 'in', 'grade', 'school', '.', '.', '.', 'she', 'was', 'always', 'there', 'hound', '##ing', 'me', 'and', 'pushing', 'me', 'to', 'do', 'homework', 'and', 'wake', 'up', 'on', 'time', 'for', 'school', 'etc', 'etc', '.', 'well', 'college', 'started', 'and', 'i', 'no', 'longer', 'had', 'that', 'structure', '.', '.', '.', 'or', 'any', 'type', 'of', 'structure', 'to', 'be', 'honest', '.', 'i', 'started', 'overs', '##lee', '##ping', ',', 'not', 'completing', 'homework', 'assignments', ',', 'not', 'studying', 'and', 'ultimately', 'failing', 'exams', '.', 'i', 'kept', 'telling', 'myself', 'that', 'i', 'would', 'be', 'able', 'to', 'put', 'my', 'head', 'down', 'and', 'push', 'through', 'it', 'but', 'i', 'was', 'dead', 'wrong', '.', 'anxiety', 'and', 'depression', 'started', 'to', 'set', 'in', 'by', 'this', 'point', 'and', 'unfortunately', 'i', 'continued', 'to', 'fail', 'and', 'ultimately', 'flu', '##nk', '##ed', 'out', 'of', 'school', '.', 'this', 'was', 'my', 'first', 'taste', 'of', 'failure', 'and', 'i', 'had', 'never', 'been', 'more', 'devastated', '.', '.', 'and', 'neither', 'had', 'my', 'parents', '.', 'it', 'was', 'at', 'this', 'point', 'that', 'my', 'relationship', 'with', 'them', 'really', 'started', 'to', 'tank', '.', 'i', 'was', 'an', 'embarrassment', 'to', 'them', 'and', 'i', 'could', 'tell', '.', '.', '.', 'it', 'was', 'an', 'awful', 'feeling', 'and', 'i', 'started', 'to', 'slip', 'into', 'a', 'deep', 'depression', '.', 'i', 'was', 'constantly', 'being', 'threatened', 'with', 'being', 'kicked', 'out', 'of', 'my', 'house', 'and', 'my', 'parents', 'would', 'yell', 'at', 'me', 'and', 'just', 'flat', 'out', 'told', 'me', 'how', 'much', 'i', 'let', 'them', 'down', '.', 'it', 'was', 'incredibly', 'painful', '.', 'at', 'this', 'point', 'i', 'had', 'fallen', 'so', 'low', 'that', 'all', 'i', 'could', 'do', 'was', 'en', '##tro', '##ll', 'into', 'a', 'junior', 'college', 'and', 'try', 'to', 'get', 'my', 'crap', 'together', 'and', 'hopefully', 'transfer', 'into', 'a', '4', 'year', 'school', '.', 'that', 'first', 'semester', 'of', 'jr', 'college', 'was', '.', '.', '.', '.', 'awful', '.', '.', '.', 'there', '##s', 'just', 'no', 'other', 'way', 'to', 'say', 'it', '.', 'i', 'felt', 'like', 'i', 'was', 'surrounded', 'by', 'med', '##io', '##cr', '##ity', 'and', 'that', 'i', 'was', 'doomed', 'to', 'just', 'wait', 'tables', '.', 'on'], ['top', 'of', 'that', '.', '.', '.', 'i', 'was', 'still', 'doing', 'bad', 'in', 'my', 'classes', '.', 'th', 'depression', '.', '.', '.', 'the', 'anxiety', '.', '.', '.', 'my', 'relationship', 'with', 'my', 'family', '.', '.', '.', '.', 'it', 'just', 'kept', 'getting', 'worse', '.', 'this', 'was', 'the', 'point', 'where', 'i', 'finally', 'decided', 'to', 'let', 'go', 'of', 'my', 'pride', 'and', 'asked', 'for', 'help', '.', 'i', 'called', 'around', 'and', 'i', 'found', 'a', 'psychologist', 'who', 'actually', 'specialized', 'in', 'young', 'adults', 'with', 'ad', '##hd', '.', 'i', 'told', 'him', 'my', 'situation', 'and', 'this', 'is', 'when', 'i', 'learned', 'that', 'he', 'too', 'had', 'originally', 'flu', '##nk', '##ed', 'out', 'of', 'school', 'and', 'worked', 'his', 'ass', 'off', 'and', 'became', 'a', 'doctor', 'and', 'that', 'he', 'wanted', 'to', 'work', 'with', 'me', 'especially', 'and', 'that', 'he', 'would', 'see', 'me', 'once', 'a', 'week', 'and', 'treat', 'me', 'and', 'all', 'of', 'this', 'for', 'free', '.', 'my', 'first', 'time', 'actually', 'visiting', 'him', 'and', 'talking', 'to', 'him', 'was', 'really', 'brutal', '.', '.', '.', 'it', 'was', 'the', 'first', 'time', 'i', 'actually', 'talked', 'to', 'someone', 'about', 'everything', 'i', 'was', 'going', 'through', 'and', 'i', 'broke', 'down', 'right', 'there', '.', 'i', 'ba', '##wled', 'like', 'a', 'little', 'girl', 'and', 'that', '##s', 'when', 'we', 'finally', 'got', 'started', '.', 'he', 'put', 'me', 'back', 'on', 'med', '##s', 'and', 'counsel', '##ed', 'me', 'every', 'week', 'like', 'he', 'said', 'he', 'would', '.', 'even', 'though', 'i', 'was', 'back', 'on', 'med', '##s', ',', 'it', 'was', 'a', 'slow', 'start', '.', 'i', 'still', 'did', 'bad', 'in', 'class', 'at', 'first', 'but', 'other', 'things', 'started', 'coming', 'together', '.', 'so', 'fast', 'forward', 'a', 'year', 'and', 'a', 'half', '.', '.', '.', '.', 'a', 'very', 'frustrating', 'year', 'and', 'a', 'half', '.', 'i', 'was', 'still', 'dealing', 'with', 'depression', 'and', 'anxiety', 'and', 'for', 'the', 'most', 'part', 'my', 'parents', 'had', 'no', 'faith', 'in', 'me', 'and', 'still', 'saw', 'me', 'as', 'a', 'disappointment', 'but', 'this', 'is', 'where', 'things', 'started', 'to', 'change', '.', 'every', 'year', 'since', 'i', 'flu', '##nk', '##ed', 'out', 'i', 'would', 'apply', 'to', 'multiple', 'schools', ',', 'hoping', 'that', 'one', 'of', 'them', 'would', 'give', 'me', 'a', 'chance', 'and', 'of', 'course', 'every', 'single', 'one', 'would', 'reject', 'me', '.', 'by', 'this', 'point', 'in', 'life', 'i', 'had', 'become', 'incredibly', 'used', 'to', 'rejection', 'and', 'even', 'though', 'it', 'hurt', 'every', 'time', 'i', 'never', 'quit', '.', 'but', 'this', 'time', 'it', 'was', 'different', '.', '.', '.', '.', 'every', 'single', 'school', 'i', 'applied', 'to', 'actually', 'accepted', 'me', '.', 'i', 'remember', 'the', 'day', 'i', 'got', 'all', 'of', 'those', 'acceptance', 'letters', 'i', 'sprinted', 'upstairs', 'to', 'my', 'mother', \"'\", 's', 'room', 'and', 'i', 'broke', 'down', 'in', 'front', 'of', 'her', 'over', 'finally', 'being', 'able', 'to', 'go', 'back', 'to', 'a', '4', 'year', 'school', '.', 'easily', 'one', 'of', 'the', 'ha', '##pp', '##iest', 'days', 'of', 'my', 'life', '.', 'so', 'after', 'a', 'year', 'and', 'a', 'half', 'with', 'my', 'doctor', 'i', 'finally', 'said', 'goodbye', 'but', 'we', 'agreed', 'that', 'i', 'would', 'call', 'him', 'at', 'least', 'once', 'a', 'month', 'and', 'he', 'would', 'still', 'write', 'me', 'scripts', 'for', 'med', '##s', '.', 'i', 'went', 'off', 'to', 'school', 'and', 'after', 'a', 'shaky', 'on', 'my', 'own', 'i', 'finally', 'found', 'my', 'groove', '.', 'at', 'first', 'i', 'was', 'really', 'embarrassed', 'around', 'everyone', 'because', 'at', 'the', 'end', 'of', 'the', 'day', 'i', 'still', 'felt', 'like', 'a', 'failure', '.', 'during', 'my', 'entire', 'time', 'in', 'school', 'i'], ['had', 'become', 'so', 'self', 'conscious', 'over', 'the', 'fact', 'that', 'i', 'felt', 'like', 'i', 'was', 'behind', 'so', 'i', 'would', 'never', 'really', 'ever', 'open', 'up', 'to', 'anyone', '.', '.', '.', 'and', 'on', 'top', 'of', 'that', 'my', 'counselor', '##s', 'could', 'not', 'tell', 'me', 'what', 'my', 'predicted', 'graduation', 'date', 'would', 'be', '.', '.', '.', 'it', 'seemed', 'like', 'the', 'finish', 'line', 'was', 'still', 'out', 'of', 'sight', '.', 'well', 'after', 'getting', 'the', 'ball', 'rolling', '.', '.', '.', 'it', 'started', 'to', 'roll', 'fast', ',', '2', 'years', 'later', 'i', 'actually', 'graduated', 'with', 'two', 'degrees', 'from', 'a', 'top', 'name', 'school', '.', 'that', 'was', 'the', 'next', 'time', 'i', 'cried', 'ha', '##ha', '.', 'it', 'was', 'embarrassing', '.', '.', '.', '.', 'i', 'was', 'so', 'proud', 'though', '.', 'by', 'this', 'time', 'i', 'had', 'already', 'sent', 'out', 'hundreds', 'of', 'resume', '##s', 'and', 'was', 'just', 'hoping', 'for', 'a', 'call', 'back', '.', 'like', 'most', 'others', 'who', 'graduated', 'in', '2010', '.', '.', '.', 'finding', 'a', 'job', 'was', 'hard', 'but', '6', 'months', 'after', 'graduation', 'i', 'got', 'a', 'call', 'from', 'one', 'of', 'the', 'largest', 'banks', 'in', 'the', 'country', '.', 'they', 'actually', 'wanted', 'to', 'interview', 'me', '.', 'needles', '##s', 'to', 'say', ',', 'i', 'rocked', 'my', 'interview', 'and', 'somehow', 'got', 'a', 'job', '.', 'it', 'was', 'at', 'that', 'point', '.', '.', '.', 'that', 'definitive', 'point', 'that', 'i', 'knew', 'everything', 'was', 'going', 'to', 'be', 'ok', '.', 'turns', 'out', 'the', 'job', 'i', 'was', 'offered', 'was', 'one', 'of', 'those', '\"', 'if', 'you', 'don', '##t', 'take', 'this', 'job', 'you', 'are', 'a', 'fucking', 'idiot', '\"', 'opportunities', '.', 'i', 'am', 'now', 'a', 'financial', 'analyst', 'being', 'groom', '##ed', 'for', 'portfolio', 'management', 'and', 'i', 'happen', 'to', 'be', 'very', 'good', 'at', 'what', 'i', 'do', '.', 'the', 'real', 'interesting', 'part', 'was', 'that', '5', 'young', 'guys', 'were', 'hired', 'into', 'this', 'group', 'for', 'the', 'specific', 'reason', 'of', 'being', 'groom', '##ed', 'for', 'future', 'management', 'positions', 'and', 'every', 'single', 'one', 'of', 'us', 'has', 'add', '.', 'i', 'am', 'a', 'year', 'and', 'a', 'half', 'into', 'my', 'job', 'and', 'have', 'never', 'been', 'happier', '.', 'my', 'parents', 'are', 'incredibly', 'proud', 'of', 'me', ',', 'i', 'keep', 'hearing', 'about', 'how', 'they', 'constantly', 'bra', '##g', 'about', 'me', 'and', 'our', 'relationship', 'is', 'amazing', '.', 'i', 'have', 'a', 'career', 'now', 'that', 'i', 'can', 'really', 'be', 'proud', 'of', '.', 'i', 'make', 'really', 'good', 'money', ',', 'i', 'love', 'what', 'i', 'do', 'and', 'i', 'have', 'a', 'lot', 'of', 'opportunities', 'in', 'front', 'of', 'me', '.', 'i', 'can', '##t', 'even', 'express', 'how', 'happy', 'i', 'am', '.', 'no', 'more', 'anxiety', ',', 'no', 'more', 'depression', '.', '.', '.', 'just', 'lots', 'of', 'hard', 'work', 'ahead', 'me', 'doing', 'something', 'i', 'really', 'love', 'to', 'do', '.', 'i', 'know', 'this', 'is', 'really', 'long', 'wind', '##ed', 'and', 'i', \"'\", 'm', 'sure', 'i', 'left', 'some', 'stuff', 'out', '.', 'i', 'have', 'to', 'be', 'honest', '.', '.', '.', 'near', 'the', 'end', 'of', 'writing', 'this', 'my', 'add', 'started', 'to', 'kick', 'in', 'ha', '##ha', '##ha', '.', 'at', 'the', 'end', 'of', 'all', 'of', 'it', 'though', ',', 'what', 'it', 'ultimately', 'took', 'was', 'a', 'lot', 'of', 'hard', 'work', 'and', 'knowing', 'when', 'to', 'ask', 'for', 'help', '.', 'if', 'you', 'have', 'questions', 'feel', 'free', 'to', 'ask', '.', 't', '##l', ';', 'dr', 'my', 'ad', '##hd', 'got', 'the', 'better', 'of', 'me', 'in', 'college', ',', 'i', 'flu', '##nk', '##ed', 'out', 'and', 'worked'], ['my', 'ass', 'off', 'to', 'get', 'back', 'in', '.', 'i', 'graduated', 'and', 'now', 'i', 'have', 'a', 'pretty', 'high', 'paying', 'job', 'that', 'i', 'love', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'ho', '##arding', '.', 'how', '##dy', 'y', \"'\", 'all', '!', '26', 'year', '-', 'old', ',', 'diagnosed', 'post', '-', 'college', ',', 'currently', 'taking', 'add', '##eral', '##l', '10', '##mg', '2', '##x', 'daily', '.', 'i', 'was', 'wondering', 'if', 'anyone', 'else', 'struggles', 'with', 'ho', '##arding', '-', 'type', 'behavior', '.', 'i', \"'\", 've', '*', 'always', '*', 'been', 'un', '##ti', '##dy', 'and', 'di', '##sor', '##gan', '##ized', '.', 'when', 'i', 'was', 'a', 'kidd', '##o', ',', 'my', 'mom', 'would', 'always', 'be', 'on', 'me', 'about', '\"', 'leaving', 'tracks', '\"', 'wherever', 'i', 'went', '.', 'just', 'finished', 'eating', 'breakfast', '?', 'the', 'cereal', 'bowl', 'and', 'spoon', 'make', 'it', 'to', 'the', 'sink', ',', 'but', 'never', 'the', 'dish', '##wash', '##er', '.', 'and', 'the', 'cereal', 'is', 'probably', 'still', 'out', 'on', 'the', 'counter', '.', 'getting', 'dressed', 'and', 'ready', 'in', 'the', 'morning', '?', 'there', \"'\", 's', 'gonna', 'be', 'water', 'spots', 'all', 'over', 'the', 'mirror', ',', 'tooth', '##brush', 'and', 'too', '##tp', '##ast', '##e', 'left', 'by', 'the', 'sink', ',', 'p', '##js', 'left', 'on', 'the', 'floor', 'in', 'my', 'room', '.', 'pretty', 'much', 'wherever', 'i', 'set', 'something', 'when', 'i', 'was', 'done', 'with', 'it', 'was', 'where', 'it', 'stayed', '.', 'any', 'kind', 'of', 'serious', 'cleaning', 'that', 'i', 'was', 'told', 'to', 'do', 'was', 'a', 'totally', 'miserable', 'experience', '.', 'generally', 'it', 'involved', 'me', 'doing', 'one', 'of', 'several', 'things', ':', '1', ')', 'staring', 'at', 'the', 'mess', 'with', 'a', 'vague', 'sense', 'of', 'helpless', '##nes', ',', 'feeling', 'overwhelmed', ',', 'and', 'having', 'no', 'idea', 'where', 'to', 'start', ',', '2', ')', 'getting', 'completely', 'lost', 'in', 'some', 'small', 'aspect', 'of', 'the', 'task', ',', 'e', '.', 'g', '.', 'organizing', 'my', 'books', '##hel', '##f', 'by', 'title', 'and', 'author', 'rather', 'than', 'tack', '##ling', 'the', 'gin', '##or', '##mous', 'mess', 'scattered', 'throughout', 'the', 'room', ',', 'or', '3', ')', 'starting', 'to', 'clean', ',', 'then', 'getting', 'distracted', 'by', 'the', 'things', 'i', 'found', '(', 'oh', 'man', ',', 'i', 'forgot', 'about', 'this', 'book', '!', 'i', 'better', 'flip', 'through', 'just', 'a', 'couple', 'pages', 'before', 'putting', 'it', 'up', '!', ')', '.', 'now', ',', 'unfortunately', 'for', 'me', ',', 'all', 'this', 'carried', 'over', 'into', 'adulthood', '.', 'to', 'this', 'day', ',', 'i', 'lo', '##ath', '##e', '-', 'let', 'me', 'emphasize', 'that', '-', '*', '*', 'lo', '##ath', '##e', '*', '*', '-', 'cleaning', '.', 'despite', 'the', 'fact', 'that', 'i', 'love', 'having', 'a', 'clean', 'house', '.', 'i', \"'\", 'm', 'fine', 'as', 'long', 'as', 'i', 'keep', 'on', 'top', 'of', 'it', 'and', 'stick', 'to', 'a', 'daily', 'cleaning', 'routine', '.', 'but', 'all', 'it', 'takes', 'is', 'one', 'day', 'off', '.', '.', '.', 'oh', ',', 'i', \"'\", 'll', 'fold', 'the', 'laundry', 'later', ',', 'or', 'man', ',', 'i', 'just', 'don', \"'\", 't', 'want', 'to', 'deal', 'with', 'running', 'the', 'vacuum', 'today', '.', '.', '.', 'and', 'it', 'just', 'totally', 'falls', 'apart', '.', 'before', 'i', 'know', 'it', ',', 'the', 'mess', 'is', 'just', 'completely', 'out', 'of', 'hand', ',', 'and', 'i', \"'\", 'm', 'overwhelmed', ',', 'and', 'shit', ',', 'i', 'just', 'don', \"'\", 't', 'want', 'to', 'deal', 'with', 'this', 'at', 'all', ',', 'it', \"'\", 's', 'such', 'a', 'mess', 'that', 'there', \"'\", 's', 'really', 'no', 'point', ',', 'so', 'i', \"'\", 'm', 'just', 'going', 'to', 'go', 'to', 'bed', 'instead', '.', '/', '/', 'deep', 'breath', 'and', 'this', 'is', 'just', 'the', 'actual', 'cleaning', 'aspect', '.', 'we', 'haven', \"'\", 't', 'even', 'gotten', 'into', 'the', 'massive', 'amounts', 'of', '*', 'stuff', '*', 'i', 'have', '.', 'and', 'the', 'amounts', '*', 'are', '*', 'massive', '.', 'it', \"'\", 's', 'not', 'even', 'that', 'i', 'feel', 'an', 'emotional', 'attachment', 'my', 'things', '.', 'if', 'it', \"'\", 's', 'out', 'of', 'sight', ',', 'it', \"'\", 's', 'out', 'of', 'mind', '.', 'and', 'i', 'would', 'honestly', 'be', 'completely', 'content', 'if', 'it', 'all', 'just', 'vanished', '/', 'was', 'thrown', 'out', '/', 'just', 'went', 'away', '.', 'but', 'the', 'idea', 'of', 'actually', 'having', 'to', 'go', '*', 'through', '*', 'the', 'massive', 'amounts', 'of', 'stuff', 'is', 'completely', 'overwhelming', '.', 'it', \"'\", 's', 'like', 'my', 'brain', 'totally', 'shut', '##s', 'down', 'when', 'i', 'even', 'think', 'about', 'it', '.', '(', 'and', 'we', \"'\", 're', 'back', 'to', 'the', 'actual', 'cleaning', 'aspect', 'again', '.', ')', 'so', 'not', 'only', 'do', 'i', 'have', 'massive', 'amounts', 'of', 'stuff', ',', 'it', \"'\", 's', 'all', 'just', 'totally', 'di', '##sor', '##gan', '##ized', '.', 'which', 'is', 'a', 'huge', 'contributing', 'factor', 'to', 'why', 'i', 'have', 'so', 'many', 'things', '.', 'need', 'a', 'ruler', 'and', 'can', \"'\", 't', 'find', 'one', '?', 'just', 'pick', 'up', 'another', 'one', 'while', 'you', \"'\", 're', 'at', 'the', 'store', '.', 'i', \"'\", 'm', 'fairly', 'confident', 'that', 'i', 'could', 'open', 'my', 'own', 'office', 'supply', 'store', 'at', 'this', 'point', '.', 'another', 'problem', 'that', 'i', 'run', 'into', 'when', 'cleaning', 'is', 'running', 'into', 'things', 'i', \"'\", 'd', 'completely', 'forgotten', 'i', 'owned', '.', 'like', ',', 'holy', 'shit', ',', '*', 'here', \"'\", 's', '*', 'my', 'hot', 'glue', 'gun', '!', 'now', 'if', 'only', 'i', 'knew', 'where', 'the', 'glue', '##stick', '##s', 'were', '.', 'i', '*', 'know', '*', 'i', 'saw', \"'\", 'em', 'just', 'the', 'other', 'day', '.', '.', '.', 'and', 'i', \"'\", 'm', 'off', 'on', 'a', 'side', 'quest', '.', 'u', '##gh', '.', 'anyway', '.', 'any', 'of', 'y', \"'\", 'all', 'struggle', 'with', 'similar', 'issues', '?', 'how', 'do', 'you', 'deal', 'with', 'it', '?', '*', '*', 't', '##ld', '##r', ':', 'my', 'house', 'is', 'a', 'mess', '.', 'and', 'i', 'own', 'way', 'too', 'much', 'shit', 'that', 'i', 'don', \"'\", 't', 'even', 'want', ',', 'but', 'trying', 'to', 'clean', 'and', 'organize', 'is', 'overwhelming', '.', '*', '*', '*', '*', 'edit', ':', '*', '*', 'wow', '!', 'i', 'can', \"'\", 't', 'tell', 'y', \"'\", 'all', 'how', 'much', 'it', 'means', 'to', 'see', 'that', 'other', 'people', 'struggle', 'with', 'this', 'too', '.', 'it', \"'\", 's', 'like', 'when', 'i', 'was', 'finally', 'diagnosed', 'with', 'ad', '##hd', '-', 'suddenly', '*', 'everything', '*', 'that', 'i', 'had', 'struggled', 'with', 'my', 'whole', 'life', '-', 'my', 'behavior', 'patterns', ',', 'my', 'thought', 'processes', ',', 'everything', '-', '*', 'made', 'sense', '.', '*', 'there', 'was', 'a', 'reason', 'i', 'was', 'the', 'way', 'i', 'am', '-', 'and', 'it', 'wasn', \"'\", 't', 'because', 'i', 'lacked', 'will', '##power', ',', 'or', 'fucked', 'up', ',', 'or', 'a', 'lou', '##sy', 'human', 'being', '-', 'but', 'because', 'my', 'brain', 'was', 'wired', 'different', '.', 'and', '-', 'holy', 'crap', '-', '*', 'other', 'people', '*', 'think', 'that', 'way', 'too', '.', 'it', 'still', 'just', 'blows', 'my', 'mind', 'to', 'see', 'that', 'other', 'people', 'feel', '/', 'think', 'the', 'same', 'way', '.']\n",
      "INFO:__main__:Number of tokens: 981\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'ho', '##arding', '.', 'how', '##dy', 'y', \"'\", 'all', '!', '26', 'year', '-', 'old', ',', 'diagnosed', 'post', '-', 'college', ',', 'currently', 'taking', 'add', '##eral', '##l', '10', '##mg', '2', '##x', 'daily', '.', 'i', 'was', 'wondering', 'if', 'anyone', 'else', 'struggles', 'with', 'ho', '##arding', '-', 'type', 'behavior', '.', 'i', \"'\", 've', '*', 'always', '*', 'been', 'un', '##ti', '##dy', 'and', 'di', '##sor', '##gan', '##ized', '.', 'when', 'i', 'was', 'a', 'kidd', '##o', ',', 'my', 'mom', 'would', 'always', 'be', 'on', 'me', 'about', '\"', 'leaving', 'tracks', '\"', 'wherever', 'i', 'went', '.', 'just', 'finished', 'eating', 'breakfast', '?', 'the', 'cereal', 'bowl', 'and', 'spoon', 'make', 'it', 'to', 'the', 'sink', ',', 'but', 'never', 'the', 'dish', '##wash', '##er', '.', 'and', 'the', 'cereal', 'is', 'probably', 'still', 'out', 'on', 'the', 'counter', '.', 'getting', 'dressed', 'and', 'ready', 'in', 'the', 'morning', '?', 'there', \"'\", 's', 'gonna', 'be', 'water', 'spots', 'all', 'over', 'the', 'mirror', ',', 'tooth', '##brush', 'and', 'too', '##tp', '##ast', '##e', 'left', 'by', 'the', 'sink', ',', 'p', '##js', 'left', 'on', 'the', 'floor', 'in', 'my', 'room', '.', 'pretty', 'much', 'wherever', 'i', 'set', 'something', 'when', 'i', 'was', 'done', 'with', 'it', 'was', 'where', 'it', 'stayed', '.', 'any', 'kind', 'of', 'serious', 'cleaning', 'that', 'i', 'was', 'told', 'to', 'do', 'was', 'a', 'totally', 'miserable', 'experience', '.', 'generally', 'it', 'involved', 'me', 'doing', 'one', 'of', 'several', 'things', ':', '1', ')', 'staring', 'at', 'the', 'mess', 'with', 'a', 'vague', 'sense', 'of', 'helpless', '##nes', ',', 'feeling', 'overwhelmed', ',', 'and', 'having', 'no', 'idea', 'where', 'to', 'start', ',', '2', ')', 'getting', 'completely', 'lost', 'in', 'some', 'small', 'aspect', 'of', 'the', 'task', ',', 'e', '.', 'g', '.', 'organizing', 'my', 'books', '##hel', '##f', 'by', 'title', 'and', 'author', 'rather', 'than', 'tack', '##ling', 'the', 'gin', '##or', '##mous', 'mess', 'scattered', 'throughout', 'the', 'room', ',', 'or', '3', ')', 'starting', 'to', 'clean', ',', 'then', 'getting', 'distracted', 'by', 'the', 'things', 'i', 'found', '(', 'oh', 'man', ',', 'i', 'forgot', 'about', 'this', 'book', '!', 'i', 'better', 'flip', 'through', 'just', 'a', 'couple', 'pages', 'before', 'putting', 'it', 'up', '!', ')', '.', 'now', ',', 'unfortunately', 'for', 'me', ',', 'all', 'this', 'carried', 'over', 'into', 'adulthood', '.', 'to', 'this', 'day', ',', 'i', 'lo', '##ath', '##e', '-', 'let', 'me', 'emphasize', 'that', '-', '*', '*', 'lo', '##ath', '##e', '*', '*', '-', 'cleaning', '.', 'despite', 'the', 'fact', 'that', 'i', 'love', 'having', 'a', 'clean', 'house', '.', 'i', \"'\", 'm', 'fine', 'as', 'long', 'as', 'i', 'keep', 'on', 'top', 'of', 'it', 'and', 'stick', 'to', 'a', 'daily', 'cleaning', 'routine', '.', 'but', 'all', 'it', 'takes', 'is', 'one', 'day', 'off', '.', '.', '.', 'oh', ',', 'i', \"'\", 'll', 'fold', 'the', 'laundry', 'later', ',', 'or', 'man', ',', 'i', 'just', 'don', \"'\", 't', 'want', 'to', 'deal', 'with', 'running', 'the', 'vacuum', 'today', '.', '.', '.', 'and', 'it', 'just', 'totally', 'falls', 'apart', '.', 'before', 'i', 'know', 'it', ',', 'the', 'mess', 'is', 'just', 'completely', 'out', 'of', 'hand', ',', 'and', 'i', \"'\", 'm', 'overwhelmed', ',', 'and', 'shit', ',', 'i', 'just', 'don', \"'\", 't', 'want', 'to', 'deal', 'with', 'this', 'at', 'all', ',', 'it', \"'\", 's', 'such', 'a', 'mess', 'that', 'there', \"'\", 's', 'really', 'no', 'point', ',', 'so', 'i', \"'\", 'm', 'just', 'going', 'to', 'go', 'to', 'bed', 'instead', '.', '/', '/', 'deep', 'breath', 'and', 'this', 'is', 'just', 'the', 'actual', 'cleaning', 'aspect', '.', 'we', 'haven', \"'\", 't', 'even', 'gotten', 'into', 'the', 'massive', 'amounts'], ['of', '*', 'stuff', '*', 'i', 'have', '.', 'and', 'the', 'amounts', '*', 'are', '*', 'massive', '.', 'it', \"'\", 's', 'not', 'even', 'that', 'i', 'feel', 'an', 'emotional', 'attachment', 'my', 'things', '.', 'if', 'it', \"'\", 's', 'out', 'of', 'sight', ',', 'it', \"'\", 's', 'out', 'of', 'mind', '.', 'and', 'i', 'would', 'honestly', 'be', 'completely', 'content', 'if', 'it', 'all', 'just', 'vanished', '/', 'was', 'thrown', 'out', '/', 'just', 'went', 'away', '.', 'but', 'the', 'idea', 'of', 'actually', 'having', 'to', 'go', '*', 'through', '*', 'the', 'massive', 'amounts', 'of', 'stuff', 'is', 'completely', 'overwhelming', '.', 'it', \"'\", 's', 'like', 'my', 'brain', 'totally', 'shut', '##s', 'down', 'when', 'i', 'even', 'think', 'about', 'it', '.', '(', 'and', 'we', \"'\", 're', 'back', 'to', 'the', 'actual', 'cleaning', 'aspect', 'again', '.', ')', 'so', 'not', 'only', 'do', 'i', 'have', 'massive', 'amounts', 'of', 'stuff', ',', 'it', \"'\", 's', 'all', 'just', 'totally', 'di', '##sor', '##gan', '##ized', '.', 'which', 'is', 'a', 'huge', 'contributing', 'factor', 'to', 'why', 'i', 'have', 'so', 'many', 'things', '.', 'need', 'a', 'ruler', 'and', 'can', \"'\", 't', 'find', 'one', '?', 'just', 'pick', 'up', 'another', 'one', 'while', 'you', \"'\", 're', 'at', 'the', 'store', '.', 'i', \"'\", 'm', 'fairly', 'confident', 'that', 'i', 'could', 'open', 'my', 'own', 'office', 'supply', 'store', 'at', 'this', 'point', '.', 'another', 'problem', 'that', 'i', 'run', 'into', 'when', 'cleaning', 'is', 'running', 'into', 'things', 'i', \"'\", 'd', 'completely', 'forgotten', 'i', 'owned', '.', 'like', ',', 'holy', 'shit', ',', '*', 'here', \"'\", 's', '*', 'my', 'hot', 'glue', 'gun', '!', 'now', 'if', 'only', 'i', 'knew', 'where', 'the', 'glue', '##stick', '##s', 'were', '.', 'i', '*', 'know', '*', 'i', 'saw', \"'\", 'em', 'just', 'the', 'other', 'day', '.', '.', '.', 'and', 'i', \"'\", 'm', 'off', 'on', 'a', 'side', 'quest', '.', 'u', '##gh', '.', 'anyway', '.', 'any', 'of', 'y', \"'\", 'all', 'struggle', 'with', 'similar', 'issues', '?', 'how', 'do', 'you', 'deal', 'with', 'it', '?', '*', '*', 't', '##ld', '##r', ':', 'my', 'house', 'is', 'a', 'mess', '.', 'and', 'i', 'own', 'way', 'too', 'much', 'shit', 'that', 'i', 'don', \"'\", 't', 'even', 'want', ',', 'but', 'trying', 'to', 'clean', 'and', 'organize', 'is', 'overwhelming', '.', '*', '*', '*', '*', 'edit', ':', '*', '*', 'wow', '!', 'i', 'can', \"'\", 't', 'tell', 'y', \"'\", 'all', 'how', 'much', 'it', 'means', 'to', 'see', 'that', 'other', 'people', 'struggle', 'with', 'this', 'too', '.', 'it', \"'\", 's', 'like', 'when', 'i', 'was', 'finally', 'diagnosed', 'with', 'ad', '##hd', '-', 'suddenly', '*', 'everything', '*', 'that', 'i', 'had', 'struggled', 'with', 'my', 'whole', 'life', '-', 'my', 'behavior', 'patterns', ',', 'my', 'thought', 'processes', ',', 'everything', '-', '*', 'made', 'sense', '.', '*', 'there', 'was', 'a', 'reason', 'i', 'was', 'the', 'way', 'i', 'am', '-', 'and', 'it', 'wasn', \"'\", 't', 'because', 'i', 'lacked', 'will', '##power', ',', 'or', 'fucked', 'up', ',', 'or', 'a', 'lou', '##sy', 'human', 'being', '-', 'but', 'because', 'my', 'brain', 'was', 'wired', 'different', '.', 'and', '-', 'holy', 'crap', '-', '*', 'other', 'people', '*', 'think', 'that', 'way', 'too', '.', 'it', 'still', 'just', 'blows', 'my', 'mind', 'to', 'see', 'that', 'other', 'people', 'feel', '/', 'think', 'the', 'same', 'way', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['for', 'those', 'of', 'us', 'who', 'have', 'tried', 'rita', '##lin', 'and', 'st', '##rat', '##tera', ',', 'what', 'other', 'options', 'remain', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['for', 'those', 'of', 'us', 'who', 'have', 'tried', 'rita', '##lin', 'and', 'st', '##rat', '##tera', ',', 'what', 'other', 'options', 'remain', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'sometimes', 'dr', '##ool', 'a', 'bit', 'because', 'they', 'lost', 'focus', 'on', 'their', 'mouth', '?', 'happens', 'to', 'me', 'at', 'least', 'daily', ',', 'i', 'swear', '.', 'i', 'get', 'really', 'into', 'something', ',', 'and', 'the', 'next', 'moment', 'i', 'can', 'feel', 'a', 'file', '##t', 'of', 'saliva', 'hanging', 'from', 'my', 'mouth', '.', ':', 'x']\n",
      "INFO:__main__:Number of tokens: 50\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'sometimes', 'dr', '##ool', 'a', 'bit', 'because', 'they', 'lost', 'focus', 'on', 'their', 'mouth', '?', 'happens', 'to', 'me', 'at', 'least', 'daily', ',', 'i', 'swear', '.', 'i', 'get', 'really', 'into', 'something', ',', 'and', 'the', 'next', 'moment', 'i', 'can', 'feel', 'a', 'file', '##t', 'of', 'saliva', 'hanging', 'from', 'my', 'mouth', '.', ':', 'x']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hello', '/', 'r', '/', 'ad', '##hd', ',', 'please', 'say', 'hello', 'to', 'the', 'quiet', 'place']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hello', '/', 'r', '/', 'ad', '##hd', ',', 'please', 'say', 'hello', 'to', 'the', 'quiet', 'place']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['to', 'r', '/', 'ad', '##hd', ':', 'you', 'guys', 'are', 'awesome', '!']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['to', 'r', '/', 'ad', '##hd', ':', 'you', 'guys', 'are', 'awesome', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['r', '/', 'ad', '##hd', 'demographics', 'survey', '!']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['r', '/', 'ad', '##hd', 'demographics', 'survey', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'what', 'is', 'one', 'thing', 'you', 'can', 'be', 'proud', 'about', '?', '[', 'week', '5', ']', '*', '*', 'welcome', 'to', 'the', '5th', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', 'so', 'here', 'is', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', 'if', 'you', 'can', \"'\", 't', 'think', 'of', 'any', '\"', 'wins', '\"', 'you', 'may', 'put', 'something', 'you', 'are', 'grateful', 'for', '.', 'we', 'all', 'can', 'express', 'some', 'gratitude', '.', '*', '*', '*', '*', '*', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', '*', '*', '*', 'some', 'examples', 'from', 'past', 'weeks', '[UNK]', '*', '*', 'started', 'taking', 'ad', '##hd', 'medication', '*', '*', 'x', '##2', '[UNK]', '*', '*', 'called', 'doctor', '*', '*', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '[UNK]', '*', '*', 'working', 'out', '*', '*', 'and', 'eating', 'a', 'healthy', 'diet', '[UNK]', 'got', 'to', 'work', '.', '.', '.', '*', '*', 'on', 'time', '*', '*', '!', '[UNK]', 'went', 'to', '*', '*', 'sleep', 'by', '2a', '##m', '*', '*', 'for', '6', 'nights', '[UNK]', 'finally', 'got', 'a', '*', '*', 'diagnosis', '*', '*', '(', 'a', 'few', 'people', ')', '*', '*', 'very', 'awesome', '*', '*', '[UNK]', 'started', '*', '*', 'working', 'out', '*', '*', 'again', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', '*', '*', '*', 'week', '1', ':', '12', 'participants', 'week', '2', ':', '24', 'week', '3', ':', '15', 'week', '4', ':', '15', 'week', '5', ':', '?', '?', '?', 'week', '6', ':', 'profit', '*', '*', '*', '*', '*', 'edit', ':', 'reminder', 'to', 'up', '##vot', '##e', 'this', 'post', 'so', 'all', 'subscribers', 'can', 'see', 'it', 'in', 'their', 'front', 'page', '.', '*', '*', 'i', 'know', 'sometimes', 'we', 'forget', 'to', 'do', 'things', '(', 'i', 'even', 'forgot', 'to', 'flush', 'the', 'other', 'day', ')', '&', '#', '323', '##2', ';', '\\\\', '_', '&', '#', '323', '##2', ';', '&', '#', '323', '##2', ';', '\\\\', '_', '&', '#', '323', '##2', ';', '&', '#', '323', '##2', ';', '\\\\', '_', '&', '#', '323', '##2', ';']\n",
      "INFO:__main__:Number of tokens: 480\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', 'all', 'share', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '.', 'what', 'is', 'one', 'thing', 'you', 'can', 'be', 'proud', 'about', '?', '[', 'week', '5', ']', '*', '*', 'welcome', 'to', 'the', '5th', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', 'so', 'here', 'is', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', 'if', 'you', 'can', \"'\", 't', 'think', 'of', 'any', '\"', 'wins', '\"', 'you', 'may', 'put', 'something', 'you', 'are', 'grateful', 'for', '.', 'we', 'all', 'can', 'express', 'some', 'gratitude', '.', '*', '*', '*', '*', '*', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', '*', '*', '*', 'some', 'examples', 'from', 'past', 'weeks', '[UNK]', '*', '*', 'started', 'taking', 'ad', '##hd', 'medication', '*', '*', 'x', '##2', '[UNK]', '*', '*', 'called', 'doctor', '*', '*', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '[UNK]', '*', '*', 'working', 'out', '*', '*', 'and', 'eating', 'a', 'healthy', 'diet', '[UNK]', 'got', 'to', 'work', '.', '.', '.', '*', '*', 'on', 'time', '*', '*', '!', '[UNK]', 'went', 'to', '*', '*', 'sleep', 'by', '2a', '##m', '*', '*', 'for', '6', 'nights', '[UNK]', 'finally', 'got', 'a', '*', '*', 'diagnosis', '*', '*', '(', 'a', 'few', 'people', ')', '*', '*', 'very', 'awesome', '*', '*', '[UNK]', 'started', '*', '*', 'working', 'out', '*', '*', 'again', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', '*', '*', '*', 'week', '1', ':', '12', 'participants', 'week', '2', ':', '24', 'week', '3', ':', '15', 'week', '4', ':', '15', 'week', '5', ':', '?', '?', '?', 'week', '6', ':', 'profit', '*', '*', '*', '*', '*', 'edit', ':', 'reminder', 'to', 'up', '##vot', '##e', 'this', 'post', 'so', 'all', 'subscribers', 'can', 'see', 'it', 'in', 'their', 'front', 'page', '.', '*', '*', 'i', 'know', 'sometimes', 'we', 'forget', 'to', 'do', 'things', '(', 'i', 'even', 'forgot', 'to', 'flush', 'the', 'other', 'day', ')', '&', '#', '323', '##2', ';', '\\\\', '_', '&', '#', '323', '##2', ';', '&', '#', '323', '##2', ';', '\\\\', '_', '&', '#', '323', '##2', ';', '&', '#', '323', '##2', ';', '\\\\', '_', '&', '#', '323', '##2', ';']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'had', 'a', 'freak', '-', 'out', 'the', 'other', 'day', 'filled', 'with', 'self', '-', 'hating', 'rage', 'when', 'paying', 'taxes', '.', '.', '.', '.', 'over', 'nothing', '.', 'i', 'have', 'to', 'pay', 'estimated', 'taxes', 'quarterly', 'because', 'of', 'my', 'job', '(', 'long', 'story', ')', ',', 'and', 'when', 'we', 'filed', 'this', 'year', 'i', 'realized', 'that', 'forgot', 'to', 'pay', 'twice', ',', 'and', 'the', 'two', 'times', 'i', 'did', 'pay', '(', 'adding', 'extra', 'for', 'the', 'ones', 'i', 'missed', ')', ',', 'i', 'was', 'at', 'least', 'a', 'week', 'late', '.', 'so', 'i', 'freaked', 'the', 'hell', 'out', 'because', 'i', 'was', 'so', 'incredibly', 'angry', 'with', 'myself', '.', 'having', 'to', 'hit', '4', 'spread', 'out', 'deadline', '##s', 'with', 'no', 'immediate', 'consequences', 'is', 'extremely', 'hard', 'for', 'me', ',', 'and', 'i', 'was', 'pissed', '.', 'total', 'penalty', 'for', 'the', 'missed', 'deadline', '##s', ':', '$', '16', '.']\n",
      "INFO:__main__:Number of tokens: 129\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'had', 'a', 'freak', '-', 'out', 'the', 'other', 'day', 'filled', 'with', 'self', '-', 'hating', 'rage', 'when', 'paying', 'taxes', '.', '.', '.', '.', 'over', 'nothing', '.', 'i', 'have', 'to', 'pay', 'estimated', 'taxes', 'quarterly', 'because', 'of', 'my', 'job', '(', 'long', 'story', ')', ',', 'and', 'when', 'we', 'filed', 'this', 'year', 'i', 'realized', 'that', 'forgot', 'to', 'pay', 'twice', ',', 'and', 'the', 'two', 'times', 'i', 'did', 'pay', '(', 'adding', 'extra', 'for', 'the', 'ones', 'i', 'missed', ')', ',', 'i', 'was', 'at', 'least', 'a', 'week', 'late', '.', 'so', 'i', 'freaked', 'the', 'hell', 'out', 'because', 'i', 'was', 'so', 'incredibly', 'angry', 'with', 'myself', '.', 'having', 'to', 'hit', '4', 'spread', 'out', 'deadline', '##s', 'with', 'no', 'immediate', 'consequences', 'is', 'extremely', 'hard', 'for', 'me', ',', 'and', 'i', 'was', 'pissed', '.', 'total', 'penalty', 'for', 'the', 'missed', 'deadline', '##s', ':', '$', '16', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', \"'\", 's', 'your', 'go', '-', 'to', 'song', 'to', 'get', 'you', 'started', ',', 'pumped', 'up', 'and', 'ready', 'to', 'go', '?', 'music', 'can', 'help', 'a', 'lot', 'with', 'getting', 'out', 'of', 'those', 'negative', 'sling', '##s', 'and', 'with', 'overcoming', 'obstacles', ',', 'what', \"'\", 's', 'your', 'favorite', '*', 'get', '-', 'pumped', '*', 'song', '?', 'my', 'submission', ':', '[', 'prototype', '##ra', '##pt', '##or', '-', 'drive', 'hard', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'j', '##y', '##q', '##d', '##w', '-', '_', '_', 'ae', '##0', ')', '.', 'lovely', 'energetic', 'mix', 'of', 'electro', 'house', 'and', 'jazz', ':', 'd', '!']\n",
      "INFO:__main__:Number of tokens: 100\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', \"'\", 's', 'your', 'go', '-', 'to', 'song', 'to', 'get', 'you', 'started', ',', 'pumped', 'up', 'and', 'ready', 'to', 'go', '?', 'music', 'can', 'help', 'a', 'lot', 'with', 'getting', 'out', 'of', 'those', 'negative', 'sling', '##s', 'and', 'with', 'overcoming', 'obstacles', ',', 'what', \"'\", 's', 'your', 'favorite', '*', 'get', '-', 'pumped', '*', 'song', '?', 'my', 'submission', ':', '[', 'prototype', '##ra', '##pt', '##or', '-', 'drive', 'hard', ']', '(', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'j', '##y', '##q', '##d', '##w', '-', '_', '_', 'ae', '##0', ')', '.', 'lovely', 'energetic', 'mix', 'of', 'electro', 'house', 'and', 'jazz', ':', 'd', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['don', \"'\", 't', 'work', '.', 'be', 'hated', '.', 'love', 'someone', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['don', \"'\", 't', 'work', '.', 'be', 'hated', '.', 'love', 'someone', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['lets', 'discuss', ':', 'coping', 'mechanisms']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['lets', 'discuss', ':', 'coping', 'mechanisms']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'thoughts', 'while', 'doing', 'daily', 'household', 'chores', 'after', '3', 'months', 'on', 'rita', '##lin', '.']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'thoughts', 'while', 'doing', 'daily', 'household', 'chores', 'after', '3', 'months', 'on', 'rita', '##lin', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'creativity', '(', 'just', 'diagnosed', 'at', '33', ')']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'creativity', '(', 'just', 'diagnosed', 'at', '33', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sick', 'of', 'this', 'shit', '.']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sick', 'of', 'this', 'shit', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'hide', 'your', 'medication', 'from', 'roommate', '##s', 'and', 'friends']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'hide', 'your', 'medication', 'from', 'roommate', '##s', 'and', 'friends']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'psychiatrist', 'is', 'trying', 'to', 'avoid', 'any', 'st', '##im', '##ula', '##nt', 'medication', '.', 'what', 'do', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'psychiatrist', 'is', 'trying', 'to', 'avoid', 'any', 'st', '##im', '##ula', '##nt', 'medication', '.', 'what', 'do', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['attention', 'deficit', 'hyper', '##act', '##ivity', 'disorder', 'ad', '##hd', 'linked', 'to', 'sleep', 'ap', '##nea']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['attention', 'deficit', 'hyper', '##act', '##ivity', 'disorder', 'ad', '##hd', 'linked', 'to', 'sleep', 'ap', '##nea']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hi', 'ad', '##hd', '.', 'i', 'should', 'have', 'gone', 'to', 'bed', 'hours', 'ago', '.']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hi', 'ad', '##hd', '.', 'i', 'should', 'have', 'gone', 'to', 'bed', 'hours', 'ago', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'ad', '##hd', 'means', 'i', 'never', 'lose', 'an', 'argument', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'ad', '##hd', 'means', 'i', 'never', 'lose', 'an', 'argument', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'hyper', 'focus', 'on', 'reading', '?']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'hyper', 'focus', 'on', 'reading', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hey', ',', 'r', '/', 'ad', '##hd', '!', 'i', 'made', 'a', 'video', 'about', 'what', 'being', 'ad', '##hd', 'feels', 'like', 'for', 'an', 'art', 'class', ',', 'here', 'you', 'go', '!']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hey', ',', 'r', '/', 'ad', '##hd', '!', 'i', 'made', 'a', 'video', 'about', 'what', 'being', 'ad', '##hd', 'feels', 'like', 'for', 'an', 'art', 'class', ',', 'here', 'you', 'go', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'got', 'prescribed', ',', 'looking', 'for', 'advice']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'got', 'prescribed', ',', 'looking', 'for', 'advice']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['who', 'said', 'st', '##im', '##ula', '##nts', '?', '?', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['who', 'said', 'st', '##im', '##ula', '##nts', '?', '?', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['eu', '##ph', '##oria', 'that', 'comes', 'with', 'add', '##eral', '##l', 'scares', 'me', 'a', 'little', '.']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['eu', '##ph', '##oria', 'that', 'comes', 'with', 'add', '##eral', '##l', 'scares', 'me', 'a', 'little', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['social', 'situations', ':', 'a', 'game', 'everyone', 'is', 'playing', '.', '.', 'that', 'i', 'don', \"'\", 't', 'know', 'the', 'rules', 'to', '?', 'as', 'the', 'title', 'says', ',', 'that', \"'\", 's', 'basically', 'how', 'i', 'feel', 'in', 'most', 'social', 'situations', 'where', 'it', \"'\", 's', 'more', 'than', 'just', 'me', 'and', 'one', 'other', 'person', '.', 'it', \"'\", 's', 'as', 'if', 'there', \"'\", 's', 'a', 'complex', 'game', 'going', 'on', ',', 'everyone', 'knows', 'it', \"'\", 's', 'a', 'game', 'and', 'most', 'of', 'it', 'is', 'just', 'pretend', '.', '.', 'i', 'try', 'to', 'play', 'the', 'game', ',', 'but', 'it', \"'\", 's', 'like', 'i', 'don', \"'\", 't', 'have', 'the', 'fog', '##gies', '##t', 'idea', 'of', 'the', 'rules', ',', 'and', 'even', 'if', 'i', 'did', ',', 'i', 'wouldn', \"'\", 't', 'be', 'able', 'to', 'bring', 'myself', 'to', 'play', '.', '.', '.', 'i', \"'\", 'm', 'not', 'a', 'very', 'good', 'actor', 'or', 'liar', '.', 'am', 'i', 'the', 'only', 'one', 'who', 'feels', 'this', 'way', '?', '(', 'side', 'note', ':', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'ad', '##hd', 'since', 'i', 'was', 'a', 'child', ',', 'recently', 'restarted', 'treatment', 'with', 'concert', '##a', ',', 'with', 'positive', 'results', '.', '.', '.', 'but', 'this', 'feeling', 'has', 'been', 'there', 'for', 'my', 'entire', 'adult', 'life', ')']\n",
      "INFO:__main__:Number of tokens: 189\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['social', 'situations', ':', 'a', 'game', 'everyone', 'is', 'playing', '.', '.', 'that', 'i', 'don', \"'\", 't', 'know', 'the', 'rules', 'to', '?', 'as', 'the', 'title', 'says', ',', 'that', \"'\", 's', 'basically', 'how', 'i', 'feel', 'in', 'most', 'social', 'situations', 'where', 'it', \"'\", 's', 'more', 'than', 'just', 'me', 'and', 'one', 'other', 'person', '.', 'it', \"'\", 's', 'as', 'if', 'there', \"'\", 's', 'a', 'complex', 'game', 'going', 'on', ',', 'everyone', 'knows', 'it', \"'\", 's', 'a', 'game', 'and', 'most', 'of', 'it', 'is', 'just', 'pretend', '.', '.', 'i', 'try', 'to', 'play', 'the', 'game', ',', 'but', 'it', \"'\", 's', 'like', 'i', 'don', \"'\", 't', 'have', 'the', 'fog', '##gies', '##t', 'idea', 'of', 'the', 'rules', ',', 'and', 'even', 'if', 'i', 'did', ',', 'i', 'wouldn', \"'\", 't', 'be', 'able', 'to', 'bring', 'myself', 'to', 'play', '.', '.', '.', 'i', \"'\", 'm', 'not', 'a', 'very', 'good', 'actor', 'or', 'liar', '.', 'am', 'i', 'the', 'only', 'one', 'who', 'feels', 'this', 'way', '?', '(', 'side', 'note', ':', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'ad', '##hd', 'since', 'i', 'was', 'a', 'child', ',', 'recently', 'restarted', 'treatment', 'with', 'concert', '##a', ',', 'with', 'positive', 'results', '.', '.', '.', 'but', 'this', 'feeling', 'has', 'been', 'there', 'for', 'my', 'entire', 'adult', 'life', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'to', 'add', 'and', 'this', 'forum', 'but', 'what', 'is', 'the', 'common', 'perspective', 'on', 'marijuana', 'use', 'and', 'add', '?']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'to', 'add', 'and', 'this', 'forum', 'but', 'what', 'is', 'the', 'common', 'perspective', 'on', 'marijuana', 'use', 'and', 'add', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', 'induced', 'anxiety', ',', 'or', 'anxiety', 'induced', 'add', '?', 'hello', ',', 'i', 'recently', 'started', 'reading', 'this', 'sub', '##red', '##dit', 'and', 'i', 'felt', 'al', '##ot', 'of', 'the', 'posts', 'spoke', 'to', 'me', '.', 'this', 'is', 'long', '##wind', '##ed', 'but', 'i', \"'\", 'll', 'try', 'to', 'sum', '##mar', '##ize', 'what', 'my', 'life', 'is', 'like', 'at', 'the', 'moment', '.', 'since', 'sixth', 'form', '(', 'ale', '##vel', '##s', ')', 'i', 'have', 'experienced', 'failure', '.', 'i', 'have', 'failed', 'the', 'first', 'year', 'of', 'university', 'three', 'times', '.', 'i', 'have', 'a', 'problem', 'finishing', 'course', '##work', ',', 'and', 'attending', 'lectures', 'after', 'i', 'miss', 'a', 'few', '(', 'anxiety', ')', 'my', 'room', 'is', 'always', 'a', 'tip', ',', 'every', 'room', 'i', \"'\", 've', 'lived', 'in', ',', 'i', \"'\", 've', 'made', 'them', 'an', 'utter', 'em', '##bara', '##ss', '##ment', 'very', 'quickly', ',', 'not', 'just', 'cl', '##utter', 'but', 'rubbish', 'and', 'other', 'junk', '.', 'at', 'the', 'end', 'of', 'semester', '1', 'of', 'this', 'third', 'year', ',', 'i', 'knew', 'something', 'was', 'up', 'and', 'suspected', 'it', 'to', 'be', 'anxiety', ',', 'the', 'doctors', 'then', 'diagnosed', 'me', 'with', 'it', '.', 'the', 'counseling', 'office', 'never', 'got', 'back', 'to', 'me', 'after', 'an', 'initial', 'session', 'to', 'evaluate', 'me', ',', 'however', 'they', 'confirmed', 'what', 'i', 'knew', '.', 'i', 'tried', 'to', 'start', 'semester', '2', 'with', 'a', 'positive', 'outlook', ',', 'trying', 'to', 'deal', 'with', 'this', 'thing', 'myself', '.', 'it', 'began', 'well', ',', 'like', 'it', 'always', 'does', '.', 'i', 'was', 'going', 'to', 'the', 'gym', ',', 'attending', 'lectures', ',', 'making', 'friends', '.', 'then', 'i', 'found', 'out', 'my', 'family', \"'\", 's', 'financial', 'situ', '##tation', 'was', 'bad', ',', 'and', 'i', 'just', 'fell', 'into', 'the', 'hole', 'again', '.', 'i', 'stopped', 'going', 'to', 'a', 'few', 'lectures', ',', 'and', 'then', 'eventually', 'all', 'of', 'them', '.', '(', 'this', 'pattern', 'by', 'the', 'way', ',', 'happen', '##d', 'in', 'the', 'other', 'two', 'years', ',', 'my', 'attendance', 'slowly', 'drops', 'till', 'i', 'am', 'non', 'exist', '##ant', ',', 'even', 'in', 'sixth', 'form', ')', 'i', 'think', 'it', \"'\", 's', 'a', 'fear', 'of', 'being', 'judged', ',', 'being', 'looked', 'at', 'as', 'a', 'failure', '.', 'i', 'know', 'i', \"'\", 'm', 'intelligent', 'but', 'i', 'feel', 'almost', 'em', '##bara', '##ssed', 'when', 'i', 'don', \"'\", 't', 'pass', 'things', 'or', 'fail', 'to', 'hand', 'in', 'work', '.', 'it', \"'\", 's', 'humiliating', 'in', 'school', 'to', 'find', 'that', 'people', 'that', 'i', 'helped', 'do', 'work', 'for', 'get', 'higher', 'grades', 'because', 'i', 'didn', \"'\", 't', 'do', 'any', 'for', 'myself', '.', 'i', 'think', 'the', 'only', 'reason', 'why', 'it', \"'\", 's', 'apparent', 'now', 'is', 'that', 'because', 'as', 'i', 'got', 'older', 'more', 'of', 'my', 'work', 'was', 'project', '/', 'course', '##work', 'based', ',', 'and', 'required', 'al', '##ot', 'more', 'motivation', 'and', 'attention', 'span', 'to', 'complete', '.', 'i', 'seem', 'to', 'fail', 'at', 'finishing', 'projects', 'i', 'start', ',', 'even', 'if', 'it', \"'\", 's', 'a', 'hobby', '.', 'i', 'look', 'at', 'myself', 'as', 'the', 'jack', 'of', 'all', 'trades', 'but', 'the', 'master', 'of', 'none', '.', 'my', 'attention', 'seems', 'to', 'switch', 'to', 'something', 'else', 'very', 'quickly', '.', 'i', \"'\", 'll', 'have', 'a', 'large', 'burst', 'of', 'interest', 'in', 'one', 'area', ',', 'and', 'then', 'soon', 'look', 'at', 'something', 'else', '.', 'i', \"'\", 'll', 'try', 'explain', 'it', 'a', 'little', 'better', ',', 'i', 'spend', 'more', 'time', 'looking', 'at', 'alternative', 'ways', 'to', 'tackle', 'a', 'problem', 'like', 'a', 'project', ',', 'rather', 'than', 'actually', 'doing', 'it', '.', 'for', 'example', ',', 'if', 'i', 'wanted', 'to', 'build', 'myself', 'a', 'guitar', ',', 'i', 'spend', 'all', 'my', 'time', 'looking', 'at', 'possible', 'options', 'like', 'wood', 'and', 'paint', ',', 'rather', 'than', 'doing', 'the', 'ask', '.', 'i', 'don', \"'\", 't', 'make', 'my', 'mind', 'up', 'and', 'eventually', 'drop', 'it', '.', 'so', 'back', 'to', 'the', 'topic', 'title', '.', 'at', 'first', 'i', 'convinced', 'myself', 'that', 'this', 'lack', 'of', 'attention', 'was', 'just', 'a', 'proxy', 'from', 'anxiety', '.', 'that', 'i', 'pro', '##cr', '##ast', '##inate', 'to', 'forget', 'about', 'the', 'troubles', 'in', 'my', 'world', '.', 'while', 'this', 'is', 'true', ',', 'i', 'feel', 'like', 'there', 'is', 'more', 'to', 'it', '.', 'i', \"'\", 'll', 'sum', '##mar', '##ize', 'what', 'i', 'feel', 'to', 'be', 'add', 'like', 'about', 'myself', 'below', ',', 'i', \"'\", 'd', 'like', 'some', 'opinions', 'from', 'people', 'who', 'know', 'more', 'about', 'this', 'subject', ':', '-', '*', '*', 'forget', '##ful', ',', 'always', 'mis', '##pl', '##acing', 'things', ',', 'like', 'my', 'keys', 'and', 'wallet', '.', 'i', \"'\", 'll', 'place', 'my', 'keys', 'on', 'my', 'table', ',', 'about', '5', 'minutes', 'later', 'i', 'will', 'think', '\"', 'where', 'are', 'my', 'keys', '\"', 'and', 'i', \"'\", 'll', 'tip', 'my', 'room', 'up', 'looking', 'for', 'them', ',', 'until', 'i', 'realize', 'they', 'were', 'on', 'my', 'desk', 'the', 'whole', 'time', '*', '*', '-', '*', '*', 'miss', 'out', 'key', 'bits', 'of', 'information', '*', '*', '*', '*', 'i', 'thought', 'my', 'assignment', 'said', 'you', 'can', 'not', 'pass', 'without', 'doing', 'both', 'parts', 'of', 'the', 'work', ',', 'it', 'didn', \"'\", 't', 'actually', 'say', 'that', '.', '*', '*', '-', '*', '*', 'i', 'thought', 'my', 'ap', '##oint', '##ment', 'was', 'at', '3', ':', '30', 'pm', ',', 'even', 'though', 'it', 'said', '3', '##pm', 'on', 'the', 'note', 'i', 'wrote', '*', '*', '-', '*', '*', 'the', 'rooms', 'i', 'live', 'in', 'are', 'always', 'a', 'tip', '*', '*', '-', '*', '*', 'i', 'have', 'poor', 'time', 'management', 'skills', '*', '*', '-', '*', '*', 'i', 'can', '##t', 'organise', 'anything', ',', 'my', 'room', 'is', 'always', 'cl', '##uttered', '*', '*', '-', '*', '*', 'i', 'am', 'a', 'night', 'owl', ',', 'i', 'feel', 'more', 'at', 'ease', 'at', 'night', ',', 'and', 'my', 'sleep', 'patterns', 'are', 'all', 'over', 'the', 'place', '*', '*', '-', '*', '*', 'i', 'gain', 'and', 'lose', 'interests', 'very', 'quickly', ',', 'i', 'find', 'i', 'can', \"'\", 't', 'progress', 'far', 'enough', 'into', 'anything', 'without', 'moving', 'on', 'to', 'something', 'else', '*', '*', '-', '*', '*', 'i', 'zone', 'out', 'when', 'i', \"'\", 'm', 'talking', 'with', 'someone', ',', 'i', 'will', 'nod', 'and', 'say', '\"', 'yeah', ',', 'yes', '\"', 'and', 'just', 'go', 'with', 'the', 'flow', 'of', 'the', 'conversation', '*', '*', '-', '*', '*', 'i', 'say', 'things', 'and', 'then', 'think', 'about', 'them', 'later', 'and', 'wonder', 'why', 'i', 'said', 'it', '*', '*', '-', '*', '*', 'i', 'jump', 'into', 'things', 'without', 'much', 'thought', '.', '*', '*', '-', '*', '*', 'i', \"'\", 'll', 'struggle', 'to', 'start', 'work', ',', 'i', \"'\", 'll', 'stare', 'at', 'an', 'empty', 'screen', 'for', 'a', 'long', 'time', '.', 'rarely', 'do', 'i', 'jump', 'into', ',', 'i', 'find', 'once', 'i', 'do', 'i', 'am', 'ok', 'but', 'as', 'soon', 'as', 'i', 'take', 'a', 'break', ',', 'i', 'can', \"'\", 't', 'start', 'it', 'again', '.', '*', '*', '-', '*', '*', 'i', 'finish', 'other', 'peoples', 'sentences', 'without', 'even', 'thinking', 'about', 'it', ',', 'it', 'just', 'comes', 'out', 'of', 'my', 'mouth', '.', '*', '*', 'the', 'final', 'straw', 'for', 'me', 'was', 'when', 'the', 'only', 'piece', 'of', 'course', '##work', 'i', 'finished', 'on', 'time', 'and', 'was', 'happy', 'with', 'i', 'failed', 'to', 'hand', 'it', 'in', ',', 'because', 'i', 'ended', 'up', 'leaving', 'my', 'dorm', 'room', '5', 'minutes', 'before', 'it', 'was', 'due', ',', 'and', 'failed', 'to', 'print', 'it', 'on', 'time', ',', 'not', 'only', 'that', 'but', 'i', 'left', 'my', 'student', 'card', 'at', 'home', 'so', 'i', 'could', 'not', 'hand', 'it', 'in', 'anyway', '.', 'i', 'felt', 'like', 'such', 'an', 'idiot', ',', 'the', 'only', 'successful', 'thing', 'that', 'i', 'managed', 'to', 'do', 'during', 'easter', 'and', 'i', 'didn', \"'\", 't', 'hand', 'it', 'in', '.', 'i', 'kept', 'looking', 'at', 'the', 'time', 'and', 'panic', '##ing', 'even', 'though', 'it', 'was', 'done', ',', 'and', 'i', \"'\", 'd', 'just', 'go', 'on', 'youtube', 'and', 'read', 'stuff', 'while', 'the', 'time', 'ticked', 'away', ',', 'until', 'it', 'was', 'literally', 'the', 'last', 'minute', '.', 'almost', 'every', 'single', 'course', '##work', 'piece', 'i', \"'\", 've', 'done', 'in', 'my', 'entire', 'life', 'has', 'been', 'a', 'disaster', ',', 'and', 'it', \"'\", 's', 'always', 'em', '##bara', '##ssing', 'as', 'my', 'school', 'teachers', 'knew', 'i', 'wasn', \"'\", 't', 'stupid', ',', 'in', '##fa', '##ct', 'i', 'used', 'to', 'help', 'my', 'peers', 'out', 'in', 'a', 'few', 'subjects', ',', 'i', 'guess', 'this', 'anxiety', 'is', 'coming', 'from', 'pride', 'and', 'ar', '##ragan', '##ce', 'on', 'my', 'part', ',', 'but', 'it', \"'\", 's', 'eating', 'away', 'at', 'me', 'inside', '.', 'so', 'today', 'i', 'spoke', 'to', 'a', 'counselor', ',', 'and', 'they', 'were', 'prepared', 'me', 'al', '##ot', 'of', 'helpful', 'stuff', 'about', 'anxiety', ',', 'i', 'asked', 'the', 'person', 'about', 'the', 'possibility', 'of', 'ad', '##hd', '/', 'add', 'and', 'they', 'said', 'that', 'it', \"'\", 's', 'probably', 'just', 'an', 'after', 'effect', 'of', 'this', 'anxiety', 'disorder', ',', 'however', 'they', 'were', 'very', 'quick', 'to', 'dismiss', 'it', 'and', 'i', \"'\", 'm', 'not', 'convinced', '.', '*', '*', 't', '##ld', '##r', ':', 'can', 'anxiety', 'cause', 'add', ',', 'and', 'can', 'the', 'same', 'thing', 'happen', 'the', 'other', 'way', 'round', '?', 'counselor', 'dismissed', 'me', 'having', 'add', ',', 'but', 'i', 'think', 'i', 'do', '?', '*', '*', 'i', \"'\", 'm', 'really', 'sorry', 'for', 'the', 'long', 'post', ',', 'i', 'hope', 'it', \"'\", 's', 'read', '##able', '.', 'just', 'writing', 'this', 'down', 'feels', 'like', 'therapy', '.']\n",
      "INFO:__main__:Number of tokens: 1371\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['add', 'induced', 'anxiety', ',', 'or', 'anxiety', 'induced', 'add', '?', 'hello', ',', 'i', 'recently', 'started', 'reading', 'this', 'sub', '##red', '##dit', 'and', 'i', 'felt', 'al', '##ot', 'of', 'the', 'posts', 'spoke', 'to', 'me', '.', 'this', 'is', 'long', '##wind', '##ed', 'but', 'i', \"'\", 'll', 'try', 'to', 'sum', '##mar', '##ize', 'what', 'my', 'life', 'is', 'like', 'at', 'the', 'moment', '.', 'since', 'sixth', 'form', '(', 'ale', '##vel', '##s', ')', 'i', 'have', 'experienced', 'failure', '.', 'i', 'have', 'failed', 'the', 'first', 'year', 'of', 'university', 'three', 'times', '.', 'i', 'have', 'a', 'problem', 'finishing', 'course', '##work', ',', 'and', 'attending', 'lectures', 'after', 'i', 'miss', 'a', 'few', '(', 'anxiety', ')', 'my', 'room', 'is', 'always', 'a', 'tip', ',', 'every', 'room', 'i', \"'\", 've', 'lived', 'in', ',', 'i', \"'\", 've', 'made', 'them', 'an', 'utter', 'em', '##bara', '##ss', '##ment', 'very', 'quickly', ',', 'not', 'just', 'cl', '##utter', 'but', 'rubbish', 'and', 'other', 'junk', '.', 'at', 'the', 'end', 'of', 'semester', '1', 'of', 'this', 'third', 'year', ',', 'i', 'knew', 'something', 'was', 'up', 'and', 'suspected', 'it', 'to', 'be', 'anxiety', ',', 'the', 'doctors', 'then', 'diagnosed', 'me', 'with', 'it', '.', 'the', 'counseling', 'office', 'never', 'got', 'back', 'to', 'me', 'after', 'an', 'initial', 'session', 'to', 'evaluate', 'me', ',', 'however', 'they', 'confirmed', 'what', 'i', 'knew', '.', 'i', 'tried', 'to', 'start', 'semester', '2', 'with', 'a', 'positive', 'outlook', ',', 'trying', 'to', 'deal', 'with', 'this', 'thing', 'myself', '.', 'it', 'began', 'well', ',', 'like', 'it', 'always', 'does', '.', 'i', 'was', 'going', 'to', 'the', 'gym', ',', 'attending', 'lectures', ',', 'making', 'friends', '.', 'then', 'i', 'found', 'out', 'my', 'family', \"'\", 's', 'financial', 'situ', '##tation', 'was', 'bad', ',', 'and', 'i', 'just', 'fell', 'into', 'the', 'hole', 'again', '.', 'i', 'stopped', 'going', 'to', 'a', 'few', 'lectures', ',', 'and', 'then', 'eventually', 'all', 'of', 'them', '.', '(', 'this', 'pattern', 'by', 'the', 'way', ',', 'happen', '##d', 'in', 'the', 'other', 'two', 'years', ',', 'my', 'attendance', 'slowly', 'drops', 'till', 'i', 'am', 'non', 'exist', '##ant', ',', 'even', 'in', 'sixth', 'form', ')', 'i', 'think', 'it', \"'\", 's', 'a', 'fear', 'of', 'being', 'judged', ',', 'being', 'looked', 'at', 'as', 'a', 'failure', '.', 'i', 'know', 'i', \"'\", 'm', 'intelligent', 'but', 'i', 'feel', 'almost', 'em', '##bara', '##ssed', 'when', 'i', 'don', \"'\", 't', 'pass', 'things', 'or', 'fail', 'to', 'hand', 'in', 'work', '.', 'it', \"'\", 's', 'humiliating', 'in', 'school', 'to', 'find', 'that', 'people', 'that', 'i', 'helped', 'do', 'work', 'for', 'get', 'higher', 'grades', 'because', 'i', 'didn', \"'\", 't', 'do', 'any', 'for', 'myself', '.', 'i', 'think', 'the', 'only', 'reason', 'why', 'it', \"'\", 's', 'apparent', 'now', 'is', 'that', 'because', 'as', 'i', 'got', 'older', 'more', 'of', 'my', 'work', 'was', 'project', '/', 'course', '##work', 'based', ',', 'and', 'required', 'al', '##ot', 'more', 'motivation', 'and', 'attention', 'span', 'to', 'complete', '.', 'i', 'seem', 'to', 'fail', 'at', 'finishing', 'projects', 'i', 'start', ',', 'even', 'if', 'it', \"'\", 's', 'a', 'hobby', '.', 'i', 'look', 'at', 'myself', 'as', 'the', 'jack', 'of', 'all', 'trades', 'but', 'the', 'master', 'of', 'none', '.', 'my', 'attention', 'seems', 'to', 'switch', 'to', 'something', 'else', 'very', 'quickly', '.', 'i', \"'\", 'll', 'have', 'a', 'large', 'burst', 'of', 'interest', 'in', 'one', 'area', ',', 'and', 'then', 'soon', 'look', 'at', 'something', 'else', '.', 'i', \"'\", 'll', 'try', 'explain', 'it', 'a', 'little', 'better', ',', 'i', 'spend', 'more', 'time', 'looking', 'at', 'alternative', 'ways', 'to', 'tackle', 'a', 'problem', 'like', 'a', 'project', ',', 'rather', 'than', 'actually', 'doing', 'it'], ['.', 'for', 'example', ',', 'if', 'i', 'wanted', 'to', 'build', 'myself', 'a', 'guitar', ',', 'i', 'spend', 'all', 'my', 'time', 'looking', 'at', 'possible', 'options', 'like', 'wood', 'and', 'paint', ',', 'rather', 'than', 'doing', 'the', 'ask', '.', 'i', 'don', \"'\", 't', 'make', 'my', 'mind', 'up', 'and', 'eventually', 'drop', 'it', '.', 'so', 'back', 'to', 'the', 'topic', 'title', '.', 'at', 'first', 'i', 'convinced', 'myself', 'that', 'this', 'lack', 'of', 'attention', 'was', 'just', 'a', 'proxy', 'from', 'anxiety', '.', 'that', 'i', 'pro', '##cr', '##ast', '##inate', 'to', 'forget', 'about', 'the', 'troubles', 'in', 'my', 'world', '.', 'while', 'this', 'is', 'true', ',', 'i', 'feel', 'like', 'there', 'is', 'more', 'to', 'it', '.', 'i', \"'\", 'll', 'sum', '##mar', '##ize', 'what', 'i', 'feel', 'to', 'be', 'add', 'like', 'about', 'myself', 'below', ',', 'i', \"'\", 'd', 'like', 'some', 'opinions', 'from', 'people', 'who', 'know', 'more', 'about', 'this', 'subject', ':', '-', '*', '*', 'forget', '##ful', ',', 'always', 'mis', '##pl', '##acing', 'things', ',', 'like', 'my', 'keys', 'and', 'wallet', '.', 'i', \"'\", 'll', 'place', 'my', 'keys', 'on', 'my', 'table', ',', 'about', '5', 'minutes', 'later', 'i', 'will', 'think', '\"', 'where', 'are', 'my', 'keys', '\"', 'and', 'i', \"'\", 'll', 'tip', 'my', 'room', 'up', 'looking', 'for', 'them', ',', 'until', 'i', 'realize', 'they', 'were', 'on', 'my', 'desk', 'the', 'whole', 'time', '*', '*', '-', '*', '*', 'miss', 'out', 'key', 'bits', 'of', 'information', '*', '*', '*', '*', 'i', 'thought', 'my', 'assignment', 'said', 'you', 'can', 'not', 'pass', 'without', 'doing', 'both', 'parts', 'of', 'the', 'work', ',', 'it', 'didn', \"'\", 't', 'actually', 'say', 'that', '.', '*', '*', '-', '*', '*', 'i', 'thought', 'my', 'ap', '##oint', '##ment', 'was', 'at', '3', ':', '30', 'pm', ',', 'even', 'though', 'it', 'said', '3', '##pm', 'on', 'the', 'note', 'i', 'wrote', '*', '*', '-', '*', '*', 'the', 'rooms', 'i', 'live', 'in', 'are', 'always', 'a', 'tip', '*', '*', '-', '*', '*', 'i', 'have', 'poor', 'time', 'management', 'skills', '*', '*', '-', '*', '*', 'i', 'can', '##t', 'organise', 'anything', ',', 'my', 'room', 'is', 'always', 'cl', '##uttered', '*', '*', '-', '*', '*', 'i', 'am', 'a', 'night', 'owl', ',', 'i', 'feel', 'more', 'at', 'ease', 'at', 'night', ',', 'and', 'my', 'sleep', 'patterns', 'are', 'all', 'over', 'the', 'place', '*', '*', '-', '*', '*', 'i', 'gain', 'and', 'lose', 'interests', 'very', 'quickly', ',', 'i', 'find', 'i', 'can', \"'\", 't', 'progress', 'far', 'enough', 'into', 'anything', 'without', 'moving', 'on', 'to', 'something', 'else', '*', '*', '-', '*', '*', 'i', 'zone', 'out', 'when', 'i', \"'\", 'm', 'talking', 'with', 'someone', ',', 'i', 'will', 'nod', 'and', 'say', '\"', 'yeah', ',', 'yes', '\"', 'and', 'just', 'go', 'with', 'the', 'flow', 'of', 'the', 'conversation', '*', '*', '-', '*', '*', 'i', 'say', 'things', 'and', 'then', 'think', 'about', 'them', 'later', 'and', 'wonder', 'why', 'i', 'said', 'it', '*', '*', '-', '*', '*', 'i', 'jump', 'into', 'things', 'without', 'much', 'thought', '.', '*', '*', '-', '*', '*', 'i', \"'\", 'll', 'struggle', 'to', 'start', 'work', ',', 'i', \"'\", 'll', 'stare', 'at', 'an', 'empty', 'screen', 'for', 'a', 'long', 'time', '.', 'rarely', 'do', 'i', 'jump', 'into', ',', 'i', 'find', 'once', 'i', 'do', 'i', 'am', 'ok', 'but', 'as', 'soon', 'as', 'i', 'take', 'a', 'break', ',', 'i', 'can', \"'\", 't', 'start', 'it', 'again', '.', '*', '*', '-', '*', '*', 'i', 'finish', 'other', 'peoples', 'sentences', 'without', 'even', 'thinking', 'about', 'it', ',', 'it', 'just', 'comes', 'out', 'of', 'my', 'mouth'], ['.', '*', '*', 'the', 'final', 'straw', 'for', 'me', 'was', 'when', 'the', 'only', 'piece', 'of', 'course', '##work', 'i', 'finished', 'on', 'time', 'and', 'was', 'happy', 'with', 'i', 'failed', 'to', 'hand', 'it', 'in', ',', 'because', 'i', 'ended', 'up', 'leaving', 'my', 'dorm', 'room', '5', 'minutes', 'before', 'it', 'was', 'due', ',', 'and', 'failed', 'to', 'print', 'it', 'on', 'time', ',', 'not', 'only', 'that', 'but', 'i', 'left', 'my', 'student', 'card', 'at', 'home', 'so', 'i', 'could', 'not', 'hand', 'it', 'in', 'anyway', '.', 'i', 'felt', 'like', 'such', 'an', 'idiot', ',', 'the', 'only', 'successful', 'thing', 'that', 'i', 'managed', 'to', 'do', 'during', 'easter', 'and', 'i', 'didn', \"'\", 't', 'hand', 'it', 'in', '.', 'i', 'kept', 'looking', 'at', 'the', 'time', 'and', 'panic', '##ing', 'even', 'though', 'it', 'was', 'done', ',', 'and', 'i', \"'\", 'd', 'just', 'go', 'on', 'youtube', 'and', 'read', 'stuff', 'while', 'the', 'time', 'ticked', 'away', ',', 'until', 'it', 'was', 'literally', 'the', 'last', 'minute', '.', 'almost', 'every', 'single', 'course', '##work', 'piece', 'i', \"'\", 've', 'done', 'in', 'my', 'entire', 'life', 'has', 'been', 'a', 'disaster', ',', 'and', 'it', \"'\", 's', 'always', 'em', '##bara', '##ssing', 'as', 'my', 'school', 'teachers', 'knew', 'i', 'wasn', \"'\", 't', 'stupid', ',', 'in', '##fa', '##ct', 'i', 'used', 'to', 'help', 'my', 'peers', 'out', 'in', 'a', 'few', 'subjects', ',', 'i', 'guess', 'this', 'anxiety', 'is', 'coming', 'from', 'pride', 'and', 'ar', '##ragan', '##ce', 'on', 'my', 'part', ',', 'but', 'it', \"'\", 's', 'eating', 'away', 'at', 'me', 'inside', '.', 'so', 'today', 'i', 'spoke', 'to', 'a', 'counselor', ',', 'and', 'they', 'were', 'prepared', 'me', 'al', '##ot', 'of', 'helpful', 'stuff', 'about', 'anxiety', ',', 'i', 'asked', 'the', 'person', 'about', 'the', 'possibility', 'of', 'ad', '##hd', '/', 'add', 'and', 'they', 'said', 'that', 'it', \"'\", 's', 'probably', 'just', 'an', 'after', 'effect', 'of', 'this', 'anxiety', 'disorder', ',', 'however', 'they', 'were', 'very', 'quick', 'to', 'dismiss', 'it', 'and', 'i', \"'\", 'm', 'not', 'convinced', '.', '*', '*', 't', '##ld', '##r', ':', 'can', 'anxiety', 'cause', 'add', ',', 'and', 'can', 'the', 'same', 'thing', 'happen', 'the', 'other', 'way', 'round', '?', 'counselor', 'dismissed', 'me', 'having', 'add', ',', 'but', 'i', 'think', 'i', 'do', '?', '*', '*', 'i', \"'\", 'm', 'really', 'sorry', 'for', 'the', 'long', 'post', ',', 'i', 'hope', 'it', \"'\", 's', 'read', '##able', '.', 'just', 'writing', 'this', 'down', 'feels', 'like', 'therapy', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'i', 'love', 'about', 'my', 'add', '.', 'i', 'have', 'ina', '##tten', '##tive', 'add', 'and', 'recently', 'decided', 'that', 'med', '##s', 'weren', \"'\", 't', 'right', 'for', 'me', '.', 'so', 'with', 'the', 'permission', 'of', 'my', 'doctor', 'i', 'have', 'been', 'off', 'any', 'kind', 'of', 'medication', 'for', 'about', 'six', 'months', 'now', '.', 'i', 'couldn', \"'\", 't', 'be', 'happier', ',', 'honestly', 'my', 'life', 'has', 'changed', 'a', 'lot', 'and', 'my', 'personality', 'is', 'back', '.', 'however', ',', 'it', 'is', 'harder', 'to', 'focus', 'on', 'school', 'work', '(', 'obviously', ')', 'but', 'i', 'have', 'found', 'a', 'way', 'to', 'help', ',', 'and', 'thought', 'i', 'would', 'pass', 'on', 'this', 'tip', 'to', 'anyone', 'else', 'who', 'might', 'need', 'it', '.', 'when', 'i', 'write', 'papers', 'i', 'find', 'myself', 'doing', 'about', '10', 'minutes', 'of', 'work', 'and', 'then', 'taking', 'a', 'break', '.', 'it', 'can', 'be', 'frustrating', 'at', 'first', 'to', 'try', 'to', 'cr', '##am', 'in', 'important', 'work', 'in', '10', 'minutes', ',', 'but', 'after', 'a', 'while', 'i', 'figured', 'out', 'how', 'to', 'do', 'it', '.', 'then', 'i', 'take', 'a', 'five', 'or', 'so', 'minute', 'break', '.', 'sometimes', 'i', 'go', '20', 'or', '30', 'minutes', 'of', 'work', ',', 'then', 'a', '10', 'minute', 'break', '.', 'just', 'something', 'to', 'relax', 'my', 'mind', 'for', 'a', 'bit', 'before', 'i', 'dive', 'back', 'in', '.', 'it', 'really', 'helps', ',', 'and', 'i', 'don', \"'\", 't', 'get', 'frustrated', 'with', 'my', 'add', 'acting', 'up', 'while', 'i', \"'\", 'm', 'trying', 'to', 'work', '.', 'i', \"'\", 'm', 'just', 'glad', 'i', 'found', 'a', 'happy', 'medium', 'between', 'add', 'and', 'college', '.', 'it', 'is', 'possible', '.']\n",
      "INFO:__main__:Number of tokens: 238\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'i', 'love', 'about', 'my', 'add', '.', 'i', 'have', 'ina', '##tten', '##tive', 'add', 'and', 'recently', 'decided', 'that', 'med', '##s', 'weren', \"'\", 't', 'right', 'for', 'me', '.', 'so', 'with', 'the', 'permission', 'of', 'my', 'doctor', 'i', 'have', 'been', 'off', 'any', 'kind', 'of', 'medication', 'for', 'about', 'six', 'months', 'now', '.', 'i', 'couldn', \"'\", 't', 'be', 'happier', ',', 'honestly', 'my', 'life', 'has', 'changed', 'a', 'lot', 'and', 'my', 'personality', 'is', 'back', '.', 'however', ',', 'it', 'is', 'harder', 'to', 'focus', 'on', 'school', 'work', '(', 'obviously', ')', 'but', 'i', 'have', 'found', 'a', 'way', 'to', 'help', ',', 'and', 'thought', 'i', 'would', 'pass', 'on', 'this', 'tip', 'to', 'anyone', 'else', 'who', 'might', 'need', 'it', '.', 'when', 'i', 'write', 'papers', 'i', 'find', 'myself', 'doing', 'about', '10', 'minutes', 'of', 'work', 'and', 'then', 'taking', 'a', 'break', '.', 'it', 'can', 'be', 'frustrating', 'at', 'first', 'to', 'try', 'to', 'cr', '##am', 'in', 'important', 'work', 'in', '10', 'minutes', ',', 'but', 'after', 'a', 'while', 'i', 'figured', 'out', 'how', 'to', 'do', 'it', '.', 'then', 'i', 'take', 'a', 'five', 'or', 'so', 'minute', 'break', '.', 'sometimes', 'i', 'go', '20', 'or', '30', 'minutes', 'of', 'work', ',', 'then', 'a', '10', 'minute', 'break', '.', 'just', 'something', 'to', 'relax', 'my', 'mind', 'for', 'a', 'bit', 'before', 'i', 'dive', 'back', 'in', '.', 'it', 'really', 'helps', ',', 'and', 'i', 'don', \"'\", 't', 'get', 'frustrated', 'with', 'my', 'add', 'acting', 'up', 'while', 'i', \"'\", 'm', 'trying', 'to', 'work', '.', 'i', \"'\", 'm', 'just', 'glad', 'i', 'found', 'a', 'happy', 'medium', 'between', 'add', 'and', 'college', '.', 'it', 'is', 'possible', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['using', 'medication', 'for', 'ad', '##hd', 'is', 'tan', '##tam', '##ount', 'to', 'insulin', 'to', 'a', 'dia', '##bet', '##ic', '.']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['using', 'medication', 'for', 'ad', '##hd', 'is', 'tan', '##tam', '##ount', 'to', 'insulin', 'to', 'a', 'dia', '##bet', '##ic', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'taking', 'to', 'much', 'concert', '##a', '?', 'i', 'took', '54', '##mg', 'of', 'methyl', '##ph', '##eni', '##date', 'er', 'at', 'noon', '(', 'i', 'forgot', 'to', 'take', 'it', 'this', 'morning', 'when', 'i', \"'\", 'm', 'supposed', 'too', ')', '.', 'i', \"'\", 've', 'had', 'a', 'headache', 'all', 'day', 'so', 'after', 'i', 'got', 'out', 'of', 'my', 'last', 'class', 'at', '5', '##pm', 'i', 'took', '2', 'ad', '##vil', '.', 'except', 'then', 'i', 'looked', 'at', 'it', 'and', 'i', 'realized', 'i', 'actually', 'took', '36', '##mg', 'of', 'more', 'methyl', '##ph', '##eni', '##date', 'er', '.', '.', '.', 'i', 'talked', 'to', 'my', 'mom', 'whose', 'a', 'nurse', 'and', 'she', 'said', 'i', 'should', 'be', 'ok', 'but', 'i', 'was', 'just', 'wondering', 'if', 'anyone', 'else', 'has', 'done', 'a', 'similar', 'thing', '?', 'on', 'one', 'hand', 'i', \"'\", 'm', 'not', 'looking', 'forward', 'to', 'lo', '##osing', 'more', 'sleep', 'tonight', 'but', 'on', 'the', 'other', 'hand', 'i', 'have', 'a', 'lot', 'of', 'school', '##work', 'to', 'do', 'anyway', '##s', '.', 'then', 'again', 'being', 'on', '90', '##mg', 'might', 'make', 'that', 'even', 'harder', 'than', 'usual', '.']\n",
      "INFO:__main__:Number of tokens: 161\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'taking', 'to', 'much', 'concert', '##a', '?', 'i', 'took', '54', '##mg', 'of', 'methyl', '##ph', '##eni', '##date', 'er', 'at', 'noon', '(', 'i', 'forgot', 'to', 'take', 'it', 'this', 'morning', 'when', 'i', \"'\", 'm', 'supposed', 'too', ')', '.', 'i', \"'\", 've', 'had', 'a', 'headache', 'all', 'day', 'so', 'after', 'i', 'got', 'out', 'of', 'my', 'last', 'class', 'at', '5', '##pm', 'i', 'took', '2', 'ad', '##vil', '.', 'except', 'then', 'i', 'looked', 'at', 'it', 'and', 'i', 'realized', 'i', 'actually', 'took', '36', '##mg', 'of', 'more', 'methyl', '##ph', '##eni', '##date', 'er', '.', '.', '.', 'i', 'talked', 'to', 'my', 'mom', 'whose', 'a', 'nurse', 'and', 'she', 'said', 'i', 'should', 'be', 'ok', 'but', 'i', 'was', 'just', 'wondering', 'if', 'anyone', 'else', 'has', 'done', 'a', 'similar', 'thing', '?', 'on', 'one', 'hand', 'i', \"'\", 'm', 'not', 'looking', 'forward', 'to', 'lo', '##osing', 'more', 'sleep', 'tonight', 'but', 'on', 'the', 'other', 'hand', 'i', 'have', 'a', 'lot', 'of', 'school', '##work', 'to', 'do', 'anyway', '##s', '.', 'then', 'again', 'being', 'on', '90', '##mg', 'might', 'make', 'that', 'even', 'harder', 'than', 'usual', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['all', 'night', '##er', 'on', 'v', '##y', '##van', '##se', '?', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['all', 'night', '##er', 'on', 'v', '##y', '##van', '##se', '?', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'sleep', 'patterns', '(', 'xp', '##ost', 'from', 'r', '/', 'gi', '##fs', ')']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'sleep', 'patterns', '(', 'xp', '##ost', 'from', 'r', '/', 'gi', '##fs', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'ask', '/', 'r', '/', 'ad', '##hd', ']', 'request', 'for', 'topics', '/', 'questions', 'i', 'can', 'pose', 'to', 'the', 'community', 'to', 'create', 'a', 'definitive', 'post', 'on', 'common', 'topics', '.', 'these', 'posts', 'will', 'be', 'linked', 'in', 'side', '##bar', '/', 'fa', '##q', '.', 'yeah', 'it', \"'\", 's', 'me', 'again', '!', 'i', 'was', 'thinking', 'about', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', '.', '.', '.', '*', 'i', 'swear', 'i', 'spend', 'too', 'much', 'time', '/', 'thought', 'about', 'this', 'place', '*', '.', '.', '.', 'and', 'i', 'came', 'up', 'with', 'another', 'idea', '.', '(', 'if', 'you', 'have', 'a', 'good', 'idea', 'for', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'feel', 'free', 'to', 'either', '[', 'pm', 'me', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '/', '?', 'to', '=', 'computer', '##psy', '##ch', ')', 'or', 'send', 'a', '[', 'message', 'to', 'the', 'moderator', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '?', 'to', '=', '%', '2', '##fr', '%', '2', '##fa', '##dh', '##d', ')', '.', ')', '*', '*', '*', 'here', 'is', 'my', 'vision', 'on', 'how', 'this', 'will', 'work', '(', 'if', 'you', 'have', 'a', 'better', 'idea', 'let', 'me', 'know', ')', '*', 'each', 'week', '(', 'or', 'bi', '-', 'weekly', ')', 'i', 'can', 'post', 'a', '*', '*', 'common', 'question', '/', 'topic', 'and', 'invite', 'discussion', '*', '*', '.', '*', 'the', 'post', 'will', 'then', 'be', 'linked', 'on', 'a', 'new', 'announcement', 'sticky', '-', 'note', '(', 'and', 'possibly', 'side', '##bar', ')', 'for', 'at', 'least', '2', 'weeks', '.', '*', 'this', 'will', 'help', 'keep', 'the', 'thread', 'active', 'before', 'scrolling', 'off', 'the', 'front', '-', 'page', 'after', 'a', 'day', '.', '*', '*', 'the', 'op', 'will', '*', '*', 'include', 'links', 'to', 'similar', '/', 'exact', 'posts', '*', '*', 'in', 'the', 'past', 'which', 'i', 'can', 'find', 'via', 'search', '.', 'you', 'all', 'can', 'let', 'me', 'know', 'if', 'i', 'missed', 'any', 'and', 'i', 'will', 'add', 'more', '.', '*', '*', '*', 'i', 'will', 'continuously', 'update', 'the', 'op', '*', '*', 'with', 'a', 'brief', 'summary', 'of', 'the', 'best', 'information', 'from', 'the', 'comment', 'section', 'so', 'people', 'new', 'to', 'red', '##dit', 'can', 'easily', 'scan', 'the', 'information', '.', '*', 'i', 'think', 'this', 'will', 'serve', 'as', 'a', '*', '*', 'supplement', 'to', 'the', 'fa', '##q', '*', '*', 'and', 'offer', 'a', 'more', '*', '*', 'diverse', '/', 'detailed', 'experience', '*', '*', '(', 'each', 'of', 'our', 'experiences', 'to', 'medication', 'is', 'unique', ')', '.', '*', 'i', 'believe', 'i', 'will', 'be', 'able', 'to', 'create', 'a', 'link', 'in', 'the', 'side', '##bar', 'which', 'will', 'list', 'all', 'the', '[', 'ask', 'ad', '##hd', ']', 'posts', 'so', '*', '*', 'people', 'can', 'easily', 'brows', '##e', 'those', 'at', 'once', '*', '*', '.', '*', 'this', 'should', 'help', '*', '*', 'st', '##ave', 'off', 'duplicate', 'posts', '*', '*', 'and', 'when', 'there', 'is', 'a', 'post', 'which', 'asks', 'a', 'question', 'answered', 'in', '[', 'ask', 'r', '/', 'ad', '##hd', ']', 'someone', 'can', 'comment', 'with', 'a', 'link', 'to', 'that', 'post', 'and', 'a', 'mod', 'can', 'remove', 'thread', '(', '*', '*', 'poster', 'gets', 'help', ',', 'cl', '##utter', 'removed', '*', '*', ')', '.', '*', 'i', 'would', 'like', 'to', 'keep', 'topics', '*', '*', 'focused', 'on', 'subjective', 'experience', '*', '*', 'instead', 'of', 'topics', '/', 'questions', 'which', 'can', 'be', 'answered', 'more', 'definitive', '##ly', '.', '*', '*', '*', 'example', '*', '*', ':', 'how', 'did', 'you', 'react', 'to', 'the', 'different', 'st', '##im', '##ula', '##nts', 'you', 'have', 'tried', 'over', 'the', 'first', 'month', '?', '(', 'great', 'question', ')', 'vs', 'i', 'just', 'started', 'taking', 'add', '##eral', '##l', '.', '.', '.', 'what', 'do', 'i', 'need', 'to', 'know', '?', '(', 'better', 'question', 'for', 'fa', '##q', ')', '.', 'if', 'you', 'had', 'a', 'good', 'comment', 'on', 'that', 'topic', 'previously', ',', 'feel', 'free', 'to', 'copy', '/', 'paste', 'it', 'into', 'the', '[', 'ask', '/', 'r', '/', 'ad', '##hd', ']', 'comments', '.', 'i', 'will', 'also', 'try', 'to', 'search', 'previous', 'related', 'threads', 'and', 'ask', 'permission', 'to', 'include', 'highly', 'voted', 'comments', 'in', 'op', '.', 'here', 'are', 'a', 'couple', 'topics', 'which', 'come', 'to', 'mind', ':', '*', 'medication', 'tolerance', '/', 'dependence', 'and', 'do', 'you', 'take', 'it', 'daily', 'or', 'as', 'needed', '?', '*', 'what', 'are', 'some', 'good', 'iphone', '/', 'android', '/', 'windows', '/', 'mac', 'apps', 'you', 'use', 'to', 'help', 'with', 'your', 'ad', '##hd', 'symptoms', '?', '*', '*', '*', '*', '*', '*', 'please', 'post', 'one', 'topic', 'per', 'comment', '(', 'so', 'i', 'can', 'identify', 'what', 'the', 'most', 'up', '##vot', '##ed', 'topics', 'are', 'and', 'start', 'with', 'those', 'ones', ')', '!', '*', '*', '*', '*', '*', '*', '*', 'side', '##note', ':', 'i', 'made', 'a', 'cs', '##s', 't', '##we', '##ak', 'for', 'an', 'alternating', 'background', '-', 'color', 'for', 'the', 'front', 'page', 'of', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', '*', '*', '(', 'be', 'sure', 'to', 'include', 'sub', '##red', '##dit', 'styling', ')', '.', 'i', 'feel', 'this', 'will', 'help', 'provide', 'a', 'easy', 'non', '-', 'distracting', 'visual', 'separation', 'between', 'posts', '.', '*', 'i', 'will', 'fix', 'some', 'small', 'issues', 'with', 'it', 'later', 'tonight', '.', '*', 'if', 'you', 'see', 'a', 'style', 'in', 'any', 'other', 'sub', '##red', '##dit', 'which', 'you', 'think', 'would', 'be', 'useful', 'here', '[', 'pm', 'me', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '/', '?', 'to', '=', 'computer', '##psy', '##ch', ')', '.', '*', '*', '*', '*', '*', 't', '##ld', '##r', ':', 'computer', '##psy', '##ch', 'spends', 'too', 'much', 'time', 'here', '.', 'i', 'want', 'topics', 'for', '[', 'ask', '/', 'r', '/', 'ad', '##hd', ']', 'posts', 'which', 'would', 'benefit', 'from', 'getting', 'a', 'wide', 'variety', 'of', 'responses', 'based', 'on', 'the', 'personal', 'experience', 'of', 'us', 'read', '##hdi', '##tors', '.', 'post', '1', 'topic', 'per', 'comment', '.', '[', 'pm', 'me', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '/', '?', 'to', '=', 'computer', '##psy', '##ch', ')', 'any', 'ideas', 'you', 'have', 'for', '/', 'r', '/', 'ad', '##hd', '.', '*', '*']\n",
      "INFO:__main__:Number of tokens: 939\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['[', 'ask', '/', 'r', '/', 'ad', '##hd', ']', 'request', 'for', 'topics', '/', 'questions', 'i', 'can', 'pose', 'to', 'the', 'community', 'to', 'create', 'a', 'definitive', 'post', 'on', 'common', 'topics', '.', 'these', 'posts', 'will', 'be', 'linked', 'in', 'side', '##bar', '/', 'fa', '##q', '.', 'yeah', 'it', \"'\", 's', 'me', 'again', '!', 'i', 'was', 'thinking', 'about', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', '.', '.', '.', '*', 'i', 'swear', 'i', 'spend', 'too', 'much', 'time', '/', 'thought', 'about', 'this', 'place', '*', '.', '.', '.', 'and', 'i', 'came', 'up', 'with', 'another', 'idea', '.', '(', 'if', 'you', 'have', 'a', 'good', 'idea', 'for', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', 'feel', 'free', 'to', 'either', '[', 'pm', 'me', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '/', '?', 'to', '=', 'computer', '##psy', '##ch', ')', 'or', 'send', 'a', '[', 'message', 'to', 'the', 'moderator', '##s', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '?', 'to', '=', '%', '2', '##fr', '%', '2', '##fa', '##dh', '##d', ')', '.', ')', '*', '*', '*', 'here', 'is', 'my', 'vision', 'on', 'how', 'this', 'will', 'work', '(', 'if', 'you', 'have', 'a', 'better', 'idea', 'let', 'me', 'know', ')', '*', 'each', 'week', '(', 'or', 'bi', '-', 'weekly', ')', 'i', 'can', 'post', 'a', '*', '*', 'common', 'question', '/', 'topic', 'and', 'invite', 'discussion', '*', '*', '.', '*', 'the', 'post', 'will', 'then', 'be', 'linked', 'on', 'a', 'new', 'announcement', 'sticky', '-', 'note', '(', 'and', 'possibly', 'side', '##bar', ')', 'for', 'at', 'least', '2', 'weeks', '.', '*', 'this', 'will', 'help', 'keep', 'the', 'thread', 'active', 'before', 'scrolling', 'off', 'the', 'front', '-', 'page', 'after', 'a', 'day', '.', '*', '*', 'the', 'op', 'will', '*', '*', 'include', 'links', 'to', 'similar', '/', 'exact', 'posts', '*', '*', 'in', 'the', 'past', 'which', 'i', 'can', 'find', 'via', 'search', '.', 'you', 'all', 'can', 'let', 'me', 'know', 'if', 'i', 'missed', 'any', 'and', 'i', 'will', 'add', 'more', '.', '*', '*', '*', 'i', 'will', 'continuously', 'update', 'the', 'op', '*', '*', 'with', 'a', 'brief', 'summary', 'of', 'the', 'best', 'information', 'from', 'the', 'comment', 'section', 'so', 'people', 'new', 'to', 'red', '##dit', 'can', 'easily', 'scan', 'the', 'information', '.', '*', 'i', 'think', 'this', 'will', 'serve', 'as', 'a', '*', '*', 'supplement', 'to', 'the', 'fa', '##q', '*', '*', 'and', 'offer', 'a', 'more', '*', '*', 'diverse', '/', 'detailed', 'experience', '*', '*', '(', 'each', 'of', 'our', 'experiences', 'to', 'medication', 'is', 'unique', ')', '.', '*', 'i', 'believe', 'i', 'will', 'be', 'able', 'to', 'create', 'a', 'link', 'in', 'the', 'side', '##bar', 'which', 'will', 'list', 'all', 'the', '[', 'ask', 'ad', '##hd', ']', 'posts', 'so', '*', '*', 'people', 'can', 'easily', 'brows', '##e', 'those', 'at', 'once', '*', '*', '.', '*', 'this', 'should', 'help', '*', '*', 'st', '##ave', 'off', 'duplicate', 'posts', '*', '*', 'and', 'when', 'there', 'is', 'a', 'post', 'which', 'asks', 'a', 'question', 'answered', 'in', '[', 'ask', 'r', '/', 'ad', '##hd', ']', 'someone', 'can', 'comment', 'with', 'a', 'link', 'to', 'that', 'post', 'and', 'a', 'mod', 'can', 'remove', 'thread', '(', '*', '*', 'poster', 'gets', 'help', ',', 'cl', '##utter', 'removed', '*', '*', ')', '.', '*', 'i', 'would', 'like', 'to', 'keep', 'topics', '*', '*', 'focused', 'on', 'subjective', 'experience', '*', '*', 'instead'], ['of', 'topics', '/', 'questions', 'which', 'can', 'be', 'answered', 'more', 'definitive', '##ly', '.', '*', '*', '*', 'example', '*', '*', ':', 'how', 'did', 'you', 'react', 'to', 'the', 'different', 'st', '##im', '##ula', '##nts', 'you', 'have', 'tried', 'over', 'the', 'first', 'month', '?', '(', 'great', 'question', ')', 'vs', 'i', 'just', 'started', 'taking', 'add', '##eral', '##l', '.', '.', '.', 'what', 'do', 'i', 'need', 'to', 'know', '?', '(', 'better', 'question', 'for', 'fa', '##q', ')', '.', 'if', 'you', 'had', 'a', 'good', 'comment', 'on', 'that', 'topic', 'previously', ',', 'feel', 'free', 'to', 'copy', '/', 'paste', 'it', 'into', 'the', '[', 'ask', '/', 'r', '/', 'ad', '##hd', ']', 'comments', '.', 'i', 'will', 'also', 'try', 'to', 'search', 'previous', 'related', 'threads', 'and', 'ask', 'permission', 'to', 'include', 'highly', 'voted', 'comments', 'in', 'op', '.', 'here', 'are', 'a', 'couple', 'topics', 'which', 'come', 'to', 'mind', ':', '*', 'medication', 'tolerance', '/', 'dependence', 'and', 'do', 'you', 'take', 'it', 'daily', 'or', 'as', 'needed', '?', '*', 'what', 'are', 'some', 'good', 'iphone', '/', 'android', '/', 'windows', '/', 'mac', 'apps', 'you', 'use', 'to', 'help', 'with', 'your', 'ad', '##hd', 'symptoms', '?', '*', '*', '*', '*', '*', '*', 'please', 'post', 'one', 'topic', 'per', 'comment', '(', 'so', 'i', 'can', 'identify', 'what', 'the', 'most', 'up', '##vot', '##ed', 'topics', 'are', 'and', 'start', 'with', 'those', 'ones', ')', '!', '*', '*', '*', '*', '*', '*', '*', 'side', '##note', ':', 'i', 'made', 'a', 'cs', '##s', 't', '##we', '##ak', 'for', 'an', 'alternating', 'background', '-', 'color', 'for', 'the', 'front', 'page', 'of', '[', '/', 'r', '/', 'ad', '##hd', ']', '(', '/', 'r', '/', 'ad', '##hd', ')', '*', '*', '(', 'be', 'sure', 'to', 'include', 'sub', '##red', '##dit', 'styling', ')', '.', 'i', 'feel', 'this', 'will', 'help', 'provide', 'a', 'easy', 'non', '-', 'distracting', 'visual', 'separation', 'between', 'posts', '.', '*', 'i', 'will', 'fix', 'some', 'small', 'issues', 'with', 'it', 'later', 'tonight', '.', '*', 'if', 'you', 'see', 'a', 'style', 'in', 'any', 'other', 'sub', '##red', '##dit', 'which', 'you', 'think', 'would', 'be', 'useful', 'here', '[', 'pm', 'me', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '/', '?', 'to', '=', 'computer', '##psy', '##ch', ')', '.', '*', '*', '*', '*', '*', 't', '##ld', '##r', ':', 'computer', '##psy', '##ch', 'spends', 'too', 'much', 'time', 'here', '.', 'i', 'want', 'topics', 'for', '[', 'ask', '/', 'r', '/', 'ad', '##hd', ']', 'posts', 'which', 'would', 'benefit', 'from', 'getting', 'a', 'wide', 'variety', 'of', 'responses', 'based', 'on', 'the', 'personal', 'experience', 'of', 'us', 'read', '##hdi', '##tors', '.', 'post', '1', 'topic', 'per', 'comment', '.', '[', 'pm', 'me', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'message', '/', 'compose', '/', '?', 'to', '=', 'computer', '##psy', '##ch', ')', 'any', 'ideas', 'you', 'have', 'for', '/', 'r', '/', 'ad', '##hd', '.', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['chances', 'of', 'having', 'depression', 'and', 'add', '?', 'i', 'know', 'that', 'scattered', 'thoughts', 'and', 'irregular', 'thought', 'patterns', 'is', 'a', 'common', 'sy', '##mpt', '##om', 'of', 'depression', ',', 'but', 'does', 'my', 'depression', 'explain', 'everything', 'about', 'what', 'i', 'am', 'experiencing', '?', 'i', 'can', \"'\", 't', 'organize', 'my', 'thoughts', 'in', 'the', 'way', 'i', 'want', 'to', '.', 'there', 'is', 'a', 'large', 'disco', '##nne', '##ct', 'from', 'brain', 'to', 'paper', 'to', 'word', '.', 'sometimes', 'when', 'i', 'write', 'really', 'strange', 'things', 'happen', '.', 'for', 'example', ',', 'instead', 'or', 'writing', 'the', 'letter', \"'\", 'r', \"'\", 'sometimes', 'i', 'write', 'the', 'number', '2', '.', 'also', ',', 'instead', 'of', 'one', 'word', 'i', 'will', 'write', 'a', 'slightly', 'similar', 'one', ';', 'for', 'example', 'instead', 'of', 'writing', \"'\", 'already', \"'\", 'i', 'will', 'write', \"'\", 'hardly', \"'\", '.', 'these', 'things', 'happen', 'more', 'than', 'often', '.', 'i', 'don', \"'\", 't', 'think', 'it', 'is', 'd', '##ys', '##le', '##xia', '.', 'i', 'write', 'very', 'well', ',', 'and', 'i', 'am', 'an', 'english', 'major', ',', 'so', 'i', 'have', 'never', 'had', 'a', 'problem', 'reading', '.', 'other', 'things', ':', 'i', 'day', '##dre', '##am', 'constantly', ',', 'i', 'get', 'bored', 'with', 'ideas', 'easily', '.', 'for', 'example', ',', 'when', 'i', 'write', 'a', 'paper', 'i', 'just', 'want', 'to', 'keep', 'moving', 'on', 'and', 'have', 'trouble', 'restraining', 'my', 'ideas', 'because', 'i', 'get', 'bored', '.', 'does', 'this', 'sound', 'like', 'affects', 'of', 'depression', 'and', 'not', 'add', '?', 'if', 'so', ',', 'are', 'there', 'medications', 'for', 'depression', 'that', 'help', 'with', 'concentration', 'specifically', '?', '18', '/', 'f', '/', 'been', 'on', 'ci', '##tal', '##op', '##ram', 'for', '4', 'months']\n",
      "INFO:__main__:Number of tokens: 241\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['chances', 'of', 'having', 'depression', 'and', 'add', '?', 'i', 'know', 'that', 'scattered', 'thoughts', 'and', 'irregular', 'thought', 'patterns', 'is', 'a', 'common', 'sy', '##mpt', '##om', 'of', 'depression', ',', 'but', 'does', 'my', 'depression', 'explain', 'everything', 'about', 'what', 'i', 'am', 'experiencing', '?', 'i', 'can', \"'\", 't', 'organize', 'my', 'thoughts', 'in', 'the', 'way', 'i', 'want', 'to', '.', 'there', 'is', 'a', 'large', 'disco', '##nne', '##ct', 'from', 'brain', 'to', 'paper', 'to', 'word', '.', 'sometimes', 'when', 'i', 'write', 'really', 'strange', 'things', 'happen', '.', 'for', 'example', ',', 'instead', 'or', 'writing', 'the', 'letter', \"'\", 'r', \"'\", 'sometimes', 'i', 'write', 'the', 'number', '2', '.', 'also', ',', 'instead', 'of', 'one', 'word', 'i', 'will', 'write', 'a', 'slightly', 'similar', 'one', ';', 'for', 'example', 'instead', 'of', 'writing', \"'\", 'already', \"'\", 'i', 'will', 'write', \"'\", 'hardly', \"'\", '.', 'these', 'things', 'happen', 'more', 'than', 'often', '.', 'i', 'don', \"'\", 't', 'think', 'it', 'is', 'd', '##ys', '##le', '##xia', '.', 'i', 'write', 'very', 'well', ',', 'and', 'i', 'am', 'an', 'english', 'major', ',', 'so', 'i', 'have', 'never', 'had', 'a', 'problem', 'reading', '.', 'other', 'things', ':', 'i', 'day', '##dre', '##am', 'constantly', ',', 'i', 'get', 'bored', 'with', 'ideas', 'easily', '.', 'for', 'example', ',', 'when', 'i', 'write', 'a', 'paper', 'i', 'just', 'want', 'to', 'keep', 'moving', 'on', 'and', 'have', 'trouble', 'restraining', 'my', 'ideas', 'because', 'i', 'get', 'bored', '.', 'does', 'this', 'sound', 'like', 'affects', 'of', 'depression', 'and', 'not', 'add', '?', 'if', 'so', ',', 'are', 'there', 'medications', 'for', 'depression', 'that', 'help', 'with', 'concentration', 'specifically', '?', '18', '/', 'f', '/', 'been', 'on', 'ci', '##tal', '##op', '##ram', 'for', '4', 'months']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'parents', 'see', 'add', 'as', 'an', '\"', 'excuse', '\"', 'for', 'la', '##zine', '##ss', 'i', 'was', 'never', 'actually', 'diagnosed', ',', 'but', 'i', 'was', 'prescribed', 'add', '##eral', '##l', 'and', 'it', 'helps', 'wonders', '.', 'i', 'was', 'only', 'given', '30', '20', '##mg', 'ir', 'for', 'a', 'month', ',', 'mainly', 'because', 'my', 'doctor', 'is', 'an', 'idiot', '.', 'but', 'ye', '##a', ',', 'my', 'mother', 'thinks', 'that', 'the', 'med', '##s', 'aren', \"'\", 't', 'needed', 'and', 'i', 'can', 'just', 'push', 'myself', ',', 'but', 'i', 'can', \"'\", 't', '.', 'i', 'am', 'not', 'hyper', '##active', 'in', 'anyway', ',', 'but', 'my', 'focus', 'has', 'always', 'been', 'off', 'and', 'i', 'can', 'not', 'concentrate', '.', 'i', 'always', 'have', 'to', 'fi', '##dget', '.', 'but', 'ye', '##a', ',', 'is', 'there', 'a', 'way', 'to', 'have', 'my', 'mother', 'see', 'that', 'i', 'am', 'not', 'some', 'lazy', 'bastard', 'but', 'instead', 'a', 'good', 'kid', 'with', 'a', 'mental', 'illness', '?']\n",
      "INFO:__main__:Number of tokens: 137\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'parents', 'see', 'add', 'as', 'an', '\"', 'excuse', '\"', 'for', 'la', '##zine', '##ss', 'i', 'was', 'never', 'actually', 'diagnosed', ',', 'but', 'i', 'was', 'prescribed', 'add', '##eral', '##l', 'and', 'it', 'helps', 'wonders', '.', 'i', 'was', 'only', 'given', '30', '20', '##mg', 'ir', 'for', 'a', 'month', ',', 'mainly', 'because', 'my', 'doctor', 'is', 'an', 'idiot', '.', 'but', 'ye', '##a', ',', 'my', 'mother', 'thinks', 'that', 'the', 'med', '##s', 'aren', \"'\", 't', 'needed', 'and', 'i', 'can', 'just', 'push', 'myself', ',', 'but', 'i', 'can', \"'\", 't', '.', 'i', 'am', 'not', 'hyper', '##active', 'in', 'anyway', ',', 'but', 'my', 'focus', 'has', 'always', 'been', 'off', 'and', 'i', 'can', 'not', 'concentrate', '.', 'i', 'always', 'have', 'to', 'fi', '##dget', '.', 'but', 'ye', '##a', ',', 'is', 'there', 'a', 'way', 'to', 'have', 'my', 'mother', 'see', 'that', 'i', 'am', 'not', 'some', 'lazy', 'bastard', 'but', 'instead', 'a', 'good', 'kid', 'with', 'a', 'mental', 'illness', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', '3', 'quarters', 'are', 'com', '##men', '##sur', '##ate', 'with', 'my', 'grades', 'since', 'middle', 'school', '.', '4th', 'quarter', 'is', 'after', 'a', 'diagnosis', ':', ']', 'study', 'on', ',', 'friends', '.', 'study', 'on', '.']\n",
      "INFO:__main__:Number of tokens: 31\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', '3', 'quarters', 'are', 'com', '##men', '##sur', '##ate', 'with', 'my', 'grades', 'since', 'middle', 'school', '.', '4th', 'quarter', 'is', 'after', 'a', 'diagnosis', ':', ']', 'study', 'on', ',', 'friends', '.', 'study', 'on', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'just', 'started', 'on', 'st', '##rat', '##tera', '.', 'when', 'i', 'saw', 'the', 'pills', 'the', 'first', 'thing', 'i', 'thought', 'of', 'was', '\"', 'the', 'matrix', '\"', '.']\n",
      "INFO:__main__:Number of tokens: 25\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'just', 'started', 'on', 'st', '##rat', '##tera', '.', 'when', 'i', 'saw', 'the', 'pills', 'the', 'first', 'thing', 'i', 'thought', 'of', 'was', '\"', 'the', 'matrix', '\"', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['after', '5', 'months', 'of', 'waiting', ',', 'i', 'finally', 'got', 'to', 'see', 'a', 'psychiatrist', 'and', '.', '.', '.', 'he', 'said', 'early', 'on', 'in', 'our', 'discussion', 'that', 'he', 'believes', '\"', 'ad', '##hd', 'is', 'strongly', 'over', '##dia', '##gno', '##sed', '\"', 'and', 'that', 'he', '\"', 'doesn', \"'\", 't', 'believe', 'that', 'most', 'of', 'the', 'people', 'who', 'are', 'diagnosed', 'actually', 'have', 'it', '\"', '.', 'he', 'also', 'expressed', 'that', 'he', 'does', 'not', 'believe', 'st', '##im', '##ula', '##nts', 'to', 'be', 'effective', 'or', 'necessary', 'for', 'most', 'people', 'and', 'says', 'he', 'personally', 'does', 'not', 'like', 'to', 'pre', '##scribe', 'them', 'at', 'all', '.', 'he', 'is', 'the', 'only', 'psychiatrist', 'in', 'the', 'area', 'right', 'now', '(', 'the', 'only', 'other', 'one', 'i', 'could', 'have', 'seen', 'has', 'been', 'on', 'a', '4', 'month', 'vacation', 'and', 'when', 'he', 'returns', 'will', 'be', 'backed', 'up', 'beyond', 'belief', ')', 'and', 'really', 'after', 'all', 'this', 'time', 'waiting', 'i', 'am', 'not', 'very', 'happy', 'with', 'the', 'outcome', '.', 'he', 'likened', 'my', 'troubles', 'and', 'symptoms', 'to', 'problems', 'that', 'everyone', 'has', ',', 'saying', 'none', 'of', 'them', 'are', 'really', 'anything', 'specific', 'to', 'warrant', 'a', 'diagnosis', '.', 'he', 'is', 'an', 'old', 'doctor', '(', '76', 'years', 'old', ')', 'and', 'a', 'part', 'of', 'me', 'thinks', 'that', 'it', 'is', 'because', 'he', 'is', 'older', 'that', 'he', 'doesn', \"'\", 't', 'see', 'this', 'as', 'a', 'real', 'issue', '.', 'i', \"'\", 'm', 'going', 'back', 'to', 'see', 'my', 'family', 'doctor', 'later', 'today', 'and', 'i', 'am', 'going', 'to', 'tell', 'him', 'that', 'i', 'did', 'not', 'get', 'anything', 'of', 'use', 'out', 'of', 'the', 'session', 'with', 'the', 'ps', '##ych', 'that', 'i', 'waited', 'half', 'a', 'year', 'for', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'will', 'happen', 'from', 'there', 'but', 'i', 'just', 'wanted', 'to', 'vent', 'some', 'frustration', '.']\n",
      "INFO:__main__:Number of tokens: 267\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['after', '5', 'months', 'of', 'waiting', ',', 'i', 'finally', 'got', 'to', 'see', 'a', 'psychiatrist', 'and', '.', '.', '.', 'he', 'said', 'early', 'on', 'in', 'our', 'discussion', 'that', 'he', 'believes', '\"', 'ad', '##hd', 'is', 'strongly', 'over', '##dia', '##gno', '##sed', '\"', 'and', 'that', 'he', '\"', 'doesn', \"'\", 't', 'believe', 'that', 'most', 'of', 'the', 'people', 'who', 'are', 'diagnosed', 'actually', 'have', 'it', '\"', '.', 'he', 'also', 'expressed', 'that', 'he', 'does', 'not', 'believe', 'st', '##im', '##ula', '##nts', 'to', 'be', 'effective', 'or', 'necessary', 'for', 'most', 'people', 'and', 'says', 'he', 'personally', 'does', 'not', 'like', 'to', 'pre', '##scribe', 'them', 'at', 'all', '.', 'he', 'is', 'the', 'only', 'psychiatrist', 'in', 'the', 'area', 'right', 'now', '(', 'the', 'only', 'other', 'one', 'i', 'could', 'have', 'seen', 'has', 'been', 'on', 'a', '4', 'month', 'vacation', 'and', 'when', 'he', 'returns', 'will', 'be', 'backed', 'up', 'beyond', 'belief', ')', 'and', 'really', 'after', 'all', 'this', 'time', 'waiting', 'i', 'am', 'not', 'very', 'happy', 'with', 'the', 'outcome', '.', 'he', 'likened', 'my', 'troubles', 'and', 'symptoms', 'to', 'problems', 'that', 'everyone', 'has', ',', 'saying', 'none', 'of', 'them', 'are', 'really', 'anything', 'specific', 'to', 'warrant', 'a', 'diagnosis', '.', 'he', 'is', 'an', 'old', 'doctor', '(', '76', 'years', 'old', ')', 'and', 'a', 'part', 'of', 'me', 'thinks', 'that', 'it', 'is', 'because', 'he', 'is', 'older', 'that', 'he', 'doesn', \"'\", 't', 'see', 'this', 'as', 'a', 'real', 'issue', '.', 'i', \"'\", 'm', 'going', 'back', 'to', 'see', 'my', 'family', 'doctor', 'later', 'today', 'and', 'i', 'am', 'going', 'to', 'tell', 'him', 'that', 'i', 'did', 'not', 'get', 'anything', 'of', 'use', 'out', 'of', 'the', 'session', 'with', 'the', 'ps', '##ych', 'that', 'i', 'waited', 'half', 'a', 'year', 'for', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'will', 'happen', 'from', 'there', 'but', 'i', 'just', 'wanted', 'to', 'vent', 'some', 'frustration', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['un', '##med', '##icated', 'for', '9', 'years', 'thinking', 'about', 'going', 'back', 'on', 'st', '##rat', '##tera', '.', 'advice', '?', 'so', 'throughout', 'school', 'until', 'i', 'graduated', 'high', 'school', ',', 'i', 'was', 'on', 'add', '##eral', '##l', 'then', 'st', '##rat', '##tera', '.', 'i', 'quit', 'due', 'to', 'money', 'problems', ',', 'and', 'was', 'really', 'hind', '##ered', 'during', 'college', '.', 'however', ',', 'i', \"'\", 've', 'been', 'able', 'to', 'get', '2', 'associates', 'degrees', 'and', 'a', 'good', 'job', 'and', 'i', \"'\", 've', 'been', 'successful', 'at', 'my', 'job', 'since', 'i', \"'\", 've', 'learned', 'how', 'to', 'deal', 'with', 'it', 'with', 'my', 'job', '.', 'still', ',', 'i', 'have', 'motivation', 'issues', 'unless', 'i', 'have', 'someone', 'pushing', 'me', 'to', 'do', 'something', ',', 'or', 'made', 'plans', 'with', 'someone', '.', 'advice', '?', 'ideas', '?', 'opinions', '?']\n",
      "INFO:__main__:Number of tokens: 119\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['un', '##med', '##icated', 'for', '9', 'years', 'thinking', 'about', 'going', 'back', 'on', 'st', '##rat', '##tera', '.', 'advice', '?', 'so', 'throughout', 'school', 'until', 'i', 'graduated', 'high', 'school', ',', 'i', 'was', 'on', 'add', '##eral', '##l', 'then', 'st', '##rat', '##tera', '.', 'i', 'quit', 'due', 'to', 'money', 'problems', ',', 'and', 'was', 'really', 'hind', '##ered', 'during', 'college', '.', 'however', ',', 'i', \"'\", 've', 'been', 'able', 'to', 'get', '2', 'associates', 'degrees', 'and', 'a', 'good', 'job', 'and', 'i', \"'\", 've', 'been', 'successful', 'at', 'my', 'job', 'since', 'i', \"'\", 've', 'learned', 'how', 'to', 'deal', 'with', 'it', 'with', 'my', 'job', '.', 'still', ',', 'i', 'have', 'motivation', 'issues', 'unless', 'i', 'have', 'someone', 'pushing', 'me', 'to', 'do', 'something', ',', 'or', 'made', 'plans', 'with', 'someone', '.', 'advice', '?', 'ideas', '?', 'opinions', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hoping', 'for', 'some', 'advice', 'from', 'this', 'community', 'hey', 'all', '.', 'first', 'i', 'wanted', 'to', 'say', 'thanks', 'in', 'advance', 'for', 'any', 'help', 'or', 'experiences', 'anyone', 'is', 'willing', 'to', 'share', '.', 'sorry', 'this', 'is', 'long', ',', 'but', 'i', 'hope', 'the', 'supplemental', 'information', 'is', 'helpful', '.', 'as', 'a', 'disc', '##lai', '##mer', 'i', \"'\", 've', 'already', 'read', 'the', 'fa', '##q', ',', 'and', 'while', 'a', 'few', 'links', '(', 'such', 'as', '\"', 'exploring', 'prescription', '##s', ';', 'from', 'concert', 'to', 'v', '##y', '##van', '##se', '\"', 'and', '\"', 'what', \"'\", 's', 'it', 'supposed', 'to', 'feel', 'like', '\"', ')', 'were', 'helpful', ',', 'i', 'didn', \"'\", 't', 'think', 'my', 'question', 'was', 'answered', '.', 'you', 'guys', 'have', 'all', 'been', 'so', 'supportive', ',', 'patient', 'and', 'helpful', 'with', 'others', 'that', 'i', 'thought', 'i', \"'\", 'd', 'crowd', 'source', 'your', 'wisdom', 'on', 'here', '.', 'i', 'realize', 'that', 'i', 'should', 'be', 'bringing', 'this', 'to', 'a', 'psychiatrist', 'and', 'i', 'think', 'i', 'will', ',', 'but', 'i', 'just', 'moved', 'across', 'the', 'country', 'and', 'my', 'old', 'psychiatrist', 'was', 'quite', 'frankly', 'pretty', 'di', '##sin', '##ter', '##ested', '/', 'busy', 'and', 'seemingly', 'inc', '##omp', '##ete', '##nt', '.', 'i', 'would', 'doubt', 'the', 'diagnosis', 'in', 'the', 'first', 'place', 'if', 'it', 'weren', \"'\", 't', 'for', 'the', 'fact', 'that', 'i', 'have', 'scored', 'off', 'the', 'charts', 'for', 'non', '-', 'hyper', '##active', 'add', 'on', 'every', 'test', 'i', \"'\", 've', 'taken', ',', 'and', 'if', 'i', 'weren', \"'\", 't', 'the', '15th', 'of', 'my', '4', 'siblings', 'and', '10', 'cousins', 'to', 'get', 'diagnosed', '(', 'ad', '##hd', 'especially', ',', 'but', 'also', 'depression', ',', 'alcoholism', 'and', 'tour', '##ette', \"'\", 's', 'run', 'in', 'my', 'family', ',', 'and', 'i', \"'\", 'm', 'not', 'sure', 'i', \"'\", 'm', 'free', 'of', 'the', 'middle', 'two', ',', 'but', 'that', \"'\", 's', 'another', 'thread', ')', '.', 'i', \"'\", 'm', 'not', 'saying', 'i', '*', 'know', '*', 'that', 'i', 'have', 'add', ',', 'but', 'i', 'have', 'no', 'reason', 'to', 'believe', 'i', 'don', \"'\", 't', ',', 'except', 'possibly', 'for', 'the', 'fore', '##going', 'problem', '.', 'some', 'additional', 'background', ',', 'the', 'likes', 'of', 'which', 'a', 'few', 'people', 'also', 'provided', 'in', 'their', 'posts', '(', 'feel', 'free', 'to', 'skip', 'to', 'the', 'next', 'paragraph', ',', 'but', 'i', 'thought', 'i', \"'\", 'd', 'provide', 'it', 'in', 'case', 'it', 'was', 'helpful', ')', ':', 'i', \"'\", 'm', '27', ',', 'male', ',', '6', \"'\", '0', '\"', 'and', '155', 'lbs', '.', 'i', 'was', 'diagnosed', 'in', '2011', '-', 'despite', 'earlier', 'suspicions', '-', 'due', 'to', 'the', 'fact', 'that', 'i', \"'\", 've', 'always', 'been', 'pretty', 'successful', 'at', 'what', 'i', \"'\", 've', 'done', '.', 'i', 'graduated', 'with', 'latin', 'honors', 'from', 'a', 'top', 'us', 'university', ',', 'on', 'time', ',', 'and', 'i', 'am', 'headed', 'to', 'law', 'school', 'at', 'a', 'top', '10', 'program', 'this', 'coming', 'fall', '(', 'part', 'of', 'my', 'anxiety', 'despite', 'past', 'success', 'stems', 'from', 'the', 'increased', 'rig', '##or', 'i', 'will', 'face', 'in', 'law', 'school', ')', '.', 'in', 'college', 'and', 'in', 'my', 'jobs', 'i', 'got', 'by', 'on', 'intelligence', '(', 'this', 'sounds', 'really', 'do', '##uche', '##y', ',', 'and', 'i', 'know', 'it', \"'\", 's', 'not', 'necessarily', 'a', 'reliable', 'metric', ',', 'but', 'my', 'iq', 'was', 'twice', 'tested', 'above', '160', ')', ',', 'but', 'i', 'would', 'cut', 'corners', ',', 'take', 'too', 'long', 'to', 'do', 'things', ',', 'and', 'mess', 'simple', 'things', 'up', 'because', 'i', 'couldn', \"'\", 't', 'pay', 'attention', 'or', 'concentrate', 'or', 'mo', '##tiv', '##ate', 'myself', '.', 'i', 'began', 'to', 'realize', 'that', 'my', 'di', '##sor', '##gan', '##ization', ',', 'di', '##sin', '##ter', '##est', ',', 'inability', 'to', 'concentrate', ',', 'forget', '##fulness', 'and', 'mess', '##iness', '(', 'among', 'other', 'things', ')', 'were', 'beginning', 'to', 'finally', 'catch', 'up', 'with', 'me', 'at', 'my', 'job', ',', 'and', 'i', 'was', 'not', 'performing', 'to', 'my', 'potential', '.', 'in', 'college', 'i', 'obviously', 'got', 'good', 'marks', ',', 'but', 'i', 'probably', 'had', 'to', 'work', 'twice', 'as', 'hard', 'as', 'people', 'who', 'were', 'as', 'or', 'less', 'intelligent', 'than', 'me', 'to', 'get', 'the', 'same', 'grades', ',', 'mostly', 'because', 'i', 'would', 'pro', '##cr', '##ast', '##inate', ',', 'dick', 'around', ',', 'forget', 'assignments', 'and', 'have', 'to', 'make', 'up', 'for', 'it', 'later', ',', 'read', 'things', 'incorrectly', ',', 'etc', '.', '(', 'it', 'was', 'not', 'uncommon', 'for', 'me', 'to', 'work', 'for', '30', 'hours', 'straight', 'on', 'a', 'paper', 'right', 'before', 'it', 'was', 'due', ',', 'or', 'to', 'slack', 'off', 'until', 'a', 'few', 'days', 'before', 'the', 'exam', 'and', 'then', 'pull', 'an', 'all', '-', 'night', '##er', ',', 'even', 'if', 'the', 'cumulative', 'effort', 'of', 'my', 'friends', 'was', 'less', 'than', '15', 'hours', 'for', 'the', 'paper', ',', 'or', 'exam', ')', '.', 'still', ',', 'college', 'was', 'sort', 'of', 'in', 'my', 'wheel', '##house', ',', 'and', 'the', 'working', 'world', 'wasn', \"'\", 't', '.', 'toward', 'the', 'end', 'of', 'my', 'time', 'in', 'my', 'former', 'state', 'of', 'residence', 'i', 'went', 'to', 'a', 'psychiatrist', 'and', 'she', 'diagnosed', 'me', 'with', 'adult', 'add', '(', 'sorry', 'if', 'i', 'have', 'the', 'terminology', 'wrong', ')', ',', 'and', 'shortly', 'thereafter', 'i', 'moved', 'to', 'the', 'state', 'i', 'just', 'recently', 'moved', 'out', 'of', '.', 'i', 'was', 'pretty', 'happy', ',', 'because', 'i', 'always', 'felt', 'that', 'i', 'had', 'add', 'but', 'was', 'ignored', 'because', 'i', 'was', 'high', 'achieving', ',', 'even', 'if', 'that', 'achievement', 'required', 'di', '##sp', '##rop', '##ort', '##ion', '##ate', 'amounts', 'of', 'effort', '.', 'additionally', ',', 'on', '5', '-', '10', 'occasions', 'in', 'high', 'school', 'or', 'college', ',', 'i', 'would', 'sneak', '/', 'buy', 'my', 'siblings', \"'\", 'concert', '##a', ',', 'or', 'rita', '##lin', 'or', 'add', '##er', '##ral', ',', 'and', 'even', 'if', 'i', 'used', 'it', 'for', '3', '-', '4', 'days', 'at', 'a', 'time', 'it', 'did', 'complete', 'wonders', 'for', 'me', '.', 'i', 'was', 'a', 'machine', ',', 'could', 'work', '20', '+', 'hr', '##s', 'straight', ',', 'could', 'concentrate', 'and', 'work', 'on', 'the', 'most', 'mundane', 'tasks', ',', 'was', 'hyper', '-', 'efficient', ',', 'etc', '.', ',', 'so', 'i', 'thought', 'that', 'medication', 'would', 'have', 'a', 'lot', 'of', 'potential', '.', 'fast', 'forward', 'to', 'the', 'problem', 'at', 'hand', '.', 'in', 'sept', '.', '2011', 'i', 'was', 'prescribed', 'add', '##er', '##ral', 'x', '##r', 'at', 'the', 'rate', 'of', '20', '##mg', 'per', 'day', '.', 'the', 'first', 'day', 'was', 'like', 'i', 'had', 'remembered', '-', 'i', 'was', 'a', 'superhuman', ',', 'but', 'now', 'the', 'problem', 'i', 'come', 'here', 'for', 'help', 'with', 'is', 'that', 'i', 'feel', 'almost', 'nothing', ',', 'or', 'at', 'least', 'nothing', 'i', 'was', 'seeking', 'through', 'medication', '.', 'the', 'best', '/', 'most', 'useful', 'thing', 'it', 'does', 'for', 'me', 'is', 'keep', 'me', 'awake', 'from', 'the', 'time', 'i', 'rise', 'until', '1', '-', '2a', '##m', '(', 'i', \"'\", 'm', 'a', 'late', 'night', 'person', 'anyway', ')', ',', 'whereas', 'i', 'used', 'to', 'have', 'to', 'rely', 'on', 'tons', 'of', 'caf', '##fe', '##ine', 'to', 'get', 'through', 'the', 'day', '.', 'after', 'that', ',', 'the', 'benefits', 'are', 'marginal', '.', '.', '.', 'i', 'never', 'have', 'to', 'worry', 'about', 'gaining', 'weight', '(', 'not', 'a', 'huge', 'problem', 'beforehand', ')', ',', 'i', 'get', 'that', 'eu', '##ph', '##oric', 'period', 'for', 'about', '30', 'minutes', 'an', 'hour', 'after', 'i', 'ing', '##est', 'the', 'pill', '(', 'but', 'it', \"'\", 's', 'sort', 'of', 'like', ',', 'who', 'cares', '?', ')', ',', 'and', 'that', \"'\", 's', 'about', 'it', '.', 'the', 'staying', 'awake', 'thing', 'is', 'nice', 'to', 'have', 'when', 'i', 'absolutely', 'need', 'to', 'get', 'things', 'done', ',', 'but', 'if', 'i', \"'\", 'm', 'not', 'more', 'productive', 'then', 'it', \"'\", 's', 'just', 'spreading', 'the', 'same', 'problem', 'out', 'over', 'a', 'longer', 'period', '.', 'the', 'downs', '##ides', 'are', 'nothing', 'horrific', 'but', 'they', 'are', 'noticeable', '-', '-', 'i', 'occasionally', 'grind', 'my', 'teeth', 'now', ',', 'i', 'can', 'be', 'slightly', 'more', 'aggressive', 'or', 'short', '-', 'tempered', ',', 'conversation', 'is', 'slightly', 'less', 'fluid', ',', 'i', 'don', \"'\", 't', 'feel', 'like', 'a', 'zombie', 'but', 'i', 'do', 'believe', 'my', 'ability', 'to', 'write', 'or', 'be', 'creative', 'is', 'slightly', 'stunt', '##ed', ',', 'and', 'it', 'may', 'be', 'affecting', 'my', 'li', '##bid', '##o', 'negatively', '.', 'on', 'days', 'or', 'periods', 'when', 'i', 'don', \"'\", 't', 'take', 'it', ',', 'these', 'good', 'and', 'bad', 'symptoms', 'disappear', 'for', 'the', 'most', 'part', ',', 'but', 'with', 'the', 'exception', 'of', 'being', 'able', 'to', 'stay', 'awake', 'and', 'loss', 'of', 'appetite', 'they', \"'\", 're', 'all', 'so', 'subtle', 'that', 'it', \"'\", 's', 'almost', 'like', 'i', 'wouldn', \"'\", 't', 'know', 'if', 'someone', 'crushed', 'a', 'pill', 'up', 'in', 'my', 'food', 'or', 'not', '.', 'ultimately', ',', 'i', 'want', 'that', 'increase', 'in', 'focus', 'that', 'people', 'report', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'get', 'high', ',', 'and', 'i', 'don', \"'\", 't', 'care', 'about', 'the', 'things', 'listed', 'above', '(', 'whether', 'they', 'disappear', 'or', 'remain', 'the', 'same', ')', ',', 'but', 'i', 'am', 'concerned', 'about', 'the', 'inability', 'to', 'concentrate', 'or', 'improve', 'my', 'productivity', '.', 'honestly', 'i', \"'\", 'd', 'say', 'that', 'if', 'anything', ',', 'my', 'average', 'productivity', 'has', 'decreased', ',', 'and', 'i', 'get', 'into', 'these', 'ja', '##gs', 'of', 'crazy', 'pro', '##cr', '##ast', '##ination', 'that', 'almost', 'feel', 'enabled', 'by', 'the', 'add', '##er', '##ral', '.', 'has', 'anyone', 'experienced', 'the', 'same', 'or', 'anything', 'similar', '?', 'i', \"'\", 'm', 'very', 'worried', 'about', 'handling', 'law', 'school', '(', 'where', 'organization', ',', 'time', 'management', 'and', 'concentration', 'are', 'ind', '##is', '##pen', '##sable', ')', 'with', 'these', 'problems', '.', 'i', 'want', 'to', 'try', 'v', '##y', '##van', '##se', 'based', 'on', 'what', 'i', \"'\", 've', 'heard', 'here', ',', 'but', 'if', 'add', '##er', '##ral', 'used', 'to', 'be', 'a', 'gods', '##end', 'and', 'for', 'whatever', 'reason', 'does', 'very', 'little', 'now', ',', 'i', 'am', 'worried', 'that', 'i', 'might', 'be', 'beyond', 'pharmaceutical', 'intervention', '.', 'thanks', 'guys', '!']\n",
      "INFO:__main__:Number of tokens: 1442\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['hoping', 'for', 'some', 'advice', 'from', 'this', 'community', 'hey', 'all', '.', 'first', 'i', 'wanted', 'to', 'say', 'thanks', 'in', 'advance', 'for', 'any', 'help', 'or', 'experiences', 'anyone', 'is', 'willing', 'to', 'share', '.', 'sorry', 'this', 'is', 'long', ',', 'but', 'i', 'hope', 'the', 'supplemental', 'information', 'is', 'helpful', '.', 'as', 'a', 'disc', '##lai', '##mer', 'i', \"'\", 've', 'already', 'read', 'the', 'fa', '##q', ',', 'and', 'while', 'a', 'few', 'links', '(', 'such', 'as', '\"', 'exploring', 'prescription', '##s', ';', 'from', 'concert', 'to', 'v', '##y', '##van', '##se', '\"', 'and', '\"', 'what', \"'\", 's', 'it', 'supposed', 'to', 'feel', 'like', '\"', ')', 'were', 'helpful', ',', 'i', 'didn', \"'\", 't', 'think', 'my', 'question', 'was', 'answered', '.', 'you', 'guys', 'have', 'all', 'been', 'so', 'supportive', ',', 'patient', 'and', 'helpful', 'with', 'others', 'that', 'i', 'thought', 'i', \"'\", 'd', 'crowd', 'source', 'your', 'wisdom', 'on', 'here', '.', 'i', 'realize', 'that', 'i', 'should', 'be', 'bringing', 'this', 'to', 'a', 'psychiatrist', 'and', 'i', 'think', 'i', 'will', ',', 'but', 'i', 'just', 'moved', 'across', 'the', 'country', 'and', 'my', 'old', 'psychiatrist', 'was', 'quite', 'frankly', 'pretty', 'di', '##sin', '##ter', '##ested', '/', 'busy', 'and', 'seemingly', 'inc', '##omp', '##ete', '##nt', '.', 'i', 'would', 'doubt', 'the', 'diagnosis', 'in', 'the', 'first', 'place', 'if', 'it', 'weren', \"'\", 't', 'for', 'the', 'fact', 'that', 'i', 'have', 'scored', 'off', 'the', 'charts', 'for', 'non', '-', 'hyper', '##active', 'add', 'on', 'every', 'test', 'i', \"'\", 've', 'taken', ',', 'and', 'if', 'i', 'weren', \"'\", 't', 'the', '15th', 'of', 'my', '4', 'siblings', 'and', '10', 'cousins', 'to', 'get', 'diagnosed', '(', 'ad', '##hd', 'especially', ',', 'but', 'also', 'depression', ',', 'alcoholism', 'and', 'tour', '##ette', \"'\", 's', 'run', 'in', 'my', 'family', ',', 'and', 'i', \"'\", 'm', 'not', 'sure', 'i', \"'\", 'm', 'free', 'of', 'the', 'middle', 'two', ',', 'but', 'that', \"'\", 's', 'another', 'thread', ')', '.', 'i', \"'\", 'm', 'not', 'saying', 'i', '*', 'know', '*', 'that', 'i', 'have', 'add', ',', 'but', 'i', 'have', 'no', 'reason', 'to', 'believe', 'i', 'don', \"'\", 't', ',', 'except', 'possibly', 'for', 'the', 'fore', '##going', 'problem', '.', 'some', 'additional', 'background', ',', 'the', 'likes', 'of', 'which', 'a', 'few', 'people', 'also', 'provided', 'in', 'their', 'posts', '(', 'feel', 'free', 'to', 'skip', 'to', 'the', 'next', 'paragraph', ',', 'but', 'i', 'thought', 'i', \"'\", 'd', 'provide', 'it', 'in', 'case', 'it', 'was', 'helpful', ')', ':', 'i', \"'\", 'm', '27', ',', 'male', ',', '6', \"'\", '0', '\"', 'and', '155', 'lbs', '.', 'i', 'was', 'diagnosed', 'in', '2011', '-', 'despite', 'earlier', 'suspicions', '-', 'due', 'to', 'the', 'fact', 'that', 'i', \"'\", 've', 'always', 'been', 'pretty', 'successful', 'at', 'what', 'i', \"'\", 've', 'done', '.', 'i', 'graduated', 'with', 'latin', 'honors', 'from', 'a', 'top', 'us', 'university', ',', 'on', 'time', ',', 'and', 'i', 'am', 'headed', 'to', 'law', 'school', 'at', 'a', 'top', '10', 'program', 'this', 'coming', 'fall', '(', 'part', 'of', 'my', 'anxiety', 'despite', 'past', 'success', 'stems', 'from', 'the', 'increased', 'rig', '##or', 'i', 'will', 'face', 'in', 'law', 'school', ')', '.', 'in', 'college', 'and', 'in', 'my', 'jobs', 'i', 'got', 'by', 'on', 'intelligence', '(', 'this', 'sounds', 'really', 'do', '##uche', '##y', ',', 'and', 'i', 'know', 'it', \"'\", 's', 'not', 'necessarily', 'a', 'reliable', 'metric', ',', 'but', 'my', 'iq', 'was', 'twice', 'tested', 'above', '160', ')', ',', 'but', 'i', 'would', 'cut', 'corners', ',', 'take', 'too', 'long', 'to', 'do', 'things', ',', 'and', 'mess', 'simple', 'things', 'up', 'because', 'i', 'couldn', \"'\", 't', 'pay', 'attention', 'or', 'concentrate', 'or'], ['mo', '##tiv', '##ate', 'myself', '.', 'i', 'began', 'to', 'realize', 'that', 'my', 'di', '##sor', '##gan', '##ization', ',', 'di', '##sin', '##ter', '##est', ',', 'inability', 'to', 'concentrate', ',', 'forget', '##fulness', 'and', 'mess', '##iness', '(', 'among', 'other', 'things', ')', 'were', 'beginning', 'to', 'finally', 'catch', 'up', 'with', 'me', 'at', 'my', 'job', ',', 'and', 'i', 'was', 'not', 'performing', 'to', 'my', 'potential', '.', 'in', 'college', 'i', 'obviously', 'got', 'good', 'marks', ',', 'but', 'i', 'probably', 'had', 'to', 'work', 'twice', 'as', 'hard', 'as', 'people', 'who', 'were', 'as', 'or', 'less', 'intelligent', 'than', 'me', 'to', 'get', 'the', 'same', 'grades', ',', 'mostly', 'because', 'i', 'would', 'pro', '##cr', '##ast', '##inate', ',', 'dick', 'around', ',', 'forget', 'assignments', 'and', 'have', 'to', 'make', 'up', 'for', 'it', 'later', ',', 'read', 'things', 'incorrectly', ',', 'etc', '.', '(', 'it', 'was', 'not', 'uncommon', 'for', 'me', 'to', 'work', 'for', '30', 'hours', 'straight', 'on', 'a', 'paper', 'right', 'before', 'it', 'was', 'due', ',', 'or', 'to', 'slack', 'off', 'until', 'a', 'few', 'days', 'before', 'the', 'exam', 'and', 'then', 'pull', 'an', 'all', '-', 'night', '##er', ',', 'even', 'if', 'the', 'cumulative', 'effort', 'of', 'my', 'friends', 'was', 'less', 'than', '15', 'hours', 'for', 'the', 'paper', ',', 'or', 'exam', ')', '.', 'still', ',', 'college', 'was', 'sort', 'of', 'in', 'my', 'wheel', '##house', ',', 'and', 'the', 'working', 'world', 'wasn', \"'\", 't', '.', 'toward', 'the', 'end', 'of', 'my', 'time', 'in', 'my', 'former', 'state', 'of', 'residence', 'i', 'went', 'to', 'a', 'psychiatrist', 'and', 'she', 'diagnosed', 'me', 'with', 'adult', 'add', '(', 'sorry', 'if', 'i', 'have', 'the', 'terminology', 'wrong', ')', ',', 'and', 'shortly', 'thereafter', 'i', 'moved', 'to', 'the', 'state', 'i', 'just', 'recently', 'moved', 'out', 'of', '.', 'i', 'was', 'pretty', 'happy', ',', 'because', 'i', 'always', 'felt', 'that', 'i', 'had', 'add', 'but', 'was', 'ignored', 'because', 'i', 'was', 'high', 'achieving', ',', 'even', 'if', 'that', 'achievement', 'required', 'di', '##sp', '##rop', '##ort', '##ion', '##ate', 'amounts', 'of', 'effort', '.', 'additionally', ',', 'on', '5', '-', '10', 'occasions', 'in', 'high', 'school', 'or', 'college', ',', 'i', 'would', 'sneak', '/', 'buy', 'my', 'siblings', \"'\", 'concert', '##a', ',', 'or', 'rita', '##lin', 'or', 'add', '##er', '##ral', ',', 'and', 'even', 'if', 'i', 'used', 'it', 'for', '3', '-', '4', 'days', 'at', 'a', 'time', 'it', 'did', 'complete', 'wonders', 'for', 'me', '.', 'i', 'was', 'a', 'machine', ',', 'could', 'work', '20', '+', 'hr', '##s', 'straight', ',', 'could', 'concentrate', 'and', 'work', 'on', 'the', 'most', 'mundane', 'tasks', ',', 'was', 'hyper', '-', 'efficient', ',', 'etc', '.', ',', 'so', 'i', 'thought', 'that', 'medication', 'would', 'have', 'a', 'lot', 'of', 'potential', '.', 'fast', 'forward', 'to', 'the', 'problem', 'at', 'hand', '.', 'in', 'sept', '.', '2011', 'i', 'was', 'prescribed', 'add', '##er', '##ral', 'x', '##r', 'at', 'the', 'rate', 'of', '20', '##mg', 'per', 'day', '.', 'the', 'first', 'day', 'was', 'like', 'i', 'had', 'remembered', '-', 'i', 'was', 'a', 'superhuman', ',', 'but', 'now', 'the', 'problem', 'i', 'come', 'here', 'for', 'help', 'with', 'is', 'that', 'i', 'feel', 'almost', 'nothing', ',', 'or', 'at', 'least', 'nothing', 'i', 'was', 'seeking', 'through', 'medication', '.', 'the', 'best', '/', 'most', 'useful', 'thing', 'it', 'does', 'for', 'me', 'is', 'keep', 'me', 'awake', 'from', 'the', 'time', 'i', 'rise', 'until', '1', '-', '2a', '##m', '(', 'i', \"'\", 'm', 'a', 'late', 'night', 'person', 'anyway', ')', ',', 'whereas', 'i', 'used', 'to', 'have', 'to', 'rely', 'on', 'tons', 'of', 'caf', '##fe', '##ine', 'to', 'get', 'through', 'the', 'day', '.', 'after', 'that', ',', 'the', 'benefits', 'are'], ['marginal', '.', '.', '.', 'i', 'never', 'have', 'to', 'worry', 'about', 'gaining', 'weight', '(', 'not', 'a', 'huge', 'problem', 'beforehand', ')', ',', 'i', 'get', 'that', 'eu', '##ph', '##oric', 'period', 'for', 'about', '30', 'minutes', 'an', 'hour', 'after', 'i', 'ing', '##est', 'the', 'pill', '(', 'but', 'it', \"'\", 's', 'sort', 'of', 'like', ',', 'who', 'cares', '?', ')', ',', 'and', 'that', \"'\", 's', 'about', 'it', '.', 'the', 'staying', 'awake', 'thing', 'is', 'nice', 'to', 'have', 'when', 'i', 'absolutely', 'need', 'to', 'get', 'things', 'done', ',', 'but', 'if', 'i', \"'\", 'm', 'not', 'more', 'productive', 'then', 'it', \"'\", 's', 'just', 'spreading', 'the', 'same', 'problem', 'out', 'over', 'a', 'longer', 'period', '.', 'the', 'downs', '##ides', 'are', 'nothing', 'horrific', 'but', 'they', 'are', 'noticeable', '-', '-', 'i', 'occasionally', 'grind', 'my', 'teeth', 'now', ',', 'i', 'can', 'be', 'slightly', 'more', 'aggressive', 'or', 'short', '-', 'tempered', ',', 'conversation', 'is', 'slightly', 'less', 'fluid', ',', 'i', 'don', \"'\", 't', 'feel', 'like', 'a', 'zombie', 'but', 'i', 'do', 'believe', 'my', 'ability', 'to', 'write', 'or', 'be', 'creative', 'is', 'slightly', 'stunt', '##ed', ',', 'and', 'it', 'may', 'be', 'affecting', 'my', 'li', '##bid', '##o', 'negatively', '.', 'on', 'days', 'or', 'periods', 'when', 'i', 'don', \"'\", 't', 'take', 'it', ',', 'these', 'good', 'and', 'bad', 'symptoms', 'disappear', 'for', 'the', 'most', 'part', ',', 'but', 'with', 'the', 'exception', 'of', 'being', 'able', 'to', 'stay', 'awake', 'and', 'loss', 'of', 'appetite', 'they', \"'\", 're', 'all', 'so', 'subtle', 'that', 'it', \"'\", 's', 'almost', 'like', 'i', 'wouldn', \"'\", 't', 'know', 'if', 'someone', 'crushed', 'a', 'pill', 'up', 'in', 'my', 'food', 'or', 'not', '.', 'ultimately', ',', 'i', 'want', 'that', 'increase', 'in', 'focus', 'that', 'people', 'report', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'get', 'high', ',', 'and', 'i', 'don', \"'\", 't', 'care', 'about', 'the', 'things', 'listed', 'above', '(', 'whether', 'they', 'disappear', 'or', 'remain', 'the', 'same', ')', ',', 'but', 'i', 'am', 'concerned', 'about', 'the', 'inability', 'to', 'concentrate', 'or', 'improve', 'my', 'productivity', '.', 'honestly', 'i', \"'\", 'd', 'say', 'that', 'if', 'anything', ',', 'my', 'average', 'productivity', 'has', 'decreased', ',', 'and', 'i', 'get', 'into', 'these', 'ja', '##gs', 'of', 'crazy', 'pro', '##cr', '##ast', '##ination', 'that', 'almost', 'feel', 'enabled', 'by', 'the', 'add', '##er', '##ral', '.', 'has', 'anyone', 'experienced', 'the', 'same', 'or', 'anything', 'similar', '?', 'i', \"'\", 'm', 'very', 'worried', 'about', 'handling', 'law', 'school', '(', 'where', 'organization', ',', 'time', 'management', 'and', 'concentration', 'are', 'ind', '##is', '##pen', '##sable', ')', 'with', 'these', 'problems', '.', 'i', 'want', 'to', 'try', 'v', '##y', '##van', '##se', 'based', 'on', 'what', 'i', \"'\", 've', 'heard', 'here', ',', 'but', 'if', 'add', '##er', '##ral', 'used', 'to', 'be', 'a', 'gods', '##end', 'and', 'for', 'whatever', 'reason', 'does', 'very', 'little', 'now', ',', 'i', 'am', 'worried', 'that', 'i', 'might', 'be', 'beyond', 'pharmaceutical', 'intervention', '.', 'thanks', 'guys', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'friday', 'f', '##oll', '##ies', ']', 'what', 'ridiculous', 'scrape', '##s', 'did', 'your', 'ad', '##hd', 'get', 'you', 'into', 'this', 'week', '?', '(', 'or', 'ever', '.', ')', '=', 'p']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'friday', 'f', '##oll', '##ies', ']', 'what', 'ridiculous', 'scrape', '##s', 'did', 'your', 'ad', '##hd', 'get', 'you', 'into', 'this', 'week', '?', '(', 'or', 'ever', '.', ')', '=', 'p']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'u', 'was', 'diagnosed', 'with', 'ad', '##hd', 'about', '10', 'years', 'ago', ',', 'and', 'for', 'the', 'last', '4', 'years', 'i', 'haven', \"'\", 't', 'had', 'any', 'problems', ',', 'until', 'i', 'walk', 'into', 'this', 'place', '.', 'something', 'about', 'it', 'makes', 'me', 'go', 'crazy']\n",
      "INFO:__main__:Number of tokens: 40\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', 'u', 'was', 'diagnosed', 'with', 'ad', '##hd', 'about', '10', 'years', 'ago', ',', 'and', 'for', 'the', 'last', '4', 'years', 'i', 'haven', \"'\", 't', 'had', 'any', 'problems', ',', 'until', 'i', 'walk', 'into', 'this', 'place', '.', 'something', 'about', 'it', 'makes', 'me', 'go', 'crazy']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['advice', 'for', 'dealing', 'with', 'a', 'possibly', 'skeptical', 'doctor', ',', 'and', 'old', '-', 'school', 'parents', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['advice', 'for', 'dealing', 'with', 'a', 'possibly', 'skeptical', 'doctor', ',', 'and', 'old', '-', 'school', 'parents', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['should', 'i', 'find', 'a', 'new', 'psychiatrist', '?']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['should', 'i', 'find', 'a', 'new', 'psychiatrist', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', '.', '.', '.', 'what', 'are', 'ad', '##hd', 'med', '##s', 'supposed', 'to', 'actually', 'do', '?', 'so', 'far', ',', 'i', \"'\", 've', 'tried', 'focal', '##in', 'ir', '(', 'didn', \"'\", 't', 'do', 'much', 'at', 'all', ')', ',', 'focal', '##in', 'x', '##r', '(', 'made', 'me', 'more', 'alert', '/', 'motivated', ',', 'but', 'not', 'focused', ',', 'and', 'came', 'with', 'terrible', 'mood', 'swings', ')', ',', 'st', '##rat', '##tera', '(', 'didn', \"'\", 't', 'seem', 'to', 'do', 'anything', ')', ',', 'and', 'now', 'v', '##y', '##van', '##se', '(', 'same', 'as', 'focal', '##in', 'x', '##r', ',', 'but', 'without', 'the', 'mood', 'swings', ')', '.', 'i', 'have', 'the', 'ina', '##tten', '##tive', 'type', '.', 'so', 'basically', 'what', 'the', 'v', '##y', '##van', '##se', 'is', 'doing', 'for', 'me', 'is', 'taking', 'me', 'from', 'being', 'a', 'kind', 'of', 'spa', '##cy', '/', 'dream', '##y', 'person', 'to', 'an', 'alert', '/', 'motivated', 'one', ',', 'but', 'it', 'does', 'nothing', 'to', 'improve', 'my', 'focus', '.', 'i', 'feel', 'like', 'a', 'rabbit', 'when', 'i', \"'\", 'm', 'on', 'med', '##s', '-', 'aware', 'of', 'everything', ',', 'wanting', 'to', 'get', 'up', 'and', 'go', '.', 'soo', '##o', 'this', 'is', 'counter', '-', 'productive', 'when', 'what', 'i', 'need', 'to', 'be', 'doing', 'is', 'reading', 'a', 'boring', 'textbook', '.', 'i', \"'\", 'm', 'just', 'wondering', 'how', 'i', 'will', 'know', 'when', 'i', 'find', 'a', 'medication', 'that', \"'\", 's', '\"', 'right', '\"', 'for', 'me', '.', 'i', 'always', 'hear', 'about', 'people', 'who', 'don', \"'\", 't', 'have', 'ad', '##hd', 'buying', 'med', '##s', 'from', 'people', 'who', 'do', ',', 'so', 'they', 'can', 'use', 'them', 'to', 'study', '.', 'and', 'i', 'just', 'don', \"'\", 't', 'understand', 'why', '!', 'what', 'am', 'i', 'missing', 'out', 'on', '?', '(', 'side', 'note', ':', 'i', 'also', 'have', 'depression', ',', 'and', 'maybe', 'anxiety', ',', 'so', 'that', 'makes', 'everything', 'more', 'confusing', '.', ')', 'edited', 'to', 'add', ':', 'another', 'thing', 'i', 'have', 'a', 'problem', 'with', 'is', 'that', 'the', 'v', '##y', '##van', '##se', 'is', 'supposed', 'to', 'last', '10', '-', '12', 'hours', ',', 'but', 'it', 'only', 'lasts', 'about', '8', 'for', 'me', '.', 'this', 'causes', 'scheduling', 'issues', ':', 'it', 'takes', 'about', '2', 'hours', 'to', 'kick', 'in', ',', 'so', 'i', 'take', 'it', 'around', '10', 'am', ',', 'because', 'then', 'it', 'wears', 'off', 'around', '8', 'pm', '(', 'i', 'try', 'to', 'be', 'done', 'with', 'homework', 'by', 'then', ')', '.', 'it', 'would', 'be', 'fantastic', 'to', 'find', 'something', 'that', 'could', 'actually', 'work', 'all', 'day', '.']\n",
      "INFO:__main__:Number of tokens: 367\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', '.', '.', '.', 'what', 'are', 'ad', '##hd', 'med', '##s', 'supposed', 'to', 'actually', 'do', '?', 'so', 'far', ',', 'i', \"'\", 've', 'tried', 'focal', '##in', 'ir', '(', 'didn', \"'\", 't', 'do', 'much', 'at', 'all', ')', ',', 'focal', '##in', 'x', '##r', '(', 'made', 'me', 'more', 'alert', '/', 'motivated', ',', 'but', 'not', 'focused', ',', 'and', 'came', 'with', 'terrible', 'mood', 'swings', ')', ',', 'st', '##rat', '##tera', '(', 'didn', \"'\", 't', 'seem', 'to', 'do', 'anything', ')', ',', 'and', 'now', 'v', '##y', '##van', '##se', '(', 'same', 'as', 'focal', '##in', 'x', '##r', ',', 'but', 'without', 'the', 'mood', 'swings', ')', '.', 'i', 'have', 'the', 'ina', '##tten', '##tive', 'type', '.', 'so', 'basically', 'what', 'the', 'v', '##y', '##van', '##se', 'is', 'doing', 'for', 'me', 'is', 'taking', 'me', 'from', 'being', 'a', 'kind', 'of', 'spa', '##cy', '/', 'dream', '##y', 'person', 'to', 'an', 'alert', '/', 'motivated', 'one', ',', 'but', 'it', 'does', 'nothing', 'to', 'improve', 'my', 'focus', '.', 'i', 'feel', 'like', 'a', 'rabbit', 'when', 'i', \"'\", 'm', 'on', 'med', '##s', '-', 'aware', 'of', 'everything', ',', 'wanting', 'to', 'get', 'up', 'and', 'go', '.', 'soo', '##o', 'this', 'is', 'counter', '-', 'productive', 'when', 'what', 'i', 'need', 'to', 'be', 'doing', 'is', 'reading', 'a', 'boring', 'textbook', '.', 'i', \"'\", 'm', 'just', 'wondering', 'how', 'i', 'will', 'know', 'when', 'i', 'find', 'a', 'medication', 'that', \"'\", 's', '\"', 'right', '\"', 'for', 'me', '.', 'i', 'always', 'hear', 'about', 'people', 'who', 'don', \"'\", 't', 'have', 'ad', '##hd', 'buying', 'med', '##s', 'from', 'people', 'who', 'do', ',', 'so', 'they', 'can', 'use', 'them', 'to', 'study', '.', 'and', 'i', 'just', 'don', \"'\", 't', 'understand', 'why', '!', 'what', 'am', 'i', 'missing', 'out', 'on', '?', '(', 'side', 'note', ':', 'i', 'also', 'have', 'depression', ',', 'and', 'maybe', 'anxiety', ',', 'so', 'that', 'makes', 'everything', 'more', 'confusing', '.', ')', 'edited', 'to', 'add', ':', 'another', 'thing', 'i', 'have', 'a', 'problem', 'with', 'is', 'that', 'the', 'v', '##y', '##van', '##se', 'is', 'supposed', 'to', 'last', '10', '-', '12', 'hours', ',', 'but', 'it', 'only', 'lasts', 'about', '8', 'for', 'me', '.', 'this', 'causes', 'scheduling', 'issues', ':', 'it', 'takes', 'about', '2', 'hours', 'to', 'kick', 'in', ',', 'so', 'i', 'take', 'it', 'around', '10', 'am', ',', 'because', 'then', 'it', 'wears', 'off', 'around', '8', 'pm', '(', 'i', 'try', 'to', 'be', 'done', 'with', 'homework', 'by', 'then', ')', '.', 'it', 'would', 'be', 'fantastic', 'to', 'find', 'something', 'that', 'could', 'actually', 'work', 'all', 'day', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'did', 'fuck', 'all', 'work', 'tonight', ',', 'but', 'i', 'did', 'basically', 'cobb', '##le', 'together', 'a', 'software', 'suite', 'made', 'from', 'fire', '##fo', '##x', 'plug', '##ins', 'and', 'reference', 'management', 'software', 'that', 'automatically', 'references', 'citations', 'upon', 'been', 'highlighted', ',', 'in', 'websites', ',', 'text', ',', 'pdf', ',', 'google', 'doc', 'etc', '.']\n",
      "INFO:__main__:Number of tokens: 47\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'did', 'fuck', 'all', 'work', 'tonight', ',', 'but', 'i', 'did', 'basically', 'cobb', '##le', 'together', 'a', 'software', 'suite', 'made', 'from', 'fire', '##fo', '##x', 'plug', '##ins', 'and', 'reference', 'management', 'software', 'that', 'automatically', 'references', 'citations', 'upon', 'been', 'highlighted', ',', 'in', 'websites', ',', 'text', ',', 'pdf', ',', 'google', 'doc', 'etc', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anti', '##de', '##press', '##ant', 'medication', 'help', 'with', 'ad', '##hd', '-', 'pi', '?', 'hello', 'everyone', ',', 'so', 'i', 'am', 'having', 'one', 'of', 'my', 'depression', 'periods', 'as', 'i', 'type', 'this', ',', 'over', '5', 'years', 'i', 'have', 'been', 'constantly', 'depressed', 'and', 'had', 'trouble', 'concentrating', '(', 'i', 'have', 'always', 'had', 'issues', 'with', 'this', ',', 'since', 'kindergarten', 'but', 'it', 'did', 'not', 'hurt', 'me', 'all', 'that', 'much', 'until', 'hs', 'started', ')', '.', 'i', 'never', 'once', 'took', 'action', 'against', 'this', 'until', 'now', 'as', 'i', 'feel', 'i', 'have', 'reached', 'a', 'critical', 'point', '.', 'i', 'seem', 'to', 'have', 'symptoms', 'of', 'depression', 'and', 'ad', '##hd', '-', 'pi', ',', 'basically', 'i', 'feel', 'dread', 'and', 'when', 'i', 'try', 'to', 'do', 'something', 'or', 'think', 'about', 'something', 'my', 'line', 'of', 'thought', 'gets', 'interruption', '##s', 'as', 'it', 'always', 'has', '.', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'fix', 'this', 'so', 'i', 'am', 'going', 'to', 'seek', 'medical', 'help', 'however', 'i', 'am', 'wondering', 'if', 'i', 'get', 'prescribed', 'pills', 'for', 'depression', 'it', 'will', 'also', 'aid', 'me', 'in', 'my', 'concentration', 'issues', 'and', 'collecting', 'my', 'thoughts', 'in', 'an', 'orderly', 'manner', '?', 'my', 'brain', 'just', 'feels', 'so', 'scattered', 'and', 'slow', '.']\n",
      "INFO:__main__:Number of tokens: 181\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anti', '##de', '##press', '##ant', 'medication', 'help', 'with', 'ad', '##hd', '-', 'pi', '?', 'hello', 'everyone', ',', 'so', 'i', 'am', 'having', 'one', 'of', 'my', 'depression', 'periods', 'as', 'i', 'type', 'this', ',', 'over', '5', 'years', 'i', 'have', 'been', 'constantly', 'depressed', 'and', 'had', 'trouble', 'concentrating', '(', 'i', 'have', 'always', 'had', 'issues', 'with', 'this', ',', 'since', 'kindergarten', 'but', 'it', 'did', 'not', 'hurt', 'me', 'all', 'that', 'much', 'until', 'hs', 'started', ')', '.', 'i', 'never', 'once', 'took', 'action', 'against', 'this', 'until', 'now', 'as', 'i', 'feel', 'i', 'have', 'reached', 'a', 'critical', 'point', '.', 'i', 'seem', 'to', 'have', 'symptoms', 'of', 'depression', 'and', 'ad', '##hd', '-', 'pi', ',', 'basically', 'i', 'feel', 'dread', 'and', 'when', 'i', 'try', 'to', 'do', 'something', 'or', 'think', 'about', 'something', 'my', 'line', 'of', 'thought', 'gets', 'interruption', '##s', 'as', 'it', 'always', 'has', '.', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'fix', 'this', 'so', 'i', 'am', 'going', 'to', 'seek', 'medical', 'help', 'however', 'i', 'am', 'wondering', 'if', 'i', 'get', 'prescribed', 'pills', 'for', 'depression', 'it', 'will', 'also', 'aid', 'me', 'in', 'my', 'concentration', 'issues', 'and', 'collecting', 'my', 'thoughts', 'in', 'an', 'orderly', 'manner', '?', 'my', 'brain', 'just', 'feels', 'so', 'scattered', 'and', 'slow', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', 'am', 'i', 'still', 'stimulated', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', 'am', 'i', 'still', 'stimulated', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['in', 'blur', 'of', 'a', '.', 'd', '.', 'h', '.', 'd', '.', ',', 'sleep', 'troubles', 'may', 'be', 'a', 'cu', '##lp', '##rit', '-', 'ny', '##time', '##s', '.', 'com']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['in', 'blur', 'of', 'a', '.', 'd', '.', 'h', '.', 'd', '.', ',', 'sleep', 'troubles', 'may', 'be', 'a', 'cu', '##lp', '##rit', '-', 'ny', '##time', '##s', '.', 'com']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['did', 'med', '##s', 'improve', 'your', 'handwriting', '?', 'i', \"'\", 'm', 'embark', '##ing', 'on', 'the', 'whole', 'diagnosis', '/', 'med', '##s', 'thing', 'and', 'i', \"'\", 'm', 'curious', 'about', 'one', 'area', 'that', 'has', 'been', 'really', 'affected', 'by', 'my', 'add', ':', 'handwriting', '.', 'mine', 'is', 'terrible', ',', 'just', 'sc', '##ra', '##wl', '##y', 'and', 'ni', '##gh', 'un', '##rea', '##dable', 'unless', 'i', \"'\", 'm', 'really', 'trying', '.', 'i', \"'\", 'm', 'starting', 'a', 'career', 'that', 'en', '##tails', 'a', 'lot', 'of', 'charting', 'and', 'data', 'recording', ',', 'much', 'of', 'it', 'done', 'old', '-', 'school', 'style', '##e', 'on', 'sheets', 'of', 'paper', '(', '*', 'head', '##des', '##k', '*', ')', '.', 'in', 'college', 'and', 'since', ',', 'i', \"'\", 've', 'steered', 'clear', 'of', 'writing', 'by', 'hand', 'as', 'much', 'as', 'possible', ',', 'helped', 'of', 'course', 'by', 'the', 'prevalence', 'of', 'computers', ',', 'but', 'it', 'would', 'be', 'nice', 'to', 'feel', 'like', 'a', 'professional', 'adult', '^', 't', '##m', 'with', 'do', '##able', 'handwriting', 'skills', '.', 'did', 'med', '##s', 'help', 'you', 'with', 'handwriting', 'issues', 'at', 'all', '?', 'if', 'you', \"'\", 're', 'in', 'a', 'medical', '/', 'medical', '##ish', 'field', ',', 'how', 'big', 'of', 'a', 'deal', 'is', 'charting', 'and', 'handwriting', 'vis', 'a', 'vis', 'your', 'add', '?']\n",
      "INFO:__main__:Number of tokens: 185\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['did', 'med', '##s', 'improve', 'your', 'handwriting', '?', 'i', \"'\", 'm', 'embark', '##ing', 'on', 'the', 'whole', 'diagnosis', '/', 'med', '##s', 'thing', 'and', 'i', \"'\", 'm', 'curious', 'about', 'one', 'area', 'that', 'has', 'been', 'really', 'affected', 'by', 'my', 'add', ':', 'handwriting', '.', 'mine', 'is', 'terrible', ',', 'just', 'sc', '##ra', '##wl', '##y', 'and', 'ni', '##gh', 'un', '##rea', '##dable', 'unless', 'i', \"'\", 'm', 'really', 'trying', '.', 'i', \"'\", 'm', 'starting', 'a', 'career', 'that', 'en', '##tails', 'a', 'lot', 'of', 'charting', 'and', 'data', 'recording', ',', 'much', 'of', 'it', 'done', 'old', '-', 'school', 'style', '##e', 'on', 'sheets', 'of', 'paper', '(', '*', 'head', '##des', '##k', '*', ')', '.', 'in', 'college', 'and', 'since', ',', 'i', \"'\", 've', 'steered', 'clear', 'of', 'writing', 'by', 'hand', 'as', 'much', 'as', 'possible', ',', 'helped', 'of', 'course', 'by', 'the', 'prevalence', 'of', 'computers', ',', 'but', 'it', 'would', 'be', 'nice', 'to', 'feel', 'like', 'a', 'professional', 'adult', '^', 't', '##m', 'with', 'do', '##able', 'handwriting', 'skills', '.', 'did', 'med', '##s', 'help', 'you', 'with', 'handwriting', 'issues', 'at', 'all', '?', 'if', 'you', \"'\", 're', 'in', 'a', 'medical', '/', 'medical', '##ish', 'field', ',', 'how', 'big', 'of', 'a', 'deal', 'is', 'charting', 'and', 'handwriting', 'vis', 'a', 'vis', 'your', 'add', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'testing', 'i', 'have', 'reason', 'to', 'suspect', 'i', 'have', 'ad', '##hd', 'or', 'something', 'along', 'those', 'lines', '.', 'is', 'there', 'any', 'scientific', 'testing', 'to', 'determine', 'if', 'this', 'is', 'so', '?', 'are', 'ad', '##hd', 'diagnosis', \"'\", 's', 'entirely', 'made', 'based', 'on', 'patient', 'history', '?', 'i', \"'\", 'm', 'interested', 'in', 'being', 'tested', ',', 'but', 'i', \"'\", 'm', 'somewhat', 'sc', '##ept', '##ical', 'of', 'a', 'diagnosis', 'not', 'based', 'on', 'some', 'kind', 'of', 'objective', 'testing', '.', 'finally', ',', 'does', 'anyone', 'know', 'of', 'a', 'place', 'in', 'toronto', 'in', 'canada', 'that', 'does', 'this', 'kind', 'of', 'testing', '?', 'thanks']\n",
      "INFO:__main__:Number of tokens: 91\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'testing', 'i', 'have', 'reason', 'to', 'suspect', 'i', 'have', 'ad', '##hd', 'or', 'something', 'along', 'those', 'lines', '.', 'is', 'there', 'any', 'scientific', 'testing', 'to', 'determine', 'if', 'this', 'is', 'so', '?', 'are', 'ad', '##hd', 'diagnosis', \"'\", 's', 'entirely', 'made', 'based', 'on', 'patient', 'history', '?', 'i', \"'\", 'm', 'interested', 'in', 'being', 'tested', ',', 'but', 'i', \"'\", 'm', 'somewhat', 'sc', '##ept', '##ical', 'of', 'a', 'diagnosis', 'not', 'based', 'on', 'some', 'kind', 'of', 'objective', 'testing', '.', 'finally', ',', 'does', 'anyone', 'know', 'of', 'a', 'place', 'in', 'toronto', 'in', 'canada', 'that', 'does', 'this', 'kind', 'of', 'testing', '?', 'thanks']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sherlock', \"'\", 's', 'mind', 'palace', 'and', 'me', '.', 'now', ',', 'i', \"'\", 'm', 'a', 'huge', 'fan', 'of', 'sherlock', 'holmes', '.', 'the', 'books', ',', 'the', 'guy', 'ritchie', 'films', ',', 'the', 'granada', 'series', ',', 'all', 'of', 'it', '.', 'especially', 'the', 'bbc', 'modern', 'series', '\"', 'sherlock', '\"', '.', 'now', 'to', 'fill', 'you', 'in', 'mr', 'holmes', 'needs', 'to', 'think', 'and', 'so', 'all', 'the', 'key', 'elements', 'of', 'the', 'case', 'his', 'three', 'main', 'clues', ',', 'the', 'words', ';', 'liberty', ',', 'hound', 'and', 'inn', '.', 'so', 'he', 'enters', 'his', 'mind', 'palace', 'where', 'all', 'his', 'ideas', 'spin', 'and', 'zoom', 'and', 'go', 'woo', '##sh', '!', 'here', \"'\", 's', 'the', 'clip', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'v', '##mu', '##eus', '##r', '##j', '##rz', '##c', '&', 'feature', '=', 're', '##lm', '##fu', 'now', 'for', 'the', 'actual', 'part', 'about', 'ad', '##hd', ',', 'the', 'hardest', 'part', 'for', 'me', 'is', 'explaining', 'the', 'ins', '##de', 'of', 'my', 'head', 'to', 'people', ',', 'it', \"'\", 's', 'like', 'explaining', 'the', 'colour', 'blue', 'to', 'blind', 'person', '.', 'you', 'need', 'to', 'see', 'it', 'to', 'understand', 'it', '.', 'but', 'then', 'this', 'clip', 'came', 'along', 'and', 'i', 'watched', 'it', 'thinking', ',', 'that', \"'\", 's', 'my', 'fucking', 'brain', 'all', 'the', 'time', '.', 'someone', 'says', 'a', 'key', 'word', 'or', 'anything', 'my', 'brain', 'spins', 'round', 'everything', 'i', 'know', 'about', 'that', 'topic', '.', 'so', 'yeah', ',', 'i', 'didn', \"'\", 't', 'really', 'know', 'a', 'good', 'way', 'to', 'end', 'this', '.', 't', '##l', ';', 'dr', ':', 'i', 'think', 'sherlock', \"'\", 's', 'mind', 'palace', 'is', 'a', 'great', 'way', 'for', 'me', 'to', 'explain', 'what', 'having', 'ad', '##hd', 'is', 'like', '.', 'edit', ':', 'also', ',', 'this', 'clip', 'is', 'a', 'lot', 'like', 'my', 'usual', 'reaction', 'to', 'when', 'people', 'can', \"'\", 't', 'follow', 'my', 'train', 'of', 'thought', '.', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'fc', '##x', '##hed', '##n', '##w', '##2', '-', '0', '&', 'feature', '=', 're', '##lm', '##fu']\n",
      "INFO:__main__:Number of tokens: 311\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sherlock', \"'\", 's', 'mind', 'palace', 'and', 'me', '.', 'now', ',', 'i', \"'\", 'm', 'a', 'huge', 'fan', 'of', 'sherlock', 'holmes', '.', 'the', 'books', ',', 'the', 'guy', 'ritchie', 'films', ',', 'the', 'granada', 'series', ',', 'all', 'of', 'it', '.', 'especially', 'the', 'bbc', 'modern', 'series', '\"', 'sherlock', '\"', '.', 'now', 'to', 'fill', 'you', 'in', 'mr', 'holmes', 'needs', 'to', 'think', 'and', 'so', 'all', 'the', 'key', 'elements', 'of', 'the', 'case', 'his', 'three', 'main', 'clues', ',', 'the', 'words', ';', 'liberty', ',', 'hound', 'and', 'inn', '.', 'so', 'he', 'enters', 'his', 'mind', 'palace', 'where', 'all', 'his', 'ideas', 'spin', 'and', 'zoom', 'and', 'go', 'woo', '##sh', '!', 'here', \"'\", 's', 'the', 'clip', ':', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'v', '##mu', '##eus', '##r', '##j', '##rz', '##c', '&', 'feature', '=', 're', '##lm', '##fu', 'now', 'for', 'the', 'actual', 'part', 'about', 'ad', '##hd', ',', 'the', 'hardest', 'part', 'for', 'me', 'is', 'explaining', 'the', 'ins', '##de', 'of', 'my', 'head', 'to', 'people', ',', 'it', \"'\", 's', 'like', 'explaining', 'the', 'colour', 'blue', 'to', 'blind', 'person', '.', 'you', 'need', 'to', 'see', 'it', 'to', 'understand', 'it', '.', 'but', 'then', 'this', 'clip', 'came', 'along', 'and', 'i', 'watched', 'it', 'thinking', ',', 'that', \"'\", 's', 'my', 'fucking', 'brain', 'all', 'the', 'time', '.', 'someone', 'says', 'a', 'key', 'word', 'or', 'anything', 'my', 'brain', 'spins', 'round', 'everything', 'i', 'know', 'about', 'that', 'topic', '.', 'so', 'yeah', ',', 'i', 'didn', \"'\", 't', 'really', 'know', 'a', 'good', 'way', 'to', 'end', 'this', '.', 't', '##l', ';', 'dr', ':', 'i', 'think', 'sherlock', \"'\", 's', 'mind', 'palace', 'is', 'a', 'great', 'way', 'for', 'me', 'to', 'explain', 'what', 'having', 'ad', '##hd', 'is', 'like', '.', 'edit', ':', 'also', ',', 'this', 'clip', 'is', 'a', 'lot', 'like', 'my', 'usual', 'reaction', 'to', 'when', 'people', 'can', \"'\", 't', 'follow', 'my', 'train', 'of', 'thought', '.', 'http', ':', '/', '/', 'www', '.', 'youtube', '.', 'com', '/', 'watch', '?', 'v', '=', 'fc', '##x', '##hed', '##n', '##w', '##2', '-', '0', '&', 'feature', '=', 're', '##lm', '##fu']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 've', 'been', 'diagnosed', 'with', 'ad', '##hd', ',', 'but', 'i', \"'\", 've', 'never', 'been', 'hyper', '##active', ',', 'and', 'have', 'always', 'been', 'quiet', ',', 'calm', ',', 'and', 'organized', '.', 'am', 'i', 'the', 'only', 'one', '?', 'i', 'feel', 'anyone', 'i', \"'\", 've', 'ever', 'met', 'on', 'ad', '##hd', 'medication', 'was', 'bouncing', 'off', 'the', 'walls', 'without', 'their', 'med', '##s', '.', 'even', 'the', 'adults', 'that', 'i', \"'\", 've', 'met', 'were', 'generally', 'extremely', 'robust', 'and', 'loud', 'in', 'social', '##izing', 'with', 'them', '.', 'i', 'fit', 'none', 'of', 'these', 'characteristics', '.', 'i', 'am', 'quiet', ',', 'une', '##mot', '##ional', ',', 'neat', ',', 'and', 'organized', '.', 'i', 'dropped', 'out', 'of', 'college', 'due', 'to', 'problems', 'concentrating', ',', 'because', 'i', 'would', 'get', 'extremely', 'tired', 'after', '15', 'minutes', 'of', 'concentrating', ',', 'but', 'that', 'was', 'it', '.', '(', 'when', 'i', 'got', 'put', 'on', 'v', '##y', '##van', '##se', ',', 'i', 'went', 'back', 'and', 'began', 'making', 'honors', 'gp', '##a', ')', '.', 'when', 'off', 'v', '##y', '##van', '##se', ',', 'i', 'did', 'have', 'mood', 'swings', ',', 'but', 'i', 'never', 'let', 'anyone', 'see', 'them', 'and', 'if', 'i', 'lost', 'my', 'shit', ',', 'i', 'did', 'it', 'in', 'private', '.', 'when', 'i', 'discuss', 'having', 'ad', '##hd', 'with', 'certain', 'people', ',', 'they', 'always', 'see', 'that', 'they', 'don', \"'\", 't', 'see', 'me', 'as', 'as', 'fitting', 'the', 'ad', '##hd', 'bill', '.']\n",
      "INFO:__main__:Number of tokens: 207\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 've', 'been', 'diagnosed', 'with', 'ad', '##hd', ',', 'but', 'i', \"'\", 've', 'never', 'been', 'hyper', '##active', ',', 'and', 'have', 'always', 'been', 'quiet', ',', 'calm', ',', 'and', 'organized', '.', 'am', 'i', 'the', 'only', 'one', '?', 'i', 'feel', 'anyone', 'i', \"'\", 've', 'ever', 'met', 'on', 'ad', '##hd', 'medication', 'was', 'bouncing', 'off', 'the', 'walls', 'without', 'their', 'med', '##s', '.', 'even', 'the', 'adults', 'that', 'i', \"'\", 've', 'met', 'were', 'generally', 'extremely', 'robust', 'and', 'loud', 'in', 'social', '##izing', 'with', 'them', '.', 'i', 'fit', 'none', 'of', 'these', 'characteristics', '.', 'i', 'am', 'quiet', ',', 'une', '##mot', '##ional', ',', 'neat', ',', 'and', 'organized', '.', 'i', 'dropped', 'out', 'of', 'college', 'due', 'to', 'problems', 'concentrating', ',', 'because', 'i', 'would', 'get', 'extremely', 'tired', 'after', '15', 'minutes', 'of', 'concentrating', ',', 'but', 'that', 'was', 'it', '.', '(', 'when', 'i', 'got', 'put', 'on', 'v', '##y', '##van', '##se', ',', 'i', 'went', 'back', 'and', 'began', 'making', 'honors', 'gp', '##a', ')', '.', 'when', 'off', 'v', '##y', '##van', '##se', ',', 'i', 'did', 'have', 'mood', 'swings', ',', 'but', 'i', 'never', 'let', 'anyone', 'see', 'them', 'and', 'if', 'i', 'lost', 'my', 'shit', ',', 'i', 'did', 'it', 'in', 'private', '.', 'when', 'i', 'discuss', 'having', 'ad', '##hd', 'with', 'certain', 'people', ',', 'they', 'always', 'see', 'that', 'they', 'don', \"'\", 't', 'see', 'me', 'as', 'as', 'fitting', 'the', 'ad', '##hd', 'bill', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'eat', 'non', '-', 'breakfast', 'foods', 'for', 'breakfast', '?', 'is', 'this', 'an', 'ad', '##hd', 'thing', '?']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'eat', 'non', '-', 'breakfast', 'foods', 'for', 'breakfast', '?', 'is', 'this', 'an', 'ad', '##hd', 'thing', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['/', 'r', '/', 'ad', '##hd', '!', 'i', 'had', 'success', '!', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '-', 'i', 'last', 'year', ',', 'and', 'due', 'to', 'some', 'bad', 'side', 'effects', ',', 'am', 'not', 'currently', 'able', 'to', 'be', 'on', 'drugs', '.', 'because', 'of', 'this', ',', 'i', \"'\", 've', 'had', 'great', 'difficulty', 'in', 'my', 'calculus', '-', 'based', 'physics', 'class', '.', 'i', 'want', 'to', 'pay', 'attention', '(', 'and', 'in', 'the', 'moments', 'i', 'do', ',', 'i', 'find', 'myself', 'very', 'interested', 'in', 'what', 'the', 'professor', 'is', 'saying', ')', ',', 'but', 'for', 'some', 'reason', 'cannot', 'stay', 'focused', 'in', 'class', ',', 'or', 'on', 'the', 'homework', ',', 'or', '(', 'and', 'this', 'is', 'the', 'worst', 'bit', ')', 'on', 'studying', 'for', 'the', 'tests', '.', 'well', ',', 'i', 'have', 'a', 'test', 'on', 'monday', ',', 'and', 'as', 'usual', ',', 'i', 'sat', 'down', 'at', 'my', 'computer', 'to', 'study', ',', 'and', 'ended', 'up', 'on', 'red', '##dit', '.', 'i', 'read', 'the', 'story', 'on', 'the', 'front', 'page', 'about', 'how', 'sitting', 'down', 'may', 'make', 'you', 'more', 'likely', 'to', 'die', ',', 'and', 'thought', 'to', 'myself', ',', '\"', 'man', ',', 'i', 'sit', 'down', 'a', 'lot', ',', 'i', \"'\", 'm', 'going', 'to', 'try', 'standing', '.', '\"', 'so', 'i', 'stood', '.', 'soon', 'i', 'got', 'off', 'of', 'red', '##dit', ',', 'and', 'surprisingly', ',', 'find', 'myself', 'willing', 'to', 'study', 'physics', '.', 'i', 'alternate', '##d', 'between', 'sitting', 'down', 'and', 'standing', '(', 'i', 'didn', \"'\", 't', 'really', 'keep', 'track', ';', 'i', 'just', 'found', 'myself', 'changing', 'position', 'every', 'so', 'often', ')', 'and', 'three', 'hours', 'passed', 'without', 'me', 'noticing', 'the', 'time', '!', 'i', 'also', 'didn', \"'\", 't', 'become', 'frustrated', 'when', 'i', 'wasn', \"'\", 't', 'able', 'to', 'figure', 'out', 'something', 'within', '10', 'minutes', ',', 'and', 'calmly', 'learned', 'how', 'to', 'work', 'with', 'the', 'bio', '##t', '-', 'sava', '##rt', 'law', ',', 'as', 'well', 'as', 'amp', '##her', '##e', \"'\", 's', ',', 'which', 'i', \"'\", 've', 'never', 'understood', 'before', '.', 'this', 'is', 'the', 'first', 'time', 'that', 'studying', 'physics', 'has', 'ever', 'been', 'easy', '(', 'and', 'even', 'enjoyable', 'for', 'me', ')', ',', 'and', 'so', 'i', 'felt', 'like', 'sharing', 'with', 'people', 'i', 'felt', 'could', 'appreciate', '(', 'and', 'possibly', 'benefit', 'from', '?', ')', 'this', 'random', 'accomplishment', '.', 'i', 'feel', 'like', 'it', \"'\", 's', 'a', 'little', 'weird', 'that', 'it', \"'\", 's', 'standing', 'that', 'works', 'for', 'me', ',', 'but', 'i', 'suppose', 'that', \"'\", 's', 'why', 'i', 'do', 'so', 'well', 'in', 'art', 'when', 'i', \"'\", 'm', 'standing', 'at', 'an', 'ease', '##l', '.', 'has', 'anybody', 'else', 'noticed', 'weird', 'effects', 'like', 'this', 'when', 'you', 'stand', 'while', 'working', 'on', 'something', '?', 't', '##l', ';', 'dr', ':', 'i', 'don', \"'\", 't', 'usually', 'study', 'physics', ',', 'but', 'when', 'i', 'stand', ',', 'suddenly', 'everything', 'makes', 'sense', '.']\n",
      "INFO:__main__:Number of tokens: 420\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['/', 'r', '/', 'ad', '##hd', '!', 'i', 'had', 'success', '!', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', '-', 'i', 'last', 'year', ',', 'and', 'due', 'to', 'some', 'bad', 'side', 'effects', ',', 'am', 'not', 'currently', 'able', 'to', 'be', 'on', 'drugs', '.', 'because', 'of', 'this', ',', 'i', \"'\", 've', 'had', 'great', 'difficulty', 'in', 'my', 'calculus', '-', 'based', 'physics', 'class', '.', 'i', 'want', 'to', 'pay', 'attention', '(', 'and', 'in', 'the', 'moments', 'i', 'do', ',', 'i', 'find', 'myself', 'very', 'interested', 'in', 'what', 'the', 'professor', 'is', 'saying', ')', ',', 'but', 'for', 'some', 'reason', 'cannot', 'stay', 'focused', 'in', 'class', ',', 'or', 'on', 'the', 'homework', ',', 'or', '(', 'and', 'this', 'is', 'the', 'worst', 'bit', ')', 'on', 'studying', 'for', 'the', 'tests', '.', 'well', ',', 'i', 'have', 'a', 'test', 'on', 'monday', ',', 'and', 'as', 'usual', ',', 'i', 'sat', 'down', 'at', 'my', 'computer', 'to', 'study', ',', 'and', 'ended', 'up', 'on', 'red', '##dit', '.', 'i', 'read', 'the', 'story', 'on', 'the', 'front', 'page', 'about', 'how', 'sitting', 'down', 'may', 'make', 'you', 'more', 'likely', 'to', 'die', ',', 'and', 'thought', 'to', 'myself', ',', '\"', 'man', ',', 'i', 'sit', 'down', 'a', 'lot', ',', 'i', \"'\", 'm', 'going', 'to', 'try', 'standing', '.', '\"', 'so', 'i', 'stood', '.', 'soon', 'i', 'got', 'off', 'of', 'red', '##dit', ',', 'and', 'surprisingly', ',', 'find', 'myself', 'willing', 'to', 'study', 'physics', '.', 'i', 'alternate', '##d', 'between', 'sitting', 'down', 'and', 'standing', '(', 'i', 'didn', \"'\", 't', 'really', 'keep', 'track', ';', 'i', 'just', 'found', 'myself', 'changing', 'position', 'every', 'so', 'often', ')', 'and', 'three', 'hours', 'passed', 'without', 'me', 'noticing', 'the', 'time', '!', 'i', 'also', 'didn', \"'\", 't', 'become', 'frustrated', 'when', 'i', 'wasn', \"'\", 't', 'able', 'to', 'figure', 'out', 'something', 'within', '10', 'minutes', ',', 'and', 'calmly', 'learned', 'how', 'to', 'work', 'with', 'the', 'bio', '##t', '-', 'sava', '##rt', 'law', ',', 'as', 'well', 'as', 'amp', '##her', '##e', \"'\", 's', ',', 'which', 'i', \"'\", 've', 'never', 'understood', 'before', '.', 'this', 'is', 'the', 'first', 'time', 'that', 'studying', 'physics', 'has', 'ever', 'been', 'easy', '(', 'and', 'even', 'enjoyable', 'for', 'me', ')', ',', 'and', 'so', 'i', 'felt', 'like', 'sharing', 'with', 'people', 'i', 'felt', 'could', 'appreciate', '(', 'and', 'possibly', 'benefit', 'from', '?', ')', 'this', 'random', 'accomplishment', '.', 'i', 'feel', 'like', 'it', \"'\", 's', 'a', 'little', 'weird', 'that', 'it', \"'\", 's', 'standing', 'that', 'works', 'for', 'me', ',', 'but', 'i', 'suppose', 'that', \"'\", 's', 'why', 'i', 'do', 'so', 'well', 'in', 'art', 'when', 'i', \"'\", 'm', 'standing', 'at', 'an', 'ease', '##l', '.', 'has', 'anybody', 'else', 'noticed', 'weird', 'effects', 'like', 'this', 'when', 'you', 'stand', 'while', 'working', 'on', 'something', '?', 't', '##l', ';', 'dr', ':', 'i', 'don', \"'\", 't', 'usually', 'study', 'physics', ',', 'but', 'when', 'i', 'stand', ',', 'suddenly', 'everything', 'makes', 'sense', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'notice', 'that', 'they', \"'\", 're', 'worse', 'at', 'multi', '-', 'task', '##ing', 'on', 'med', '##s', '?', 'i', 'know', 'ad', '##hd', 'people', 'aren', \"'\", 't', 'supposed', 'to', 'actually', 'be', 'able', 'to', 'multi', '-', 'task', 'well', 'but', 'i', \"'\", 've', 'noticed', 'when', 'i', 'try', 'to', 'watch', 'a', 'tv', 'show', 'in', 'one', 'window', 'while', 'doing', 'something', 'else', 'in', 'another', 'i', 'completely', 'miss', 'what', \"'\", 's', 'going', 'on', 'in', 'the', 'show', '.', 'i', 'know', 'this', 'is', 'a', 'dumb', 'idea', 'in', 'the', 'first', 'place', 'but', 'it', \"'\", 's', 'been', 'a', 'habit', 'for', 'a', 'really', 'long', 'time', '.', 'before', 'i', 'started', 'med', '##s', 'i', 'would', 'do', 'this', 'all', 'the', 'time', 'but', 'i', 'would', 'still', 'know', 'what', 'was', 'going', 'on', 'in', 'the', 'show', 'while', 'surfing', 'the', 'web', '.']\n",
      "INFO:__main__:Number of tokens: 123\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'notice', 'that', 'they', \"'\", 're', 'worse', 'at', 'multi', '-', 'task', '##ing', 'on', 'med', '##s', '?', 'i', 'know', 'ad', '##hd', 'people', 'aren', \"'\", 't', 'supposed', 'to', 'actually', 'be', 'able', 'to', 'multi', '-', 'task', 'well', 'but', 'i', \"'\", 've', 'noticed', 'when', 'i', 'try', 'to', 'watch', 'a', 'tv', 'show', 'in', 'one', 'window', 'while', 'doing', 'something', 'else', 'in', 'another', 'i', 'completely', 'miss', 'what', \"'\", 's', 'going', 'on', 'in', 'the', 'show', '.', 'i', 'know', 'this', 'is', 'a', 'dumb', 'idea', 'in', 'the', 'first', 'place', 'but', 'it', \"'\", 's', 'been', 'a', 'habit', 'for', 'a', 'really', 'long', 'time', '.', 'before', 'i', 'started', 'med', '##s', 'i', 'would', 'do', 'this', 'all', 'the', 'time', 'but', 'i', 'would', 'still', 'know', 'what', 'was', 'going', 'on', 'in', 'the', 'show', 'while', 'surfing', 'the', 'web', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'can', 'i', 'convince', 'my', 'doctor', 'my', 'problems', 'are', 'from', 'add', '/', 'ad', '##hd', '?', 'i', \"'\", 've', 'been', 'seeing', 'this', 'guy', 'for', 'over', 'a', 'year', 'now', 'and', 'every', 'time', 'i', 'tell', 'him', 'an', 'anti', '##de', '##press', '##ant', 'either', 'isn', \"'\", 't', 'doing', 'anything', 'or', 'it', 'makes', 'me', 'feel', 'crazy', ',', 'he', 'just', 'gives', 'me', 'a', 'different', 'one', '.', 'i', 'told', 'him', 'i', 'think', 'my', 'problems', 'are', 'coming', 'from', 'not', 'being', 'able', 'to', 'focus', 'and', 'that', \"'\", 's', 'why', 'i', 'don', \"'\", 't', 'feel', 'like', 'doing', 'anything', '.', 'but', 'according', 'to', 'him', 'if', 'i', 'can', \"'\", 't', 'focus', 'it', 'is', 'because', 'of', 'depression', '.', 'has', 'any', 'body', 'had', 'troubles', 'with', 'this', '?', 'i', \"'\", 'm', 'going', 'absolutely', 'mad', 'and', 'feeling', 'worse', 'than', 'before', '.']\n",
      "INFO:__main__:Number of tokens: 124\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'can', 'i', 'convince', 'my', 'doctor', 'my', 'problems', 'are', 'from', 'add', '/', 'ad', '##hd', '?', 'i', \"'\", 've', 'been', 'seeing', 'this', 'guy', 'for', 'over', 'a', 'year', 'now', 'and', 'every', 'time', 'i', 'tell', 'him', 'an', 'anti', '##de', '##press', '##ant', 'either', 'isn', \"'\", 't', 'doing', 'anything', 'or', 'it', 'makes', 'me', 'feel', 'crazy', ',', 'he', 'just', 'gives', 'me', 'a', 'different', 'one', '.', 'i', 'told', 'him', 'i', 'think', 'my', 'problems', 'are', 'coming', 'from', 'not', 'being', 'able', 'to', 'focus', 'and', 'that', \"'\", 's', 'why', 'i', 'don', \"'\", 't', 'feel', 'like', 'doing', 'anything', '.', 'but', 'according', 'to', 'him', 'if', 'i', 'can', \"'\", 't', 'focus', 'it', 'is', 'because', 'of', 'depression', '.', 'has', 'any', 'body', 'had', 'troubles', 'with', 'this', '?', 'i', \"'\", 'm', 'going', 'absolutely', 'mad', 'and', 'feeling', 'worse', 'than', 'before', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['every', 'single', 'day', '.', 'and', 'then', 'i', 'fail', 'my', 'classes', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['every', 'single', 'day', '.', 'and', 'then', 'i', 'fail', 'my', 'classes', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', 'ad', '##hd', '\"', 'go', 'away', '\"', '?', 'i', 'know', 'there', 'are', 'several', 'temporal', 'treatments', 'for', 'ad', '##hd', 'but', 'are', 'there', 'any', 'documented', 'cases', 'where', 'people', 'were', '\"', 'cured', '\"', 'of', 'their', 'ad', '##hd', '##s', 'or', 'rather', 'stopped', 'having', 'do', '##pa', '##mine', 'deficiency', 'which', 'is', 'supposed', 'to', 'be', 'the', 'main', 'cause', 'of', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 55\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', 'ad', '##hd', '\"', 'go', 'away', '\"', '?', 'i', 'know', 'there', 'are', 'several', 'temporal', 'treatments', 'for', 'ad', '##hd', 'but', 'are', 'there', 'any', 'documented', 'cases', 'where', 'people', 'were', '\"', 'cured', '\"', 'of', 'their', 'ad', '##hd', '##s', 'or', 'rather', 'stopped', 'having', 'do', '##pa', '##mine', 'deficiency', 'which', 'is', 'supposed', 'to', 'be', 'the', 'main', 'cause', 'of', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'med', '##s', 'have', 'you', 'tried', '?', 'why', 'did', 'you', 'choose', 'to', 'move', 'on', 'to', 'a', 'different', 'one', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'med', '##s', 'have', 'you', 'tried', '?', 'why', 'did', 'you', 'choose', 'to', 'move', 'on', 'to', 'a', 'different', 'one', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['50', 'successful', 'people', 'with', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['50', 'successful', 'people', 'with', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', 'you', 'really', \"'\", 'channel', \"'\", 'your', 'ad', '##hd', 'into', 'something', 'positive', '?', 'and', 'if', 'so', ',', 'how', '?', 'there', \"'\", 's', 'a', 'post', 'about', 'some', 'successful', 'people', 'who', 'achieved', 'success', 'despite', 'their', 'ad', '##hd', 'by', 'supposedly', 'utilizing', 'it', 'to', 'achieve', 'their', 'success', '.', 'is', 'that', 'really', 'possible', '?', 'if', 'so', 'how', '?', 'what', 'are', 'the', 'aspects', 'of', 'ad', '##hd', 'that', 'you', 'can', 'utilize', 'to', 'contribute', 'to', 'your', 'success', '.', 'i', '.', 'e', '.', 'how', 'can', 'you', 'succeed', 'because', 'of', 'your', 'ad', '##hd', 'and', 'not', 'in', 'spite', 'of', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 90\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', 'you', 'really', \"'\", 'channel', \"'\", 'your', 'ad', '##hd', 'into', 'something', 'positive', '?', 'and', 'if', 'so', ',', 'how', '?', 'there', \"'\", 's', 'a', 'post', 'about', 'some', 'successful', 'people', 'who', 'achieved', 'success', 'despite', 'their', 'ad', '##hd', 'by', 'supposedly', 'utilizing', 'it', 'to', 'achieve', 'their', 'success', '.', 'is', 'that', 'really', 'possible', '?', 'if', 'so', 'how', '?', 'what', 'are', 'the', 'aspects', 'of', 'ad', '##hd', 'that', 'you', 'can', 'utilize', 'to', 'contribute', 'to', 'your', 'success', '.', 'i', '.', 'e', '.', 'how', 'can', 'you', 'succeed', 'because', 'of', 'your', 'ad', '##hd', 'and', 'not', 'in', 'spite', 'of', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['quarter', 'system', 'vs', '.', 'semester', 'system', 'i', 'am', 'in', 'my', 'second', 'semester', 'of', 'nursing', 'school', '.', 'i', 'am', 'doing', 'ok', ',', 'but', 'not', 'nearly', 'as', 'well', 'as', 'i', 'did', 'at', 'my', 'previous', 'college', '.', 'it', 'was', 'on', 'quarters', ',', 'so', 'classes', 'were', 'only', '~', '1', 'hr', 'and', 'multiple', 'times', 'a', 'week', '.', 'i', 'got', 'strait', 'as', '.', 'nursing', 'school', 'is', 'on', 'semester', '##s', '.', 'most', 'classes', 'are', '2', 'hours', 'and', 'only', 'once', 'a', 'week', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'it', 'is', ',', 'after', 'like', '20', 'minutes', 'i', 'am', 'zone', '##d', 'out', '.', 'my', 'longest', 'class', 'is', '3', 'hours', ',', 'it', 'is', 'tour', '##ture', '.', 'when', 'i', 'go', 'to', 'study', 'and', 'realize', 'how', 'much', 'info', 'i', 'have', 'to', 'go', 'over', 'my', 'mind', 'just', 'shut', '##s', 'off', '.', 'has', 'anyone', 'else', 'noticed', 'that', 'grades', '/', 'performance', 'is', 'better', 'at', 'quarter', 'schools', 'compared', 'to', 'semester', '?', 'and', 'what', 'can', 'you', 'do', 'in', 'these', 'long', 'classes', 'to', 'pay', 'attention', '?', 'i', 'sit', 'near', 'the', 'front', '.']\n",
      "INFO:__main__:Number of tokens: 164\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['quarter', 'system', 'vs', '.', 'semester', 'system', 'i', 'am', 'in', 'my', 'second', 'semester', 'of', 'nursing', 'school', '.', 'i', 'am', 'doing', 'ok', ',', 'but', 'not', 'nearly', 'as', 'well', 'as', 'i', 'did', 'at', 'my', 'previous', 'college', '.', 'it', 'was', 'on', 'quarters', ',', 'so', 'classes', 'were', 'only', '~', '1', 'hr', 'and', 'multiple', 'times', 'a', 'week', '.', 'i', 'got', 'strait', 'as', '.', 'nursing', 'school', 'is', 'on', 'semester', '##s', '.', 'most', 'classes', 'are', '2', 'hours', 'and', 'only', 'once', 'a', 'week', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'it', 'is', ',', 'after', 'like', '20', 'minutes', 'i', 'am', 'zone', '##d', 'out', '.', 'my', 'longest', 'class', 'is', '3', 'hours', ',', 'it', 'is', 'tour', '##ture', '.', 'when', 'i', 'go', 'to', 'study', 'and', 'realize', 'how', 'much', 'info', 'i', 'have', 'to', 'go', 'over', 'my', 'mind', 'just', 'shut', '##s', 'off', '.', 'has', 'anyone', 'else', 'noticed', 'that', 'grades', '/', 'performance', 'is', 'better', 'at', 'quarter', 'schools', 'compared', 'to', 'semester', '?', 'and', 'what', 'can', 'you', 'do', 'in', 'these', 'long', 'classes', 'to', 'pay', 'attention', '?', 'i', 'sit', 'near', 'the', 'front', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'to', 'f', '*', 'ck', '##ing', 'decide', 'on', 'a', 'career', 'path', '?', 'since', 'i', 'finished', 'my', 'phd', 'thesis', '(', 'literature', '-', 'i', 'know', '!', ')', ',', 'i', 'am', 'living', 'in', 'limb', '##o', '.', 'meanwhile', ',', 'life', 'happened', '.', 'i', 'am', '34', 'years', 'old', ',', 'single', ',', 'diagnosed', 'with', 'add', 'and', '(', 'recently', ')', 'a', 'depression', ',', 'soon', 'with', 'that', 'phd', 'title', ',', 'but', 'i', '\"', 'still', 'haven', \"'\", 't', 'found', 'what', 'i', \"'\", 'm', 'looking', 'for', '\"', '.', 'i', 'doubt', 'that', 'i', 'still', 'have', 'the', 'chance', 'to', 'become', 'a', 'professor', ',', 'i', 'am', 'not', 'even', 'excited', 'enough', 'to', 'go', 'for', 'it', '.', 'i', 'feel', 'that', 'at', 'this', 'point', 'in', 'my', 'life', ',', 'i', 'want', 'to', 'find', 'something', 'durable', 'and', 'become', 'more', 'stable', '.', 'the', 'people', 'around', 'me', 'expect', 'me', 'to', 'either', 'go', 'for', 'it', 'and', 'commit', 'to', 'a', 'poor', 'but', 'interesting', 'life', ',', 'while', 'others', 'want', 'me', 'to', 'drop', 'it', 'all', ',', 'start', 'something', 'predictable', ',', 'have', 'a', 'family', 'and', 'so', 'on', '.', 'i', 'want', 'both', ':', 'an', 'interesting', 'life', 'and', 'financial', 'stability', '.', 'every', 'day', 'i', 'read', 'through', 'job', 'advertisements', ',', 'but', 'i', 'always', 'see', 'many', 'reasons', 'not', 'to', 'apply', 'at', 'all', '.', 'this', 'cannot', 'go', 'on', 'like', 'this', '.', 'if', 'you', 'have', 'any', 'advice', 'on', 'how', 'to', 'pick', 'a', 'carr', '##eer', ',', 'it', 'would', 'be', 'much', 'appreciated', '.', 'i', 'know', 'this', 'is', 'a', 'luxury', 'problem', ':', 'too', 'many', 'things', 'to', 'chose', 'from', '.', 'but', 'how', '?', 'current', 'problems', ':', '*', 'nearly', 'unemployed', '(', 'side', 'job', 'as', 'online', 'editor', ')', '*', 'nearly', 'broke', '(', 'debt', 'ceiling', 'reached', 'in', 'about', 'a', 'year', 'max', ')', '*', 'mild', 'depression', '(', 'constantly', 'tired', ',', 'everything', 'is', '3', '##x', 'more', 'difficult', ')', '*', 'ins', '##ati', '##able', 'longing', 'to', 'write', 'stories', ',', 'no', 'matter', 'what', 'else', 'i', 'do', 'skills', ':', '*', 'able', 'to', 'add', 'meaning', 'to', 'anything', '(', 'analyzing', ',', 'interpreting', 'the', 'world', ')', '*', 'able', 'to', 'speak', 'in', 'public', 'without', 'shame', 'or', 'fear', '/', 'risk', 'take', '##r', '*', 'html', ',', 'cs', '##s', ',', 'joo', '##ml', '##a', ',', '-', 'a', 'certain', 'talent', 'about', 'things', 'digital', '*', 'knows', 'how', 'to', 'tell', 'or', 'deco', '##nst', '##ru', '##ct', 'stories', '*', 'good', 'at', 'explaining', 'complicated', 'things', 'simply', '*', 'interested', 'in', 'everything', ',', 'able', 'to', 'research', 'any', 'topic', ',', 'fast', 'my', 'problem', ':', 'i', 'am', 'looking', 'for', 'a', 'job', 'that', 'is', 'able', 'to', 'support', 'myself', 'in', 'the', 'long', 'run', ',', 'without', 'becoming', 'boring', '.', 'for', 'every', 'idea', 'i', 'have', ',', 'something', 'inside', 'me', 'tells', 'me', 'to', 'stop', 'reaching', 'for', 'it', '.', 'for', 'example', ',', 'my', 'current', 'ideas', 'for', 'a', 'career', ':', '*', 'getting', 'funded', 'as', 'a', 'scholar', '/', 'writer', '(', 'fear', ':', 'failure', 'likely', ',', 'won', \"'\", 't', 'get', 'out', 'of', 'debt', ',', 'might', 'have', 'to', 'relocate', '=', 'des', '##ta', '##bil', '##izing', ',', 'job', 'needs', '120', '%', 'time', 'and', 'attention', ',', 'no', 'time', 'for', 'writing', ')', '*', 'online', 'marketing', 'manager', '(', 'fear', ':', 'i', 'will', 'work', 'like', 'a', 'slave', 'all', 'day', 'on', 'boring', 'stuff', 'and', 'lose', 'sight', 'of', 'interesting', 'things', ',', 'material', '##istic', 'cow', '##or', '##kers', ',', 'lack', 'of', 'ideal', '##ism', ')', '*', 'freelance', 'web', '##des', '##ign', '##er', '/', 'tourist', 'guide', 'and', 'wat', '##not', '.', '(', 'fear', ':', 'low', 'pay', ',', 'lots', 'of', 'work', ',', 'depression', 'gets', 'worse', ')', '*', 'pr', '(', 'fear', ':', 'full', 'time', 'job', ',', 'boring', ',', 'no', 'time', 'for', 'literature', ',', 'media', 'manipulation', ')', '*', 'strategic', 'marketing', 'consultant', '(', 'fear', ':', 'full', 'time', 'job', ',', 'needs', 'more', 'passion', ',', 'thinking', 'about', 'brand', 'identity', 'all', 'day', '=', 'boring', ')', 'so', ',', 'what', 'do', '?', 'in', 'all', 'of', 'these', 'fields', ',', 'i', 'can', 'think', 'of', 'positive', 'factors', 'as', 'well', ',', 'but', 'i', 'am', 'not', 'really', 'interested', 'in', 'making', 'a', '\"', 'career', '\"', 'in', 'any', 'of', 'these', 'jobs', '.', 'i', 'am', 'looking', 'for', 'something', 'where', 'i', 'can', 'pay', 'my', 'bills', ',', 'work', 'with', 'nice', 'people', ',', 'spend', 'my', 'time', 'with', 'something', 'worth', '##wil', '##e', ',', 'have', 'some', 'free', 'time', 'for', 'own', 'writing', 'projects', '.', 'i', 'know', 'i', 'shouldn', \"'\", 't', 'tell', 'anybody', ',', 'but', 'in', 'some', 'fields', ',', 'people', 'might', 'be', 'more', 'open', 'to', 'me', 'have', '##ing', 'side', 'projects', 'that', 'matter', 'to', 'me', '.', 't', '##l', ';', 'dr', ':', 'too', 'many', 'things', 'to', 'choose', 'from', ',', 'a', 'lot', 'of', 'fear', ',', 'feeling', 'tired', 'and', 'depressed', '.', '*', '*', 'this', 'was', 'already', 'helpful', '.', 'thanks', 'for', 'your', 'time', '!', '*', '*']\n",
      "INFO:__main__:Number of tokens: 711\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['how', 'to', 'f', '*', 'ck', '##ing', 'decide', 'on', 'a', 'career', 'path', '?', 'since', 'i', 'finished', 'my', 'phd', 'thesis', '(', 'literature', '-', 'i', 'know', '!', ')', ',', 'i', 'am', 'living', 'in', 'limb', '##o', '.', 'meanwhile', ',', 'life', 'happened', '.', 'i', 'am', '34', 'years', 'old', ',', 'single', ',', 'diagnosed', 'with', 'add', 'and', '(', 'recently', ')', 'a', 'depression', ',', 'soon', 'with', 'that', 'phd', 'title', ',', 'but', 'i', '\"', 'still', 'haven', \"'\", 't', 'found', 'what', 'i', \"'\", 'm', 'looking', 'for', '\"', '.', 'i', 'doubt', 'that', 'i', 'still', 'have', 'the', 'chance', 'to', 'become', 'a', 'professor', ',', 'i', 'am', 'not', 'even', 'excited', 'enough', 'to', 'go', 'for', 'it', '.', 'i', 'feel', 'that', 'at', 'this', 'point', 'in', 'my', 'life', ',', 'i', 'want', 'to', 'find', 'something', 'durable', 'and', 'become', 'more', 'stable', '.', 'the', 'people', 'around', 'me', 'expect', 'me', 'to', 'either', 'go', 'for', 'it', 'and', 'commit', 'to', 'a', 'poor', 'but', 'interesting', 'life', ',', 'while', 'others', 'want', 'me', 'to', 'drop', 'it', 'all', ',', 'start', 'something', 'predictable', ',', 'have', 'a', 'family', 'and', 'so', 'on', '.', 'i', 'want', 'both', ':', 'an', 'interesting', 'life', 'and', 'financial', 'stability', '.', 'every', 'day', 'i', 'read', 'through', 'job', 'advertisements', ',', 'but', 'i', 'always', 'see', 'many', 'reasons', 'not', 'to', 'apply', 'at', 'all', '.', 'this', 'cannot', 'go', 'on', 'like', 'this', '.', 'if', 'you', 'have', 'any', 'advice', 'on', 'how', 'to', 'pick', 'a', 'carr', '##eer', ',', 'it', 'would', 'be', 'much', 'appreciated', '.', 'i', 'know', 'this', 'is', 'a', 'luxury', 'problem', ':', 'too', 'many', 'things', 'to', 'chose', 'from', '.', 'but', 'how', '?', 'current', 'problems', ':', '*', 'nearly', 'unemployed', '(', 'side', 'job', 'as', 'online', 'editor', ')', '*', 'nearly', 'broke', '(', 'debt', 'ceiling', 'reached', 'in', 'about', 'a', 'year', 'max', ')', '*', 'mild', 'depression', '(', 'constantly', 'tired', ',', 'everything', 'is', '3', '##x', 'more', 'difficult', ')', '*', 'ins', '##ati', '##able', 'longing', 'to', 'write', 'stories', ',', 'no', 'matter', 'what', 'else', 'i', 'do', 'skills', ':', '*', 'able', 'to', 'add', 'meaning', 'to', 'anything', '(', 'analyzing', ',', 'interpreting', 'the', 'world', ')', '*', 'able', 'to', 'speak', 'in', 'public', 'without', 'shame', 'or', 'fear', '/', 'risk', 'take', '##r', '*', 'html', ',', 'cs', '##s', ',', 'joo', '##ml', '##a', ',', '-', 'a', 'certain', 'talent', 'about', 'things', 'digital', '*', 'knows', 'how', 'to', 'tell', 'or', 'deco', '##nst', '##ru', '##ct', 'stories', '*', 'good', 'at', 'explaining', 'complicated', 'things', 'simply', '*', 'interested', 'in', 'everything', ',', 'able', 'to', 'research', 'any', 'topic', ',', 'fast', 'my', 'problem', ':', 'i', 'am', 'looking', 'for', 'a', 'job', 'that', 'is', 'able', 'to', 'support', 'myself', 'in', 'the', 'long', 'run', ',', 'without', 'becoming', 'boring', '.', 'for', 'every', 'idea', 'i', 'have', ',', 'something', 'inside', 'me', 'tells', 'me', 'to', 'stop', 'reaching', 'for', 'it', '.', 'for', 'example', ',', 'my', 'current', 'ideas', 'for', 'a', 'career', ':', '*', 'getting', 'funded', 'as', 'a', 'scholar', '/', 'writer', '(', 'fear', ':', 'failure', 'likely', ',', 'won', \"'\", 't', 'get', 'out', 'of', 'debt', ',', 'might', 'have', 'to', 'relocate', '=', 'des', '##ta', '##bil', '##izing', ',', 'job', 'needs', '120', '%', 'time', 'and', 'attention', ',', 'no', 'time', 'for', 'writing', ')', '*', 'online', 'marketing', 'manager', '(', 'fear', ':', 'i', 'will', 'work', 'like', 'a', 'slave', 'all', 'day', 'on', 'boring', 'stuff', 'and', 'lose', 'sight', 'of', 'interesting', 'things', ',', 'material', '##istic', 'cow', '##or', '##kers', ',', 'lack', 'of', 'ideal', '##ism', ')', '*', 'freelance', 'web', '##des', '##ign', '##er', '/', 'tourist', 'guide', 'and'], ['wat', '##not', '.', '(', 'fear', ':', 'low', 'pay', ',', 'lots', 'of', 'work', ',', 'depression', 'gets', 'worse', ')', '*', 'pr', '(', 'fear', ':', 'full', 'time', 'job', ',', 'boring', ',', 'no', 'time', 'for', 'literature', ',', 'media', 'manipulation', ')', '*', 'strategic', 'marketing', 'consultant', '(', 'fear', ':', 'full', 'time', 'job', ',', 'needs', 'more', 'passion', ',', 'thinking', 'about', 'brand', 'identity', 'all', 'day', '=', 'boring', ')', 'so', ',', 'what', 'do', '?', 'in', 'all', 'of', 'these', 'fields', ',', 'i', 'can', 'think', 'of', 'positive', 'factors', 'as', 'well', ',', 'but', 'i', 'am', 'not', 'really', 'interested', 'in', 'making', 'a', '\"', 'career', '\"', 'in', 'any', 'of', 'these', 'jobs', '.', 'i', 'am', 'looking', 'for', 'something', 'where', 'i', 'can', 'pay', 'my', 'bills', ',', 'work', 'with', 'nice', 'people', ',', 'spend', 'my', 'time', 'with', 'something', 'worth', '##wil', '##e', ',', 'have', 'some', 'free', 'time', 'for', 'own', 'writing', 'projects', '.', 'i', 'know', 'i', 'shouldn', \"'\", 't', 'tell', 'anybody', ',', 'but', 'in', 'some', 'fields', ',', 'people', 'might', 'be', 'more', 'open', 'to', 'me', 'have', '##ing', 'side', 'projects', 'that', 'matter', 'to', 'me', '.', 't', '##l', ';', 'dr', ':', 'too', 'many', 'things', 'to', 'choose', 'from', ',', 'a', 'lot', 'of', 'fear', ',', 'feeling', 'tired', 'and', 'depressed', '.', '*', '*', 'this', 'was', 'already', 'helpful', '.', 'thanks', 'for', 'your', 'time', '!', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 've', 'always', 'hated', 'exercise', 'no', 'matter', 'how', 'long', 'i', 'do', 'it', ',', 'what', 'type', 'it', 'is', ',', 'or', 'who', 'i', 'do', 'it', 'with', '.', 'the', 'only', 'aspect', 'of', 'it', 'i', \"'\", 've', 'enjoyed', 'is', 'the', 'social', 'aspect', 'when', 'i', 'ran', 'track', '.', 'i', \"'\", 've', 'never', 'felt', 'any', 'kind', 'of', 'stress', '-', 'release', 'or', 'relaxing', 'aspect', 'from', 'it', '.', 'is', 'there', 'something', 'wrong', 'with', 'me', '?']\n",
      "INFO:__main__:Number of tokens: 68\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 've', 'always', 'hated', 'exercise', 'no', 'matter', 'how', 'long', 'i', 'do', 'it', ',', 'what', 'type', 'it', 'is', ',', 'or', 'who', 'i', 'do', 'it', 'with', '.', 'the', 'only', 'aspect', 'of', 'it', 'i', \"'\", 've', 'enjoyed', 'is', 'the', 'social', 'aspect', 'when', 'i', 'ran', 'track', '.', 'i', \"'\", 've', 'never', 'felt', 'any', 'kind', 'of', 'stress', '-', 'release', 'or', 'relaxing', 'aspect', 'from', 'it', '.', 'is', 'there', 'something', 'wrong', 'with', 'me', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['\"', 'conditional', 'acceptance', '\"', 'meeting', 'help', 'needed', 'so', 'a', 'little', 'back', 'story', '.', 'i', 'am', 'currently', 'an', 'early', 'childhood', 'ed', 'major', '.', 'last', 'semester', 'was', 'the', 'semester', 'i', 'was', 'introduced', 'to', 'the', 'lab', 'preschool', '.', 'so', 'my', 'thinking', 'was', 'to', 'be', 'a', 'fun', 'and', 'happy', 'person', 'was', 'to', 'go', 'and', 'take', 'my', 'v', '##y', '##van', '##se', 'as', 'needed', '.', 'this', 'was', 'not', 'good', '.', 'the', 'constant', 'going', 'on', 'and', 'off', 'the', 'med', '##s', 'made', 'me', 'very', 'ir', '##rita', '##ble', 'and', 'was', 'unable', 'to', 'demonstrate', 'the', 'person', 'they', 'needed', '(', 'they', 'were', 'looking', 'for', 'bright', 'and', 'bubble', 'people', ')', '.', 'the', 'classroom', 'had', '6', 'teachers', 'in', 'the', 'room', ',', 'and', 'i', 'was', 'pushed', 'to', 'the', 'side', '.', 'instead', 'of', 'making', 'the', 'effort', 'to', 'be', 'included', ',', 'i', 'excluded', 'myself', '.', 'this', 'lead', 'to', 'depression', '.', 'i', 'was', 'thousands', 'of', 'miles', 'from', 'home', 'and', 'i', 'let', 'it', 'bother', 'me', '.', 'so', 'i', 'email', '##ed', 'the', 'teacher', 'in', 'charge', 'asking', 'her', 'if', 'i', 'could', 'observe', 'the', 'children', 'instead', 'of', 'being', 'with', 'them', 'just', 'for', 'one', 'day', '.', 'she', 'told', 'me', 'no', 'and', 'email', '##ed', 'my', 'adviser', 'about', 'her', 'doubts', 'about', 'me', '.', '.', 'following', 'effect', '##ed', 'another', 'class', 'where', 'i', 'had', 'to', 'do', 'some', 'sort', 'of', 'culture', 'awareness', 'i', 'had', 'an', 'anxiety', 'attack', '.', 'this', 'lead', 'that', 'professor', 'email', '##ing', 'my', 'adviser', '.', 'i', 'had', 'two', 'professors', 'email', 'my', 'adviser', 'last', 'semester', '.', 'fast', 'forward', 'to', 'feb', '.', 'i', 'was', 'meeting', 'with', 'my', 'adviser', 'and', 'another', 'professor', 'about', 'transportation', 'issues', 'to', 'an', 'off', 'campus', 'lab', '.', 'it', 'was', 'there', 'i', 'was', 'told', 'that', 'i', 'might', 'not', 'be', 'admitted', 'to', '\"', 'methods', '\"', '.', 'this', 'blow', 'was', 'the', 'worst', 'feeling', 'i', 'ever', 'felt', 'because', 'of', 'the', 'two', 'emails', '(', 'i', 'should', 'have', 'been', 'told', 'of', 'this', 'last', 'semester', 'but', 'that', 'is', 'another', 'story', ')', '.', 'the', 'adviser', 'told', 'me', 'that', 'she', 'had', 'two', 'professors', 'email', 'her', '.', 'i', 'asked', 'her', 'my', 'options', '.', 'she', 'tried', 'to', 'get', 'me', 'to', 'switch', 'majors', ',', 'i', 'said', 'no', 'what', 'are', 'my', 'options', '.', 'she', 'said', 'to', 'transfer', 'and', 'apply', 'to', 'the', 'program', 'just', 'in', 'case', '.', 'so', 'i', 'got', 'into', 'a', 'school', 'closer', 'to', 'home', 'and', 'submitted', 'my', 'application', 'to', 'the', 'program', 'here', '##with', 'a', 'letter', 'explaining', 'my', 'situation', '.', 'now', ',', 'the', 'committee', 'wants', 'to', 'meet', 'with', 'me', 'to', 'discuss', 'conditional', 'acceptance', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'expect', '.', '.', '.', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'bash', '##ed', 'nor', 'do', 'i', 'want', 'the', 'conditions', 'to', 'be', 'like', '\"', 'get', 'on', 'anti', 'de', '##press', '##ants', '\"', '.', 'i', 'accepted', 'the', 'fact', 'that', 'it', 'was', 'a', 'long', 'shot', 'of', 'me', 'getting', 'in', '.', 'i', 'was', 'upset', 'that', 'this', 'happened', '.', 'i', 'don', \"'\", 't', 'know', 'how', 'i', 'will', 'be', 'able', 'to', 'control', 'my', 'issues', 'or', 'if', 'i', 'even', 'want', 'to', 'keep', 'going', 'here', '.', 'it', 'has', 'been', 'a', 'long', 'road', 'that', 'i', 'feel', 'like', 'no', 'one', 'is', 'fighting', 'for', 'me', 'and', 'i', 'have', 'to', 'battle', 'this', 'school', '.', 'i', 'mean', 'if', 'one', 'person', 'reached', 'out', 'maybe', 'i', 'would', 'have', 'had', 'a', 'better', 'semester', '.', 'what', 'can', 'i', 'expect', 'for', 'a', 'meeting', 'like', 'this', '?', 'i', 'have', 'never', 'been', 'in', 'a', 'situation', 'like', 'this', '.']\n",
      "INFO:__main__:Number of tokens: 532\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['\"', 'conditional', 'acceptance', '\"', 'meeting', 'help', 'needed', 'so', 'a', 'little', 'back', 'story', '.', 'i', 'am', 'currently', 'an', 'early', 'childhood', 'ed', 'major', '.', 'last', 'semester', 'was', 'the', 'semester', 'i', 'was', 'introduced', 'to', 'the', 'lab', 'preschool', '.', 'so', 'my', 'thinking', 'was', 'to', 'be', 'a', 'fun', 'and', 'happy', 'person', 'was', 'to', 'go', 'and', 'take', 'my', 'v', '##y', '##van', '##se', 'as', 'needed', '.', 'this', 'was', 'not', 'good', '.', 'the', 'constant', 'going', 'on', 'and', 'off', 'the', 'med', '##s', 'made', 'me', 'very', 'ir', '##rita', '##ble', 'and', 'was', 'unable', 'to', 'demonstrate', 'the', 'person', 'they', 'needed', '(', 'they', 'were', 'looking', 'for', 'bright', 'and', 'bubble', 'people', ')', '.', 'the', 'classroom', 'had', '6', 'teachers', 'in', 'the', 'room', ',', 'and', 'i', 'was', 'pushed', 'to', 'the', 'side', '.', 'instead', 'of', 'making', 'the', 'effort', 'to', 'be', 'included', ',', 'i', 'excluded', 'myself', '.', 'this', 'lead', 'to', 'depression', '.', 'i', 'was', 'thousands', 'of', 'miles', 'from', 'home', 'and', 'i', 'let', 'it', 'bother', 'me', '.', 'so', 'i', 'email', '##ed', 'the', 'teacher', 'in', 'charge', 'asking', 'her', 'if', 'i', 'could', 'observe', 'the', 'children', 'instead', 'of', 'being', 'with', 'them', 'just', 'for', 'one', 'day', '.', 'she', 'told', 'me', 'no', 'and', 'email', '##ed', 'my', 'adviser', 'about', 'her', 'doubts', 'about', 'me', '.', '.', 'following', 'effect', '##ed', 'another', 'class', 'where', 'i', 'had', 'to', 'do', 'some', 'sort', 'of', 'culture', 'awareness', 'i', 'had', 'an', 'anxiety', 'attack', '.', 'this', 'lead', 'that', 'professor', 'email', '##ing', 'my', 'adviser', '.', 'i', 'had', 'two', 'professors', 'email', 'my', 'adviser', 'last', 'semester', '.', 'fast', 'forward', 'to', 'feb', '.', 'i', 'was', 'meeting', 'with', 'my', 'adviser', 'and', 'another', 'professor', 'about', 'transportation', 'issues', 'to', 'an', 'off', 'campus', 'lab', '.', 'it', 'was', 'there', 'i', 'was', 'told', 'that', 'i', 'might', 'not', 'be', 'admitted', 'to', '\"', 'methods', '\"', '.', 'this', 'blow', 'was', 'the', 'worst', 'feeling', 'i', 'ever', 'felt', 'because', 'of', 'the', 'two', 'emails', '(', 'i', 'should', 'have', 'been', 'told', 'of', 'this', 'last', 'semester', 'but', 'that', 'is', 'another', 'story', ')', '.', 'the', 'adviser', 'told', 'me', 'that', 'she', 'had', 'two', 'professors', 'email', 'her', '.', 'i', 'asked', 'her', 'my', 'options', '.', 'she', 'tried', 'to', 'get', 'me', 'to', 'switch', 'majors', ',', 'i', 'said', 'no', 'what', 'are', 'my', 'options', '.', 'she', 'said', 'to', 'transfer', 'and', 'apply', 'to', 'the', 'program', 'just', 'in', 'case', '.', 'so', 'i', 'got', 'into', 'a', 'school', 'closer', 'to', 'home', 'and', 'submitted', 'my', 'application', 'to', 'the', 'program', 'here', '##with', 'a', 'letter', 'explaining', 'my', 'situation', '.', 'now', ',', 'the', 'committee', 'wants', 'to', 'meet', 'with', 'me', 'to', 'discuss', 'conditional', 'acceptance', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'expect', '.', '.', '.', '.', 'i', 'don', \"'\", 't', 'want', 'to', 'be', 'bash', '##ed', 'nor', 'do', 'i', 'want', 'the', 'conditions', 'to', 'be', 'like', '\"', 'get', 'on', 'anti', 'de', '##press', '##ants', '\"', '.', 'i', 'accepted', 'the', 'fact', 'that', 'it', 'was', 'a', 'long', 'shot', 'of', 'me', 'getting', 'in', '.', 'i', 'was', 'upset', 'that', 'this', 'happened', '.', 'i', 'don', \"'\", 't', 'know', 'how', 'i', 'will', 'be', 'able', 'to', 'control', 'my', 'issues', 'or', 'if', 'i', 'even', 'want', 'to', 'keep', 'going', 'here', '.', 'it', 'has', 'been', 'a', 'long', 'road', 'that', 'i', 'feel', 'like', 'no', 'one', 'is', 'fighting', 'for', 'me', 'and', 'i', 'have', 'to', 'battle', 'this', 'school', '.', 'i', 'mean', 'if', 'one', 'person', 'reached', 'out', 'maybe', 'i', 'would', 'have', 'had', 'a', 'better', 'semester', '.'], ['what', 'can', 'i', 'expect', 'for', 'a', 'meeting', 'like', 'this', '?', 'i', 'have', 'never', 'been', 'in', 'a', 'situation', 'like', 'this', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'can', \"'\", 't', 'get', 'motivated', 'to', 'do', 'things', 'i', 'need', 'to', 'do', 'and', 'it', \"'\", 's', 'causing', 'big', 'issues', '.', 'help', '?', 'i', \"'\", 've', 'been', 'taking', 'concert', '##a', 'for', 'the', 'past', '11', 'or', 'so', 'years', ',', 'and', 'i', 'am', 'definitely', 'a', 'more', 'focused', 'person', 'when', 'i', \"'\", 'm', 'on', 'it', '.', 'i', 'just', 'can', \"'\", 't', 'get', 'motivated', ',', 'though', '.', 'i', 'can', 'sit', 'down', 'and', 'do', 'something', 'i', \"'\", 'm', 'interested', 'for', 'hours', ',', 'but', 'starting', 'to', 'write', 'a', 'paper', 'for', 'a', 'subject', 'i', 'don', \"'\", 't', 'care', 'about', 'does', 'not', 'happen', '.', 'it', \"'\", 's', 'getting', 'to', 'the', 'point', 'where', 'my', 'gp', '##a', 'and', 'honors', 'status', 'are', 'in', 'extreme', 'danger', '.', 'i', 'know', 'that', 'will', '##power', 'is', 'a', 'lot', 'of', 'it', ',', 'but', 'i', 'just', 'can', \"'\", 't', 'seem', 'to', 'muster', 'the', 'will', '##power', 'to', 'do', 'what', \"'\", 's', 'expected', 'of', 'me', '.', 'i', 'would', 'greatly', 'appreciate', 'any', 'tips', 'or', 'strategies', 'that', 'might', 'help', '.', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 161\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'can', \"'\", 't', 'get', 'motivated', 'to', 'do', 'things', 'i', 'need', 'to', 'do', 'and', 'it', \"'\", 's', 'causing', 'big', 'issues', '.', 'help', '?', 'i', \"'\", 've', 'been', 'taking', 'concert', '##a', 'for', 'the', 'past', '11', 'or', 'so', 'years', ',', 'and', 'i', 'am', 'definitely', 'a', 'more', 'focused', 'person', 'when', 'i', \"'\", 'm', 'on', 'it', '.', 'i', 'just', 'can', \"'\", 't', 'get', 'motivated', ',', 'though', '.', 'i', 'can', 'sit', 'down', 'and', 'do', 'something', 'i', \"'\", 'm', 'interested', 'for', 'hours', ',', 'but', 'starting', 'to', 'write', 'a', 'paper', 'for', 'a', 'subject', 'i', 'don', \"'\", 't', 'care', 'about', 'does', 'not', 'happen', '.', 'it', \"'\", 's', 'getting', 'to', 'the', 'point', 'where', 'my', 'gp', '##a', 'and', 'honors', 'status', 'are', 'in', 'extreme', 'danger', '.', 'i', 'know', 'that', 'will', '##power', 'is', 'a', 'lot', 'of', 'it', ',', 'but', 'i', 'just', 'can', \"'\", 't', 'seem', 'to', 'muster', 'the', 'will', '##power', 'to', 'do', 'what', \"'\", 's', 'expected', 'of', 'me', '.', 'i', 'would', 'greatly', 'appreciate', 'any', 'tips', 'or', 'strategies', 'that', 'might', 'help', '.', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['red', '##dit', ',', 'what', 'do', 'you', 'eat', 'on', 'your', 'med', '##s', '?', 'just', 'out', 'of', 'curiosity', ',', 'what', 'foods', 'can', 'you', 'eat', 'without', 'wanting', 'to', '.', '.', '.', 'not', 'eat', 'them', '?', 'i', 'prefer', 'orange', '##s', 'for', 'some', 'reason', '.', 'edit', ':', 'thanks', 'for', 'the', 'input', 'guys', ',', 'sounds', 'delicious', '.']\n",
      "INFO:__main__:Number of tokens: 51\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['red', '##dit', ',', 'what', 'do', 'you', 'eat', 'on', 'your', 'med', '##s', '?', 'just', 'out', 'of', 'curiosity', ',', 'what', 'foods', 'can', 'you', 'eat', 'without', 'wanting', 'to', '.', '.', '.', 'not', 'eat', 'them', '?', 'i', 'prefer', 'orange', '##s', 'for', 'some', 'reason', '.', 'edit', ':', 'thanks', 'for', 'the', 'input', 'guys', ',', 'sounds', 'delicious', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['finally', 'going', 'back', 'on', 'med', '##s']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['finally', 'going', 'back', 'on', 'med', '##s']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'momentum', 'monday', ']', 'to', 'get', 'your', 'week', 'started', 'earlier', ',', 'and', 'have', 'more', 'free', '/', 'fun', 'time', 'on', 'the', 'weekend', '!', '#', '#', 'welcome', 'momentum', 'monday', '!', 'share', 'one', '(', 'or', 'a', 'couple', ')', 'of', 'things', 'you', 'want', 'to', 'get', 'done', 'this', 'week', '.', 'i', 'will', 'give', 'you', 'a', 'suggested', 'format', 'at', 'the', 'end', '.', '*', '*', '*', '#', '#', 'new', 'options', '!', '*', '*', 'fun', 'thing', '*', '-', '-', 'something', 'fun', 'you', \"'\", 've', 'been', 'wanting', 'to', 'do', '.', '*', '*', 'daily', 'habit', '*', '-', '-', 'a', 'habit', 'that', 'takes', '<', '1', 'minute', 'that', 'you', 'want', 'to', 'develop', '.', '*', '*', 'pro', '##cr', '##ast', '##inated', 'project', '*', '-', '-', 'something', 'you', \"'\", 've', 'been', 'putting', 'off', ',', 'that', 'will', 'likely', 'take', 'less', 'time', 'than', 'your', 'anxiety', 'tells', 'you', '!', '*', '*', '*', '*', '*', 'everyone', 'can', 'feel', 'free', 'to', 'check', '-', 'in', 'with', 'people', 'who', 'request', 'it', '.', 'i', 'was', 'too', 'busy', '/', 'distracted', 'last', 'week', 'so', 'i', 'would', 'appreciate', 'help', 'from', 'everyone', 'in', 'doing', 'follow', '##up', '!', '*', '*', '*', '*', '*', '[', 'check', 'out', 'last', 'weeks', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'se', '##f', '##j', '##v', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '3', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '2', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '1', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'r', '##g', '##1', '##q', '##f', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '*', 'suggested', 'rules', '*', '[UNK]', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to', 'do', 'that', 'too', '!', ')', '[UNK]', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'committing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '[UNK]', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '[UNK]', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', '*', '*', 'tips', '*', '*', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', 'one', 'hour', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '.', '*', '*', '*', '[UNK]', '*', '*', 'examples', 'from', 'previous', 'weeks', ':', '*', '*', '[UNK]', '[UNK]', 'un', '##load', 'the', 'dish', '##wash', '##er', '[UNK]', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '[UNK]', 'make', 'an', 'appointment', 'with', 'doctor', '[UNK]', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '[UNK]', '*', '*', 'start', '*', '*', 'something', '.', 'spend', '5', 'minutes', 'on', 'writing', 'my', 'paper', '.', '[UNK]', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '*', 'to', 'respond', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '*', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?', 'when', '?', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-', '*', '*', '*', 'hopefully', 'most', 'of', 'you', 'can', 'also', 'participate', 'in', 'win', 'wednesday', '.', 'we', 'had', 'a', 'drop', 'in', 'participation', 'in', 'both', 'these', 'threads', 'last', 'week', '.', 'if', 'you', 'have', 'any', 'other', 'suggestions', 'for', 'weekly', 'threads', 'or', 'improvements', 'to', 'these', 'ones', 'let', 'me', 'know', '!']\n",
      "INFO:__main__:Number of tokens: 1001\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['[', 'momentum', 'monday', ']', 'to', 'get', 'your', 'week', 'started', 'earlier', ',', 'and', 'have', 'more', 'free', '/', 'fun', 'time', 'on', 'the', 'weekend', '!', '#', '#', 'welcome', 'momentum', 'monday', '!', 'share', 'one', '(', 'or', 'a', 'couple', ')', 'of', 'things', 'you', 'want', 'to', 'get', 'done', 'this', 'week', '.', 'i', 'will', 'give', 'you', 'a', 'suggested', 'format', 'at', 'the', 'end', '.', '*', '*', '*', '#', '#', 'new', 'options', '!', '*', '*', 'fun', 'thing', '*', '-', '-', 'something', 'fun', 'you', \"'\", 've', 'been', 'wanting', 'to', 'do', '.', '*', '*', 'daily', 'habit', '*', '-', '-', 'a', 'habit', 'that', 'takes', '<', '1', 'minute', 'that', 'you', 'want', 'to', 'develop', '.', '*', '*', 'pro', '##cr', '##ast', '##inated', 'project', '*', '-', '-', 'something', 'you', \"'\", 've', 'been', 'putting', 'off', ',', 'that', 'will', 'likely', 'take', 'less', 'time', 'than', 'your', 'anxiety', 'tells', 'you', '!', '*', '*', '*', '*', '*', 'everyone', 'can', 'feel', 'free', 'to', 'check', '-', 'in', 'with', 'people', 'who', 'request', 'it', '.', 'i', 'was', 'too', 'busy', '/', 'distracted', 'last', 'week', 'so', 'i', 'would', 'appreciate', 'help', 'from', 'everyone', 'in', 'doing', 'follow', '##up', '!', '*', '*', '*', '*', '*', '[', 'check', 'out', 'last', 'weeks', 'thread', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'se', '##f', '##j', '##v', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '3', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '2', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rr', '##7', '##em', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '[', 'week', '1', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'r', '##g', '##1', '##q', '##f', '/', 'to', '_', 'accomplish', '_', 'tuesday', '_', 'share', '_', 'one', '_', 'thing', '_', 'you', '_', 'would', '/', ')', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'public', '##ally', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '*', 'suggested', 'rules', '*', '[UNK]', '*', '*', 'just', 'put', 'one', 'goal', '.', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to', 'do', 'that', 'too', '!', ')', '[UNK]', '*', '*', 'when', 'would', 'you'], ['like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'committing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '[UNK]', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '[UNK]', '*', '*', 'create', 'another', 'when', 'finished', '*', '*', '-', 'when', 'you', 'finished', 'your', 'first', 'task', 'feel', 'free', 'to', 'add', 'another', '!', '*', '*', '*', '*', '*', 'tips', '*', '*', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', 'one', 'hour', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '.', '*', '*', '*', '[UNK]', '*', '*', 'examples', 'from', 'previous', 'weeks', ':', '*', '*', '[UNK]', '[UNK]', 'un', '##load', 'the', 'dish', '##wash', '##er', '[UNK]', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '[UNK]', 'make', 'an', 'appointment', 'with', 'doctor', '[UNK]', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '[UNK]', '*', '*', 'start', '*', '*', 'something', '.', 'spend', '5', 'minutes', 'on', 'writing', 'my', 'paper', '.', '[UNK]', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '*', 'to', 'respond', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '*', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?', 'when', '?', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-', '*', '*', '*', 'hopefully', 'most', 'of', 'you', 'can', 'also', 'participate', 'in', 'win', 'wednesday', '.', 'we', 'had', 'a', 'drop', 'in', 'participation', 'in', 'both', 'these', 'threads', 'last', 'week', '.', 'if', 'you', 'have', 'any', 'other', 'suggestions', 'for', 'weekly', 'threads', 'or', 'improvements', 'to', 'these', 'ones', 'let', 'me', 'know', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['if', 'you', 'could', 'push', 'a', 'button', 'and', 'permanently', 'transform', 'your', 'brain', 'into', 'that', 'of', 'a', '\"', 'normal', '\"', 'person', ',', 'would', 'you', '?']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['if', 'you', 'could', 'push', 'a', 'button', 'and', 'permanently', 'transform', 'your', 'brain', 'into', 'that', 'of', 'a', '\"', 'normal', '\"', 'person', ',', 'would', 'you', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['why', 'don', \"'\", 't', 'i', 'feel', 'different', 'on', 'med', '##s', 'or', 'off', 'med', '##s', 'i', 'was', 'diagnosed', 'in', '2003', '.', 'i', 'currently', 'take', '60', '##mg', 'of', 'v', '##y', '##vance', 'daily', 'and', 'have', 'been', 'since', '2011', '.', 'i', 'take', 'my', 'medicine', 'every', 'morning', '.', 'when', 'i', 'wake', 'up', 'late', 'and', 'can', \"'\", 't', 'take', 'my', 'med', '##s', 'in', 'the', 'morning', ',', 'i', 'still', 'feel', 'the', 'same', '.', 'i', 'feel', 'hyper', 'on', 'and', 'off', 'medication', ',', 'and', 'i', 'can', \"'\", 't', 'focus', 'either', 'way', '.', 'am', 'i', 'already', 'tolerant', 'of', 'my', 'new', 'medication', 'or', 'what', 'could', 'it', 'be', '?']\n",
      "INFO:__main__:Number of tokens: 97\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['why', 'don', \"'\", 't', 'i', 'feel', 'different', 'on', 'med', '##s', 'or', 'off', 'med', '##s', 'i', 'was', 'diagnosed', 'in', '2003', '.', 'i', 'currently', 'take', '60', '##mg', 'of', 'v', '##y', '##vance', 'daily', 'and', 'have', 'been', 'since', '2011', '.', 'i', 'take', 'my', 'medicine', 'every', 'morning', '.', 'when', 'i', 'wake', 'up', 'late', 'and', 'can', \"'\", 't', 'take', 'my', 'med', '##s', 'in', 'the', 'morning', ',', 'i', 'still', 'feel', 'the', 'same', '.', 'i', 'feel', 'hyper', 'on', 'and', 'off', 'medication', ',', 'and', 'i', 'can', \"'\", 't', 'focus', 'either', 'way', '.', 'am', 'i', 'already', 'tolerant', 'of', 'my', 'new', 'medication', 'or', 'what', 'could', 'it', 'be', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'hyper', '##thy', '##roid', '##ism', '?', 'need', 'advice', '/', 'info', '/', 'anything', '.', '.', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'years', 'ago', 'and', 'it', 'is', 'still', 'a', 'challenge', 'i', 'deal', 'with', 'everyday', '(', 'es', '##p', 'as', 'a', 'college', 'student', ')', '.', 'i', 'recently', 'went', 'in', 'to', 'see', 'a', 'psychiatrist', 'about', 'a', 'sudden', 'onset', 'of', 'frequent', 'panic', 'attacks', ',', 'anxiety', ',', 'and', 'depression', '(', 'which', 'has', 'never', 'been', 'an', 'issue', 'before', ')', '.', 'after', 'having', 'routine', 'blood', 'work', 'done', ',', 'and', 'finding', 'that', 'my', 'ts', '##h', 'levels', 'were', 'low', 'i', 'am', 'now', 'being', 'tested', 'for', 'possible', 'hyper', '##thy', '##roid', '##ism', '.', 'after', 'doing', 'some', 'research', 'i', 'found', 'several', 'articles', 'and', 'books', 'that', 'relate', 'ad', '##hd', 'to', 'hyper', '##thy', '##roid', '##ism', '.', 'i', 'was', 'wondering', 'if', 'any', 'of', 'y', \"'\", 'all', 'out', 'there', 'have', 'been', 'diagnosed', 'with', 'ad', '##hd', 'only', 'to', 'later', 'be', 'diagnosed', 'with', 'hyper', '##thy', '##roid', '##ism', '?', 'and', 'once', 'treatment', 'for', 'hyper', '##thy', '##roid', '##ism', 'was', 'started', ',', 'did', 'you', 'notice', 'any', 'difference', 'in', 'mood', ',', 'ability', 'to', 'concentrate', ',', 'interest', ',', 'ec', '##t', '?', 'i', \"'\", 've', 'also', 'recently', 'been', 'having', 'trouble', 'remembering', 'simple', 'things', 'that', 'i', 'normally', 'do', 'not', 'have', 'trouble', 'with', '.', 'it', \"'\", 's', 'really', 'become', 'a', 'problem', 'with', 'studying', '.', 'i', 'can', 'study', 'for', 'hours', 'in', 'the', 'method', 'where', 'i', 'learn', 'most', 'efficient', '.', 'but', 'come', 'back', 'to', 'a', 'problem', 'after', '5', '-', '10', 'min', '##s', 'and', 'i', 'will', 'have', 'forgotten', 'everything', '.', 'i', 'understand', 'the', 'problem', ',', 'but', 'can', \"'\", 't', 'remember', 'the', 'reaction', '(', 'i', 'am', 'a', 'chemistry', 'major', ')', '.', 'with', 'this', 'new', 'development', ',', 'i', \"'\", 've', 'also', 'started', 'having', 'panic', 'attacks', ',', 'anxiety', 'and', 'depression', '.', 'i', \"'\", 'm', 'not', 'my', 'normal', 'self', ',', 'and', 'it', 'feels', 'like', 'i', 'don', \"'\", 't', 'even', 'know', 'who', 'i', 'am', 'anymore', '.', 'i', 'have', 'trouble', 'identifying', 'how', 'i', 'feel', 'about', 'certain', 'things', '.', 'i', \"'\", 'm', 'losing', 'confidence', 'in', 'myself', ',', 'doubt', '##ing', 'my', 'ability', 'to', 'finish', 'school', 'and', 'my', 'intelligence', '.', 'overall', 'i', 'feel', 'just', '.', '.', '.', 'helpless', '.', 'any', 'advice', 'from', 'someone', 'who', 'has', 'had', 'similar', 'issues', 'or', 'know', 'what', 'i', \"'\", 'm', 'talking', 'about', 'would', 'be', 'great', '.', 'i', 'feel', 'alone', 'with', 'this', 'particular', 'issue', ',', 'can', \"'\", 't', 'find', 'anyone', 'who', 'relates', '.']\n",
      "INFO:__main__:Number of tokens: 379\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'hyper', '##thy', '##roid', '##ism', '?', 'need', 'advice', '/', 'info', '/', 'anything', '.', '.', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'years', 'ago', 'and', 'it', 'is', 'still', 'a', 'challenge', 'i', 'deal', 'with', 'everyday', '(', 'es', '##p', 'as', 'a', 'college', 'student', ')', '.', 'i', 'recently', 'went', 'in', 'to', 'see', 'a', 'psychiatrist', 'about', 'a', 'sudden', 'onset', 'of', 'frequent', 'panic', 'attacks', ',', 'anxiety', ',', 'and', 'depression', '(', 'which', 'has', 'never', 'been', 'an', 'issue', 'before', ')', '.', 'after', 'having', 'routine', 'blood', 'work', 'done', ',', 'and', 'finding', 'that', 'my', 'ts', '##h', 'levels', 'were', 'low', 'i', 'am', 'now', 'being', 'tested', 'for', 'possible', 'hyper', '##thy', '##roid', '##ism', '.', 'after', 'doing', 'some', 'research', 'i', 'found', 'several', 'articles', 'and', 'books', 'that', 'relate', 'ad', '##hd', 'to', 'hyper', '##thy', '##roid', '##ism', '.', 'i', 'was', 'wondering', 'if', 'any', 'of', 'y', \"'\", 'all', 'out', 'there', 'have', 'been', 'diagnosed', 'with', 'ad', '##hd', 'only', 'to', 'later', 'be', 'diagnosed', 'with', 'hyper', '##thy', '##roid', '##ism', '?', 'and', 'once', 'treatment', 'for', 'hyper', '##thy', '##roid', '##ism', 'was', 'started', ',', 'did', 'you', 'notice', 'any', 'difference', 'in', 'mood', ',', 'ability', 'to', 'concentrate', ',', 'interest', ',', 'ec', '##t', '?', 'i', \"'\", 've', 'also', 'recently', 'been', 'having', 'trouble', 'remembering', 'simple', 'things', 'that', 'i', 'normally', 'do', 'not', 'have', 'trouble', 'with', '.', 'it', \"'\", 's', 'really', 'become', 'a', 'problem', 'with', 'studying', '.', 'i', 'can', 'study', 'for', 'hours', 'in', 'the', 'method', 'where', 'i', 'learn', 'most', 'efficient', '.', 'but', 'come', 'back', 'to', 'a', 'problem', 'after', '5', '-', '10', 'min', '##s', 'and', 'i', 'will', 'have', 'forgotten', 'everything', '.', 'i', 'understand', 'the', 'problem', ',', 'but', 'can', \"'\", 't', 'remember', 'the', 'reaction', '(', 'i', 'am', 'a', 'chemistry', 'major', ')', '.', 'with', 'this', 'new', 'development', ',', 'i', \"'\", 've', 'also', 'started', 'having', 'panic', 'attacks', ',', 'anxiety', 'and', 'depression', '.', 'i', \"'\", 'm', 'not', 'my', 'normal', 'self', ',', 'and', 'it', 'feels', 'like', 'i', 'don', \"'\", 't', 'even', 'know', 'who', 'i', 'am', 'anymore', '.', 'i', 'have', 'trouble', 'identifying', 'how', 'i', 'feel', 'about', 'certain', 'things', '.', 'i', \"'\", 'm', 'losing', 'confidence', 'in', 'myself', ',', 'doubt', '##ing', 'my', 'ability', 'to', 'finish', 'school', 'and', 'my', 'intelligence', '.', 'overall', 'i', 'feel', 'just', '.', '.', '.', 'helpless', '.', 'any', 'advice', 'from', 'someone', 'who', 'has', 'had', 'similar', 'issues', 'or', 'know', 'what', 'i', \"'\", 'm', 'talking', 'about', 'would', 'be', 'great', '.', 'i', 'feel', 'alone', 'with', 'this', 'particular', 'issue', ',', 'can', \"'\", 't', 'find', 'anyone', 'who', 'relates', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'are', 'you', 'affected', 'by', 'sugar', '##s', ';', 'caf', '##fe', '##ine', '?', 'first', 'post', 'in', 'this', 'sub', '##red', '##dit', 'because', 'i', \"'\", 've', 'just', 'been', 'diagnosed', 'this', 'last', 'week', '.', 'i', \"'\", 've', 'just', 'started', 'my', 'research', 'to', 'find', 'my', 'own', 'action', 'steps', '.', 'i', \"'\", 've', 'noticed', 'that', 'i', 'used', 'to', 'self', '-', 'med', '##icate', 'with', 'caf', '##fe', '##ine', 'before', 'when', 'i', 'couldn', \"'\", 't', 'concentrate', 'or', 'mo', '##tiv', '##ate', 'myself', 'on', 'a', 'specific', 'task', '.', 'i', 'couldn', \"'\", 't', 'drink', 'any', 'caf', '##fe', '##inated', 'drink', 'with', 'a', 'large', 'amount', 'of', 'sugar', '##s', 'in', 'it', 'because', 'it', 'would', 'always', 'just', 'put', 'me', 'to', 'sleep', '.', 'i', 'think', 'that', 'sugar', '##s', 'sl', '##oo', '##oo', '##w', 'me', 'down', '.', 'can', 'this', 'be', 'true', '?', 'now', 'that', 'i', 'think', 'more', 'about', 'it', ',', 'i', 'am', 'not', 'certain', 'that', 'the', 'caf', '##fe', '##ine', 'was', 'even', 'helping', 'me', 'do', 'anything', '.', 'it', 'could', 'have', 'been', 'a', 'place', '##bo', 'when', 'it', 'worked', 'and', ',', 'at', 'times', ',', 'calmed', 'me', 'down', '.', 'what', 'have', 'been', 'your', 'experiences', 'with', 'caf', '##fe', '##ine', 'and', 'sugar', '##s', '?', 'how', 'do', 'you', 'intake', 'them', 'now', '?']\n",
      "INFO:__main__:Number of tokens: 186\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'are', 'you', 'affected', 'by', 'sugar', '##s', ';', 'caf', '##fe', '##ine', '?', 'first', 'post', 'in', 'this', 'sub', '##red', '##dit', 'because', 'i', \"'\", 've', 'just', 'been', 'diagnosed', 'this', 'last', 'week', '.', 'i', \"'\", 've', 'just', 'started', 'my', 'research', 'to', 'find', 'my', 'own', 'action', 'steps', '.', 'i', \"'\", 've', 'noticed', 'that', 'i', 'used', 'to', 'self', '-', 'med', '##icate', 'with', 'caf', '##fe', '##ine', 'before', 'when', 'i', 'couldn', \"'\", 't', 'concentrate', 'or', 'mo', '##tiv', '##ate', 'myself', 'on', 'a', 'specific', 'task', '.', 'i', 'couldn', \"'\", 't', 'drink', 'any', 'caf', '##fe', '##inated', 'drink', 'with', 'a', 'large', 'amount', 'of', 'sugar', '##s', 'in', 'it', 'because', 'it', 'would', 'always', 'just', 'put', 'me', 'to', 'sleep', '.', 'i', 'think', 'that', 'sugar', '##s', 'sl', '##oo', '##oo', '##w', 'me', 'down', '.', 'can', 'this', 'be', 'true', '?', 'now', 'that', 'i', 'think', 'more', 'about', 'it', ',', 'i', 'am', 'not', 'certain', 'that', 'the', 'caf', '##fe', '##ine', 'was', 'even', 'helping', 'me', 'do', 'anything', '.', 'it', 'could', 'have', 'been', 'a', 'place', '##bo', 'when', 'it', 'worked', 'and', ',', 'at', 'times', ',', 'calmed', 'me', 'down', '.', 'what', 'have', 'been', 'your', 'experiences', 'with', 'caf', '##fe', '##ine', 'and', 'sugar', '##s', '?', 'how', 'do', 'you', 'intake', 'them', 'now', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'know', 'a', 'good', 'doctor', 'in', 'san', 'francisco', '?', 'i', 'recently', 'started', 'wondering', 'if', 'i', 'might', 'have', 'ad', '##hd', '.', 'my', 'brother', 'was', 'diagnosed', 'when', 'he', 'was', 'little', ',', 'but', 'i', \"'\", 've', 'never', 'been', 'tested', '.', 'i', \"'\", 've', 'read', 'a', 'lot', 'of', 'lists', 'of', 'symptoms', ',', 'and', 'i', 'seem', 'to', 'have', 'the', 'majority', '.', 'anyway', ',', 'does', 'anyone', 'on', 'here', 'know', 'a', 'good', 'doctor', 'in', 'san', 'francisco', 'who', 'can', 'test', 'me', 'and', 'hopefully', 'not', 'incorrectly', 'dia', '##gno', '##se', 'me', '?']\n",
      "INFO:__main__:Number of tokens: 83\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'know', 'a', 'good', 'doctor', 'in', 'san', 'francisco', '?', 'i', 'recently', 'started', 'wondering', 'if', 'i', 'might', 'have', 'ad', '##hd', '.', 'my', 'brother', 'was', 'diagnosed', 'when', 'he', 'was', 'little', ',', 'but', 'i', \"'\", 've', 'never', 'been', 'tested', '.', 'i', \"'\", 've', 'read', 'a', 'lot', 'of', 'lists', 'of', 'symptoms', ',', 'and', 'i', 'seem', 'to', 'have', 'the', 'majority', '.', 'anyway', ',', 'does', 'anyone', 'on', 'here', 'know', 'a', 'good', 'doctor', 'in', 'san', 'francisco', 'who', 'can', 'test', 'me', 'and', 'hopefully', 'not', 'incorrectly', 'dia', '##gno', '##se', 'me', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['career', 'and', 'post', '-', 'college', 'advice', ',', 'can', 'you', 'all', 'offer', 'me', 'some', 'advice', '?', '*', '*', 'note', ':', '*', '*', 'i', 'saw', 'a', 'similar', 'post', 'recently', '(', '[', 'here', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'so', '##48', '##v', '/', 'how', '_', 'to', '_', 'fc', '##king', '_', 'decide', '_', 'on', '_', 'a', '_', 'career', '_', 'path', '/', ')', ')', 'and', 'found', 'some', 'of', 'the', 'comments', 'to', 'be', 'insight', '##ful', 'and', 'helpful', '.', 'so', 'i', 'thought', 'i', 'would', 'ask', 'a', 'similar', 'question', '-', 'maybe', 'even', 'a', 'series', 'of', 'these', 'would', 'be', 'beneficial', 'to', 'the', 'community', '?', '*', '*', '*', '*', '*', '*', 'background', '*', '*', 'i', 'am', 'about', 'to', 'graduate', 'from', 'the', 'university', 'of', 'maryland', 'in', 'less', 'than', 'a', 'month', ',', 'with', 'a', 'degree', 'in', 'government', 'and', 'politics', '(', '*', 'um', '##d', \"'\", 's', 'political', 'science', 'major', ')', '.', 'before', 'i', 'get', 'cha', '##stis', '##ed', 'for', 'picking', 'a', 'liberal', 'arts', 'major', ',', 'i', 'justified', 'my', 'decision', 'for', 'two', 'significant', 'reasons', ':', '*', 'politics', 'and', 'government', 'are', 'my', 'true', 'passion', ',', 'and', 'the', 'one', 'subject', 'that', 'i', 'never', 'seem', 'to', 'get', 'bored', 'with', '.', '*', 'i', 'had', 'a', 'number', 'of', 'significant', 'high', 'school', '(', 'student', 'school', 'board', 'member', ',', 'state', 'legislative', 'page', ')', 'and', 'college', 'internship', '##s', 'with', 'federal', '(', 'usd', '##ot', ')', ',', 'state', '(', 'md', 'house', 'of', 'delegates', ',', '[', 'cap', '##c', ']', '(', 'cap', '##c', '.', 'um', '##d', '.', 'ed', '##u', ')', ')', 'and', 'local', 'government', '(', 'rent', 'control', 'board', 'member', ',', 'city', 'council', 'liaison', ')', '.', 'my', 'gp', '##a', 'is', 'a', 'bit', 'embarrassing', ',', 'at', 'a', 'solid', '2', '.', '86', '.', 'throughout', 'college', 'i', 'have', 'consistently', 'worked', 'a', 'part', '-', 'time', 'job', ',', 'struggled', 'with', 'severe', 'depression', ',', 'worked', 'as', 'an', 'ra', ',', 'and', 'was', 'elected', 'to', 'student', 'government', '.', 'additionally', 'i', 'see', 'a', 'therapist', 'weekly', 'for', 'depression', '/', 'anxiety', 'issues', ',', 'and', 'i', 'am', 'currently', 'taking', '10', '##mg', 'add', '##eral', '##l', 'three', 'times', 'daily', '.', '*', '*', '*', '*', '*', '*', 'current', 'problems', '*', '*', '*', 'no', 'employment', 'prospects', '*', 'over', '##be', '##aring', 'parents', '*', 'can', \"'\", 't', 'maintain', 'enough', 'savings', 'from', 'part', '-', 'time', 'income', '.', '*', 'ins', '##ati', '##able', 'desire', 'to', 'run', 'for', 'public', 'office', ',', 'or', 'be', 'involved', 'in', 'public', 'service', '.', '*', '*', '*', '*', '*', '*', 'personal', 'skills', '*', '*', '*', 'i', 'am', 'quick', 'to', 'read', 'and', 'understand', 'people', ',', 'and', 'am', 'quick', 'to', 'em', '##path', '##ize', '/', 'relate', 'to', 'other', 'people', \"'\", 's', 'problems', '.', '*', 'good', 'debate', '##r', '/', 'public', 'speaker', '(', 'can', 'imp', '##rov', '##ise', 'speeches', 'off', 'the', 'top', 'of', 'my', 'head', ')', '.', '*', 'i', 'can', 'learn', 'really', 'quick', ',', 'and', 'i', 'research', 'everything', '-', 'wasting', 'many', 'hour', 'sc', '##ouring', 'wikipedia', 'for', 'no', 'reason', '.', '*', 'i', \"'\", 'm', 'relatively', 'proficient', 'with', 'pcs', 'and', 'widely', '-', 'used', 'office', 'software', '.', '*', 'i', 'love', 'meeting', 'people', 'and', 'being', 'on', 'the', 'move', '.', 'i', 'would', 'be', 'miserable', 'at', 'a', 'desk', 'job', '/', 'cubic', '##le', 'job', '.', '*', 'i', 'have', 'a', 'strong', 'work', 'et', '##hic', 'when', 'i', 'enjoy', 'what', 'i', 'do', ',', 'but', 'i', 'am', 'terribly', 'lazy', 'when', 'i', 'am', 'bored', 'or', 'un', '##int', '##eres', '##ted', 'in', 'the', 'work', '.', '*', 'i', 'can', 'explain', 'complicated', 'and', 'inter', '-', 'related', 'concepts', 'in', 'lay', '##man', \"'\", 's', 'terms', '.', 'i', \"'\", 've', 'been', 'told', 'i', 'would', 'be', 'a', 'good', 'teacher', '.', '*', 'i', 'like', 'to', 'think', 'i', 'write', 'well', ',', 'and', 'i', 'weird', '##ly', 'ob', '##ses', '##s', 'over', 'order', '##liness', 'despite', 'the', 'fact', 'that', 'my', 'home', 'life', 'is', 'messy', '(', 'laundry', ',', 'dishes', ',', 'etc', '.', ')', '*', '*', '*', '*', 'my', 'problem', ':', 'i', 'am', 'interested', 'in', 'pursuing', 'more', 'education', '-', 'particularly', 'law', 'school', 'or', 'a', 'mp', '##a', '/', 'mba', '.', 'though', 'i', 'am', 'almost', 'positive', 'that', 'my', 'gp', '##a', 'and', 'college', 'performance', 'makes', 'this', 'un', '##real', '##istic', '.', 'i', 'don', \"'\", 't', 'really', 'care', 'if', 'i', 'make', 'a', 'massive', 'salary', '-', 'i', \"'\", 'm', 'more', 'interested', 'in', 'finding', 'a', 'career', 'that', 'i', 'am', 'passionate', 'about', ',', 'that', 'won', \"'\", 't', 'be', 'hind', '##ered', 'by', 'my', 'ad', '##hd', '.', '*', '*', '*', '*', 't', '##l', ';', 'dr', 'i', 'am', 'an', 'anxiety', '-', 'ridden', 'ad', '##hd', 'graduating', 'college', 'senior', 'who', 'has', 'a', 'med', '##io', '##cre', 'gp', '##a', 'but', 'as', '##pire', '##s', 'to', 'be', 'a', 'successful', 'and', 'happy', '.', 'please', 'ask', 'questions', '!']\n",
      "INFO:__main__:Number of tokens: 723\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['career', 'and', 'post', '-', 'college', 'advice', ',', 'can', 'you', 'all', 'offer', 'me', 'some', 'advice', '?', '*', '*', 'note', ':', '*', '*', 'i', 'saw', 'a', 'similar', 'post', 'recently', '(', '[', 'here', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'so', '##48', '##v', '/', 'how', '_', 'to', '_', 'fc', '##king', '_', 'decide', '_', 'on', '_', 'a', '_', 'career', '_', 'path', '/', ')', ')', 'and', 'found', 'some', 'of', 'the', 'comments', 'to', 'be', 'insight', '##ful', 'and', 'helpful', '.', 'so', 'i', 'thought', 'i', 'would', 'ask', 'a', 'similar', 'question', '-', 'maybe', 'even', 'a', 'series', 'of', 'these', 'would', 'be', 'beneficial', 'to', 'the', 'community', '?', '*', '*', '*', '*', '*', '*', 'background', '*', '*', 'i', 'am', 'about', 'to', 'graduate', 'from', 'the', 'university', 'of', 'maryland', 'in', 'less', 'than', 'a', 'month', ',', 'with', 'a', 'degree', 'in', 'government', 'and', 'politics', '(', '*', 'um', '##d', \"'\", 's', 'political', 'science', 'major', ')', '.', 'before', 'i', 'get', 'cha', '##stis', '##ed', 'for', 'picking', 'a', 'liberal', 'arts', 'major', ',', 'i', 'justified', 'my', 'decision', 'for', 'two', 'significant', 'reasons', ':', '*', 'politics', 'and', 'government', 'are', 'my', 'true', 'passion', ',', 'and', 'the', 'one', 'subject', 'that', 'i', 'never', 'seem', 'to', 'get', 'bored', 'with', '.', '*', 'i', 'had', 'a', 'number', 'of', 'significant', 'high', 'school', '(', 'student', 'school', 'board', 'member', ',', 'state', 'legislative', 'page', ')', 'and', 'college', 'internship', '##s', 'with', 'federal', '(', 'usd', '##ot', ')', ',', 'state', '(', 'md', 'house', 'of', 'delegates', ',', '[', 'cap', '##c', ']', '(', 'cap', '##c', '.', 'um', '##d', '.', 'ed', '##u', ')', ')', 'and', 'local', 'government', '(', 'rent', 'control', 'board', 'member', ',', 'city', 'council', 'liaison', ')', '.', 'my', 'gp', '##a', 'is', 'a', 'bit', 'embarrassing', ',', 'at', 'a', 'solid', '2', '.', '86', '.', 'throughout', 'college', 'i', 'have', 'consistently', 'worked', 'a', 'part', '-', 'time', 'job', ',', 'struggled', 'with', 'severe', 'depression', ',', 'worked', 'as', 'an', 'ra', ',', 'and', 'was', 'elected', 'to', 'student', 'government', '.', 'additionally', 'i', 'see', 'a', 'therapist', 'weekly', 'for', 'depression', '/', 'anxiety', 'issues', ',', 'and', 'i', 'am', 'currently', 'taking', '10', '##mg', 'add', '##eral', '##l', 'three', 'times', 'daily', '.', '*', '*', '*', '*', '*', '*', 'current', 'problems', '*', '*', '*', 'no', 'employment', 'prospects', '*', 'over', '##be', '##aring', 'parents', '*', 'can', \"'\", 't', 'maintain', 'enough', 'savings', 'from', 'part', '-', 'time', 'income', '.', '*', 'ins', '##ati', '##able', 'desire', 'to', 'run', 'for', 'public', 'office', ',', 'or', 'be', 'involved', 'in', 'public', 'service', '.', '*', '*', '*', '*', '*', '*', 'personal', 'skills', '*', '*', '*', 'i', 'am', 'quick', 'to', 'read', 'and', 'understand', 'people', ',', 'and', 'am', 'quick', 'to', 'em', '##path', '##ize', '/', 'relate', 'to', 'other', 'people', \"'\", 's', 'problems', '.', '*', 'good', 'debate', '##r', '/', 'public', 'speaker', '(', 'can', 'imp', '##rov', '##ise', 'speeches', 'off', 'the', 'top', 'of', 'my', 'head', ')', '.', '*', 'i', 'can', 'learn', 'really', 'quick', ',', 'and', 'i', 'research', 'everything', '-', 'wasting', 'many', 'hour', 'sc', '##ouring', 'wikipedia', 'for', 'no', 'reason', '.', '*', 'i', \"'\", 'm', 'relatively', 'proficient', 'with', 'pcs', 'and', 'widely', '-', 'used', 'office', 'software', '.', '*', 'i', 'love', 'meeting', 'people', 'and', 'being', 'on', 'the', 'move', '.', 'i', 'would', 'be', 'miserable', 'at', 'a', 'desk', 'job', '/', 'cubic', '##le', 'job', '.', '*', 'i', 'have', 'a', 'strong', 'work', 'et', '##hic', 'when', 'i', 'enjoy', 'what'], ['i', 'do', ',', 'but', 'i', 'am', 'terribly', 'lazy', 'when', 'i', 'am', 'bored', 'or', 'un', '##int', '##eres', '##ted', 'in', 'the', 'work', '.', '*', 'i', 'can', 'explain', 'complicated', 'and', 'inter', '-', 'related', 'concepts', 'in', 'lay', '##man', \"'\", 's', 'terms', '.', 'i', \"'\", 've', 'been', 'told', 'i', 'would', 'be', 'a', 'good', 'teacher', '.', '*', 'i', 'like', 'to', 'think', 'i', 'write', 'well', ',', 'and', 'i', 'weird', '##ly', 'ob', '##ses', '##s', 'over', 'order', '##liness', 'despite', 'the', 'fact', 'that', 'my', 'home', 'life', 'is', 'messy', '(', 'laundry', ',', 'dishes', ',', 'etc', '.', ')', '*', '*', '*', '*', 'my', 'problem', ':', 'i', 'am', 'interested', 'in', 'pursuing', 'more', 'education', '-', 'particularly', 'law', 'school', 'or', 'a', 'mp', '##a', '/', 'mba', '.', 'though', 'i', 'am', 'almost', 'positive', 'that', 'my', 'gp', '##a', 'and', 'college', 'performance', 'makes', 'this', 'un', '##real', '##istic', '.', 'i', 'don', \"'\", 't', 'really', 'care', 'if', 'i', 'make', 'a', 'massive', 'salary', '-', 'i', \"'\", 'm', 'more', 'interested', 'in', 'finding', 'a', 'career', 'that', 'i', 'am', 'passionate', 'about', ',', 'that', 'won', \"'\", 't', 'be', 'hind', '##ered', 'by', 'my', 'ad', '##hd', '.', '*', '*', '*', '*', 't', '##l', ';', 'dr', 'i', 'am', 'an', 'anxiety', '-', 'ridden', 'ad', '##hd', 'graduating', 'college', 'senior', 'who', 'has', 'a', 'med', '##io', '##cre', 'gp', '##a', 'but', 'as', '##pire', '##s', 'to', 'be', 'a', 'successful', 'and', 'happy', '.', 'please', 'ask', 'questions', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', ',', 'have', 'trouble', 'trying', 'to', 'explain', 'information', 'to', 'others', '?', 'when', 'ever', 'i', 'try', 'to', 'explain', 'things', 'to', 'people', 'i', 'can', 'get', 'my', 'basic', 'point', 'across', '.', 'if', 'i', 'have', 'to', 'go', 'into', 'detail', 'though', 'i', 'usually', 'have', 'so', 'much', 'going', 'through', 'my', 'head', 'its', 'hard', 'to', 'form', 'a', 'structured', 'intelligent', 'sentence', ',', 'and', 'i', 'usually', 'just', 'dumb', 'it', 'wa', '##aa', '##ay', 'down', '.', 'if', 'i', 'get', 'into', 'a', 'debate', 'it', 'gets', 'worse', '.', 'i', 'feel', 'like', 'it', 'makes', 'me', 'come', 'off', 'as', 'dumb', 'and', 'like', 'i', 'have', 'no', 'idea', 'what', 'i', \"'\", 'm', 'talking', 'about', '.', 'does', 'that', 'happen', 'to', 'anyone', 'else', '?']\n",
      "INFO:__main__:Number of tokens: 106\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', ',', 'have', 'trouble', 'trying', 'to', 'explain', 'information', 'to', 'others', '?', 'when', 'ever', 'i', 'try', 'to', 'explain', 'things', 'to', 'people', 'i', 'can', 'get', 'my', 'basic', 'point', 'across', '.', 'if', 'i', 'have', 'to', 'go', 'into', 'detail', 'though', 'i', 'usually', 'have', 'so', 'much', 'going', 'through', 'my', 'head', 'its', 'hard', 'to', 'form', 'a', 'structured', 'intelligent', 'sentence', ',', 'and', 'i', 'usually', 'just', 'dumb', 'it', 'wa', '##aa', '##ay', 'down', '.', 'if', 'i', 'get', 'into', 'a', 'debate', 'it', 'gets', 'worse', '.', 'i', 'feel', 'like', 'it', 'makes', 'me', 'come', 'off', 'as', 'dumb', 'and', 'like', 'i', 'have', 'no', 'idea', 'what', 'i', \"'\", 'm', 'talking', 'about', '.', 'does', 'that', 'happen', 'to', 'anyone', 'else', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ps', '##a', ':', 'locks', '##mith', '##s', 'are', 'expensive', '!', 'make', 'sure', 'you', 'have', 'your', 'keys', 'before', 'you', 'close', 'your', 'door', '!', 'last', 'weekend', ',', 'i', 'was', 'running', 'some', 'er', '##rand', '##s', 'before', 'a', 'date', 'i', 'had', 'that', 'afternoon', '.', 'when', 'i', 'pulled', 'into', 'the', 'parking', 'lot', 'of', 'the', 'shopping', 'center', ',', 'i', 'decided', 'to', 'clean', 'out', 'my', 'car', '.', 'i', 'turned', 'off', 'the', 'engine', ',', 'but', 'left', 'the', 'keys', 'in', 'the', 'ignition', ',', 'so', 'i', 'could', 'still', 'listen', 'to', 'music', '.', '.', '.', 'i', 'then', 'pull', 'the', 'bag', 'of', 'trash', 'out', 'of', 'my', 'car', ',', 'and', 'close', 'the', 'door', '.', 'i', 'suppose', 'anticipating', 'my', 'date', ',', 'and', 'going', 'though', 'my', 'list', 'of', 'things', 'i', 'wanted', 'to', 'get', 'done', 'beforehand', 'kept', 'me', 'from', 'remembering', 'to', 'make', 'sure', 'i', 'had', 'my', 'keys', 'before', 'closing', 'the', 'door', '!', 'called', 'locks', '##mith', ',', 'they', 'charged', '230', 'dollars', 'to', 'open', 'the', 'door', '.', 'ff', '##ff', '##fu', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', ':', '-', '/', 'make', 'sure', 'you', 'always', 'have', 'your', 'keys', 'before', 'you', 'close', 'your', 'car', '/', 'house', '/', 'apartment', '/', 'etc', 'door', '.']\n",
      "INFO:__main__:Number of tokens: 186\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ps', '##a', ':', 'locks', '##mith', '##s', 'are', 'expensive', '!', 'make', 'sure', 'you', 'have', 'your', 'keys', 'before', 'you', 'close', 'your', 'door', '!', 'last', 'weekend', ',', 'i', 'was', 'running', 'some', 'er', '##rand', '##s', 'before', 'a', 'date', 'i', 'had', 'that', 'afternoon', '.', 'when', 'i', 'pulled', 'into', 'the', 'parking', 'lot', 'of', 'the', 'shopping', 'center', ',', 'i', 'decided', 'to', 'clean', 'out', 'my', 'car', '.', 'i', 'turned', 'off', 'the', 'engine', ',', 'but', 'left', 'the', 'keys', 'in', 'the', 'ignition', ',', 'so', 'i', 'could', 'still', 'listen', 'to', 'music', '.', '.', '.', 'i', 'then', 'pull', 'the', 'bag', 'of', 'trash', 'out', 'of', 'my', 'car', ',', 'and', 'close', 'the', 'door', '.', 'i', 'suppose', 'anticipating', 'my', 'date', ',', 'and', 'going', 'though', 'my', 'list', 'of', 'things', 'i', 'wanted', 'to', 'get', 'done', 'beforehand', 'kept', 'me', 'from', 'remembering', 'to', 'make', 'sure', 'i', 'had', 'my', 'keys', 'before', 'closing', 'the', 'door', '!', 'called', 'locks', '##mith', ',', 'they', 'charged', '230', 'dollars', 'to', 'open', 'the', 'door', '.', 'ff', '##ff', '##fu', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', '##u', ':', '-', '/', 'make', 'sure', 'you', 'always', 'have', 'your', 'keys', 'before', 'you', 'close', 'your', 'car', '/', 'house', '/', 'apartment', '/', 'etc', 'door', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['cannot', 'believe', 'it', 'took', 'me', 'this', 'long', ',', 'but', 'i', 'have', 'found', 'my', 'new', 'favorite', 'sub', '##red', '##dit', 'as', 'a', 'college', 'student', ',', 'struggling', 'and', 'excel', '##ling', 'in', 'classes', 'and', 'social', 'life', ',', 'i', 'feel', 'like', 'i', 'finally', 'found', 'people', 'like', 'me', 'on', 'red', '##dit', '.', 'i', 'have', 'a', 'lot', 'to', 'say', ',', 'and', 'a', 'lot', 'to', 'think', 'about', '.', 'this', 'may', 'be', 'the', 'best', 'and', 'worst', 'thing', 'to', 'happen', 'to', 'me', 'today', ',', 'much', 'like', 'having', 'ad', '##hd', 'is', 'the', 'best', 'and', 'worst', 'thing', 'to', 'ever', 'happen', 'to', 'me', '.']\n",
      "INFO:__main__:Number of tokens: 91\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['cannot', 'believe', 'it', 'took', 'me', 'this', 'long', ',', 'but', 'i', 'have', 'found', 'my', 'new', 'favorite', 'sub', '##red', '##dit', 'as', 'a', 'college', 'student', ',', 'struggling', 'and', 'excel', '##ling', 'in', 'classes', 'and', 'social', 'life', ',', 'i', 'feel', 'like', 'i', 'finally', 'found', 'people', 'like', 'me', 'on', 'red', '##dit', '.', 'i', 'have', 'a', 'lot', 'to', 'say', ',', 'and', 'a', 'lot', 'to', 'think', 'about', '.', 'this', 'may', 'be', 'the', 'best', 'and', 'worst', 'thing', 'to', 'happen', 'to', 'me', 'today', ',', 'much', 'like', 'having', 'ad', '##hd', 'is', 'the', 'best', 'and', 'worst', 'thing', 'to', 'ever', 'happen', 'to', 'me', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'almost', 'electro', '##cute', 'themselves', 'when', 'trying', 'to', 'take', 'a', 'bath', '?', 'if', 'i', 'ever', 'decide', 'that', 'i', 'want', 'a', 'bath', ',', 'i', 'usually', 'get', 'in', 'and', 'out', 'many', 'times', 'causing', 'my', 'floor', 'to', 'be', 'very', 'wet', '.', 'the', 'bath', 'ends', 'before', 'i', 'actually', 'decide', 'to', 'clean', '##se', 'myself', '.', 'the', 'only', 'way', 'i', 'am', 'able', 'to', 'sit', 'and', '\"', 'relax', '\"', 'during', 'a', 'bath', 'is', 'if', 'i', 'have', 'my', 'computer', 'balancing', 'very', 'un', '##stead', '##ily', 'on', 'the', 'side', 'of', 'the', 'tub', ';', 'then', 'i', \"'\", 'm', 'in', 'te', '##pid', 'water', 'until', 'i', \"'\", 'm', 'pr', '##une', '##y', '-', 'to', '-', 'the', '-', 'max', '.', '(', 'no', 'med', '##s', ')']\n",
      "INFO:__main__:Number of tokens: 111\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'almost', 'electro', '##cute', 'themselves', 'when', 'trying', 'to', 'take', 'a', 'bath', '?', 'if', 'i', 'ever', 'decide', 'that', 'i', 'want', 'a', 'bath', ',', 'i', 'usually', 'get', 'in', 'and', 'out', 'many', 'times', 'causing', 'my', 'floor', 'to', 'be', 'very', 'wet', '.', 'the', 'bath', 'ends', 'before', 'i', 'actually', 'decide', 'to', 'clean', '##se', 'myself', '.', 'the', 'only', 'way', 'i', 'am', 'able', 'to', 'sit', 'and', '\"', 'relax', '\"', 'during', 'a', 'bath', 'is', 'if', 'i', 'have', 'my', 'computer', 'balancing', 'very', 'un', '##stead', '##ily', 'on', 'the', 'side', 'of', 'the', 'tub', ';', 'then', 'i', \"'\", 'm', 'in', 'te', '##pid', 'water', 'until', 'i', \"'\", 'm', 'pr', '##une', '##y', '-', 'to', '-', 'the', '-', 'max', '.', '(', 'no', 'med', '##s', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'd', 'really', 'like', 'some', 'opinions', 'hey', 'r', '/', 'ad', '##hd', ',', 'i', \"'\", 've', 'kind', 'of', 'been', 'going', 'through', 'a', 'crisis', 'the', 'last', 'couple', 'of', 'months', 'now', 'that', 'i', 'have', 'to', 'do', 'some', 'exams', 'that', 'actually', 'have', 'importance', 'for', 'the', 'first', 'time', 'in', 'my', 'life', '.', 'generally', ',', 'i', 'do', 'pretty', 'badly', 'in', 'my', 'tests', 'because', 'i', 'never', 'usually', 'study', 'or', 'even', 'really', 'attempt', 'to', 'do', 'so', 'and', 'this', 'is', 'ok', 'with', 'me', ',', 'i', 'understand', 'that', 'if', 'i', 'don', \"'\", 't', 'put', 'in', 'effort', ',', 'i', 'won', \"'\", 't', 'get', 'results', ';', 'that', \"'\", 's', 'fine', '.', 'but', 'soon', 'as', 'i', \"'\", 've', 'previously', 'mentioned', 'i', \"'\", 've', 'got', 'the', 'most', 'important', 'exam', 'of', 'my', 'life', '(', 'or', 'so', 'i', \"'\", 'm', 'told', ')', 'coming', 'up', '.', 'this', 'has', 'motivated', 'me', 'to', 'actually', 'try', 'to', 'do', 'well', '.', 'the', 'problem', 'is', 'that', 'i', 'can', \"'\", 't', 'seem', 'to', 'do', 'it', '.', 'i', 'force', 'myself', 'to', 'de', '##dicate', 'at', 'least', 'three', 'hours', 'a', 'day', 'after', 'school', 'to', 'study', 'but', 'i', 'just', 'can', \"'\", 't', 'focus', 'on', 'what', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'doing', '.', 'for', 'example', ':', 'i', \"'\", 'll', 'open', 'up', 'a', 'book', ',', 'start', 'reading', ',', 'then', 'realize', 'about', '10', 'minutes', 'later', 'that', 'i', 'haven', \"'\", 't', 'actually', 'read', 'anything', 'on', 'the', 'page', '.', 'i', 'was', 'just', 'kind', 'of', 'drifting', 'off', '.', 'this', 'isn', \"'\", 't', 'only', 'a', 'problem', 'with', 'my', 'studies', 'i', \"'\", 'm', 'sort', 'of', 'known', 'for', 'being', 'absent', 'minded', 'and', 'forget', '##ful', ',', 'or', 'in', 'the', 'words', 'of', 'those', 'who', 'know', 'me', '\"', 'off', 'in', 'his', 'own', 'world', '\"', '.', 'despite', 'my', 'efforts', 'i', \"'\", 've', 'had', 'little', 'noticeable', 'improvement', 'in', 'months', '.', 'generally', 'i', 'have', 'a', 'tendency', 'to', 'forget', 'things', ',', 'i', \"'\", 'm', 'often', 'accused', 'of', 'being', 'messy', 'and', 'i', \"'\", 'm', 'an', 'expert', 'in', 'pro', '##cr', '##ast', '##ination', '.', '(', 'sometimes', 'i', 'can', 'even', 'convince', 'myself', 'that', 'what', 'i', \"'\", 'm', 'doing', 'is', 'somehow', 'important', 'and', 'i', \"'\", 'm', 'not', 'actually', 'pro', '##cr', '##ast', '##inating', '.', ')', 'anyway', '##s', 'before', 'i', 'go', 'off', 'to', 'any', 'doctors', 'for', 'consultancy', 'i', 'thought', 'i', \"'\", 'd', 'hear', 'what', 'red', '##dit', 'has', 'to', 'say', 'on', 'the', 'matter', '.', '(', 'some', 'info', 'that', 'might', 'be', 'relevant', ',', 'i', \"'\", 'm', '18', 'and', 'i', \"'\", 'm', 'not', '\"', 'hyper', '##active', '\"', 'quite', 'the', 'opposite', 'actually', '.', 'i', 'live', 'in', 'ireland', 'and', 'i', \"'\", 'm', 'a', 'lad', '.', ')']\n",
      "INFO:__main__:Number of tokens: 403\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'd', 'really', 'like', 'some', 'opinions', 'hey', 'r', '/', 'ad', '##hd', ',', 'i', \"'\", 've', 'kind', 'of', 'been', 'going', 'through', 'a', 'crisis', 'the', 'last', 'couple', 'of', 'months', 'now', 'that', 'i', 'have', 'to', 'do', 'some', 'exams', 'that', 'actually', 'have', 'importance', 'for', 'the', 'first', 'time', 'in', 'my', 'life', '.', 'generally', ',', 'i', 'do', 'pretty', 'badly', 'in', 'my', 'tests', 'because', 'i', 'never', 'usually', 'study', 'or', 'even', 'really', 'attempt', 'to', 'do', 'so', 'and', 'this', 'is', 'ok', 'with', 'me', ',', 'i', 'understand', 'that', 'if', 'i', 'don', \"'\", 't', 'put', 'in', 'effort', ',', 'i', 'won', \"'\", 't', 'get', 'results', ';', 'that', \"'\", 's', 'fine', '.', 'but', 'soon', 'as', 'i', \"'\", 've', 'previously', 'mentioned', 'i', \"'\", 've', 'got', 'the', 'most', 'important', 'exam', 'of', 'my', 'life', '(', 'or', 'so', 'i', \"'\", 'm', 'told', ')', 'coming', 'up', '.', 'this', 'has', 'motivated', 'me', 'to', 'actually', 'try', 'to', 'do', 'well', '.', 'the', 'problem', 'is', 'that', 'i', 'can', \"'\", 't', 'seem', 'to', 'do', 'it', '.', 'i', 'force', 'myself', 'to', 'de', '##dicate', 'at', 'least', 'three', 'hours', 'a', 'day', 'after', 'school', 'to', 'study', 'but', 'i', 'just', 'can', \"'\", 't', 'focus', 'on', 'what', 'i', \"'\", 'm', 'supposed', 'to', 'be', 'doing', '.', 'for', 'example', ':', 'i', \"'\", 'll', 'open', 'up', 'a', 'book', ',', 'start', 'reading', ',', 'then', 'realize', 'about', '10', 'minutes', 'later', 'that', 'i', 'haven', \"'\", 't', 'actually', 'read', 'anything', 'on', 'the', 'page', '.', 'i', 'was', 'just', 'kind', 'of', 'drifting', 'off', '.', 'this', 'isn', \"'\", 't', 'only', 'a', 'problem', 'with', 'my', 'studies', 'i', \"'\", 'm', 'sort', 'of', 'known', 'for', 'being', 'absent', 'minded', 'and', 'forget', '##ful', ',', 'or', 'in', 'the', 'words', 'of', 'those', 'who', 'know', 'me', '\"', 'off', 'in', 'his', 'own', 'world', '\"', '.', 'despite', 'my', 'efforts', 'i', \"'\", 've', 'had', 'little', 'noticeable', 'improvement', 'in', 'months', '.', 'generally', 'i', 'have', 'a', 'tendency', 'to', 'forget', 'things', ',', 'i', \"'\", 'm', 'often', 'accused', 'of', 'being', 'messy', 'and', 'i', \"'\", 'm', 'an', 'expert', 'in', 'pro', '##cr', '##ast', '##ination', '.', '(', 'sometimes', 'i', 'can', 'even', 'convince', 'myself', 'that', 'what', 'i', \"'\", 'm', 'doing', 'is', 'somehow', 'important', 'and', 'i', \"'\", 'm', 'not', 'actually', 'pro', '##cr', '##ast', '##inating', '.', ')', 'anyway', '##s', 'before', 'i', 'go', 'off', 'to', 'any', 'doctors', 'for', 'consultancy', 'i', 'thought', 'i', \"'\", 'd', 'hear', 'what', 'red', '##dit', 'has', 'to', 'say', 'on', 'the', 'matter', '.', '(', 'some', 'info', 'that', 'might', 'be', 'relevant', ',', 'i', \"'\", 'm', '18', 'and', 'i', \"'\", 'm', 'not', '\"', 'hyper', '##active', '\"', 'quite', 'the', 'opposite', 'actually', '.', 'i', 'live', 'in', 'ireland', 'and', 'i', \"'\", 'm', 'a', 'lad', '.', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'new', 'favorite', 'website', 'i', 'use', 'this', 'all', 'the', 'time', 'now', '.', 'it', \"'\", 's', 'an', 'online', 'stop', '##watch', 'that', 'lets', 'you', 'set', 'a', 'timer', 'in', 'any', 'interval', '.', 'this', 'is', 'especially', 'helpful', 'for', 'people', 'struggling', 'with', 'add', '/', 'ad', '##hd', 'as', 'finals', 'are', 'approaching', 'for', 'many', 'of', 'us', '.', 'cheers', '!']\n",
      "INFO:__main__:Number of tokens: 52\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'new', 'favorite', 'website', 'i', 'use', 'this', 'all', 'the', 'time', 'now', '.', 'it', \"'\", 's', 'an', 'online', 'stop', '##watch', 'that', 'lets', 'you', 'set', 'a', 'timer', 'in', 'any', 'interval', '.', 'this', 'is', 'especially', 'helpful', 'for', 'people', 'struggling', 'with', 'add', '/', 'ad', '##hd', 'as', 'finals', 'are', 'approaching', 'for', 'many', 'of', 'us', '.', 'cheers', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'smoking', '(', 'nico', '##tine', ')', 'i', 'found', 'myself', 'smoking', 'upwards', 'of', 'three', 'quarters', 'of', 'a', 'pack', 'of', 'cigarettes', 'a', 'day', 'before', 'i', 'was', 'prescribed', 'concert', '##a', '.', 'because', 'i', 'have', 'a', 'history', 'with', 'addiction', ',', 'my', 'psychiatrist', 'had', 'me', 'try', 'out', 'a', 'few', 'different', 'medications', 'before', 'i', 'was', 'prescribed', 'methyl', '##ph', '##eni', '##date', '.', 'one', 'of', 'the', 'techniques', 'he', 'suggested', 'was', 'wearing', 'the', 'nico', '##tine', 'patch', '.', 'i', 'quit', 'for', '9', 'weeks', 'and', 'felt', 'the', 'edge', 'was', 'taken', 'somewhat', 'off', 'of', 'my', 'concentration', 'issues', '.', 'i', 'started', 'smoking', 'again', 'after', 'well', '##bu', '##tri', '##n', '(', 'bu', '##pro', '##pio', '##n', ')', 'wasn', \"'\", 't', 'working', 'out', 'for', 'my', 'concentration', 'and', 'short', '-', 'term', 'memory', 'and', 'imp', '##uls', '##ivity', '.', '.', '.', 'at', 'all', '.', 'i', 'was', 'even', 'smoking', 'while', 'taking', 'well', '##bu', '##tri', '##n', ',', 'which', 'is', 'also', 'prescribed', 'to', 'help', 'people', 'quit', 'smoking', '.', 'it', 'just', 'seemed', 'my', 'mind', 'was', 'craving', 'the', 'calming', '/', 'stimulating', 'double', '-', 'edge', 'of', 'nico', '##tine', '.', 'i', 'was', 'under', 'the', 'impression', 'that', 'pending', 'good', 'results', 'with', 'concert', '##a', ',', 'i', \"'\", 'd', 'be', 'focusing', 'much', 'better', '.', '.', '.', 'and', 'therefore', 'would', 'be', 'able', 'to', 'stop', 'smoking', 'again', 'since', 'the', 'habit', 'isn', \"'\", 't', 'very', 'ing', '##rained', '.', 'unfortunately', 'i', 'had', 'a', 'cigarette', 'and', 'experienced', 'an', 'incredible', 'rush', 'while', 'on', 'concert', '##a', 'and', 'have', 'been', 'finding', 'myself', 'smoking', 'even', 'more', '.', 'my', 'birthday', 'is', 'a', 'few', 'days', 'away', 'and', 'i', \"'\", 'm', 'going', 'to', 'be', 'going', 'on', 'the', 'patch', 'again', '.', 'i', \"'\", 'm', 'just', 'looking', 'for', 'people', 'to', 'share', 'their', 'experiences', 'with', 'nico', '##tine', 'in', 'any', 'form', ',', 'in', 'combination', 'with', 'methyl', '##ph', '##eni', '##date', ',', 'particularly', 'wearing', 'the', 'patch', 'while', 'you', \"'\", 're', 'on', 'it', '.', 'thanks', '.', 'and', 'for', 'the', 'record', 'concert', '##a', 'has', 'worked', 'wonders', 'for', 'me', 'thus', 'far', '.', 'i', 'tried', 'cl', '##oni', '##dine', ',', 'st', '##rate', '##rra', ',', 'and', 'well', '##bu', '##tri', '##n', 'before', 'finally', 'being', 'prescribed', 'concert', '##a', '.', 'cl', '##oni', '##dine', 'helped', 'somewhat', 'but', 'the', 'side', '-', 'effects', 'after', '3', 'months', 'were', 'interfering', 'with', 'my', 'day', '-', 'to', '-', 'day', 'routine', '.', '(', 'di', '##zziness', ',', 'fatigue', ')', '.', 'i', 'experienced', 'eu', '##ph', '##oria', 'initially', 'while', 'on', 'st', '##rate', '##rra', 'and', 'after', 'the', 'first', 'few', 'days', 'it', 'subsided', 'and', 'i', 'did', 'gain', 'a', 'boost', 'in', 'focus', 'though', 'my', 'imp', '##uls', '##ivity', 'was', 'seemingly', 'the', 'same', '.', 'i', 'also', 'had', 'some', 'sexual', 'dysfunction', 'side', '-', 'effects', 'that', 'immediately', 'aba', '##ted', 'after', 'being', 'taken', 'off', 'the', 'medication', '.', 'well', '##bu', '##tri', '##n', 'really', 'didn', \"'\", 't', 'do', 'much', 'for', 'me', 'outside', 'of', 'potentially', 'helping', 'my', 'mood', ',', 'but', 'it', \"'\", 's', 'been', 'relatively', 'stable', 'regardless', '.']\n",
      "INFO:__main__:Number of tokens: 441\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'smoking', '(', 'nico', '##tine', ')', 'i', 'found', 'myself', 'smoking', 'upwards', 'of', 'three', 'quarters', 'of', 'a', 'pack', 'of', 'cigarettes', 'a', 'day', 'before', 'i', 'was', 'prescribed', 'concert', '##a', '.', 'because', 'i', 'have', 'a', 'history', 'with', 'addiction', ',', 'my', 'psychiatrist', 'had', 'me', 'try', 'out', 'a', 'few', 'different', 'medications', 'before', 'i', 'was', 'prescribed', 'methyl', '##ph', '##eni', '##date', '.', 'one', 'of', 'the', 'techniques', 'he', 'suggested', 'was', 'wearing', 'the', 'nico', '##tine', 'patch', '.', 'i', 'quit', 'for', '9', 'weeks', 'and', 'felt', 'the', 'edge', 'was', 'taken', 'somewhat', 'off', 'of', 'my', 'concentration', 'issues', '.', 'i', 'started', 'smoking', 'again', 'after', 'well', '##bu', '##tri', '##n', '(', 'bu', '##pro', '##pio', '##n', ')', 'wasn', \"'\", 't', 'working', 'out', 'for', 'my', 'concentration', 'and', 'short', '-', 'term', 'memory', 'and', 'imp', '##uls', '##ivity', '.', '.', '.', 'at', 'all', '.', 'i', 'was', 'even', 'smoking', 'while', 'taking', 'well', '##bu', '##tri', '##n', ',', 'which', 'is', 'also', 'prescribed', 'to', 'help', 'people', 'quit', 'smoking', '.', 'it', 'just', 'seemed', 'my', 'mind', 'was', 'craving', 'the', 'calming', '/', 'stimulating', 'double', '-', 'edge', 'of', 'nico', '##tine', '.', 'i', 'was', 'under', 'the', 'impression', 'that', 'pending', 'good', 'results', 'with', 'concert', '##a', ',', 'i', \"'\", 'd', 'be', 'focusing', 'much', 'better', '.', '.', '.', 'and', 'therefore', 'would', 'be', 'able', 'to', 'stop', 'smoking', 'again', 'since', 'the', 'habit', 'isn', \"'\", 't', 'very', 'ing', '##rained', '.', 'unfortunately', 'i', 'had', 'a', 'cigarette', 'and', 'experienced', 'an', 'incredible', 'rush', 'while', 'on', 'concert', '##a', 'and', 'have', 'been', 'finding', 'myself', 'smoking', 'even', 'more', '.', 'my', 'birthday', 'is', 'a', 'few', 'days', 'away', 'and', 'i', \"'\", 'm', 'going', 'to', 'be', 'going', 'on', 'the', 'patch', 'again', '.', 'i', \"'\", 'm', 'just', 'looking', 'for', 'people', 'to', 'share', 'their', 'experiences', 'with', 'nico', '##tine', 'in', 'any', 'form', ',', 'in', 'combination', 'with', 'methyl', '##ph', '##eni', '##date', ',', 'particularly', 'wearing', 'the', 'patch', 'while', 'you', \"'\", 're', 'on', 'it', '.', 'thanks', '.', 'and', 'for', 'the', 'record', 'concert', '##a', 'has', 'worked', 'wonders', 'for', 'me', 'thus', 'far', '.', 'i', 'tried', 'cl', '##oni', '##dine', ',', 'st', '##rate', '##rra', ',', 'and', 'well', '##bu', '##tri', '##n', 'before', 'finally', 'being', 'prescribed', 'concert', '##a', '.', 'cl', '##oni', '##dine', 'helped', 'somewhat', 'but', 'the', 'side', '-', 'effects', 'after', '3', 'months', 'were', 'interfering', 'with', 'my', 'day', '-', 'to', '-', 'day', 'routine', '.', '(', 'di', '##zziness', ',', 'fatigue', ')', '.', 'i', 'experienced', 'eu', '##ph', '##oria', 'initially', 'while', 'on', 'st', '##rate', '##rra', 'and', 'after', 'the', 'first', 'few', 'days', 'it', 'subsided', 'and', 'i', 'did', 'gain', 'a', 'boost', 'in', 'focus', 'though', 'my', 'imp', '##uls', '##ivity', 'was', 'seemingly', 'the', 'same', '.', 'i', 'also', 'had', 'some', 'sexual', 'dysfunction', 'side', '-', 'effects', 'that', 'immediately', 'aba', '##ted', 'after', 'being', 'taken', 'off', 'the', 'medication', '.', 'well', '##bu', '##tri', '##n', 'really', 'didn', \"'\", 't', 'do', 'much', 'for', 'me', 'outside', 'of', 'potentially', 'helping', 'my', 'mood', ',', 'but', 'it', \"'\", 's', 'been', 'relatively', 'stable', 'regardless', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['this', 'was', 'posted', 'on', '/', 'r', '/', 'science', 'today', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['this', 'was', 'posted', 'on', '/', 'r', '/', 'science', 'today', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', ',', 'i', 'drank', 'a', 'bit', 'of', 'coffee', 'thinking', 'it', 'wouldn', \"'\", 't', 'affect', 'my', 'med', '##s', '.', 'i', 'was', 'so', 'wrong', '.']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', ',', 'i', 'drank', 'a', 'bit', 'of', 'coffee', 'thinking', 'it', 'wouldn', \"'\", 't', 'affect', 'my', 'med', '##s', '.', 'i', 'was', 'so', 'wrong', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['non', 'drug', 'ad', '##hd', 'options', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['non', 'drug', 'ad', '##hd', 'options', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['practical', 'tips', 'for', 'college', 'students', 'w', '/', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['practical', 'tips', 'for', 'college', 'students', 'w', '/', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'a', 'lot', 'of', 'help', '.', '.', '.', 'to', 'those', 'who', 'might', 'be', 'experts', 'on', 'depression', 'and', 'ad', '##hd', 'co', '-', 'mor', '##bid', '##ity', '.', 'i', \"'\", 'm', 'a', 'college', 'student', 'in', 'a', 'relationship', 'with', 'a', 'wonderful', 'girl', '.', 'i', \"'\", 've', 'known', 'that', 'she', 'was', 'ad', '##hd', 'for', 'some', 'time', ',', 'and', 'i', \"'\", 've', 'also', 'known', 'her', 'to', 'have', 'depressed', 'spells', 'now', 'and', 'again', '.', 'these', 'are', 'things', 'that', 'i', 'try', 'to', 'help', 'her', 'with', ',', 'but', 'recently', ',', 'things', 'have', 'gotten', 'very', 'bad', '.', 'suicidal', 'thoughts', 'bad', '.', 'she', \"'\", 's', 'going', 'to', 'need', 'to', 'repeat', 'required', 'classes', ',', 'and', 'it', \"'\", 's', 'removed', 'all', 'her', 'confidence', 'and', 'hope', ',', 'in', 'her', 'words', '.', 'things', 'that', 'i', 'say', 'just', 'seem', 'to', 'make', 'the', 'situation', 'worse', '.', '.', '.', 'i', 'try', 'to', 'suggest', 'that', 'she', 'change', 'her', 'study', 'habits', '(', 'studying', 'the', 'night', 'before', ')', 'and', 'she', 'says', 'it', 'just', 'doesn', \"'\", 't', 'work', ',', 'that', 'she', 'can', \"'\", 't', 'do', 'it', '.', 'she', \"'\", 'll', 'completely', 'shut', 'down', ',', 'say', 'how', 'much', 'of', 'a', 'burden', 'she', 'is', 'on', 'me', ',', 'say', 'that', 'cutting', 'herself', 'is', 'the', 'way', 'to', 'make', 'her', 'feel', 'a', 'little', 'better', '(', 'she', 'says', 'she', 'doesn', \"'\", 't', 'want', 'to', 'hurt', 'me', 'because', 'she', 'loves', 'me', ',', 'so', 'she', \"'\", 'd', 'rather', 'hurt', 'herself', ')', '.', 'i', 'never', 'let', 'her', 'do', 'that', ',', 'of', 'course', '.', '.', '.', 'she', 'also', 'has', 'extreme', 'anxiety', 'when', 'even', 'the', 'slightest', 'suggestion', 'of', 'speaking', 'with', 'her', 'teachers', 'comes', 'up', '.', 'she', 'quite', 'literally', 'freeze', '##s', 'up', 'and', 'can', \"'\", 't', 'ever', 'bring', 'herself', 'to', 'personally', 'speak', '.', 'even', 'email', '##ing', 'is', 'difficult', ',', 'if', 'not', 'impossible', '.', 'i', \"'\", 've', 'gotten', 'her', 'to', 'counseling', ',', 'but', 'it', 'really', 'isn', \"'\", 't', 'doing', 'anything', '.', 'it', 'seems', 'to', 'make', 'things', 'worse', ',', 'actually', '.', 'it', 'sounds', 'like', 'they', 'give', 'her', 'the', 'same', 'suggestions', 'i', 'do', ',', 'she', 'is', 'currently', 'on', 'medication', 'for', 'both', 'ad', '##hd', 'and', 'depression', '.', 'i', \"'\", 'm', 'looking', 'for', 'someone', 'with', 'tips', '/', 'techniques', 'on', 'ways', 'i', 'can', 'help', 'my', 'love', '.', 'i', 'want', 'to', 'be', 'able', 'to', 'say', 'things', 'that', 'won', \"'\", 't', 'make', 'her', 'upset', '(', 'because', 'i', 'don', \"'\", 't', 'understand', 'the', 'ad', '##hd', 'thinking', 'process', ')', ',', 'and', 'i', 'want', 'to', 'be', 'able', 'to', 'help', 'her', 'out', 'of', 'these', 'de', '##pressive', 'episodes', '.', 'i', 'hope', 'i', \"'\", 'm', 'being', 'clear', 'enough', 'and', 'i', \"'\", 'm', 'providing', 'enough', 'information', '.', 'if', 'not', ',', 'i', \"'\", 'll', 'try', 'to', 'clear', 'it', 'up', 'as', 'best', 'as', 'i', 'can', 'and', 'make', 'edit', '##s', 'to', 'this', 'post', 'as', 'needed', '.', 'thank', 'you', '.']\n",
      "INFO:__main__:Number of tokens: 438\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'a', 'lot', 'of', 'help', '.', '.', '.', 'to', 'those', 'who', 'might', 'be', 'experts', 'on', 'depression', 'and', 'ad', '##hd', 'co', '-', 'mor', '##bid', '##ity', '.', 'i', \"'\", 'm', 'a', 'college', 'student', 'in', 'a', 'relationship', 'with', 'a', 'wonderful', 'girl', '.', 'i', \"'\", 've', 'known', 'that', 'she', 'was', 'ad', '##hd', 'for', 'some', 'time', ',', 'and', 'i', \"'\", 've', 'also', 'known', 'her', 'to', 'have', 'depressed', 'spells', 'now', 'and', 'again', '.', 'these', 'are', 'things', 'that', 'i', 'try', 'to', 'help', 'her', 'with', ',', 'but', 'recently', ',', 'things', 'have', 'gotten', 'very', 'bad', '.', 'suicidal', 'thoughts', 'bad', '.', 'she', \"'\", 's', 'going', 'to', 'need', 'to', 'repeat', 'required', 'classes', ',', 'and', 'it', \"'\", 's', 'removed', 'all', 'her', 'confidence', 'and', 'hope', ',', 'in', 'her', 'words', '.', 'things', 'that', 'i', 'say', 'just', 'seem', 'to', 'make', 'the', 'situation', 'worse', '.', '.', '.', 'i', 'try', 'to', 'suggest', 'that', 'she', 'change', 'her', 'study', 'habits', '(', 'studying', 'the', 'night', 'before', ')', 'and', 'she', 'says', 'it', 'just', 'doesn', \"'\", 't', 'work', ',', 'that', 'she', 'can', \"'\", 't', 'do', 'it', '.', 'she', \"'\", 'll', 'completely', 'shut', 'down', ',', 'say', 'how', 'much', 'of', 'a', 'burden', 'she', 'is', 'on', 'me', ',', 'say', 'that', 'cutting', 'herself', 'is', 'the', 'way', 'to', 'make', 'her', 'feel', 'a', 'little', 'better', '(', 'she', 'says', 'she', 'doesn', \"'\", 't', 'want', 'to', 'hurt', 'me', 'because', 'she', 'loves', 'me', ',', 'so', 'she', \"'\", 'd', 'rather', 'hurt', 'herself', ')', '.', 'i', 'never', 'let', 'her', 'do', 'that', ',', 'of', 'course', '.', '.', '.', 'she', 'also', 'has', 'extreme', 'anxiety', 'when', 'even', 'the', 'slightest', 'suggestion', 'of', 'speaking', 'with', 'her', 'teachers', 'comes', 'up', '.', 'she', 'quite', 'literally', 'freeze', '##s', 'up', 'and', 'can', \"'\", 't', 'ever', 'bring', 'herself', 'to', 'personally', 'speak', '.', 'even', 'email', '##ing', 'is', 'difficult', ',', 'if', 'not', 'impossible', '.', 'i', \"'\", 've', 'gotten', 'her', 'to', 'counseling', ',', 'but', 'it', 'really', 'isn', \"'\", 't', 'doing', 'anything', '.', 'it', 'seems', 'to', 'make', 'things', 'worse', ',', 'actually', '.', 'it', 'sounds', 'like', 'they', 'give', 'her', 'the', 'same', 'suggestions', 'i', 'do', ',', 'she', 'is', 'currently', 'on', 'medication', 'for', 'both', 'ad', '##hd', 'and', 'depression', '.', 'i', \"'\", 'm', 'looking', 'for', 'someone', 'with', 'tips', '/', 'techniques', 'on', 'ways', 'i', 'can', 'help', 'my', 'love', '.', 'i', 'want', 'to', 'be', 'able', 'to', 'say', 'things', 'that', 'won', \"'\", 't', 'make', 'her', 'upset', '(', 'because', 'i', 'don', \"'\", 't', 'understand', 'the', 'ad', '##hd', 'thinking', 'process', ')', ',', 'and', 'i', 'want', 'to', 'be', 'able', 'to', 'help', 'her', 'out', 'of', 'these', 'de', '##pressive', 'episodes', '.', 'i', 'hope', 'i', \"'\", 'm', 'being', 'clear', 'enough', 'and', 'i', \"'\", 'm', 'providing', 'enough', 'information', '.', 'if', 'not', ',', 'i', \"'\", 'll', 'try', 'to', 'clear', 'it', 'up', 'as', 'best', 'as', 'i', 'can', 'and', 'make', 'edit', '##s', 'to', 'this', 'post', 'as', 'needed', '.', 'thank', 'you', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['diagnosis', 'confirmed', ',', 'i', 'have', 'ad', '##hd', ',', 'and', 'all', 'i', 'got', 'was', 'met', '##h', '.']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['diagnosis', 'confirmed', ',', 'i', 'have', 'ad', '##hd', ',', 'and', 'all', 'i', 'got', 'was', 'met', '##h', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'make', 'packing', 'lists', 'for', 'events', ',', 'and', 'am', 'now', 'planning', 'my', 'wedding', '.', 'as', 'requested', ',', 'here', \"'\", 's', 'the', 'packing', 'list', 'for', 'my', 'wedding', 'emergency', 'bag', '.', 'i', \"'\", 'm', 'building', 'a', 'packing', 'list', 'for', 'my', 'upcoming', 'wedding', ',', 'specifically', 'for', 'the', 'emergency', 'kit', '.', 'i', 'mentioned', 'it', 'to', 'some', 'fellow', 'r', '/', 'ad', '##hd', 'folks', 'in', 'ir', '##c', 'recently', 'and', 'there', 'was', 'some', 'interest', 'in', 'the', 'list', '.', 'i', \"'\", 've', 'edited', 'this', 'to', 'add', 'some', 'items', 'i', 'normally', 'om', '##it', 'from', 'the', 'list', 'because', 'they', \"'\", 're', 'always', 'with', 'me', '.', 'this', 'list', 'is', 'pretty', 'similar', 'to', 'what', 'i', 'pack', 'when', 'i', \"'\", 'm', 'working', 'a', 'convention', ',', 'with', 'some', 'additions', 'in', 'case', 'of', 'more', 'personal', 'disasters', 'around', 'me', '.', '(', 'actually', ',', 'it', 'occurs', 'to', 'me', 'i', \"'\", 've', 'removed', 'a', 'number', 'of', 'things', 'i', 'pack', 'for', 'conventions', '.', 'if', 'anyone', \"'\", 's', 'interested', 'in', 'my', 'usual', 'list', ',', 'let', 'me', 'know', '.', ')', 'also', ',', 'if', 'you', \"'\", 've', 'got', 'an', 'ios', 'device', ',', 'i', 'used', 'a', 'free', 'application', 'called', 'pack', '##the', '##bag', 'to', 'generate', 'this', 'list', ',', 'it', \"'\", 's', 'pretty', 'handy', 'for', 'whipping', 'up', 'quick', 'packing', 'lists', '.', 'the', 'wedding', '\"', 'oh', 'shit', '!', '\"', 'bag', 'packing', 'list', ':', '*', 'sandals', '*', 'sun', 'glasses', '*', 'ethernet', 'cable', '*', 'paper', '/', 'pens', '*', 'sharp', '##ies', '*', 'belt', '*', 'bra', '*', 'emergency', 'pants', '*', 'underwear', '*', 'duct', 'tape', '*', 'garment', 'tape', '*', 'hand', 'san', '##iti', '##zer', '*', 'wet', 'wipe', '##s', '*', 'water', 'bottle', '*', 'sun', '##block', '*', 'brush', '*', 'de', '##od', '##oran', '##t', '*', 'hair', 'clips', '*', 'lip', 'bal', '##m', '*', 'make', '-', 'up', 'accessories', '*', 'nail', 'file', '*', 'perfume', '*', 'tam', '##pon', '##s', '/', 'sanitary', 'pads', '*', 'tide', 'pens', '*', 'identification', 'card', '/', 'id', '*', 'bottle', '/', 'can', 'opener', '*', 'pen', 'and', 'paper', '*', 'pocket', 'knife', '*', 'scissors', '*', 'screw', '##drive', '##r', '*', 'sewing', 'kit', '*', 'umbrella', '*', 'cell', '##phone', 'charge', '##r', '*', 'head', '##phones', '*', 'ipad', '*', 'ipod', '/', 'mp3', 'player', '*', 'multiple', 'outlet', 'strip', '*', 'audio', 'patch', 'cable', '*', 'multi', '##to', '##ol', '*', 'board', 'games', '*', 'dice', '*', 'all', '##ergy', 'med', '##s', '*', 'sterile', 'bandages', '*', 'ib', '##up', '##ro', '##fen', '*', 'jacket']\n",
      "INFO:__main__:Number of tokens: 361\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'make', 'packing', 'lists', 'for', 'events', ',', 'and', 'am', 'now', 'planning', 'my', 'wedding', '.', 'as', 'requested', ',', 'here', \"'\", 's', 'the', 'packing', 'list', 'for', 'my', 'wedding', 'emergency', 'bag', '.', 'i', \"'\", 'm', 'building', 'a', 'packing', 'list', 'for', 'my', 'upcoming', 'wedding', ',', 'specifically', 'for', 'the', 'emergency', 'kit', '.', 'i', 'mentioned', 'it', 'to', 'some', 'fellow', 'r', '/', 'ad', '##hd', 'folks', 'in', 'ir', '##c', 'recently', 'and', 'there', 'was', 'some', 'interest', 'in', 'the', 'list', '.', 'i', \"'\", 've', 'edited', 'this', 'to', 'add', 'some', 'items', 'i', 'normally', 'om', '##it', 'from', 'the', 'list', 'because', 'they', \"'\", 're', 'always', 'with', 'me', '.', 'this', 'list', 'is', 'pretty', 'similar', 'to', 'what', 'i', 'pack', 'when', 'i', \"'\", 'm', 'working', 'a', 'convention', ',', 'with', 'some', 'additions', 'in', 'case', 'of', 'more', 'personal', 'disasters', 'around', 'me', '.', '(', 'actually', ',', 'it', 'occurs', 'to', 'me', 'i', \"'\", 've', 'removed', 'a', 'number', 'of', 'things', 'i', 'pack', 'for', 'conventions', '.', 'if', 'anyone', \"'\", 's', 'interested', 'in', 'my', 'usual', 'list', ',', 'let', 'me', 'know', '.', ')', 'also', ',', 'if', 'you', \"'\", 've', 'got', 'an', 'ios', 'device', ',', 'i', 'used', 'a', 'free', 'application', 'called', 'pack', '##the', '##bag', 'to', 'generate', 'this', 'list', ',', 'it', \"'\", 's', 'pretty', 'handy', 'for', 'whipping', 'up', 'quick', 'packing', 'lists', '.', 'the', 'wedding', '\"', 'oh', 'shit', '!', '\"', 'bag', 'packing', 'list', ':', '*', 'sandals', '*', 'sun', 'glasses', '*', 'ethernet', 'cable', '*', 'paper', '/', 'pens', '*', 'sharp', '##ies', '*', 'belt', '*', 'bra', '*', 'emergency', 'pants', '*', 'underwear', '*', 'duct', 'tape', '*', 'garment', 'tape', '*', 'hand', 'san', '##iti', '##zer', '*', 'wet', 'wipe', '##s', '*', 'water', 'bottle', '*', 'sun', '##block', '*', 'brush', '*', 'de', '##od', '##oran', '##t', '*', 'hair', 'clips', '*', 'lip', 'bal', '##m', '*', 'make', '-', 'up', 'accessories', '*', 'nail', 'file', '*', 'perfume', '*', 'tam', '##pon', '##s', '/', 'sanitary', 'pads', '*', 'tide', 'pens', '*', 'identification', 'card', '/', 'id', '*', 'bottle', '/', 'can', 'opener', '*', 'pen', 'and', 'paper', '*', 'pocket', 'knife', '*', 'scissors', '*', 'screw', '##drive', '##r', '*', 'sewing', 'kit', '*', 'umbrella', '*', 'cell', '##phone', 'charge', '##r', '*', 'head', '##phones', '*', 'ipad', '*', 'ipod', '/', 'mp3', 'player', '*', 'multiple', 'outlet', 'strip', '*', 'audio', 'patch', 'cable', '*', 'multi', '##to', '##ol', '*', 'board', 'games', '*', 'dice', '*', 'all', '##ergy', 'med', '##s', '*', 'sterile', 'bandages', '*', 'ib', '##up', '##ro', '##fen', '*', 'jacket']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', 'lot', 'of', 'you', 'have', 'probably', 'heard', 'it', 'but', ',', 'what', 'do', 'you', 'think', 'of', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', 'lot', 'of', 'you', 'have', 'probably', 'heard', 'it', 'but', ',', 'what', 'do', 'you', 'think', 'of', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tips', 'for', 'overcoming', 'senior', '##itis', '?', 'so', 'i', 'am', 'a', 'college', 'senior', ',', 'and', 'my', 'last', 'real', 'semester', 'is', 'coming', 'to', 'a', 'close', 'in', '2', 'weeks', '(', 'i', 'technically', 'have', '2', 'summer', 'classes', ',', 'but', 'they', 'are', 'not', 'difficult', 'at', 'all', ')', '.', 'my', 'senior', 'caps', '##tone', 'project', 'is', 'basically', 'done', ',', 'and', 'i', 'no', 'longer', 'give', 'a', 'rat', \"'\", 's', 'ass', 'about', 'my', '2', 'of', 'my', '3', 'other', 'projects', '.', 'my', 'biggest', 'worry', 'is', 'not', 'the', 'projects', ',', 'but', 'my', 'finals', '.', 'i', 'have', 'one', 'normal', 'final', ',', 'and', '2', 'take', 'home', 'finals', ',', 'but', 'the', 'normal', ',', 'in', 'class', 'final', 'is', 'in', 'the', 'hardest', 'class', ',', 'with', 'the', '(', 'objective', '##ly', ')', 'worst', 'teacher', ',', 'and', 'i', 'have', 'no', 'comprehension', 'of', 'the', 'material', '.', 'are', 'there', 'any', 'tips', 'or', 'tricks', 'that', 'i', 'can', 'use', 'to', 'actually', 'learn', 'something', 'from', 'studying', '?', 'i', 'had', 'a', 'test', 'this', 'morning', ',', 'and', 'last', 'night', 'i', 'found', 'myself', 'not', 'caring', 'at', 'all', ',', 'and', 'thus', 'not', 'actually', 'absorbing', 'any', 'info', '(', 'not', 'that', 'it', 'helped', 'because', 'none', 'of', 'the', 'material', 'that', 'i', 'studied', 'appeared', 'on', 'the', 'test', ')', '.', 'i', 'don', \"'\", 't', 'think', 'i', \"'\", 'm', 'in', 'risk', 'of', 'failing', 'any', 'classes', ',', 'but', 'i', \"'\", 'd', 'rather', 'not', 'get', 'any', 'ds', 'my', 'senior', 'year', '.']\n",
      "INFO:__main__:Number of tokens: 215\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tips', 'for', 'overcoming', 'senior', '##itis', '?', 'so', 'i', 'am', 'a', 'college', 'senior', ',', 'and', 'my', 'last', 'real', 'semester', 'is', 'coming', 'to', 'a', 'close', 'in', '2', 'weeks', '(', 'i', 'technically', 'have', '2', 'summer', 'classes', ',', 'but', 'they', 'are', 'not', 'difficult', 'at', 'all', ')', '.', 'my', 'senior', 'caps', '##tone', 'project', 'is', 'basically', 'done', ',', 'and', 'i', 'no', 'longer', 'give', 'a', 'rat', \"'\", 's', 'ass', 'about', 'my', '2', 'of', 'my', '3', 'other', 'projects', '.', 'my', 'biggest', 'worry', 'is', 'not', 'the', 'projects', ',', 'but', 'my', 'finals', '.', 'i', 'have', 'one', 'normal', 'final', ',', 'and', '2', 'take', 'home', 'finals', ',', 'but', 'the', 'normal', ',', 'in', 'class', 'final', 'is', 'in', 'the', 'hardest', 'class', ',', 'with', 'the', '(', 'objective', '##ly', ')', 'worst', 'teacher', ',', 'and', 'i', 'have', 'no', 'comprehension', 'of', 'the', 'material', '.', 'are', 'there', 'any', 'tips', 'or', 'tricks', 'that', 'i', 'can', 'use', 'to', 'actually', 'learn', 'something', 'from', 'studying', '?', 'i', 'had', 'a', 'test', 'this', 'morning', ',', 'and', 'last', 'night', 'i', 'found', 'myself', 'not', 'caring', 'at', 'all', ',', 'and', 'thus', 'not', 'actually', 'absorbing', 'any', 'info', '(', 'not', 'that', 'it', 'helped', 'because', 'none', 'of', 'the', 'material', 'that', 'i', 'studied', 'appeared', 'on', 'the', 'test', ')', '.', 'i', 'don', \"'\", 't', 'think', 'i', \"'\", 'm', 'in', 'risk', 'of', 'failing', 'any', 'classes', ',', 'but', 'i', \"'\", 'd', 'rather', 'not', 'get', 'any', 'ds', 'my', 'senior', 'year', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dangers', 'of', 'combining', 'medications', '?', '(', 'st', '##rat', '##tera', 'and', 'concert', '##a', '/', 'rita', '##lin', ')']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dangers', 'of', 'combining', 'medications', '?', '(', 'st', '##rat', '##tera', 'and', 'concert', '##a', '/', 'rita', '##lin', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['unexpected', 'downs', '##ides', 'of', 'your', 'ad', '##hd', 'just', 'curious', 'if', 'anyone', 'had', 'any', 'unexpected', 'downs', '##ides', 'to', 'their', 'ad', '##hd', '(', 'aside', 'from', 'the', 'obvious', 'lack', 'of', 'focus', ',', 'inability', 'to', 'get', 'things', 'done', ',', 'etc', ')', '.', 'one', 'example', 'i', 'have', 'is', 'that', 'because', 'of', 'my', 'medication', ',', 'my', 'resting', 'heart', 'rate', 'is', 'too', 'high', 'and', 'i', 'can', \"'\", 't', 'give', 'plasma', '.']\n",
      "INFO:__main__:Number of tokens: 64\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['unexpected', 'downs', '##ides', 'of', 'your', 'ad', '##hd', 'just', 'curious', 'if', 'anyone', 'had', 'any', 'unexpected', 'downs', '##ides', 'to', 'their', 'ad', '##hd', '(', 'aside', 'from', 'the', 'obvious', 'lack', 'of', 'focus', ',', 'inability', 'to', 'get', 'things', 'done', ',', 'etc', ')', '.', 'one', 'example', 'i', 'have', 'is', 'that', 'because', 'of', 'my', 'medication', ',', 'my', 'resting', 'heart', 'rate', 'is', 'too', 'high', 'and', 'i', 'can', \"'\", 't', 'give', 'plasma', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'own', 'personal', 'ad', '##hd', 'hell', '.', 'anyone', 'else', 'ace', 'tests', 'but', 'blow', 'homework', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'own', 'personal', 'ad', '##hd', 'hell', '.', 'anyone', 'else', 'ace', 'tests', 'but', 'blow', 'homework', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['mixing', 'amp', '##het', '##amine', '##s']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['mixing', 'amp', '##het', '##amine', '##s']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'your', 'experiences', 'with', 'giving', 'friends', 'some', 'of', 'your', 'medication', '?', 'or', 'them', 'asking', 'for', 'some', '?', 'so', 'pretty', 'much', 'friends', 'and', 'your', 'medication', 'in', 'general', '.', '(', 'note', ':', 'i', 'know', 'its', 'not', 'legal', 'to', 'give', 'people', 'medication', 'that', \"'\", 's', 'not', 'prescribed', 'to', 'them', ')']\n",
      "INFO:__main__:Number of tokens: 48\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'your', 'experiences', 'with', 'giving', 'friends', 'some', 'of', 'your', 'medication', '?', 'or', 'them', 'asking', 'for', 'some', '?', 'so', 'pretty', 'much', 'friends', 'and', 'your', 'medication', 'in', 'general', '.', '(', 'note', ':', 'i', 'know', 'its', 'not', 'legal', 'to', 'give', 'people', 'medication', 'that', \"'\", 's', 'not', 'prescribed', 'to', 'them', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'else', 'not', 'have', 'any', 'ad', '##hd', 'support', 'groups', '?', 'are', 'they', 'even', 'useful', '?', 'i', 'was', 'very', 'hopeful', 'today', 'learning', 'over', 'the', 'inter', '##we', '##bs', 'that', 'there', \"'\", 's', 'a', 'ad', '/', 'hd', 'support', 'group', 'in', 'my', 'hometown', '.', 'however', ',', 'after', 'looking', 'at', 'it', ',', 'it', \"'\", 's', 'for', 'parents', 'who', 'have', 'children', 'with', 'ad', '/', 'hd', '.', 'sigh', '.', 'needles', '##s', 'to', 'say', 'my', 'hope', 'def', '##lated', 'like', 'a', 'so', '##ggy', 'balloon', '.', 'any', '##w', '##ho', '.', 'have', 'you', 'guys', 'ever', 'gone', 'to', 'a', 'support', 'group', 'meeting', '?', 'are', 'they', 'nice', 'there', '?', 'is', 'it', 'worth', 'going', 'to', '?', 'how', 'about', 'one', '-', 'on', '-', 'one', 'therapy', '?', 'is', 'it', 'okay', 'to', 'get', 'someone', 'who', 'doesn', \"'\", 't', 'special', '##ize', 'in', 'ad', '/', 'hd', '?']\n",
      "INFO:__main__:Number of tokens: 129\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'else', 'not', 'have', 'any', 'ad', '##hd', 'support', 'groups', '?', 'are', 'they', 'even', 'useful', '?', 'i', 'was', 'very', 'hopeful', 'today', 'learning', 'over', 'the', 'inter', '##we', '##bs', 'that', 'there', \"'\", 's', 'a', 'ad', '/', 'hd', 'support', 'group', 'in', 'my', 'hometown', '.', 'however', ',', 'after', 'looking', 'at', 'it', ',', 'it', \"'\", 's', 'for', 'parents', 'who', 'have', 'children', 'with', 'ad', '/', 'hd', '.', 'sigh', '.', 'needles', '##s', 'to', 'say', 'my', 'hope', 'def', '##lated', 'like', 'a', 'so', '##ggy', 'balloon', '.', 'any', '##w', '##ho', '.', 'have', 'you', 'guys', 'ever', 'gone', 'to', 'a', 'support', 'group', 'meeting', '?', 'are', 'they', 'nice', 'there', '?', 'is', 'it', 'worth', 'going', 'to', '?', 'how', 'about', 'one', '-', 'on', '-', 'one', 'therapy', '?', 'is', 'it', 'okay', 'to', 'get', 'someone', 'who', 'doesn', \"'\", 't', 'special', '##ize', 'in', 'ad', '/', 'hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['if', 'you', \"'\", 're', 'happy', 'with', 'your', 'time', '/', 'task', 'management', ',', 'please', 'tell', 'us', 'how', 'you', 'make', 'it', 'work', 'if', 'you', 'are', '(', 'mostly', ')', 'happy', 'with', 'your', 'getting', 'things', 'done', '(', 'or', 'whatever', 'you', \"'\", 'd', 'call', 'it', ')', ',', 'please', 'share', 'your', 'system', 'and', 'technique', 'and', 'also', ',', 'what', 'mo', '##tiv', '##ates', 'you', 'to', 'stick', 'with', 'it', '.', '*', 'edit', '3', ':', '*', 'forget', 'the', '\"', 'if', '\"', '-', 'part', 'of', 'the', 'question', '.', 'what', 'techniques', 'or', 'strategies', 'help', 'you', '?', 'for', 'me', ',', 'implementation', 'is', 'quite', 'problematic', '.', 'at', 'the', 'moment', ',', 'my', 'most', 'important', 'time', 'management', 'technique', 'consists', 'of', 'getting', 'into', 'the', 'library', '(', 'quiet', 'and', 'productive', 'surrounding', ')', '.', 'once', 'i', 'am', 'there', ',', 'i', 'have', 'no', 'choice', 'but', 'to', 'do', 'something', 'productive', '.', 'other', 'than', 'that', ',', 'some', 'ideas', 'that', 'helped', 'me', 'so', 'far', ':', '*', 'motivation', ':', 'i', 'think', 'about', 'death', 'often', ',', 'it', 'wakes', 'me', 'up', '.', 'life', 'feels', 'so', 'precious', ',', 'one', 'moment', 'ago', 'i', 'was', 'a', 'child', ',', 'watching', 'our', 'old', 'black', 'cat', 'chase', 'imaginary', 'birds', 'in', 'the', 'garden', ',', 'now', 'i', 'live', 'in', 'a', 'strange', 'city', 'alone', ';', 'only', 'i', 'remember', 'the', 'cat', '.', 'streets', 'and', 'faces', 'rushing', 'by', ',', 'one', 'day', 'it', 'will', 'all', 'be', 'gone', ',', 'how', 'can', 'i', 'bring', 'this', 'moment', 'of', 'my', 'fleeting', 'life', 'to', 'light', 'up', '?', 'this', ',', 'even', 'if', 'it', 'is', 'not', 'optimal', ',', 'i', 'only', 'have', 'this', ',', 'here', ',', 'now', '.', '-', 'what', 'do', '?', '*', 'consistency', ':', 'it', 'became', 'a', 'ritual', 'to', 'spend', '2', 'minutes', 'with', 'the', 'time', 'stuff', 'every', 'evening', ',', 'at', 'least', '.', 'i', 'still', 'forget', 'it', 'pretty', 'often', ',', 'some', 'mayhem', 'later', ',', 'i', 'rue', '##fully', 'come', 'back', 'to', 'it', '.', 'also', ',', 'on', 'an', 'un', '##st', '##ructured', 'day', ',', 'i', 'feel', 'crap', '##py', ',', 'which', 'reminds', 'me', 'to', 'get', 'back', 'to', 'it', 'again', '.', '*', 'paper', ':', 'everything', 'i', 'write', 'by', 'hand', 'gets', 'somehow', 'marked', 'as', 'important', '.', 'probably', 'the', 'movement', 'of', 'the', 'hand', '(', 'more', 'effort', 'makes', 'it', '\"', 'stick', '\"', ')', '.', 'on', 'the', 'ipod', '/', 'iphone', ',', 'everything', 'looks', 'so', 'beautiful', ',', 'i', 'don', \"'\", 't', 'perceive', 'it', 'as', 'real', ',', 'plus', 'i', 'get', 'distracted', '.', 'paper', 'system', '.', '*', 'month', '-', 'week', '-', 'day', ':', 'in', 'my', 'notebook', 'i', 'keep', 'a', 'list', 'with', 'the', 'things', 'i', 'hope', 'to', 'get', 'done', 'this', 'year', 'and', 'in', 'the', 'next', 'three', 'months', '.', 'also', ',', 'i', 'have', 'a', 'paper', 'calendar', 'that', 'i', \"'\", 'tuned', \"'\", 'with', 'additional', 'lists', ':', 'one', 'list', 'for', 'what', 'i', 'want', 'to', 'get', 'done', 'this', 'month', '(', 'only', 'most', 'important', ')', ',', 'one', 'list', 'for', 'most', 'important', 'things', 'to', 'get', 'done', 'this', 'week', ',', 'seven', 'individual', 'sheets', 'for', 'each', 'week', ',', 'with', 'a', 'rough', 'sketch', 'of', 'the', 'day', 'and', 'the', 'most', 'important', 'things', 'i', 'want', 'to', 'get', 'done', 'today', '.', '*', 'chains', 'for', 'the', 'regular', ':', 'for', 'regular', 'tasks', 'like', 'writing', 'and', 'doing', 'some', 'sport', ',', 'i', 'have', 'a', 'printed', 'monthly', 'calendar', 'over', 'my', 'desk', '.', 'every', 'day', 'i', 'do', 'one', 'of', 'these', 'i', 'mark', 'an', 'x', '.', 'i', 'forgot', 'who', 'invented', 'this', 'technique', 'but', 'it', 'helps', 'a', 'lot', '.', '*', 'time', '##box', '##ing', ':', 'i', 'try', 'to', 'de', '##dicate', 'longer', 'stretched', 'of', 'time', 'to', 'projects', '.', 'i', 'found', 'the', 'po', '##mo', '##dor', '##i', 'technique', 'very', 'helpful', ',', 'although', 'i', 'just', 'use', 'a', 'boiled', 'down', 'version', '(', 'working', 'in', '25', '-', 'minute', '-', 'blocks', ',', 'counting', 'them', ')', '*', 'will', '##power', ':', 'two', 'years', 'ago', 'i', 'was', 'so', 'desperate', 'about', 'my', 'ad', '##hd', 'that', 'i', 'started', 'with', 'zen', '.', 'since', 'then', ',', 'i', 'am', 'sitting', 'on', 'a', 'cushion', '(', 'more', 'or', 'less', ')', 'every', 'morning', ',', 'staring', 'against', 'the', 'wall', '.', 'this', 'became', 'a', 'habit', ',', 'and', 'the', 'days', 'usually', 'go', 'downhill', 'when', 'i', 'forget', '.', 'i', 'found', 'the', 'book', 'from', 'roy', 'ba', '##ume', '##ister', 'very', 'inspiring', '(', '\"', 'will', '##power', '\"', ')', '.', 'being', 'able', 'to', 'implement', 'the', 'plan', ',', 'i', 'find', 'it', 'more', 'important', 'than', 'the', 'details', 'of', 'the', \"'\", 'technique', \"'\", '.', 'and', 'of', 'course', ',', 'medication', 'whenever', 'necessary', '.', '*', 'challenges', ':', 'after', 'completing', 'my', 'dissertation', ',', 'suddenly', 'i', 'became', 'a', 'cave', 'man', '.', 'i', 'discovered', 'that', 'for', 'my', \"'\", 'time', 'management', \"'\", 'to', 'work', ',', 'i', 'need', 'to', 'feel', 'challenged', 'by', 'goals', 'i', 'perceive', 'as', 'worthy', '–', 'no', 'one', 'needs', 'time', 'management', 'just', 'to', 'decide', 'when', 'go', 'shopping', 'and', 'bring', 'out', 'the', 'trash', '.', 'my', 'current', 'problem', ':', 'i', 'need', 'to', 'do', 'more', '!', '*', 'inspiring', 'people', ':', 'meeting', 'people', 'who', 'have', 'a', 'lot', 'of', 'energy', ',', 'who', 'get', 'loads', 'of', 'shit', 'done', ',', 'especially', 'the', 'right', 'kind', 'of', 'it', ',', 'i', 'find', 'it', 'inspiring', '.', 'once', 'i', 'want', 'to', 'be', 'part', 'of', 'it', ',', 'i', 'become', 'more', 'productive', '.', 'i', 'am', 'glad', 'that', 'i', 'met', 'quite', 'a', 'few', 'people', 'who', 'are', 'very', 'successful', '*', 'and', '*', 'friendly', ',', 'it', 'is', 'important', 'to', 'know', 'that', 'this', 'is', 'not', 'a', 'contradiction', '.', '*', 'tactical', 'med', '##io', '##cr', '##ic', '##y', ':', 'most', 'of', 'the', 'tasks', 'i', 'should', '(', 'sic', ')', 'do', 'can', 'be', 'done', 'less', 'than', 'perfect', ',', 'so', 'i', \"'\", 'd', 'focus', 'on', 'doing', 'a', 'shit', 'job', ',', 'fast', '.', '*', 'gratitude', 'and', 'forgiveness', ':', 'all', 'that', 'said', ',', 'i', 'mess', 'up', 'pretty', 'badly', ',', 'often', '.', 'but', 'i', 'found', 'it', 'only', 'gets', 'worse', 'if', 'i', 'be', '##rate', 'myself', 'for', 'it', ',', 'or', 'feel', 'ashamed', '–', 'then', 'i', 'spiral', 'back', 'into', 'depression', 'and', 'that', \"'\", 's', 'it', 'for', 'time', 'management', '.', 'currently', 'reading', 'david', 'burns', \"'\", '\"', 'feeling', 'good', '\"', ',', 'many', 'good', 'ideas', 'here', 'for', 'self', '-', 'acceptance', '.', 'i', 'still', 'find', 'it', 'hard', 'to', 'focus', 'on', 'the', 'positive', ',', 'but', 'very', 'gr', '##ati', '##fi', '##ying', 'whenever', 'i', 'do', '.', '*', 'environment', ':', 'for', 'me', ',', 'the', 'library', 'works', 'best', '(', 'working', 'atmosphere', '+', 'people', 'working', ')', '.', 'also', ',', 'head', '##phones', 'with', 'some', 'relaxing', 'white', '/', 'brown', '/', 'pink', 'noise', 'in', 'it', '.', '*', 'internet', '-', 'less', ':', 'when', 'nothing', 'else', 'works', ',', 'applications', 'like', '\"', 'freedom', '\"', 'do', '.', 'i', 'feel', 'a', 'bit', 'ashamed', 'every', 'time', 'i', 'use', 'it', ',', 'but', 'sometimes', 'it', 'is', 'una', '##vo', '##ida', '##ble', ',', 'and', 'it', 'always', 'helps', 'to', 'ref', '##oc', '##us', '.', '*', 'motivation', 'ii', ':', 'once', 'i', 'found', 'a', 'goal', 'that', 'i', 'perceive', 'as', 'worthy', ',', 'i', 'want', 'to', 'be', 'able', 'to', 'complete', 'it', ',', 'no', 'matter', 'what', '.', 'hyper', '##fo', '##cus', ',', 'if', 'you', 'want', 'to', 'call', 'it', 'that', 'way', '.', 'still', 'figuring', 'out', 'how', 'to', 'direct', 'the', 'laser', 'beam', '.', '.', '.', '.', 'of', 'course', ',', 'what', 'i', \"'\", 've', 'done', 'for', 'the', 'past', 'hour', 'was', 'not', 'a', 'good', 'example', 'of', 'time', 'man', '##gement', 'at', 'all', '.', 'i', 'am', 'still', 'recovering', 'from', 'a', 'cold', ',', 'and', 'if', 'ba', '##ume', '##ister', 'has', 'it', 'right', ',', 'will', '##power', 'gets', 'reduced', 'then', '.', 'but', 'this', 'is', 'not', 'an', 'excuse', ':', 'goodbye', 'to', 'the', 'internet', 'now', ',', 'i', 'will', 'start', 'freedom', 'for', '60', 'minutes', 'and', 'use', 'the', 'time', 'to', 'take', 'a', 'shower', ',', 'do', 'some', 'zen', 'and', 'move', 'to', 'the', 'library', 'for', 'the', 'rest', 'of', 'the', 'day', ',', 'to', 'write', 'at', 'least', 'one', 'application', '.', '*', 'edit', '*', ':', 'almost', '3', '##pm', ',', 'i', 'am', 'still', 'at', 'home', '.', '!', '!', '!', '!', '!', 'does', 'it', 'still', 'make', 'sense', 'to', 'go', 'to', 'the', 'library', '?', 'my', 'time', 'management', 'is', 'in', 'pieces', 'at', 'the', 'moment', ',', 'i', 'hate', 'living', 'alone', '.', 'shit', '##shi', '##ts', '##hit', '.', 'anyway', '##s', ',', 'time', 'to', 'leave', ',', 'if', 'only', 'for', 'two', 'hours', '.', '*', 'edit', '2', '*', ':', 'it', 'is', 'hard', 'not', 'to', 'feel', 'like', 'a', 'failure', 'at', 'times', 'like', 'these', '.', 'i', 'decided', 'to', 'stay', 'at', 'home', ',', 'this', 'place', 'is', 'a', 'mess', '.', '1', '##h', 'of', 'cleaning', 'my', 'apartment', '.', 'desk', 'is', 'clear', 'again', ',', 'tomorrow', 'it', \"'\", 'll', 'be', 'easier', 'to', 'out', 'of', 'here', 'early', '.', 'now', 'again', ',', 'time', 'to', 'work', 'on', 'those', 'lists', '.', 'i', 'want', 'at', 'least', 'one', 'thing', 'done', 'today', '.', '*', 'edit', '4', '*', ':', 'finally', 'caught', 'the', 'day', 'by', 'its', 'tail', ',', 'sort', 'of', ':', '6', 'po', '##mo', '##dor', '##os', ',', 'clean', 'apartment', ',', 'preparations', 'for', 'tomorrow', 'done', '(', 'lay', '##ed', 'out', 'clothes', 'and', 'packed', 'my', 'bag', 'to', 'go', ')', '.', 'tomorrow', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', ':', 'despite', 'some', 'theoretical', 'knowledge', 'about', 'time', 'management', ',', 'op', 'sets', 'a', 'negative', 'example', '.', 'how', 'do', 'you', 'get', 'things', 'done', 'yourself', ',', 'how', 'do', 'you', 'get', 'out', 'of', 'a', 'ru', '##t', '?']\n",
      "INFO:__main__:Number of tokens: 1400\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['if', 'you', \"'\", 're', 'happy', 'with', 'your', 'time', '/', 'task', 'management', ',', 'please', 'tell', 'us', 'how', 'you', 'make', 'it', 'work', 'if', 'you', 'are', '(', 'mostly', ')', 'happy', 'with', 'your', 'getting', 'things', 'done', '(', 'or', 'whatever', 'you', \"'\", 'd', 'call', 'it', ')', ',', 'please', 'share', 'your', 'system', 'and', 'technique', 'and', 'also', ',', 'what', 'mo', '##tiv', '##ates', 'you', 'to', 'stick', 'with', 'it', '.', '*', 'edit', '3', ':', '*', 'forget', 'the', '\"', 'if', '\"', '-', 'part', 'of', 'the', 'question', '.', 'what', 'techniques', 'or', 'strategies', 'help', 'you', '?', 'for', 'me', ',', 'implementation', 'is', 'quite', 'problematic', '.', 'at', 'the', 'moment', ',', 'my', 'most', 'important', 'time', 'management', 'technique', 'consists', 'of', 'getting', 'into', 'the', 'library', '(', 'quiet', 'and', 'productive', 'surrounding', ')', '.', 'once', 'i', 'am', 'there', ',', 'i', 'have', 'no', 'choice', 'but', 'to', 'do', 'something', 'productive', '.', 'other', 'than', 'that', ',', 'some', 'ideas', 'that', 'helped', 'me', 'so', 'far', ':', '*', 'motivation', ':', 'i', 'think', 'about', 'death', 'often', ',', 'it', 'wakes', 'me', 'up', '.', 'life', 'feels', 'so', 'precious', ',', 'one', 'moment', 'ago', 'i', 'was', 'a', 'child', ',', 'watching', 'our', 'old', 'black', 'cat', 'chase', 'imaginary', 'birds', 'in', 'the', 'garden', ',', 'now', 'i', 'live', 'in', 'a', 'strange', 'city', 'alone', ';', 'only', 'i', 'remember', 'the', 'cat', '.', 'streets', 'and', 'faces', 'rushing', 'by', ',', 'one', 'day', 'it', 'will', 'all', 'be', 'gone', ',', 'how', 'can', 'i', 'bring', 'this', 'moment', 'of', 'my', 'fleeting', 'life', 'to', 'light', 'up', '?', 'this', ',', 'even', 'if', 'it', 'is', 'not', 'optimal', ',', 'i', 'only', 'have', 'this', ',', 'here', ',', 'now', '.', '-', 'what', 'do', '?', '*', 'consistency', ':', 'it', 'became', 'a', 'ritual', 'to', 'spend', '2', 'minutes', 'with', 'the', 'time', 'stuff', 'every', 'evening', ',', 'at', 'least', '.', 'i', 'still', 'forget', 'it', 'pretty', 'often', ',', 'some', 'mayhem', 'later', ',', 'i', 'rue', '##fully', 'come', 'back', 'to', 'it', '.', 'also', ',', 'on', 'an', 'un', '##st', '##ructured', 'day', ',', 'i', 'feel', 'crap', '##py', ',', 'which', 'reminds', 'me', 'to', 'get', 'back', 'to', 'it', 'again', '.', '*', 'paper', ':', 'everything', 'i', 'write', 'by', 'hand', 'gets', 'somehow', 'marked', 'as', 'important', '.', 'probably', 'the', 'movement', 'of', 'the', 'hand', '(', 'more', 'effort', 'makes', 'it', '\"', 'stick', '\"', ')', '.', 'on', 'the', 'ipod', '/', 'iphone', ',', 'everything', 'looks', 'so', 'beautiful', ',', 'i', 'don', \"'\", 't', 'perceive', 'it', 'as', 'real', ',', 'plus', 'i', 'get', 'distracted', '.', 'paper', 'system', '.', '*', 'month', '-', 'week', '-', 'day', ':', 'in', 'my', 'notebook', 'i', 'keep', 'a', 'list', 'with', 'the', 'things', 'i', 'hope', 'to', 'get', 'done', 'this', 'year', 'and', 'in', 'the', 'next', 'three', 'months', '.', 'also', ',', 'i', 'have', 'a', 'paper', 'calendar', 'that', 'i', \"'\", 'tuned', \"'\", 'with', 'additional', 'lists', ':', 'one', 'list', 'for', 'what', 'i', 'want', 'to', 'get', 'done', 'this', 'month', '(', 'only', 'most', 'important', ')', ',', 'one', 'list', 'for', 'most', 'important', 'things', 'to', 'get', 'done', 'this', 'week', ',', 'seven', 'individual', 'sheets', 'for', 'each', 'week', ',', 'with', 'a', 'rough', 'sketch', 'of', 'the', 'day', 'and', 'the', 'most', 'important', 'things', 'i', 'want', 'to', 'get', 'done', 'today', '.', '*', 'chains', 'for', 'the', 'regular', ':', 'for', 'regular', 'tasks', 'like', 'writing', 'and', 'doing', 'some', 'sport', ',', 'i', 'have', 'a', 'printed', 'monthly', 'calendar', 'over', 'my', 'desk', '.', 'every', 'day', 'i', 'do', 'one', 'of', 'these', 'i', 'mark', 'an', 'x', '.', 'i', 'forgot', 'who', 'invented'], ['this', 'technique', 'but', 'it', 'helps', 'a', 'lot', '.', '*', 'time', '##box', '##ing', ':', 'i', 'try', 'to', 'de', '##dicate', 'longer', 'stretched', 'of', 'time', 'to', 'projects', '.', 'i', 'found', 'the', 'po', '##mo', '##dor', '##i', 'technique', 'very', 'helpful', ',', 'although', 'i', 'just', 'use', 'a', 'boiled', 'down', 'version', '(', 'working', 'in', '25', '-', 'minute', '-', 'blocks', ',', 'counting', 'them', ')', '*', 'will', '##power', ':', 'two', 'years', 'ago', 'i', 'was', 'so', 'desperate', 'about', 'my', 'ad', '##hd', 'that', 'i', 'started', 'with', 'zen', '.', 'since', 'then', ',', 'i', 'am', 'sitting', 'on', 'a', 'cushion', '(', 'more', 'or', 'less', ')', 'every', 'morning', ',', 'staring', 'against', 'the', 'wall', '.', 'this', 'became', 'a', 'habit', ',', 'and', 'the', 'days', 'usually', 'go', 'downhill', 'when', 'i', 'forget', '.', 'i', 'found', 'the', 'book', 'from', 'roy', 'ba', '##ume', '##ister', 'very', 'inspiring', '(', '\"', 'will', '##power', '\"', ')', '.', 'being', 'able', 'to', 'implement', 'the', 'plan', ',', 'i', 'find', 'it', 'more', 'important', 'than', 'the', 'details', 'of', 'the', \"'\", 'technique', \"'\", '.', 'and', 'of', 'course', ',', 'medication', 'whenever', 'necessary', '.', '*', 'challenges', ':', 'after', 'completing', 'my', 'dissertation', ',', 'suddenly', 'i', 'became', 'a', 'cave', 'man', '.', 'i', 'discovered', 'that', 'for', 'my', \"'\", 'time', 'management', \"'\", 'to', 'work', ',', 'i', 'need', 'to', 'feel', 'challenged', 'by', 'goals', 'i', 'perceive', 'as', 'worthy', '–', 'no', 'one', 'needs', 'time', 'management', 'just', 'to', 'decide', 'when', 'go', 'shopping', 'and', 'bring', 'out', 'the', 'trash', '.', 'my', 'current', 'problem', ':', 'i', 'need', 'to', 'do', 'more', '!', '*', 'inspiring', 'people', ':', 'meeting', 'people', 'who', 'have', 'a', 'lot', 'of', 'energy', ',', 'who', 'get', 'loads', 'of', 'shit', 'done', ',', 'especially', 'the', 'right', 'kind', 'of', 'it', ',', 'i', 'find', 'it', 'inspiring', '.', 'once', 'i', 'want', 'to', 'be', 'part', 'of', 'it', ',', 'i', 'become', 'more', 'productive', '.', 'i', 'am', 'glad', 'that', 'i', 'met', 'quite', 'a', 'few', 'people', 'who', 'are', 'very', 'successful', '*', 'and', '*', 'friendly', ',', 'it', 'is', 'important', 'to', 'know', 'that', 'this', 'is', 'not', 'a', 'contradiction', '.', '*', 'tactical', 'med', '##io', '##cr', '##ic', '##y', ':', 'most', 'of', 'the', 'tasks', 'i', 'should', '(', 'sic', ')', 'do', 'can', 'be', 'done', 'less', 'than', 'perfect', ',', 'so', 'i', \"'\", 'd', 'focus', 'on', 'doing', 'a', 'shit', 'job', ',', 'fast', '.', '*', 'gratitude', 'and', 'forgiveness', ':', 'all', 'that', 'said', ',', 'i', 'mess', 'up', 'pretty', 'badly', ',', 'often', '.', 'but', 'i', 'found', 'it', 'only', 'gets', 'worse', 'if', 'i', 'be', '##rate', 'myself', 'for', 'it', ',', 'or', 'feel', 'ashamed', '–', 'then', 'i', 'spiral', 'back', 'into', 'depression', 'and', 'that', \"'\", 's', 'it', 'for', 'time', 'management', '.', 'currently', 'reading', 'david', 'burns', \"'\", '\"', 'feeling', 'good', '\"', ',', 'many', 'good', 'ideas', 'here', 'for', 'self', '-', 'acceptance', '.', 'i', 'still', 'find', 'it', 'hard', 'to', 'focus', 'on', 'the', 'positive', ',', 'but', 'very', 'gr', '##ati', '##fi', '##ying', 'whenever', 'i', 'do', '.', '*', 'environment', ':', 'for', 'me', ',', 'the', 'library', 'works', 'best', '(', 'working', 'atmosphere', '+', 'people', 'working', ')', '.', 'also', ',', 'head', '##phones', 'with', 'some', 'relaxing', 'white', '/', 'brown', '/', 'pink', 'noise', 'in', 'it', '.', '*', 'internet', '-', 'less', ':', 'when', 'nothing', 'else', 'works', ',', 'applications', 'like', '\"', 'freedom', '\"', 'do', '.', 'i', 'feel', 'a', 'bit', 'ashamed', 'every', 'time', 'i', 'use', 'it', ',', 'but', 'sometimes', 'it', 'is', 'una', '##vo', '##ida', '##ble', ',', 'and', 'it', 'always', 'helps', 'to', 'ref', '##oc', '##us'], ['.', '*', 'motivation', 'ii', ':', 'once', 'i', 'found', 'a', 'goal', 'that', 'i', 'perceive', 'as', 'worthy', ',', 'i', 'want', 'to', 'be', 'able', 'to', 'complete', 'it', ',', 'no', 'matter', 'what', '.', 'hyper', '##fo', '##cus', ',', 'if', 'you', 'want', 'to', 'call', 'it', 'that', 'way', '.', 'still', 'figuring', 'out', 'how', 'to', 'direct', 'the', 'laser', 'beam', '.', '.', '.', '.', 'of', 'course', ',', 'what', 'i', \"'\", 've', 'done', 'for', 'the', 'past', 'hour', 'was', 'not', 'a', 'good', 'example', 'of', 'time', 'man', '##gement', 'at', 'all', '.', 'i', 'am', 'still', 'recovering', 'from', 'a', 'cold', ',', 'and', 'if', 'ba', '##ume', '##ister', 'has', 'it', 'right', ',', 'will', '##power', 'gets', 'reduced', 'then', '.', 'but', 'this', 'is', 'not', 'an', 'excuse', ':', 'goodbye', 'to', 'the', 'internet', 'now', ',', 'i', 'will', 'start', 'freedom', 'for', '60', 'minutes', 'and', 'use', 'the', 'time', 'to', 'take', 'a', 'shower', ',', 'do', 'some', 'zen', 'and', 'move', 'to', 'the', 'library', 'for', 'the', 'rest', 'of', 'the', 'day', ',', 'to', 'write', 'at', 'least', 'one', 'application', '.', '*', 'edit', '*', ':', 'almost', '3', '##pm', ',', 'i', 'am', 'still', 'at', 'home', '.', '!', '!', '!', '!', '!', 'does', 'it', 'still', 'make', 'sense', 'to', 'go', 'to', 'the', 'library', '?', 'my', 'time', 'management', 'is', 'in', 'pieces', 'at', 'the', 'moment', ',', 'i', 'hate', 'living', 'alone', '.', 'shit', '##shi', '##ts', '##hit', '.', 'anyway', '##s', ',', 'time', 'to', 'leave', ',', 'if', 'only', 'for', 'two', 'hours', '.', '*', 'edit', '2', '*', ':', 'it', 'is', 'hard', 'not', 'to', 'feel', 'like', 'a', 'failure', 'at', 'times', 'like', 'these', '.', 'i', 'decided', 'to', 'stay', 'at', 'home', ',', 'this', 'place', 'is', 'a', 'mess', '.', '1', '##h', 'of', 'cleaning', 'my', 'apartment', '.', 'desk', 'is', 'clear', 'again', ',', 'tomorrow', 'it', \"'\", 'll', 'be', 'easier', 'to', 'out', 'of', 'here', 'early', '.', 'now', 'again', ',', 'time', 'to', 'work', 'on', 'those', 'lists', '.', 'i', 'want', 'at', 'least', 'one', 'thing', 'done', 'today', '.', '*', 'edit', '4', '*', ':', 'finally', 'caught', 'the', 'day', 'by', 'its', 'tail', ',', 'sort', 'of', ':', '6', 'po', '##mo', '##dor', '##os', ',', 'clean', 'apartment', ',', 'preparations', 'for', 'tomorrow', 'done', '(', 'lay', '##ed', 'out', 'clothes', 'and', 'packed', 'my', 'bag', 'to', 'go', ')', '.', 'tomorrow', '.', '*', '*', 't', '##l', ';', 'dr', '*', '*', ':', 'despite', 'some', 'theoretical', 'knowledge', 'about', 'time', 'management', ',', 'op', 'sets', 'a', 'negative', 'example', '.', 'how', 'do', 'you', 'get', 'things', 'done', 'yourself', ',', 'how', 'do', 'you', 'get', 'out', 'of', 'a', 'ru', '##t', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['philosophy', 'research', 'paper', 'revolving', 'around', 'ad', '##hd', ',', 'suggestions', 'appreciated', 'hey', 'guys', '.', 'i', 'am', 'doing', 'a', 'research', 'paper', 'in', 'my', 'philosophy', 'class', 'that', 'revolves', 'ad', '##hd', '.', 'the', 'one', 'catch', 'is', 'that', 'i', 'have', 'to', 'cite', 'either', 'rousseau', '(', 'state', 'of', 'nature', ')', ',', 'kant', ',', 'he', '##gel', ',', 'or', 'ne', '##itz', '##che', '.', 'does', 'anyone', 'know', 'of', 'any', 'articles', 'that', 'relate', 'ad', '##hd', 'to', 'something', 'any', 'of', 'these', 'philosophers', 'would', 'talk', 'about', '?', 'my', 'idea', 'is', 'that', 'i', 'could', 'find', 'something', 'revolving', 'around', 'either', 'rousseau', \"'\", 's', 'state', 'of', 'nature', 'how', 'man', 'in', 'the', 'state', 'of', 'nature', 'were', '\"', 'in', 'the', 'moment', '\"', 'had', 'no', 'thought', 'of', 'planning', 'tasks', ',', 'and', 'just', 'did', 'whatever', 'he', 'pleased', ',', 'and', 'was', 'completely', 'self', 'sufficient', '.', 'or', 'anything', 'in', '##vo', '##ling', 'ne', '##itz', '##sche', 'and', 'what', 'he', 'would', 'think', 'about', 'someone', 'who', 'has', 'ad', '##hd', 'and', 'how', 'they', 'adapt', 'to', 'society', '.']\n",
      "INFO:__main__:Number of tokens: 151\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['philosophy', 'research', 'paper', 'revolving', 'around', 'ad', '##hd', ',', 'suggestions', 'appreciated', 'hey', 'guys', '.', 'i', 'am', 'doing', 'a', 'research', 'paper', 'in', 'my', 'philosophy', 'class', 'that', 'revolves', 'ad', '##hd', '.', 'the', 'one', 'catch', 'is', 'that', 'i', 'have', 'to', 'cite', 'either', 'rousseau', '(', 'state', 'of', 'nature', ')', ',', 'kant', ',', 'he', '##gel', ',', 'or', 'ne', '##itz', '##che', '.', 'does', 'anyone', 'know', 'of', 'any', 'articles', 'that', 'relate', 'ad', '##hd', 'to', 'something', 'any', 'of', 'these', 'philosophers', 'would', 'talk', 'about', '?', 'my', 'idea', 'is', 'that', 'i', 'could', 'find', 'something', 'revolving', 'around', 'either', 'rousseau', \"'\", 's', 'state', 'of', 'nature', 'how', 'man', 'in', 'the', 'state', 'of', 'nature', 'were', '\"', 'in', 'the', 'moment', '\"', 'had', 'no', 'thought', 'of', 'planning', 'tasks', ',', 'and', 'just', 'did', 'whatever', 'he', 'pleased', ',', 'and', 'was', 'completely', 'self', 'sufficient', '.', 'or', 'anything', 'in', '##vo', '##ling', 'ne', '##itz', '##sche', 'and', 'what', 'he', 'would', 'think', 'about', 'someone', 'who', 'has', 'ad', '##hd', 'and', 'how', 'they', 'adapt', 'to', 'society', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['question', 'about', 'up', '##ping', 'your', 'dose', 'on', 'your', 'own']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['question', 'about', 'up', '##ping', 'your', 'dose', 'on', 'your', 'own']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', 'lack', 'of', 'intimacy', ':', 'a', 'portrait', 'of', 'ad', '##hd', 'in', 'adult', 'relationships']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', 'lack', 'of', 'intimacy', ':', 'a', 'portrait', 'of', 'ad', '##hd', 'in', 'adult', 'relationships']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['yer', '##ba', 'mate', 'specifically', 'for', 'people', 'who', 'have', 'amp', '##het', '##amine', 'dependency', 'from', 'their', 'medication', 'but', 'could', 'be', 'of', 'help', 'for', 'others', '.', 'for', 'a', 'year', 'now', 'i', 'have', 'been', 'drinking', 'the', 'tea', 'yer', '##ba', 'mate', 'to', 'help', 'me', 'get', 'through', 'the', 'days', 'where', 'i', 'don', \"'\", 't', 'take', 'my', 'medicine', '.', 'it', 'helps', 'with', 'my', 'withdrawal', 'symptoms', 'and', 'helps', 'me', 'focus', '.', 'i', 'email', '##ed', 'an', 'yer', '##ba', 'mate', 'distributing', 'company', 'once', 'to', 'see', 'if', 'they', 'had', 'heard', 'anything', 'about', 'it', 'helping', 'with', 'ad', '##hd', '/', 'add', 'and', 'they', 'said', 'they', 'hadn', \"'\", 't', 'but', 'would', 'love', 'to', 'see', 'if', 'it', 'actually', 'does', '.', 'does', 'anyone', 'else', 'have', 'any', 'experience', 'with', 'it', '?', '*', '*', 'edit', '*', '*', ':', 'sorry', 'new', 'here', '.', 'i', 'didn', \"'\", 't', 'mean', 'for', 'this', 'to', 'come', 'off', 'as', \"'\", 'alternative', \"'\", 'medicine', 'advice', '.', 'i', 'just', 'wanted', 'to', 'see', 'if', 'anyone', 'else', 'had', 'experience', 'with', 'it', 'helping', 'their', 'non', '-', 'med', '##icated', 'days', '.']\n",
      "INFO:__main__:Number of tokens: 161\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['yer', '##ba', 'mate', 'specifically', 'for', 'people', 'who', 'have', 'amp', '##het', '##amine', 'dependency', 'from', 'their', 'medication', 'but', 'could', 'be', 'of', 'help', 'for', 'others', '.', 'for', 'a', 'year', 'now', 'i', 'have', 'been', 'drinking', 'the', 'tea', 'yer', '##ba', 'mate', 'to', 'help', 'me', 'get', 'through', 'the', 'days', 'where', 'i', 'don', \"'\", 't', 'take', 'my', 'medicine', '.', 'it', 'helps', 'with', 'my', 'withdrawal', 'symptoms', 'and', 'helps', 'me', 'focus', '.', 'i', 'email', '##ed', 'an', 'yer', '##ba', 'mate', 'distributing', 'company', 'once', 'to', 'see', 'if', 'they', 'had', 'heard', 'anything', 'about', 'it', 'helping', 'with', 'ad', '##hd', '/', 'add', 'and', 'they', 'said', 'they', 'hadn', \"'\", 't', 'but', 'would', 'love', 'to', 'see', 'if', 'it', 'actually', 'does', '.', 'does', 'anyone', 'else', 'have', 'any', 'experience', 'with', 'it', '?', '*', '*', 'edit', '*', '*', ':', 'sorry', 'new', 'here', '.', 'i', 'didn', \"'\", 't', 'mean', 'for', 'this', 'to', 'come', 'off', 'as', \"'\", 'alternative', \"'\", 'medicine', 'advice', '.', 'i', 'just', 'wanted', 'to', 'see', 'if', 'anyone', 'else', 'had', 'experience', 'with', 'it', 'helping', 'their', 'non', '-', 'med', '##icated', 'days', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dex', '##amp', '##het', '##amine', '.', 'awesome', 'focus', '/', 'peace', 'of', 'mind', 'but', 'troubles', 'getting', 'started', '.']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dex', '##amp', '##het', '##amine', '.', 'awesome', 'focus', '/', 'peace', 'of', 'mind', 'but', 'troubles', 'getting', 'started', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'from', 'and', 'evolutionary', 'point', 'of', 'view', '?', 'has', 'anyone', 'ever', 'seen', 'any', 'research', 'on', 'this', '?', 'if', 'ad', '##hd', 'is', 'so', 'de', '##bil', '##itating', ',', 'why', 'hasn', \"'\", 't', 'it', 'been', 'selected', 'out', 'of', 'humans', '?', 'has', 'it', 'always', 'existed', ',', 'but', 'only', 'became', 'a', 'problem', 'because', 'of', 'societal', 'complexity', 'and', 'intense', 'organizational', 'demands', 'put', 'on', 'the', 'individual', 'since', 'the', 'industrial', 'revolution', '?', 'anyone', 'have', 'any', 'thoughts', 'on', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 72\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'from', 'and', 'evolutionary', 'point', 'of', 'view', '?', 'has', 'anyone', 'ever', 'seen', 'any', 'research', 'on', 'this', '?', 'if', 'ad', '##hd', 'is', 'so', 'de', '##bil', '##itating', ',', 'why', 'hasn', \"'\", 't', 'it', 'been', 'selected', 'out', 'of', 'humans', '?', 'has', 'it', 'always', 'existed', ',', 'but', 'only', 'became', 'a', 'problem', 'because', 'of', 'societal', 'complexity', 'and', 'intense', 'organizational', 'demands', 'put', 'on', 'the', 'individual', 'since', 'the', 'industrial', 'revolution', '?', 'anyone', 'have', 'any', 'thoughts', 'on', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['conditional', 'acceptance', 'update', 'contract', 'ok', '.', 'so', 'my', 'meeting', 'is', 'tomorrow', '[', 'read', 'the', 'post', 'here', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'so', '##av', '##q', '/', 'conditional', '_', 'acceptance', '_', 'meeting', '_', 'help', '_', 'needed', '/', ')', '.', 'i', 'have', 'come', 'up', 'with', 'the', 'following', 'letter', '>', 'on', 'thursday', 'april', '26', ',', '2012', ',', 'the', 'psi', '##i', 'committee', 'of', 'dr', '.', 'a', 'w', ',', 'dr', '.', 'd', 'g', ',', 'and', 'mrs', '.', 'l', ',', 'met', 'with', 'miss', 'c', 'to', 'discuss', 'the', 'terms', 'of', 'the', '“', 'conditional', 'acceptance', '”', 'that', 'have', 'been', 'placed', 'for', 'miss', 'c', 'for', 'the', 'fall', 'semester', 'of', '2012', '.', 'those', 'terms', 'are', 'the', 'following', ':', '•', '•', '•', '>', 'upon', 'agreeing', 'with', 'these', 'terms', 'miss', 'che', '##kan', 'has', 'asked', 'the', 'committee', 'to', 'consider', 'the', 'following', 'in', 'order', 'to', 'be', 'successful', '.', '•', 'previous', 'judgment', 'against', 'her', 'should', 'not', 'be', 'used', 'to', 'evaluate', 'her', 'performance', 'in', 'the', 'new', 'semester', '.', '•', 'progress', 'reports', 'should', 'be', 'email', '##ed', 'to', 'miss', 'c', 'ever', '2', 'to', '3', 'weeks', ',', 'and', 'there', 'should', 'be', 'at', 'least', 'one', 'positive', 'thing', 'in', 'the', 'report', '.', '•', 'incidents', 'of', 'concern', 'should', 'be', 'brought', 'to', 'the', 'attention', 'of', 'miss', 'c', 'first', '.', '•', 'miss', 'c', 'should', 'be', 'notified', 'information', 'about', 'her', 'is', 'going', 'to', 'be', 'distributed', 'among', 'the', 'faculty', '.', 'so', ',', 'do', 'you', 'think', 'i', 'am', 'being', 'unreasonable', '?', 'i', 'want', 'everything', 'in', 'writing', 'if', 'have', 'ad', '##hd', 'has', 'taught', 'me', 'anything', 'always', 'have', 'it', 'in', 'writing']\n",
      "INFO:__main__:Number of tokens: 256\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['conditional', 'acceptance', 'update', 'contract', 'ok', '.', 'so', 'my', 'meeting', 'is', 'tomorrow', '[', 'read', 'the', 'post', 'here', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'so', '##av', '##q', '/', 'conditional', '_', 'acceptance', '_', 'meeting', '_', 'help', '_', 'needed', '/', ')', '.', 'i', 'have', 'come', 'up', 'with', 'the', 'following', 'letter', '>', 'on', 'thursday', 'april', '26', ',', '2012', ',', 'the', 'psi', '##i', 'committee', 'of', 'dr', '.', 'a', 'w', ',', 'dr', '.', 'd', 'g', ',', 'and', 'mrs', '.', 'l', ',', 'met', 'with', 'miss', 'c', 'to', 'discuss', 'the', 'terms', 'of', 'the', '“', 'conditional', 'acceptance', '”', 'that', 'have', 'been', 'placed', 'for', 'miss', 'c', 'for', 'the', 'fall', 'semester', 'of', '2012', '.', 'those', 'terms', 'are', 'the', 'following', ':', '•', '•', '•', '>', 'upon', 'agreeing', 'with', 'these', 'terms', 'miss', 'che', '##kan', 'has', 'asked', 'the', 'committee', 'to', 'consider', 'the', 'following', 'in', 'order', 'to', 'be', 'successful', '.', '•', 'previous', 'judgment', 'against', 'her', 'should', 'not', 'be', 'used', 'to', 'evaluate', 'her', 'performance', 'in', 'the', 'new', 'semester', '.', '•', 'progress', 'reports', 'should', 'be', 'email', '##ed', 'to', 'miss', 'c', 'ever', '2', 'to', '3', 'weeks', ',', 'and', 'there', 'should', 'be', 'at', 'least', 'one', 'positive', 'thing', 'in', 'the', 'report', '.', '•', 'incidents', 'of', 'concern', 'should', 'be', 'brought', 'to', 'the', 'attention', 'of', 'miss', 'c', 'first', '.', '•', 'miss', 'c', 'should', 'be', 'notified', 'information', 'about', 'her', 'is', 'going', 'to', 'be', 'distributed', 'among', 'the', 'faculty', '.', 'so', ',', 'do', 'you', 'think', 'i', 'am', 'being', 'unreasonable', '?', 'i', 'want', 'everything', 'in', 'writing', 'if', 'have', 'ad', '##hd', 'has', 'taught', 'me', 'anything', 'always', 'have', 'it', 'in', 'writing']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['would', 'you', 'be', 'upset', 'if', 'your', 'doctor', 'was', 'not', 'telling', 'you', 'the', 'truth', '?', 'v', '##y', '##van', '##se', '60', '##mg', '-', 'once', 'a', 'day', ',', 'is', 'what', 'my', 'doctor', '(', 'psychiatrist', ')', 'has', 'prescribed', 'me', '.', 'the', 'medicine', 'is', 'great', 'and', 'i', 'can', 'not', 'complain', '.', 'my', 'problem', 'however', 'is', ',', 'my', 'doctor', 'has', 'repeatedly', 'told', 'me', 'that', 'i', 'am', 'on', 'the', 'highest', 'dose', 'of', 'v', '##y', '##van', '##se', 'they', 'make', '.', 'doing', 'some', 'research', 'i', 'found', 'information', 'on', 'the', 'dose', 'levels', 'allowed', 'by', 'the', 'fda', '.', 'they', 'all', 'say', 'that', 'shire', 'makes', 'a', '70', '##mg', 'v', '##y', '##van', '##se', 'for', 'typical', 'daily', 'use', '.', '1', '.', 'what', 'happens', 'if', 'my', 'dose', 'would', 'need', 'to', 'be', 'changed', 'later', 'on', '?', '2', '.', 'would', 'he', 'rev', '##ert', 'to', 'just', 'switching', 'types', '?', '3', '.', 'has', 'he', 'lied', 'to', 'me', 'about', 'more', 'then', 'this', '?', '4', '.', 'should', 'i', 'bring', 'this', 'up', 'with', 'him', '?', 't', '##l', ':', 'dr', '-', 'would', 'you', 'be', 'upset', 'if', 'your', 'doctor', 'lied', 'to', 'you', '?', '-']\n",
      "INFO:__main__:Number of tokens: 170\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['would', 'you', 'be', 'upset', 'if', 'your', 'doctor', 'was', 'not', 'telling', 'you', 'the', 'truth', '?', 'v', '##y', '##van', '##se', '60', '##mg', '-', 'once', 'a', 'day', ',', 'is', 'what', 'my', 'doctor', '(', 'psychiatrist', ')', 'has', 'prescribed', 'me', '.', 'the', 'medicine', 'is', 'great', 'and', 'i', 'can', 'not', 'complain', '.', 'my', 'problem', 'however', 'is', ',', 'my', 'doctor', 'has', 'repeatedly', 'told', 'me', 'that', 'i', 'am', 'on', 'the', 'highest', 'dose', 'of', 'v', '##y', '##van', '##se', 'they', 'make', '.', 'doing', 'some', 'research', 'i', 'found', 'information', 'on', 'the', 'dose', 'levels', 'allowed', 'by', 'the', 'fda', '.', 'they', 'all', 'say', 'that', 'shire', 'makes', 'a', '70', '##mg', 'v', '##y', '##van', '##se', 'for', 'typical', 'daily', 'use', '.', '1', '.', 'what', 'happens', 'if', 'my', 'dose', 'would', 'need', 'to', 'be', 'changed', 'later', 'on', '?', '2', '.', 'would', 'he', 'rev', '##ert', 'to', 'just', 'switching', 'types', '?', '3', '.', 'has', 'he', 'lied', 'to', 'me', 'about', 'more', 'then', 'this', '?', '4', '.', 'should', 'i', 'bring', 'this', 'up', 'with', 'him', '?', 't', '##l', ':', 'dr', '-', 'would', 'you', 'be', 'upset', 'if', 'your', 'doctor', 'lied', 'to', 'you', '?', '-']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'thinks', 'laptop', '##s', 'are', 'attention', 'killers', 'in', 'lectures', '?', 'i', 'stopped', 'bringing', 'my', 'laptop', 'to', 'class', 'because', 'i', 'just', 'can', \"'\", 't', 'concentrate', 'on', 'paying', 'attention', 'to', 'the', 'lecturer', '.']\n",
      "INFO:__main__:Number of tokens: 32\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'thinks', 'laptop', '##s', 'are', 'attention', 'killers', 'in', 'lectures', '?', 'i', 'stopped', 'bringing', 'my', 'laptop', 'to', 'class', 'because', 'i', 'just', 'can', \"'\", 't', 'concentrate', 'on', 'paying', 'attention', 'to', 'the', 'lecturer', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['d', '##ys', '##cal', '##cu', '##lia', '?']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['d', '##ys', '##cal', '##cu', '##lia', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['not', 'sure', 'how', 'i', \"'\", 'm', 'feeling', 'about', 'rita', '##lin', '(', 'methyl', '##ph', '##eni', '##date', ')']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['not', 'sure', 'how', 'i', \"'\", 'm', 'feeling', 'about', 'rita', '##lin', '(', 'methyl', '##ph', '##eni', '##date', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['going', 'into', 'to', '(', 'most', 'likely', ')', 'be', 'prescribed', 'med', '##s', 'for', 'ad', '##hd', '.', 'a', 'few', 'questions', ':']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['going', 'into', 'to', '(', 'most', 'likely', ')', 'be', 'prescribed', 'med', '##s', 'for', 'ad', '##hd', '.', 'a', 'few', 'questions', ':']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['diagnosed', 'with', 'ad', '##hd', 'and', 'scared', 'of', 'medication', 'after', 'being', 'kicked', 'out', 'of', 'college', 'i', 'decided', 'to', 'go', 'to', 'a', 'psychiatrist', 'to', 'see', 'if', 'i', 'had', 'ad', '##hd', 'or', 'not', '.', 'after', '2', 'visits', ',', 'he', 'thought', 'that', 'i', 'suffered', 'from', 'ad', '##hd', 'even', 'though', 'i', 'never', 'told', 'him', 'i', 'failed', 'college', '.', 'now', 'i', \"'\", 'm', 'heading', 'back', 'to', 'college', 'and', 'am', 'scared', 'that', 'the', 'same', 'thing', 'will', 'happen', 'if', 'i', 'don', \"'\", 't', 'use', 'ad', '##hd', 'medication', '.', 't', '##ld', '##r', ':', 'kicked', 'out', 'of', 'college', 'and', 'scared', 'of', 'using', 'ad', '##hd', 'medication', 'as', 'i', 'am', 'the', 'only', 'one', 'with', 'ad', '##hd', 'in', 'my', 'house', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 110\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['diagnosed', 'with', 'ad', '##hd', 'and', 'scared', 'of', 'medication', 'after', 'being', 'kicked', 'out', 'of', 'college', 'i', 'decided', 'to', 'go', 'to', 'a', 'psychiatrist', 'to', 'see', 'if', 'i', 'had', 'ad', '##hd', 'or', 'not', '.', 'after', '2', 'visits', ',', 'he', 'thought', 'that', 'i', 'suffered', 'from', 'ad', '##hd', 'even', 'though', 'i', 'never', 'told', 'him', 'i', 'failed', 'college', '.', 'now', 'i', \"'\", 'm', 'heading', 'back', 'to', 'college', 'and', 'am', 'scared', 'that', 'the', 'same', 'thing', 'will', 'happen', 'if', 'i', 'don', \"'\", 't', 'use', 'ad', '##hd', 'medication', '.', 't', '##ld', '##r', ':', 'kicked', 'out', 'of', 'college', 'and', 'scared', 'of', 'using', 'ad', '##hd', 'medication', 'as', 'i', 'am', 'the', 'only', 'one', 'with', 'ad', '##hd', 'in', 'my', 'house', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['call', 'me', 'sc', '##hl', '##ep', '##rock', 'remember', 'that', 'cave', '##man', 'dude', 'with', 'the', 'cloud', 'over', 'his', 'head', 'in', 'the', 'pebbles', 'and', 'bam', '-', 'bam', 'show', '?', 'i', 'always', 'think', 'of', 'that', 'when', 'the', 'simplest', 'things', 'in', 'my', 'life', 'turn', 'into', 'a', 'cluster', 'fu', '@', 'k', '.', 'i', 'actually', 'look', 'up', 'at', 'times', 'to', 'see', 'if', 'it', \"'\", 's', 'really', 'there', '!', 'i', 'looked', 'up', 'sc', '##hl', '##ep', '##rock', 'on', 'urban', 'dictionary', 'and', 'the', 'definition', 'was', ':', '\"', 'to', 'carry', 'heavy', 'burden', '##s', '.', 'one', 'who', 'is', 'said', 'to', 'un', '##lu', '##cky', 'or', 'extremely', 'unfortunate', '.', '\"', 'that', 'me', '.', 'here', \"'\", 's', 'my', 'latest', '.', 'i', 'suffer', 'from', 'anxiety', 'and', 'depression', 'and', 'have', 'been', 'on', 'and', 'off', 'medication', 'for', 'about', '4', 'years', '.', 'i', 'always', 'thought', 'i', 'had', 'ad', '##hd', 'and', 'recently', 'decided', 'to', 'address', 'that', '.', 'i', 'call', 'my', 'dr', '.', 'to', 'make', 'an', 'appointment', ',', 'soon', '##est', 'i', 'could', 'get', 'in', 'was', 'a', 'little', 'over', 'a', 'month', '.', 'i', 'waited', 'as', 'patiently', 'as', 'possible', 'for', 'that', 'appointment', '(', 'which', 'is', 'very', 'challenging', 'for', 'me', ')', ',', 'finally', 'the', 'day', 'arrives', '.', 'i', 'didn', \"'\", 't', 'tell', 'her', 'i', 'suspected', 'ad', '##ha', 'i', 'just', 'gave', 'her', 'an', 'list', 'of', 'my', 'complaints', '.', 'she', 'looks', 'at', 'it', 'and', 'say', \"'\", 's', '\"', 'this', 'is', 'a', 'family', 'practice', 'we', 'don', \"'\", 't', 'dia', '##gno', '##se', 'ada', '##h', '!', '\"', ',', 'and', 'told', 'me', 'i', 'need', 'to', 'see', 'a', 'psychiatrist', '.', 'i', 'say', 'okay', 'so', 'she', 'starts', 'to', 'look', 'for', 'one', 'for', 'me', 'then', 'after', 'about', 'a', 'minute', 'say', \"'\", 's', '\"', 'we', 'have', 'a', 'consular', 'here', 'do', 'you', 'want', 'to', 'talk', 'to', 'her', '?', '\"', 'sure', 'i', 'say', '.', 'make', 'another', 'appointment', ',', 'over', '3', 'week', 'wait', '.', 'patience', 'wearing', 'thin', '.', 'finally', 'my', 'appointment', 'arrives', ',', 'went', 'great', '.', 'the', 'consular', 'agrees', 'with', 'ad', '##hd', 'and', 'asked', 'if', 'i', 'wanted', 'to', 'try', 'a', 'medication', '.', 'yes', 'i', 'do', ',', 'i', \"'\", 'm', '49', 'years', 'old', 'and', 'lived', 'with', 'this', 'my', 'whole', 'life', '.', 'i', 'have', 'tried', 'unsuccessfully', 'to', 'deal', 'with', 'this', 'on', 'my', 'own', ',', 'it', 'is', 'not', 'working', '.', 'she', 'tells', 'me', 'she', 'will', 'forward', 'the', 'info', 'to', 'the', 'dr', '.', 'to', 'write', 'a', 'prescription', '.', 'that', 'was', 'monday', '9', '##a', '.', 'm', '.', 'by', 'tuesday', '4', '##p', '.', 'm', '.', 'i', 'had', 'not', 'heard', 'anything', 'and', 'wondered', 'if', 'i', 'was', 'supposed', 'to', 'make', 'an', 'appointment', 'to', 'see', 'the', 'dr', '.', 'so', 'i', 'called', 'and', 'spoke', 'to', 'a', 'nurse', 'who', 'said', 'she', 'would', 'pass', 'the', 'info', 'along', '.', 'wednesday', 'evening', 'still', 'had', 'not', 'heard', 'back', 'so', 'i', 'call', ',', 'the', 'office', 'is', 'closed', '.', 'i', 'go', 'onto', 'their', 'website', 'and', 'create', 'an', 'account', 'so', 'i', 'can', 'send', 'an', 'e', '-', 'mail', '.', 'a', 'half', 'an', 'hour', 'later', 'the', 'letter', 'is', 'typed', 'and', 'i', 'hit', 'send', ',', 'nothing', ',', 'send', 'again', ',', 'nothing', ',', 'hit', 'review', 'and', 'try', 'to', 'send', 'form', 'there', ',', 'got', 'a', 'message', 'that', 'my', 'attempt', 'was', 'unsuccessful', 'try', 'again', 'later', '.', 'patience', 'hanging', 'on', 'by', 'a', 'thread', '.', 'i', 'am', 'this', 'close', '|', '|', 'too', 'what', 'i', 'hope', 'will', 'be', 'a', 'life', 'changing', 'experience', 'and', 'just', 'can', \"'\", 't', 'seem', 'to', 'get', 'there', '!', 'but', 'this', 'is', 'my', 'life', ',', 'even', 'the', 'simplest', 'things', 'turn', 'into', 'a', 'major', 'cluster', 'f', '@', 'ck', '.', 't', '##ld', '##r', ';', 'my', 'life', 'is', 'a', 'cluster', 'f', '@', 'ck', '!', 'i', 'can', \"'\", 't', 'even', 'get', 'prescription', 'called', 'in', 'after', 'diagnosis', 'confirmed', '.', 'no', 'surprise', ',', 'welcome', 'to', 'my', 'life', '.']\n",
      "INFO:__main__:Number of tokens: 581\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['call', 'me', 'sc', '##hl', '##ep', '##rock', 'remember', 'that', 'cave', '##man', 'dude', 'with', 'the', 'cloud', 'over', 'his', 'head', 'in', 'the', 'pebbles', 'and', 'bam', '-', 'bam', 'show', '?', 'i', 'always', 'think', 'of', 'that', 'when', 'the', 'simplest', 'things', 'in', 'my', 'life', 'turn', 'into', 'a', 'cluster', 'fu', '@', 'k', '.', 'i', 'actually', 'look', 'up', 'at', 'times', 'to', 'see', 'if', 'it', \"'\", 's', 'really', 'there', '!', 'i', 'looked', 'up', 'sc', '##hl', '##ep', '##rock', 'on', 'urban', 'dictionary', 'and', 'the', 'definition', 'was', ':', '\"', 'to', 'carry', 'heavy', 'burden', '##s', '.', 'one', 'who', 'is', 'said', 'to', 'un', '##lu', '##cky', 'or', 'extremely', 'unfortunate', '.', '\"', 'that', 'me', '.', 'here', \"'\", 's', 'my', 'latest', '.', 'i', 'suffer', 'from', 'anxiety', 'and', 'depression', 'and', 'have', 'been', 'on', 'and', 'off', 'medication', 'for', 'about', '4', 'years', '.', 'i', 'always', 'thought', 'i', 'had', 'ad', '##hd', 'and', 'recently', 'decided', 'to', 'address', 'that', '.', 'i', 'call', 'my', 'dr', '.', 'to', 'make', 'an', 'appointment', ',', 'soon', '##est', 'i', 'could', 'get', 'in', 'was', 'a', 'little', 'over', 'a', 'month', '.', 'i', 'waited', 'as', 'patiently', 'as', 'possible', 'for', 'that', 'appointment', '(', 'which', 'is', 'very', 'challenging', 'for', 'me', ')', ',', 'finally', 'the', 'day', 'arrives', '.', 'i', 'didn', \"'\", 't', 'tell', 'her', 'i', 'suspected', 'ad', '##ha', 'i', 'just', 'gave', 'her', 'an', 'list', 'of', 'my', 'complaints', '.', 'she', 'looks', 'at', 'it', 'and', 'say', \"'\", 's', '\"', 'this', 'is', 'a', 'family', 'practice', 'we', 'don', \"'\", 't', 'dia', '##gno', '##se', 'ada', '##h', '!', '\"', ',', 'and', 'told', 'me', 'i', 'need', 'to', 'see', 'a', 'psychiatrist', '.', 'i', 'say', 'okay', 'so', 'she', 'starts', 'to', 'look', 'for', 'one', 'for', 'me', 'then', 'after', 'about', 'a', 'minute', 'say', \"'\", 's', '\"', 'we', 'have', 'a', 'consular', 'here', 'do', 'you', 'want', 'to', 'talk', 'to', 'her', '?', '\"', 'sure', 'i', 'say', '.', 'make', 'another', 'appointment', ',', 'over', '3', 'week', 'wait', '.', 'patience', 'wearing', 'thin', '.', 'finally', 'my', 'appointment', 'arrives', ',', 'went', 'great', '.', 'the', 'consular', 'agrees', 'with', 'ad', '##hd', 'and', 'asked', 'if', 'i', 'wanted', 'to', 'try', 'a', 'medication', '.', 'yes', 'i', 'do', ',', 'i', \"'\", 'm', '49', 'years', 'old', 'and', 'lived', 'with', 'this', 'my', 'whole', 'life', '.', 'i', 'have', 'tried', 'unsuccessfully', 'to', 'deal', 'with', 'this', 'on', 'my', 'own', ',', 'it', 'is', 'not', 'working', '.', 'she', 'tells', 'me', 'she', 'will', 'forward', 'the', 'info', 'to', 'the', 'dr', '.', 'to', 'write', 'a', 'prescription', '.', 'that', 'was', 'monday', '9', '##a', '.', 'm', '.', 'by', 'tuesday', '4', '##p', '.', 'm', '.', 'i', 'had', 'not', 'heard', 'anything', 'and', 'wondered', 'if', 'i', 'was', 'supposed', 'to', 'make', 'an', 'appointment', 'to', 'see', 'the', 'dr', '.', 'so', 'i', 'called', 'and', 'spoke', 'to', 'a', 'nurse', 'who', 'said', 'she', 'would', 'pass', 'the', 'info', 'along', '.', 'wednesday', 'evening', 'still', 'had', 'not', 'heard', 'back', 'so', 'i', 'call', ',', 'the', 'office', 'is', 'closed', '.', 'i', 'go', 'onto', 'their', 'website', 'and', 'create', 'an', 'account', 'so', 'i', 'can', 'send', 'an', 'e', '-', 'mail', '.', 'a', 'half', 'an', 'hour', 'later', 'the', 'letter', 'is', 'typed', 'and', 'i', 'hit', 'send', ',', 'nothing', ',', 'send', 'again', ',', 'nothing', ',', 'hit', 'review', 'and', 'try', 'to', 'send', 'form', 'there', ',', 'got', 'a', 'message', 'that', 'my', 'attempt', 'was', 'unsuccessful', 'try', 'again', 'later', '.', 'patience', 'hanging', 'on', 'by', 'a', 'thread', '.', 'i', 'am', 'this', 'close', '|', '|', 'too', 'what', 'i', 'hope'], ['will', 'be', 'a', 'life', 'changing', 'experience', 'and', 'just', 'can', \"'\", 't', 'seem', 'to', 'get', 'there', '!', 'but', 'this', 'is', 'my', 'life', ',', 'even', 'the', 'simplest', 'things', 'turn', 'into', 'a', 'major', 'cluster', 'f', '@', 'ck', '.', 't', '##ld', '##r', ';', 'my', 'life', 'is', 'a', 'cluster', 'f', '@', 'ck', '!', 'i', 'can', \"'\", 't', 'even', 'get', 'prescription', 'called', 'in', 'after', 'diagnosis', 'confirmed', '.', 'no', 'surprise', ',', 'welcome', 'to', 'my', 'life', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['red', '##dit', ',', 'what', 'ni', '##ft', '##y', 'organizational', 'method', 'have', 'you', 'had', 'great', 'success', 'with', '?', 'here', '##s', 'mine', '.']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['red', '##dit', ',', 'what', 'ni', '##ft', '##y', 'organizational', 'method', 'have', 'you', 'had', 'great', 'success', 'with', '?', 'here', '##s', 'mine', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['don', \"'\", 't', 'like', 'taking', 'rita', '##lin', '(', 'methyl', '##ph', '##eni', '##date', ')', '.', 'why', 'could', 'that', 'be', '?', 'i', \"'\", 'm', 'on', 'rita', '##lin', '80', 'mg', 'and', 'flu', '##ox', '##eti', '##ne', '20', 'mg', 'daily', ',', 'since', '9', 'months', '.', 'i', 'notice', 'some', 'kind', 'of', 'a', 'mental', 'resistance', 'to', 'having', 'my', 'twice', 'daily', 'doses', 'of', 'rita', '##lin', '.', 'in', 'other', 'words', ',', 'i', 'somehow', ',', 'mentally', ',', 'do', 'not', 'look', 'forward', 'to', 'my', 'rita', '##lin', 'doses', '.', 'i', 'am', 'indifferent', 'to', 'the', 'flu', '##ox', '##eti', '##ne', 'and', 'can', 'pop', 'in', 'one', 'easily', 'without', 'a', 'second', 'thought', '.', 'any', 'ideas', 'why', 'this', 'could', 'be', '?']\n",
      "INFO:__main__:Number of tokens: 104\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['don', \"'\", 't', 'like', 'taking', 'rita', '##lin', '(', 'methyl', '##ph', '##eni', '##date', ')', '.', 'why', 'could', 'that', 'be', '?', 'i', \"'\", 'm', 'on', 'rita', '##lin', '80', 'mg', 'and', 'flu', '##ox', '##eti', '##ne', '20', 'mg', 'daily', ',', 'since', '9', 'months', '.', 'i', 'notice', 'some', 'kind', 'of', 'a', 'mental', 'resistance', 'to', 'having', 'my', 'twice', 'daily', 'doses', 'of', 'rita', '##lin', '.', 'in', 'other', 'words', ',', 'i', 'somehow', ',', 'mentally', ',', 'do', 'not', 'look', 'forward', 'to', 'my', 'rita', '##lin', 'doses', '.', 'i', 'am', 'indifferent', 'to', 'the', 'flu', '##ox', '##eti', '##ne', 'and', 'can', 'pop', 'in', 'one', 'easily', 'without', 'a', 'second', 'thought', '.', 'any', 'ideas', 'why', 'this', 'could', 'be', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['a', 'personal', 'request', 'to', 'anyone', 'who', 'reads', '/', 'writes', 'on', 'this', 'part', 'of', 'red', '##dit', '.', 'look', 'guys', ',', 'i', 'can', \"'\", 't', 'speak', 'for', 'anyone', 'else', 'but', ',', 'i', 'have', 'a', 'lot', 'of', 'people', 'around', 'me', 'who', 'don', \"'\", 't', 'believe', 'ad', '##hd', 'is', 'a', 'real', 'problem', 'some', 'are', 'open', 'about', 'this', 'and', 'some', 'aren', \"'\", 't', '.', 'i', 'would', 'just', 'like', 'to', 'respectful', '##ly', 'ask', 'people', 'quit', 'trivial', '##izing', 'this', 'disorder', '.', 'this', 'is', 'the', '/', 'r', '/', 'ad', '##hd', 'forum', 'and', 'would', 'hope', 'here', 'of', 'all', 'places', 'people', 'would', 'understand', 'what', 'it', 'means', 'to', 'have', 'a', 'disorder', '.', 'every', 'time', 'i', 'see', 'a', 'post', 'where', 'someone', 'tries', 'to', 'make', 'it', 'out', 'to', 'be', 'an', 'evolutionary', 'advantage', ',', 'personality', 'type', ',', 'or', 'super', 'power', 'i', 'just', 'out', 'right', 'get', 'annoyed', '.', 'everyone', 'is', 'entitled', 'to', 'their', 'own', 'opinion', 'and', 'i', 'won', \"'\", 't', 'deny', 'them', 'that', '.', 'however', ',', 'when', 'i', 'see', 'regular', 'speculation', 'about', 'the', '\"', 'advantages', '\"', 'of', 'ad', '##hd', 'i', 'start', 'to', 'wonder', 'if', 'these', 'people', 'actually', 'have', 'the', 'same', 'disorder', 'i', 'do', 'or', 'what', '.', 'it', 'leads', 'to', 'me', 'personally', 'worrying', 'if', 'this', 'is', 'the', 'attitude', 'that', 'is', 'carried', 'over', 'then', 'what', 'claim', 'do', 'people', 'with', 'ad', '##hd', 'really', 'have', '?', 'it', \"'\", 's', 'bad', 'enough', 'when', 'people', 'who', 'don', \"'\", 't', 'have', 'the', 'disorder', 'sit', 'back', 'and', 'doubt', 'it', 'but', ',', 'when', 'people', 'who', 'do', 'have', 'the', 'disorder', 'try', 'to', 'make', 'it', 'into', 'something', 'it', \"'\", 's', 'not', 'it', 'gives', 'people', 'even', 'more', 'reason', 'to', 'doubt', 'the', 'disorder', '.', 't', '##l', ';', 'dr', 'please', 'be', '##care', '##ful', 'what', 'you', 'say', 'because', ',', 'when', 'you', 'try', 'to', 'down', '##play', 'what', 'ad', '##hd', 'is', 'you', 'give', 'reason', 'for', 'people', 'to', 'doubt', 'the', 'disorder', 'more', 'than', 'they', 'already', 'do', '.']\n",
      "INFO:__main__:Number of tokens: 294\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['a', 'personal', 'request', 'to', 'anyone', 'who', 'reads', '/', 'writes', 'on', 'this', 'part', 'of', 'red', '##dit', '.', 'look', 'guys', ',', 'i', 'can', \"'\", 't', 'speak', 'for', 'anyone', 'else', 'but', ',', 'i', 'have', 'a', 'lot', 'of', 'people', 'around', 'me', 'who', 'don', \"'\", 't', 'believe', 'ad', '##hd', 'is', 'a', 'real', 'problem', 'some', 'are', 'open', 'about', 'this', 'and', 'some', 'aren', \"'\", 't', '.', 'i', 'would', 'just', 'like', 'to', 'respectful', '##ly', 'ask', 'people', 'quit', 'trivial', '##izing', 'this', 'disorder', '.', 'this', 'is', 'the', '/', 'r', '/', 'ad', '##hd', 'forum', 'and', 'would', 'hope', 'here', 'of', 'all', 'places', 'people', 'would', 'understand', 'what', 'it', 'means', 'to', 'have', 'a', 'disorder', '.', 'every', 'time', 'i', 'see', 'a', 'post', 'where', 'someone', 'tries', 'to', 'make', 'it', 'out', 'to', 'be', 'an', 'evolutionary', 'advantage', ',', 'personality', 'type', ',', 'or', 'super', 'power', 'i', 'just', 'out', 'right', 'get', 'annoyed', '.', 'everyone', 'is', 'entitled', 'to', 'their', 'own', 'opinion', 'and', 'i', 'won', \"'\", 't', 'deny', 'them', 'that', '.', 'however', ',', 'when', 'i', 'see', 'regular', 'speculation', 'about', 'the', '\"', 'advantages', '\"', 'of', 'ad', '##hd', 'i', 'start', 'to', 'wonder', 'if', 'these', 'people', 'actually', 'have', 'the', 'same', 'disorder', 'i', 'do', 'or', 'what', '.', 'it', 'leads', 'to', 'me', 'personally', 'worrying', 'if', 'this', 'is', 'the', 'attitude', 'that', 'is', 'carried', 'over', 'then', 'what', 'claim', 'do', 'people', 'with', 'ad', '##hd', 'really', 'have', '?', 'it', \"'\", 's', 'bad', 'enough', 'when', 'people', 'who', 'don', \"'\", 't', 'have', 'the', 'disorder', 'sit', 'back', 'and', 'doubt', 'it', 'but', ',', 'when', 'people', 'who', 'do', 'have', 'the', 'disorder', 'try', 'to', 'make', 'it', 'into', 'something', 'it', \"'\", 's', 'not', 'it', 'gives', 'people', 'even', 'more', 'reason', 'to', 'doubt', 'the', 'disorder', '.', 't', '##l', ';', 'dr', 'please', 'be', '##care', '##ful', 'what', 'you', 'say', 'because', ',', 'when', 'you', 'try', 'to', 'down', '##play', 'what', 'ad', '##hd', 'is', 'you', 'give', 'reason', 'for', 'people', 'to', 'doubt', 'the', 'disorder', 'more', 'than', 'they', 'already', 'do', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'hear', 'music', 'in', 'their', 'heads', 'while', 'doing', 'a', 'boring', 'task', '?', 'whenever', 'i', 'have', 'to', 'do', 'something', 'boring', 'or', 'un', '##int', '##eres', '##ting', 'like', 'studying', 'for', 'a', 'test', 'or', 'gathering', 'information', 'for', 'a', 'project', 'at', 'work', 'and', 'it', \"'\", 's', 'completely', 'dull', '.', 'i', 'always', 'hear', 'music', ',', 'not', 'real', 'music', 'but', 'in', 'my', 'head', '.', 'sometimes', 'it', 'would', 'be', 'the', 'last', 'song', 'i', 'heard', 'or', 'it', 'could', 'be', 'my', 'favorite', 'song', 'at', 'the', 'time', '.', 'but', 'it', 'feel', 'like', 'the', 'song', 'is', 'all', 'i', 'can', 'focus', 'on', '.', 'sometimes', 'what', 'helps', 'me', 'focus', 'is', 'if', 'i', 'listen', 'to', 'different', 'music', ';', 'something', 'that', 'is', 'not', 'my', 'favorite', 'or', 'something', 'new', '.', 'something', 'i', 'can', 'just', 'listen', 'to', 'in', 'the', 'background', 'and', 'not', 'focus', 'on', '.']\n",
      "INFO:__main__:Number of tokens: 128\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'hear', 'music', 'in', 'their', 'heads', 'while', 'doing', 'a', 'boring', 'task', '?', 'whenever', 'i', 'have', 'to', 'do', 'something', 'boring', 'or', 'un', '##int', '##eres', '##ting', 'like', 'studying', 'for', 'a', 'test', 'or', 'gathering', 'information', 'for', 'a', 'project', 'at', 'work', 'and', 'it', \"'\", 's', 'completely', 'dull', '.', 'i', 'always', 'hear', 'music', ',', 'not', 'real', 'music', 'but', 'in', 'my', 'head', '.', 'sometimes', 'it', 'would', 'be', 'the', 'last', 'song', 'i', 'heard', 'or', 'it', 'could', 'be', 'my', 'favorite', 'song', 'at', 'the', 'time', '.', 'but', 'it', 'feel', 'like', 'the', 'song', 'is', 'all', 'i', 'can', 'focus', 'on', '.', 'sometimes', 'what', 'helps', 'me', 'focus', 'is', 'if', 'i', 'listen', 'to', 'different', 'music', ';', 'something', 'that', 'is', 'not', 'my', 'favorite', 'or', 'something', 'new', '.', 'something', 'i', 'can', 'just', 'listen', 'to', 'in', 'the', 'background', 'and', 'not', 'focus', 'on', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['help', 'me', 'with', 'some', 'advice', 'please', '.', 'hi', ',', 'i', 'don', \"'\", 't', 'think', 'i', 'have', 'add', ',', 'at', '##lea', '##st', 'i', 'don', '##t', 'think', 'i', 'do', '.', 'from', 'my', 'very', 'shallow', 'understanding', 'of', 'add', 'it', 'is', 'basically', 'where', 'you', 'have', 'so', 'much', 'going', 'on', 'in', 'your', 'mind', 'that', 'you', 'can', '##t', 'concentrate', '.', 'but', 'me', ',', 'i', 'have', 'exactly', 'the', 'opposite', '.', 'my', 'mind', 'is', 'constantly', 'blank', '.', 'when', 'i', 'read', 'things', 'i', 'have', 'to', 're', '##rea', '##d', 'them', 'or', 'im', 'like', '\"', 'what', 'am', 'i', 'reading', '\"', 'and', 'i', 'can', \"'\", 't', 'focus', 'in', 'class', 'or', 'pay', 'attention', 'to', 'anything', 'for', 'a', 'extended', 'amount', 'of', 'time', '.', 'iv', '##e', 'seen', 'a', 'ne', '##uro', '##logist', 'and', 'had', 'a', 'mri', 'brain', 'scan', 'and', 'it', 'turned', 'out', 'normal', '(', 'thank', 'god', ')', '.', 'i', 'guess', 'this', 'very', 'uno', '##rga', '##in', '##zed', 'comment', 'is', 'me', 'just', 'looking', 'for', 'advice', 'on', 'what', 'i', 'should', 'do', ',', 'i', 'know', 'this', 'might', 'be', 'the', 'wrong', 'forum', 'but', 'i', 'don', '##t', 'know', 'where', 'else', 'to', 'ask', '.', 'thank', 'you', '.', 'even', 'typing', 'this', 'out', 'i', 'had', 'to', 're', '##rea', '##d', 'what', 'i', 'wrote', 'because', 'i', 'already', 'forgot', 'what', 'i', 'was', 'talking', 'about', '.', '-', '_', '-', '.', 'there', 'was', 'def', '##fin', '##ately', 'a', 'not', '##ica', '##ble', 'change', 'in', 'me', ',', '4', 'months', 'ago', 'i', 'could', 'concentrate', 'fine', 'and', 'i', 'did', 'well', 'in', 'school', 'and', 'now', 'my', 'grades', 'are', 'slipping', 'and', 'i', 'can', '##t', 'focus', '.', 'please', 'help', 'me', '.']\n",
      "INFO:__main__:Number of tokens: 243\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['help', 'me', 'with', 'some', 'advice', 'please', '.', 'hi', ',', 'i', 'don', \"'\", 't', 'think', 'i', 'have', 'add', ',', 'at', '##lea', '##st', 'i', 'don', '##t', 'think', 'i', 'do', '.', 'from', 'my', 'very', 'shallow', 'understanding', 'of', 'add', 'it', 'is', 'basically', 'where', 'you', 'have', 'so', 'much', 'going', 'on', 'in', 'your', 'mind', 'that', 'you', 'can', '##t', 'concentrate', '.', 'but', 'me', ',', 'i', 'have', 'exactly', 'the', 'opposite', '.', 'my', 'mind', 'is', 'constantly', 'blank', '.', 'when', 'i', 'read', 'things', 'i', 'have', 'to', 're', '##rea', '##d', 'them', 'or', 'im', 'like', '\"', 'what', 'am', 'i', 'reading', '\"', 'and', 'i', 'can', \"'\", 't', 'focus', 'in', 'class', 'or', 'pay', 'attention', 'to', 'anything', 'for', 'a', 'extended', 'amount', 'of', 'time', '.', 'iv', '##e', 'seen', 'a', 'ne', '##uro', '##logist', 'and', 'had', 'a', 'mri', 'brain', 'scan', 'and', 'it', 'turned', 'out', 'normal', '(', 'thank', 'god', ')', '.', 'i', 'guess', 'this', 'very', 'uno', '##rga', '##in', '##zed', 'comment', 'is', 'me', 'just', 'looking', 'for', 'advice', 'on', 'what', 'i', 'should', 'do', ',', 'i', 'know', 'this', 'might', 'be', 'the', 'wrong', 'forum', 'but', 'i', 'don', '##t', 'know', 'where', 'else', 'to', 'ask', '.', 'thank', 'you', '.', 'even', 'typing', 'this', 'out', 'i', 'had', 'to', 're', '##rea', '##d', 'what', 'i', 'wrote', 'because', 'i', 'already', 'forgot', 'what', 'i', 'was', 'talking', 'about', '.', '-', '_', '-', '.', 'there', 'was', 'def', '##fin', '##ately', 'a', 'not', '##ica', '##ble', 'change', 'in', 'me', ',', '4', 'months', 'ago', 'i', 'could', 'concentrate', 'fine', 'and', 'i', 'did', 'well', 'in', 'school', 'and', 'now', 'my', 'grades', 'are', 'slipping', 'and', 'i', 'can', '##t', 'focus', '.', 'please', 'help', 'me', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'had', 'a', 'fight', 'with', 'my', 'mom', 'she', 'just', 'doesn', \"'\", 't', 'understand', 'me', '.', 'i', 'almost', 'get', 'the', 'feeling', 'she', 'thinks', 'i', \"'\", 'm', 'like', 'this', 'for', 'fun', ',', 'like', 'i', 'forget', 'to', 'tell', 'things', 'just', 'for', 'the', 'sake', 'of', 'it', '.', 'i', 'just', 'remembered', 'a', 'scholarship', 'for', 'people', 'who', 'can', \"'\", 't', 'get', 'up', 'with', 'school', ',', 'it', 'was', 'a', 'year', 'ago', 'i', 'talked', 'about', 'this', 'with', 'someone', 'from', 'my', 'school', 'and', 'at', 'the', 'time', 'i', 'thought', 'i', 'didn', \"'\", 't', 'need', 'it', '.', 'my', 'mom', 'got', 'mad', 'i', 'forgot', 'and', 'she', 'told', 'me', 'to', 'write', 'important', 'things', 'down', '.', 'how', 'on', 'earth', 'can', 'i', 'write', 'down', 'everything', 'thing', 'that', 'might', 'be', 'important', 'in', 'the', 'future', '?', 'i', 'would', 'need', 'huge', 'books', 'for', 'that', '!', 'and', 'i', \"'\", 'm', 'already', 'progressing', 'at', 'writing', 'appointments', 'and', 'stuff', 'down', ',', 'but', 'clearly', 'she', 'doesn', \"'\", 't', 'see', 'that', '.', 'so', 'i', 'just', 'told', 'her', 'i', 'don', \"'\", 't', 'like', 'to', 'be', 'like', 'this', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'forget', 'stuff', '.', 'i', 'know', 'it', \"'\", 's', 'hard', 'to', 'understand', 'as', 'a', 'normal', 'person', ',', 'but', 'it', \"'\", 's', 'even', 'harder', 'to', 'be', 'misunderstood', '.', 'just', 'wanted', 'to', 'get', 'this', 'off', 'my', 'chest', ',', 'please', 'tell', 'me', 'stories', 'like', 'this', 'about', 'how', 'your', 'family', '/', 'friends', 'don', \"'\", 't', 'understand', 'you', '!', '(', 'sorry', 'for', 'the', 'bad', 'english', ',', 'i', \"'\", 'm', 'just', 'upset', 'and', 'i', 'can', \"'\", 't', 'really', 'think', 'straight', ')']\n",
      "INFO:__main__:Number of tokens: 244\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'had', 'a', 'fight', 'with', 'my', 'mom', 'she', 'just', 'doesn', \"'\", 't', 'understand', 'me', '.', 'i', 'almost', 'get', 'the', 'feeling', 'she', 'thinks', 'i', \"'\", 'm', 'like', 'this', 'for', 'fun', ',', 'like', 'i', 'forget', 'to', 'tell', 'things', 'just', 'for', 'the', 'sake', 'of', 'it', '.', 'i', 'just', 'remembered', 'a', 'scholarship', 'for', 'people', 'who', 'can', \"'\", 't', 'get', 'up', 'with', 'school', ',', 'it', 'was', 'a', 'year', 'ago', 'i', 'talked', 'about', 'this', 'with', 'someone', 'from', 'my', 'school', 'and', 'at', 'the', 'time', 'i', 'thought', 'i', 'didn', \"'\", 't', 'need', 'it', '.', 'my', 'mom', 'got', 'mad', 'i', 'forgot', 'and', 'she', 'told', 'me', 'to', 'write', 'important', 'things', 'down', '.', 'how', 'on', 'earth', 'can', 'i', 'write', 'down', 'everything', 'thing', 'that', 'might', 'be', 'important', 'in', 'the', 'future', '?', 'i', 'would', 'need', 'huge', 'books', 'for', 'that', '!', 'and', 'i', \"'\", 'm', 'already', 'progressing', 'at', 'writing', 'appointments', 'and', 'stuff', 'down', ',', 'but', 'clearly', 'she', 'doesn', \"'\", 't', 'see', 'that', '.', 'so', 'i', 'just', 'told', 'her', 'i', 'don', \"'\", 't', 'like', 'to', 'be', 'like', 'this', ',', 'i', 'don', \"'\", 't', 'want', 'to', 'forget', 'stuff', '.', 'i', 'know', 'it', \"'\", 's', 'hard', 'to', 'understand', 'as', 'a', 'normal', 'person', ',', 'but', 'it', \"'\", 's', 'even', 'harder', 'to', 'be', 'misunderstood', '.', 'just', 'wanted', 'to', 'get', 'this', 'off', 'my', 'chest', ',', 'please', 'tell', 'me', 'stories', 'like', 'this', 'about', 'how', 'your', 'family', '/', 'friends', 'don', \"'\", 't', 'understand', 'you', '!', '(', 'sorry', 'for', 'the', 'bad', 'english', ',', 'i', \"'\", 'm', 'just', 'upset', 'and', 'i', 'can', \"'\", 't', 'really', 'think', 'straight', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['this', 'site', 'helped', 'me', 'get', 'my', 'bearings', 'as', 'i', 'started', 'wondering', 'if', 'i', 'had', 'add', '.', 'thought', 'someone', 'else', 'might', 'benefit', 'from', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['this', 'site', 'helped', 'me', 'get', 'my', 'bearings', 'as', 'i', 'started', 'wondering', 'if', 'i', 'had', 'add', '.', 'thought', 'someone', 'else', 'might', 'benefit', 'from', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['always', 'had', 'issues', 'with', 'add', ',', 'though', 'never', 'diagnosed', 'or', 'treated', ',', 'now', 'leading', 'to', 'depression', '.', 'first', 'off', 'apologies', 'for', 'this', 'text', ',', 'to', 'sum', 'it', 'up', ',', 'i', 'have', 'major', 'issues', 'with', 'writing', '.', '>', 'l', 'to', 'start', 'it', 'off', ',', 'i', 'have', 'always', 'had', 'issues', 'with', 'attention', 'and', 'concentrating', ',', 'making', 'me', 'terrible', 'at', 'assignments', 'and', 'course', '##work', 'etc', '.', 'fortunately', ',', 'i', 'grew', 'up', 'naturally', 'smart', ',', 'which', 'meant', 'i', 'could', 'do', 'assignments', 'and', 'work', 'at', 'the', 'last', 'minute', ',', 'in', 'school', 'and', 'still', 'get', 'off', 'with', 'average', 'grades', '.', 'most', 'the', 'time', 'i', 'have', 'just', 'been', 'branded', \"'\", 'lazy', \"'\", 'which', 'i', 'convinced', 'myself', 'was', 'true', '.', 'but', 'this', 'changed', 'during', '6th', 'form', '(', 'i', 'guess', 'the', 'uk', 'version', 'of', 'college', ')', 'when', 'i', 'was', 'getting', 'my', 'a', '-', 'levels', '.', '>', 'l', 'the', 'first', 'year', '(', 'as', 'levels', ')', 'i', 'did', 'ok', ',', 'but', 'mainly', 'due', 'to', 'easy', 'course', '##work', ',', 'and', '2', 'it', 'based', 'projects', 'which', 'is', 'the', 'only', 'thing', 'i', 'managed', 'to', 'do', 'properly', 'as', 'i', 'enjoyed', 'them', '.', 'but', 'for', 'the', 'a', 'levels', '(', 'year', '2', ')', 'i', 'messed', 'up', ',', 'and', 'did', 'okay', 'in', 'some', 'course', '##work', ',', 'but', 'border', '##line', 'failed', 'all', 'my', 'exams', ',', 'ending', 'up', 'with', 'under', 'average', 'grades', '(', 'cs', 'and', 'ds', ')', 'for', 'what', 'i', 'was', 'capable', 'of', 'i', 'was', 'definitely', 'disappointed', '.', '>', 'l', 'at', 'university', '(', 'studying', 'computing', ')', 'it', 'just', 'got', 'worse', ',', 'first', 'year', ',', 'i', 'did', 'very', 'well', 'in', 'the', 'pure', 'programming', 'projects', ',', 'though', 'most', 'i', 'did', 'the', 'night', 'before', ',', 'partly', 'because', 'they', 'weren', \"'\", 't', 'that', 'challenging', '.', 'but', 'for', 'the', 'modules', 'with', 'essay', 'work', 'and', 'writing', 'i', 'barely', 'got', 'over', '40', '%', 'and', 'scored', '0', 'in', 'one', 'because', 'i', 'simply', 'could', 'not', 'get', 'into', 'the', 'minds', '##et', 'to', 'doing', 'it', '.', '>', 'l', 'the', '2nd', 'year', 'was', 'pretty', 'much', 'identical', ',', 'except', 'i', 'started', 'to', 'struggle', 'in', 'even', 'the', 'projects', 'i', \"'\", 'd', 'previously', 'been', 'interested', ',', 'such', 'as', 'different', 'programming', 'languages', ',', 'and', 'i', 'barely', 'passed', 'the', 'year', 'scoring', 'an', 'average', 'of', '40', '-', '50', '%', 'in', 'most', 'modules', ',', 'but', 'also', 'scoring', '60', '-', '70', '%', 'in', 'the', 'few', 'modules', 'i', 'was', 'interested', 'in', '.', '>', 'l', 'the', '3rd', 'year', ',', 'which', 'i', 'may', 'well', 'just', 'be', 'about', 'to', 'fail', '.', 'things', 'only', 'got', 'worse', ',', 'i', 'have', 'found', 'it', 'impossible', 'to', 'concentrate', 'or', 'focus', 'my', 'mind', 'on', 'anything', 'remotely', 'challenging', 'or', 'large', 'scale', ',', 'keeping', 'on', 'pushing', 'back', 'deadline', '##s', 'until', 'the', 'very', 'last', 'minute', ',', 'and', 'i', 'am', 'talking', 'about', 'projects', 'which', 'are', 'supposed', 'to', 'take', '40', 'hours', 'the', 'day', 'before', '.', '>', 'l', 'but', 'what', 'concerns', 'me', 'the', 'most', 'is', 'how', 'in', 'the', 'past', 'year', ',', 'i', 'have', 'been', 'feeling', 'depressed', ',', 'having', 'a', 'complete', 'negative', 'outlook', 'on', 'my', 'future', ',', 'basically', 'giving', 'up', 'on', 'any', 'hopes', 'of', 'having', 'a', 'job', 'in', 'computing', ',', 'i', 'have', 'lost', 'all', 'interest', 'in', 'most', 'things', ',', 'including', 'computers', 'and', 'computer', 'games', '(', 'which', 'previously', 'was', 'my', 'life', ')', 'when', 'i', \"'\", 'm', 'on', 'the', 'computer', 'i', 'feel', 'myself', 'watching', '/', 'playing', '/', 'red', '##dit', '##ing', 'more', 'to', 'waste', 'time', 'or', 'just', 'because', 'there', 'is', 'nothing', 'else', 'to', 'do', ',', 'instead', 'of', 'actually', 'enjoying', 'it', '.', '>', 'l', 'i', 'also', 'find', 'myself', 'constantly', 'tired', ',', 'and', 'find', 'myself', 'sleeping', 'until', 'late', 'in', 'the', 'day', 'then', 'going', 'to', 'sleep', 'relatively', 'early', 'than', 'i', 'used', 'to', '.', 'this', 'year', 'as', 'well', 'i', 'have', 'not', 'managed', 'to', 'really', 'cook', 'anything', 'more', 'complicated', 'than', 'a', 'ready', 'meal', 'or', 'frozen', 'pizza', ',', 'and', 'have', 'put', 'on', 'a', 'lot', 'of', 'weight', ',', 'despite', 'recently', 'averaging', 'about', '1', 'meal', 'per', 'day', 'with', 'snacks', 'throughout', 'the', 'day', '.', '>', 'l', 'i', 'find', 'communicating', 'with', 'my', 'family', '/', 'friends', 'pretty', 'hard', ',', 'and', 'throughout', 'the', 'year', 'i', 'have', 'lied', 'to', 'them', 'about', 'my', 'progress', ',', 'how', 'i', \"'\", 'm', 'doing', '/', 'feeling', 'etc', ',', 'and', 'i', 'can', 'easily', 'put', 'on', 'a', 'happy', 'face', 'and', 'positive', 'aura', 'when', 'around', 'people', ',', 'then', 'go', 'home', 'and', 'feel', 'lonely', 'and', 'just', 'as', 'shitty', 'as', 'before', '.', '>', 'l', 'another', 'major', 'issue', 'is', 'i', 'am', 'too', 'scared', 'to', 'ask', 'for', 'help', ',', 'and', 'i', 'often', 'found', 'myself', 'skipping', 'lectures', 'and', 'practical', '##s', 'because', 'i', 'was', 'behind', ',', 'and', 'didn', \"'\", 't', 'want', 'the', 'lecturer', '##s', 'to', 'find', 'out', '.', 'this', 'is', 'also', 'the', 'main', 'reason', 'i', 'haven', \"'\", 't', 'brought', 'this', 'topic', 'up', 'before', ',', 'but', 'i', 'feel', 'its', 'got', 'to', 'a', 'point', 'where', 'i', 'can', '##t', 'get', 'anything', 'done', ',', 'and', 'will', 'be', 'finished', 'with', 'university', 'in', 'less', 'than', 'a', 'year', ',', 'with', 'little', 'career', 'prospects', '.', '>', 'l', 'this', 'is', 'also', 'probably', 'the', 'most', 'amount', 'of', 'words', 'in', 'a', 'short', 'time', 'i', 'have', 'done', 'in', 'a', 'while', ',', 'though', 'mostly', 'due', 'to', 'the', 'anonymous', 'aspect', ',', 'i', 'can', 'not', 'stand', 'anyone', 'reading', 'anything', 'i', 'read', ',', 'mostly', 'due', 'to', 'embarrassed', '.', '>', 'l', '*', '*', 'the', 'main', 'question', '*', '*', '>', 'l', 'the', 'main', 'issue', 'is', 'i', 'want', 'to', 'get', 'some', 'help', ',', 'or', 'even', 'advice', ',', 'but', 'i', 'have', 'no', 'idea', 'what', 'to', 'do', ',', 'i', 'am', 'in', 'the', 'uk', ',', 'so', 'the', 'general', 'understanding', 'is', 'that', 'i', 'contact', 'a', 'gp', ',', 'but', 'i', 'have', 'no', 'idea', 'what', 'to', 'say', ',', 'or', 'to', 'do', ',', 'and', 'if', 'i', 'need', 'to', 'be', 'registered', 'or', 'anything', '.', 'if', 'anyone', 'could', 'post', 'some', 'useful', 'advice', 'i', 'would', 'very', 'much', 'appreciate', 'it', ',', 'thanks', '.', 'edit', ':', 'apologies', 'for', 'the', 'format', '##ting', ',', 'i', 'could', 'not', 'manage', 'to', 'find', 'a', 'way', 'to', 'put', 'a', 'line', 'break', '.']\n",
      "INFO:__main__:Number of tokens: 928\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['always', 'had', 'issues', 'with', 'add', ',', 'though', 'never', 'diagnosed', 'or', 'treated', ',', 'now', 'leading', 'to', 'depression', '.', 'first', 'off', 'apologies', 'for', 'this', 'text', ',', 'to', 'sum', 'it', 'up', ',', 'i', 'have', 'major', 'issues', 'with', 'writing', '.', '>', 'l', 'to', 'start', 'it', 'off', ',', 'i', 'have', 'always', 'had', 'issues', 'with', 'attention', 'and', 'concentrating', ',', 'making', 'me', 'terrible', 'at', 'assignments', 'and', 'course', '##work', 'etc', '.', 'fortunately', ',', 'i', 'grew', 'up', 'naturally', 'smart', ',', 'which', 'meant', 'i', 'could', 'do', 'assignments', 'and', 'work', 'at', 'the', 'last', 'minute', ',', 'in', 'school', 'and', 'still', 'get', 'off', 'with', 'average', 'grades', '.', 'most', 'the', 'time', 'i', 'have', 'just', 'been', 'branded', \"'\", 'lazy', \"'\", 'which', 'i', 'convinced', 'myself', 'was', 'true', '.', 'but', 'this', 'changed', 'during', '6th', 'form', '(', 'i', 'guess', 'the', 'uk', 'version', 'of', 'college', ')', 'when', 'i', 'was', 'getting', 'my', 'a', '-', 'levels', '.', '>', 'l', 'the', 'first', 'year', '(', 'as', 'levels', ')', 'i', 'did', 'ok', ',', 'but', 'mainly', 'due', 'to', 'easy', 'course', '##work', ',', 'and', '2', 'it', 'based', 'projects', 'which', 'is', 'the', 'only', 'thing', 'i', 'managed', 'to', 'do', 'properly', 'as', 'i', 'enjoyed', 'them', '.', 'but', 'for', 'the', 'a', 'levels', '(', 'year', '2', ')', 'i', 'messed', 'up', ',', 'and', 'did', 'okay', 'in', 'some', 'course', '##work', ',', 'but', 'border', '##line', 'failed', 'all', 'my', 'exams', ',', 'ending', 'up', 'with', 'under', 'average', 'grades', '(', 'cs', 'and', 'ds', ')', 'for', 'what', 'i', 'was', 'capable', 'of', 'i', 'was', 'definitely', 'disappointed', '.', '>', 'l', 'at', 'university', '(', 'studying', 'computing', ')', 'it', 'just', 'got', 'worse', ',', 'first', 'year', ',', 'i', 'did', 'very', 'well', 'in', 'the', 'pure', 'programming', 'projects', ',', 'though', 'most', 'i', 'did', 'the', 'night', 'before', ',', 'partly', 'because', 'they', 'weren', \"'\", 't', 'that', 'challenging', '.', 'but', 'for', 'the', 'modules', 'with', 'essay', 'work', 'and', 'writing', 'i', 'barely', 'got', 'over', '40', '%', 'and', 'scored', '0', 'in', 'one', 'because', 'i', 'simply', 'could', 'not', 'get', 'into', 'the', 'minds', '##et', 'to', 'doing', 'it', '.', '>', 'l', 'the', '2nd', 'year', 'was', 'pretty', 'much', 'identical', ',', 'except', 'i', 'started', 'to', 'struggle', 'in', 'even', 'the', 'projects', 'i', \"'\", 'd', 'previously', 'been', 'interested', ',', 'such', 'as', 'different', 'programming', 'languages', ',', 'and', 'i', 'barely', 'passed', 'the', 'year', 'scoring', 'an', 'average', 'of', '40', '-', '50', '%', 'in', 'most', 'modules', ',', 'but', 'also', 'scoring', '60', '-', '70', '%', 'in', 'the', 'few', 'modules', 'i', 'was', 'interested', 'in', '.', '>', 'l', 'the', '3rd', 'year', ',', 'which', 'i', 'may', 'well', 'just', 'be', 'about', 'to', 'fail', '.', 'things', 'only', 'got', 'worse', ',', 'i', 'have', 'found', 'it', 'impossible', 'to', 'concentrate', 'or', 'focus', 'my', 'mind', 'on', 'anything', 'remotely', 'challenging', 'or', 'large', 'scale', ',', 'keeping', 'on', 'pushing', 'back', 'deadline', '##s', 'until', 'the', 'very', 'last', 'minute', ',', 'and', 'i', 'am', 'talking', 'about', 'projects', 'which', 'are', 'supposed', 'to', 'take', '40', 'hours', 'the', 'day', 'before', '.', '>', 'l', 'but', 'what', 'concerns', 'me', 'the', 'most', 'is', 'how', 'in', 'the', 'past', 'year', ',', 'i', 'have', 'been', 'feeling', 'depressed', ',', 'having', 'a', 'complete', 'negative', 'outlook', 'on', 'my', 'future', ',', 'basically', 'giving', 'up', 'on', 'any', 'hopes', 'of', 'having', 'a', 'job', 'in', 'computing', ',', 'i', 'have', 'lost', 'all', 'interest', 'in', 'most', 'things', ',', 'including', 'computers', 'and', 'computer', 'games', '(', 'which', 'previously', 'was', 'my', 'life', ')', 'when', 'i', \"'\", 'm', 'on', 'the', 'computer', 'i'], ['feel', 'myself', 'watching', '/', 'playing', '/', 'red', '##dit', '##ing', 'more', 'to', 'waste', 'time', 'or', 'just', 'because', 'there', 'is', 'nothing', 'else', 'to', 'do', ',', 'instead', 'of', 'actually', 'enjoying', 'it', '.', '>', 'l', 'i', 'also', 'find', 'myself', 'constantly', 'tired', ',', 'and', 'find', 'myself', 'sleeping', 'until', 'late', 'in', 'the', 'day', 'then', 'going', 'to', 'sleep', 'relatively', 'early', 'than', 'i', 'used', 'to', '.', 'this', 'year', 'as', 'well', 'i', 'have', 'not', 'managed', 'to', 'really', 'cook', 'anything', 'more', 'complicated', 'than', 'a', 'ready', 'meal', 'or', 'frozen', 'pizza', ',', 'and', 'have', 'put', 'on', 'a', 'lot', 'of', 'weight', ',', 'despite', 'recently', 'averaging', 'about', '1', 'meal', 'per', 'day', 'with', 'snacks', 'throughout', 'the', 'day', '.', '>', 'l', 'i', 'find', 'communicating', 'with', 'my', 'family', '/', 'friends', 'pretty', 'hard', ',', 'and', 'throughout', 'the', 'year', 'i', 'have', 'lied', 'to', 'them', 'about', 'my', 'progress', ',', 'how', 'i', \"'\", 'm', 'doing', '/', 'feeling', 'etc', ',', 'and', 'i', 'can', 'easily', 'put', 'on', 'a', 'happy', 'face', 'and', 'positive', 'aura', 'when', 'around', 'people', ',', 'then', 'go', 'home', 'and', 'feel', 'lonely', 'and', 'just', 'as', 'shitty', 'as', 'before', '.', '>', 'l', 'another', 'major', 'issue', 'is', 'i', 'am', 'too', 'scared', 'to', 'ask', 'for', 'help', ',', 'and', 'i', 'often', 'found', 'myself', 'skipping', 'lectures', 'and', 'practical', '##s', 'because', 'i', 'was', 'behind', ',', 'and', 'didn', \"'\", 't', 'want', 'the', 'lecturer', '##s', 'to', 'find', 'out', '.', 'this', 'is', 'also', 'the', 'main', 'reason', 'i', 'haven', \"'\", 't', 'brought', 'this', 'topic', 'up', 'before', ',', 'but', 'i', 'feel', 'its', 'got', 'to', 'a', 'point', 'where', 'i', 'can', '##t', 'get', 'anything', 'done', ',', 'and', 'will', 'be', 'finished', 'with', 'university', 'in', 'less', 'than', 'a', 'year', ',', 'with', 'little', 'career', 'prospects', '.', '>', 'l', 'this', 'is', 'also', 'probably', 'the', 'most', 'amount', 'of', 'words', 'in', 'a', 'short', 'time', 'i', 'have', 'done', 'in', 'a', 'while', ',', 'though', 'mostly', 'due', 'to', 'the', 'anonymous', 'aspect', ',', 'i', 'can', 'not', 'stand', 'anyone', 'reading', 'anything', 'i', 'read', ',', 'mostly', 'due', 'to', 'embarrassed', '.', '>', 'l', '*', '*', 'the', 'main', 'question', '*', '*', '>', 'l', 'the', 'main', 'issue', 'is', 'i', 'want', 'to', 'get', 'some', 'help', ',', 'or', 'even', 'advice', ',', 'but', 'i', 'have', 'no', 'idea', 'what', 'to', 'do', ',', 'i', 'am', 'in', 'the', 'uk', ',', 'so', 'the', 'general', 'understanding', 'is', 'that', 'i', 'contact', 'a', 'gp', ',', 'but', 'i', 'have', 'no', 'idea', 'what', 'to', 'say', ',', 'or', 'to', 'do', ',', 'and', 'if', 'i', 'need', 'to', 'be', 'registered', 'or', 'anything', '.', 'if', 'anyone', 'could', 'post', 'some', 'useful', 'advice', 'i', 'would', 'very', 'much', 'appreciate', 'it', ',', 'thanks', '.', 'edit', ':', 'apologies', 'for', 'the', 'format', '##ting', ',', 'i', 'could', 'not', 'manage', 'to', 'find', 'a', 'way', 'to', 'put', 'a', 'line', 'break', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['side', 'effects', 'of', 'ad', '##hd', 'medications']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['side', 'effects', 'of', 'ad', '##hd', 'medications']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['help', 'with', 'management', 'of', 'time', 'and', 'focusing', 'on', 'goals', '.', 'so', 'i', 'have', 'been', 'off', 'medication', 'for', 'add', 'for', 'a', 'while', '.', 'now', 'that', 'i', 'am', 'out', 'of', 'school', 'and', 'in', 'the', 'working', 'world', 'the', 'symptoms', 'of', 'add', 'are', 'hitting', 'back', '.', 'i', 'am', 'noticing', 'lost', 'time', ',', 'the', 'same', 'way', 'i', 'did', 'when', 'i', 'was', 'young', '.', 'i', 'will', 'start', 'to', 'do', 'something', 'and', 'then', 'suddenly', 'its', '4', 'hours', 'later', 'and', 'i', 'have', 'barely', 'even', 'started', 'my', 'task', '.', 'does', 'any', 'one', 'have', 'any', 'advice', 'or', 'solutions', 'to', 'getting', 'motivated', ',', 'staying', 'motivated', 'and', 'sticking', 'to', 'a', 'task', '?', 'prefer', '##ably', 'non', 'med', '##icated', 'solutions', '.']\n",
      "INFO:__main__:Number of tokens: 107\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['help', 'with', 'management', 'of', 'time', 'and', 'focusing', 'on', 'goals', '.', 'so', 'i', 'have', 'been', 'off', 'medication', 'for', 'add', 'for', 'a', 'while', '.', 'now', 'that', 'i', 'am', 'out', 'of', 'school', 'and', 'in', 'the', 'working', 'world', 'the', 'symptoms', 'of', 'add', 'are', 'hitting', 'back', '.', 'i', 'am', 'noticing', 'lost', 'time', ',', 'the', 'same', 'way', 'i', 'did', 'when', 'i', 'was', 'young', '.', 'i', 'will', 'start', 'to', 'do', 'something', 'and', 'then', 'suddenly', 'its', '4', 'hours', 'later', 'and', 'i', 'have', 'barely', 'even', 'started', 'my', 'task', '.', 'does', 'any', 'one', 'have', 'any', 'advice', 'or', 'solutions', 'to', 'getting', 'motivated', ',', 'staying', 'motivated', 'and', 'sticking', 'to', 'a', 'task', '?', 'prefer', '##ably', 'non', 'med', '##icated', 'solutions', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'need', 'to', 'vent', '.', 'i', 'hope', 'you', 'guys', 'don', \"'\", 't', 'mind', '.']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'need', 'to', 'vent', '.', 'i', 'hope', 'you', 'guys', 'don', \"'\", 't', 'mind', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['difficulty', 'hearing', 'musical', '\"', 'beat', '\"', 'without', 'a', 'visual', 'aid']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['difficulty', 'hearing', 'musical', '\"', 'beat', '\"', 'without', 'a', 'visual', 'aid']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'can', 'i', 'do', ',', 'i', \"'\", 'm', 'at', 'the', 'end', 'of', 'my', 'rope', ',', 'suggestions', '?', 'so', 'i', \"'\", 'm', '21', ',', 'graduating', 'with', 'pre', '-', 'law', ',', 'then', 'going', 'back', 'to', 'under', '##grad', 'to', 'finish', 'up', 'a', 'couple', 'of', 'pre', '-', 'med', 're', '##quisite', '##s', '.', 'this', 'last', 'semester', 'has', 'been', 'the', 'worst', 'when', 'it', 'comes', 'to', 'concentrating', ',', 'especially', 'since', 'i', 'haven', \"'\", 't', 'taken', 'science', 'and', 'its', 'different', 'from', 'what', 'i', \"'\", 'm', 'used', 'to', '.', 'i', 'just', 'got', 'a', 'c', 'on', 'my', 'biology', 'final', ',', 'with', 'a', 'b', 'in', 'the', 'class', ',', 'so', 'i', \"'\", 'm', 'not', 'sure', 'what', 'my', 'final', 'grade', 'will', 'be', '.', 'now', 'i', \"'\", 'm', 'not', 'the', 'sharpe', '##st', 'cr', '##ayo', '##n', ',', 'but', 'i', \"'\", 'm', 'not', 'dumb', '(', 'i', 'don', \"'\", 't', 'think', ')', 'and', 'i', 'put', 'all', 'i', 'could', 'into', 'that', 'test', 'and', 'still', 'came', 'up', 'shorter', 'than', 'i', 'wanted', '.', 'concentration', 'has', 'been', 'harder', 'to', 'hold', 'for', 'me', ',', 'and', 'my', 'anxiety', 'has', 'gone', 'from', 'moderate', 'to', 'worse', '.', 'for', 'me', ',', 'if', 'it', 'doesn', \"'\", 't', 'come', 'naturally', ',', 'i', 'have', 'to', 'ex', '##pen', '##d', 'a', 'lot', 'more', 'energy', 'than', 'my', 'peers', 'to', 'force', 'concentration', 'and', 'learning', '.', 'thing', 'is', ',', 'i', 'was', 'diagnosed', 'about', '7', 'years', 'ago', 'with', 'add', 'and', 'o', '##cd', 'from', 'a', 'couple', 'therapist', '##s', ',', 'and', 'i', 'took', 'rita', '##lin', '(', 'add', '##eral', '##l', 'gave', 'me', 'headache', '##s', ')', '.', 'i', 'stopped', 'about', 'a', 'year', 'later', 'as', 'i', 'couldn', \"'\", 't', 'tell', 'what', 'it', 'was', 'doing', 'for', 'me', 'and', 'school', 'wasn', \"'\", 't', 'that', 'demanding', '.', 'now', 'i', 'feel', 'like', 'all', 'of', 'my', 'mental', 'resources', 'are', 'fix', '##ated', 'on', 'school', ',', 'and', 'i', 'cannot', 'find', 'the', 'energy', 'to', 'even', 'keep', 'up', 'my', 'personal', 'relationships', ',', 'i', \"'\", 'm', 'isolated', 'now', '.', 'how', 'can', 'i', 'work', 'smarter', ',', 'not', 'harder', ',', 'should', 'i', 'see', 'if', 'i', 'can', 'get', 'a', 'third', 'opinion', 'on', 'my', 'add', 'and', 'gauge', 'its', 'development', 'now', '?', 't', '##l', ':', 'dr', 'more', 'i', 'try', ',', 'harder', 'it', 'gets', '.', 'diagnosed', 'years', 'ago', 'with', 'add', '/', 'o', '##cd', 'but', 'stopped', 'taking', 'med', '##s', 'shortly', 'after', ',', 'tired', 'of', 'running', 'myself', 'ragged', 'with', 'little', 'to', 'show', 'for', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 367\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'can', 'i', 'do', ',', 'i', \"'\", 'm', 'at', 'the', 'end', 'of', 'my', 'rope', ',', 'suggestions', '?', 'so', 'i', \"'\", 'm', '21', ',', 'graduating', 'with', 'pre', '-', 'law', ',', 'then', 'going', 'back', 'to', 'under', '##grad', 'to', 'finish', 'up', 'a', 'couple', 'of', 'pre', '-', 'med', 're', '##quisite', '##s', '.', 'this', 'last', 'semester', 'has', 'been', 'the', 'worst', 'when', 'it', 'comes', 'to', 'concentrating', ',', 'especially', 'since', 'i', 'haven', \"'\", 't', 'taken', 'science', 'and', 'its', 'different', 'from', 'what', 'i', \"'\", 'm', 'used', 'to', '.', 'i', 'just', 'got', 'a', 'c', 'on', 'my', 'biology', 'final', ',', 'with', 'a', 'b', 'in', 'the', 'class', ',', 'so', 'i', \"'\", 'm', 'not', 'sure', 'what', 'my', 'final', 'grade', 'will', 'be', '.', 'now', 'i', \"'\", 'm', 'not', 'the', 'sharpe', '##st', 'cr', '##ayo', '##n', ',', 'but', 'i', \"'\", 'm', 'not', 'dumb', '(', 'i', 'don', \"'\", 't', 'think', ')', 'and', 'i', 'put', 'all', 'i', 'could', 'into', 'that', 'test', 'and', 'still', 'came', 'up', 'shorter', 'than', 'i', 'wanted', '.', 'concentration', 'has', 'been', 'harder', 'to', 'hold', 'for', 'me', ',', 'and', 'my', 'anxiety', 'has', 'gone', 'from', 'moderate', 'to', 'worse', '.', 'for', 'me', ',', 'if', 'it', 'doesn', \"'\", 't', 'come', 'naturally', ',', 'i', 'have', 'to', 'ex', '##pen', '##d', 'a', 'lot', 'more', 'energy', 'than', 'my', 'peers', 'to', 'force', 'concentration', 'and', 'learning', '.', 'thing', 'is', ',', 'i', 'was', 'diagnosed', 'about', '7', 'years', 'ago', 'with', 'add', 'and', 'o', '##cd', 'from', 'a', 'couple', 'therapist', '##s', ',', 'and', 'i', 'took', 'rita', '##lin', '(', 'add', '##eral', '##l', 'gave', 'me', 'headache', '##s', ')', '.', 'i', 'stopped', 'about', 'a', 'year', 'later', 'as', 'i', 'couldn', \"'\", 't', 'tell', 'what', 'it', 'was', 'doing', 'for', 'me', 'and', 'school', 'wasn', \"'\", 't', 'that', 'demanding', '.', 'now', 'i', 'feel', 'like', 'all', 'of', 'my', 'mental', 'resources', 'are', 'fix', '##ated', 'on', 'school', ',', 'and', 'i', 'cannot', 'find', 'the', 'energy', 'to', 'even', 'keep', 'up', 'my', 'personal', 'relationships', ',', 'i', \"'\", 'm', 'isolated', 'now', '.', 'how', 'can', 'i', 'work', 'smarter', ',', 'not', 'harder', ',', 'should', 'i', 'see', 'if', 'i', 'can', 'get', 'a', 'third', 'opinion', 'on', 'my', 'add', 'and', 'gauge', 'its', 'development', 'now', '?', 't', '##l', ':', 'dr', 'more', 'i', 'try', ',', 'harder', 'it', 'gets', '.', 'diagnosed', 'years', 'ago', 'with', 'add', '/', 'o', '##cd', 'but', 'stopped', 'taking', 'med', '##s', 'shortly', 'after', ',', 'tired', 'of', 'running', 'myself', 'ragged', 'with', 'little', 'to', 'show', 'for', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['exchanging', 'writing', 'skills', 'for', 'some', 'ad', '##hd', 'coaching', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['exchanging', 'writing', 'skills', 'for', 'some', 'ad', '##hd', 'coaching', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['looking', 'for', 'advice', 'i', \"'\", 'm', '24', 'years', 'old', 'and', 'in', 'gr', '##ad', 'school', 'at', 'the', 'moment', '.', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'anxiety', 'and', 'mild', 'depression', 'in', 'the', 'past', ',', 'but', 'it', 'was', 'recently', 'brought', 'to', 'my', 'attention', 'that', 'i', 'might', 'have', 'ad', '##hd', '(', 'which', 'i', \"'\", 'm', 'told', 'often', 'comes', 'with', 'the', 'anxiety', 'and', 'depression', ')', '.', 'i', 'do', 'have', 'a', 'lot', 'of', 'the', 'symptoms', ',', 'and', 'my', 'brother', 'and', 'dad', 'both', 'have', 'it', '.', 'my', 'school', 'is', 'out', 'of', 'state', 'and', 'i', 'have', 'about', '3', 'weeks', 'left', 'before', 'i', 'head', 'home', '.', 'i', \"'\", 'd', 'like', 'to', 'get', 'tested', 'now', 'so', 'if', 'i', 'find', 'out', 'i', 'have', 'ad', '##hd', ',', 'i', 'can', 'start', 'treatment', 'and', 'maybe', 'be', 'able', 'to', 'focus', 'on', 'my', 'work', 'better', 'for', 'these', 'last', 'few', 'weeks', '.', 'my', 'worry', 'is', 'that', 'i', \"'\", 'll', 'be', 'incorrectly', 'diagnosed', 'and', 'given', 'medication', 'that', 'makes', 'it', 'even', 'harder', 'for', 'me', 'to', 'get', 'anything', 'done', '.', '(', 'or', 'correctly', 'diagnosed', 'but', 'given', 'medication', 'that', 'i', 'have', 'a', 'bad', 'reaction', 'to', ')', '.', 'so', 'my', 'question', 'is', 'this', ':', 'do', 'you', 'guys', 'think', 'it', 'would', 'be', 'a', 'better', 'idea', 'to', 'get', 'tested', 'now', ',', 'or', 'wait', 'until', 'my', 'semester', 'ends', 'and', 'i', \"'\", 'm', 'home', 'for', 'the', 'summer', '?', 'i', 'hope', 'that', 'all', 'made', 'sense', '.', '.', '.', 'i', 'lost', 'track', 'of', 'what', 'i', 'was', 'trying', 'to', 'say', 'a', 'few', 'times', 'there', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 238\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['looking', 'for', 'advice', 'i', \"'\", 'm', '24', 'years', 'old', 'and', 'in', 'gr', '##ad', 'school', 'at', 'the', 'moment', '.', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'anxiety', 'and', 'mild', 'depression', 'in', 'the', 'past', ',', 'but', 'it', 'was', 'recently', 'brought', 'to', 'my', 'attention', 'that', 'i', 'might', 'have', 'ad', '##hd', '(', 'which', 'i', \"'\", 'm', 'told', 'often', 'comes', 'with', 'the', 'anxiety', 'and', 'depression', ')', '.', 'i', 'do', 'have', 'a', 'lot', 'of', 'the', 'symptoms', ',', 'and', 'my', 'brother', 'and', 'dad', 'both', 'have', 'it', '.', 'my', 'school', 'is', 'out', 'of', 'state', 'and', 'i', 'have', 'about', '3', 'weeks', 'left', 'before', 'i', 'head', 'home', '.', 'i', \"'\", 'd', 'like', 'to', 'get', 'tested', 'now', 'so', 'if', 'i', 'find', 'out', 'i', 'have', 'ad', '##hd', ',', 'i', 'can', 'start', 'treatment', 'and', 'maybe', 'be', 'able', 'to', 'focus', 'on', 'my', 'work', 'better', 'for', 'these', 'last', 'few', 'weeks', '.', 'my', 'worry', 'is', 'that', 'i', \"'\", 'll', 'be', 'incorrectly', 'diagnosed', 'and', 'given', 'medication', 'that', 'makes', 'it', 'even', 'harder', 'for', 'me', 'to', 'get', 'anything', 'done', '.', '(', 'or', 'correctly', 'diagnosed', 'but', 'given', 'medication', 'that', 'i', 'have', 'a', 'bad', 'reaction', 'to', ')', '.', 'so', 'my', 'question', 'is', 'this', ':', 'do', 'you', 'guys', 'think', 'it', 'would', 'be', 'a', 'better', 'idea', 'to', 'get', 'tested', 'now', ',', 'or', 'wait', 'until', 'my', 'semester', 'ends', 'and', 'i', \"'\", 'm', 'home', 'for', 'the', 'summer', '?', 'i', 'hope', 'that', 'all', 'made', 'sense', '.', '.', '.', 'i', 'lost', 'track', 'of', 'what', 'i', 'was', 'trying', 'to', 'say', 'a', 'few', 'times', 'there', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['who', 'else', '.', '.', '?', 'admit', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['who', 'else', '.', '.', '?', 'admit', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'your', 'experiences', 'with', 'non', '-', 'st', '##im', '##ula', '##nt', 'medications', '?', 'i', \"'\", 've', 'tried', 'a', 'few', 'medications', 'with', 'my', 'doctor', 'but', 'st', '##im', '##ula', '##nts', 'just', 'do', 'not', 'work', 'for', 'me', '.', 'they', 'help', 'me', 'focus', 'a', 'little', 'bit', 'better', 'but', 'make', 'me', 'way', 'too', 'wired', '.', 'they', 'make', 'me', 'uncomfortable', 'and', 'it', 'becomes', 'almost', 'impossible', 'for', 'me', 'to', 'calm', 'down', 'or', 'have', 'peace', 'of', 'mind', '.', 'so', 'i', \"'\", 'm', 'thinking', 'about', 'switching', 'to', 'non', '-', 'st', '##im', '##ula', '##nts', 'like', 'st', '##rat', '##tera', 'and', 'wanted', 'to', 'know', 'if', 'others', 'have', 'had', 'similar', 'reactions', 'to', 'st', '##im', '##ula', '##nts', 'and', 'if', 'non', '-', 'st', '##im', '##s', 'have', 'helped', '.']\n",
      "INFO:__main__:Number of tokens: 113\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'your', 'experiences', 'with', 'non', '-', 'st', '##im', '##ula', '##nt', 'medications', '?', 'i', \"'\", 've', 'tried', 'a', 'few', 'medications', 'with', 'my', 'doctor', 'but', 'st', '##im', '##ula', '##nts', 'just', 'do', 'not', 'work', 'for', 'me', '.', 'they', 'help', 'me', 'focus', 'a', 'little', 'bit', 'better', 'but', 'make', 'me', 'way', 'too', 'wired', '.', 'they', 'make', 'me', 'uncomfortable', 'and', 'it', 'becomes', 'almost', 'impossible', 'for', 'me', 'to', 'calm', 'down', 'or', 'have', 'peace', 'of', 'mind', '.', 'so', 'i', \"'\", 'm', 'thinking', 'about', 'switching', 'to', 'non', '-', 'st', '##im', '##ula', '##nts', 'like', 'st', '##rat', '##tera', 'and', 'wanted', 'to', 'know', 'if', 'others', 'have', 'had', 'similar', 'reactions', 'to', 'st', '##im', '##ula', '##nts', 'and', 'if', 'non', '-', 'st', '##im', '##s', 'have', 'helped', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['my', 'daughter', 'was', 'just', 'diagnosed', '.', '.', '.', '.', 'my', 'daughter', ',', '8', 'years', 'old', ',', 'has', 'just', 'been', 'diagnosed', 'with', 'ad', '##hd', 'pre', '##dom', '##inate', '##ly', 'ina', '##tten', '##tive', '.', 'i', \"'\", 've', 'decided', 'that', 'medication', 'is', 'not', 'necessary', 'as', 'of', 'yet', ',', '&', 'i', \"'\", 'm', 'wondering', 'if', 'anyone', 'here', 'had', 'some', 'ideas', 'on', 'ways', 'i', 'can', 'help', 'my', 'daughter', 'learn', 'new', 'behaviour', '##s', 'to', 'adapt', 'to', 'ad', '##hd', '-', 'pi', '.', 'any', 'strategies', 'or', 'advice', 'i', 'would', 'be', 'more', 'than', 'happy', 'to', 'hear', '!', 'thanks', '!', '!']\n",
      "INFO:__main__:Number of tokens: 90\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['my', 'daughter', 'was', 'just', 'diagnosed', '.', '.', '.', '.', 'my', 'daughter', ',', '8', 'years', 'old', ',', 'has', 'just', 'been', 'diagnosed', 'with', 'ad', '##hd', 'pre', '##dom', '##inate', '##ly', 'ina', '##tten', '##tive', '.', 'i', \"'\", 've', 'decided', 'that', 'medication', 'is', 'not', 'necessary', 'as', 'of', 'yet', ',', '&', 'i', \"'\", 'm', 'wondering', 'if', 'anyone', 'here', 'had', 'some', 'ideas', 'on', 'ways', 'i', 'can', 'help', 'my', 'daughter', 'learn', 'new', 'behaviour', '##s', 'to', 'adapt', 'to', 'ad', '##hd', '-', 'pi', '.', 'any', 'strategies', 'or', 'advice', 'i', 'would', 'be', 'more', 'than', 'happy', 'to', 'hear', '!', 'thanks', '!', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'have', 'experience', 'with', 'v', '##y', '##van', '##se', '?', 'i', \"'\", 've', 'recently', 'been', 'diagnosed', 'with', 'ad', '##hd', '(', 'the', 'non', '-', 'hyper', 'kind', ')', 'and', 'my', 'doctor', 'put', 'me', 'on', 'v', '##y', '##van', '##se', '.', 'i', \"'\", 've', 'heard', 'that', 'st', '##im', '##ula', '##nts', ',', 'like', 'add', '##eral', '##l', ',', 'can', 'get', 'you', 'hopped', '-', 'up', 'and', 'stuff', ',', 'but', 'v', '##y', '##van', '##se', 'doesn', \"'\", 't', 'really', 'do', 'that', 'to', 'me', '.', 'i', \"'\", 'm', 'focusing', 'better', 'than', 'i', 'used', 'to', ',', 'and', 'i', 'have', 'a', 'lot', 'more', 'motivation', ',', 'but', 'does', 'the', 'lack', 'of', '\"', 'kick', '\"', 'mean', 'that', 'it', \"'\", 's', 'just', 'some', 'kind', 'of', 'place', '##bo', 'effect', ',', 'or', 'is', 'v', '##y', '##van', '##se', 'just', 'really', 'subtle', 'when', 'it', 'works', '?']\n",
      "INFO:__main__:Number of tokens: 126\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'have', 'experience', 'with', 'v', '##y', '##van', '##se', '?', 'i', \"'\", 've', 'recently', 'been', 'diagnosed', 'with', 'ad', '##hd', '(', 'the', 'non', '-', 'hyper', 'kind', ')', 'and', 'my', 'doctor', 'put', 'me', 'on', 'v', '##y', '##van', '##se', '.', 'i', \"'\", 've', 'heard', 'that', 'st', '##im', '##ula', '##nts', ',', 'like', 'add', '##eral', '##l', ',', 'can', 'get', 'you', 'hopped', '-', 'up', 'and', 'stuff', ',', 'but', 'v', '##y', '##van', '##se', 'doesn', \"'\", 't', 'really', 'do', 'that', 'to', 'me', '.', 'i', \"'\", 'm', 'focusing', 'better', 'than', 'i', 'used', 'to', ',', 'and', 'i', 'have', 'a', 'lot', 'more', 'motivation', ',', 'but', 'does', 'the', 'lack', 'of', '\"', 'kick', '\"', 'mean', 'that', 'it', \"'\", 's', 'just', 'some', 'kind', 'of', 'place', '##bo', 'effect', ',', 'or', 'is', 'v', '##y', '##van', '##se', 'just', 'really', 'subtle', 'when', 'it', 'works', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'love', 'for', 'people', 'with', 'ad', '##hd', 'and', 'in', 'a', 'college', 'fraternity', '?', 'i', 'don', \"'\", 't', 'know', 'who', 'else', 'out', 'there', 'this', 'may', 'apply', 'to', 'but', 'i', 'have', 'ad', '##hd', 'and', 'am', 'in', 'a', 'fraternity', ',', 'sigma', 'pi', ',', 'beta', 'tau', 'chapter', '.', 'anyone', 'else', 'out', 'there', 'in', 'a', 'fraternity', 'or', 'sorority', 'or', 'something', 'like', 'that', '?']\n",
      "INFO:__main__:Number of tokens: 58\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'love', 'for', 'people', 'with', 'ad', '##hd', 'and', 'in', 'a', 'college', 'fraternity', '?', 'i', 'don', \"'\", 't', 'know', 'who', 'else', 'out', 'there', 'this', 'may', 'apply', 'to', 'but', 'i', 'have', 'ad', '##hd', 'and', 'am', 'in', 'a', 'fraternity', ',', 'sigma', 'pi', ',', 'beta', 'tau', 'chapter', '.', 'anyone', 'else', 'out', 'there', 'in', 'a', 'fraternity', 'or', 'sorority', 'or', 'something', 'like', 'that', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'i', 'was', 'going', 'to', 'talk', 'to', 'my', 'psychiatrist', 'and', 'then', 'i', 'just', 'blank', 'when', 'i', \"'\", 'm', 'there', '.']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', 'i', 'was', 'going', 'to', 'talk', 'to', 'my', 'psychiatrist', 'and', 'then', 'i', 'just', 'blank', 'when', 'i', \"'\", 'm', 'there', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'know', 'i', 'have', 'ad', '##hd', 'but', 'cannot', 'tell', 'what', 'type', '.', 'i', 'have', 'looked', 'around', 'and', 'have', 'many', 'of', 'the', 'symptoms', 'of', 'hyper', 'active', ',', 'but', 'fall', 'into', 'other', 'types', 'as', 'well', '.', 'how', 'can', 'i', 'tell', 'since', 'i', 'am', 'going', 'to', 'college', 'soon', 'i', 'would', 'like', 'to', 'know', 'how', 'to', 'med', '##icate', 'finally', '.', 'i', 'have', 'not', 'taken', 'anything', 'for', 'treatment', 'since', 'my', 'second', 'year', 'of', 'third', 'grade', 'so', 'i', 'don', \"'\", 't', 'know', 'what', 'is', 'out', 'there', 'our', 'what', 'to', 'take', '.']\n",
      "INFO:__main__:Number of tokens: 85\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'know', 'i', 'have', 'ad', '##hd', 'but', 'cannot', 'tell', 'what', 'type', '.', 'i', 'have', 'looked', 'around', 'and', 'have', 'many', 'of', 'the', 'symptoms', 'of', 'hyper', 'active', ',', 'but', 'fall', 'into', 'other', 'types', 'as', 'well', '.', 'how', 'can', 'i', 'tell', 'since', 'i', 'am', 'going', 'to', 'college', 'soon', 'i', 'would', 'like', 'to', 'know', 'how', 'to', 'med', '##icate', 'finally', '.', 'i', 'have', 'not', 'taken', 'anything', 'for', 'treatment', 'since', 'my', 'second', 'year', 'of', 'third', 'grade', 'so', 'i', 'don', \"'\", 't', 'know', 'what', 'is', 'out', 'there', 'our', 'what', 'to', 'take', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['when', 'will', 'the', \"'\", 'tolerance', \"'\", 'section', 'of', '/', 'r', '/', 'ad', '##hd', 'fa', '##q', 'be', 'updated', '?', 'any', 'inputs', '?']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['when', 'will', 'the', \"'\", 'tolerance', \"'\", 'section', 'of', '/', 'r', '/', 'ad', '##hd', 'fa', '##q', 'be', 'updated', '?', 'any', 'inputs', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'representative', 'is', 'dr', 'russell', 'bark', '##ley', 'on', 'the', 'current', 'scientific', 'state', 'of', 'the', 'art', 'on', 'ad', '##hd', '?', 'i', 'think', 'lots', 'of', 'us', 'have', 'seen', 'the', 'videos', '.', 'i', 'dunn', '##o', 'about', 'you', ',', 'i', 'find', 'them', 'massive', '##ly', 'appealing', ',', 'a', 'lot', 'of', 'self', '-', 'recognition', 'and', '“', 'yu', '##p', '!', 'that', \"'\", 's', 'totally', 'me', '!', '”', 'but', 'just', 'because', 'you', 'like', 'it', 'doesn', \"'\", 't', 'mean', 'it', \"'\", 's', 'true', '.', 'anybody', 'know', 'what', 'reputation', 'his', 'work', 'actually', 'has', 'in', 'the', 'research', 'community', '(', 'and', 'how', 'you', 'get', 'that', 'impression', ')', '?', '(', 'me', '##h', ':', 'representative', '*', 'of', '*', '.', 'english', 'fail', ')']\n",
      "INFO:__main__:Number of tokens: 108\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'representative', 'is', 'dr', 'russell', 'bark', '##ley', 'on', 'the', 'current', 'scientific', 'state', 'of', 'the', 'art', 'on', 'ad', '##hd', '?', 'i', 'think', 'lots', 'of', 'us', 'have', 'seen', 'the', 'videos', '.', 'i', 'dunn', '##o', 'about', 'you', ',', 'i', 'find', 'them', 'massive', '##ly', 'appealing', ',', 'a', 'lot', 'of', 'self', '-', 'recognition', 'and', '“', 'yu', '##p', '!', 'that', \"'\", 's', 'totally', 'me', '!', '”', 'but', 'just', 'because', 'you', 'like', 'it', 'doesn', \"'\", 't', 'mean', 'it', \"'\", 's', 'true', '.', 'anybody', 'know', 'what', 'reputation', 'his', 'work', 'actually', 'has', 'in', 'the', 'research', 'community', '(', 'and', 'how', 'you', 'get', 'that', 'impression', ')', '?', '(', 'me', '##h', ':', 'representative', '*', 'of', '*', '.', 'english', 'fail', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['question', 'to', 'those', 'taking', 'st', '##im', '##ula', '##nts', ':', 'what', 'dos', '##ages', 'are', 'you', 'on', '?', 'i', \"'\", 'm', 'just', 'asking', 'out', 'of', 'curiosity', ',', 'to', 'see', 'how', 'i', 'compare', 'to', 'everyone', 'else', '.', 'i', \"'\", 'm', 'on', '27', 'mg', 'of', 'concert', '##a', '+', '10', 'mg', 'of', 'rita', '##lin', '(', 'short', 'acting', ')', 'per', 'day', '.']\n",
      "INFO:__main__:Number of tokens: 56\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['question', 'to', 'those', 'taking', 'st', '##im', '##ula', '##nts', ':', 'what', 'dos', '##ages', 'are', 'you', 'on', '?', 'i', \"'\", 'm', 'just', 'asking', 'out', 'of', 'curiosity', ',', 'to', 'see', 'how', 'i', 'compare', 'to', 'everyone', 'else', '.', 'i', \"'\", 'm', 'on', '27', 'mg', 'of', 'concert', '##a', '+', '10', 'mg', 'of', 'rita', '##lin', '(', 'short', 'acting', ')', 'per', 'day', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['discrimination', 'against', 'myself', 'in', 'my', 'major', 'so', 'i', 'had', 'a', 'conditional', 'accepted', 'meeting', 'yesterday', '.', 'in', 'order', 'to', 'get', 'this', 'i', 'composed', 'a', 'letter', 'explaining', 'that', 'i', 'was', 'on', 'medication', 'and', 'it', 'makes', 'me', 'withdrawn', '(', 'i', 'am', 'an', 'education', 'major', ')', '.', 'it', 'went', 'awful', '.', 'they', 'told', 'me', 'i', 'either', 'had', 'to', 'take', 'a', 'course', 'i', 'already', 'took', ',', 'or', 'a', 'kindergarten', 'experience', 'class', '.', 'in', 'order', 'to', 'gain', 'more', 'experience', 'socially', '.', 'there', 'is', 'a', 'catch', ',', 'they', 'want', 'to', 'disclose', 'i', 'am', 'on', 'med', '##s', 'to', 'staff', 'members', '.', 'can', 'they', 'even', 'do', 'that', '?', 'i', 'am', 'on', '40', 'mg', 'of', 'v', '##y', '##van', '##se', ',', 'i', 'am', 'adjusting', 'it', 'to', 'lower', 'to', 'see', 'if', 'that', 'helps', '.']\n",
      "INFO:__main__:Number of tokens: 122\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['discrimination', 'against', 'myself', 'in', 'my', 'major', 'so', 'i', 'had', 'a', 'conditional', 'accepted', 'meeting', 'yesterday', '.', 'in', 'order', 'to', 'get', 'this', 'i', 'composed', 'a', 'letter', 'explaining', 'that', 'i', 'was', 'on', 'medication', 'and', 'it', 'makes', 'me', 'withdrawn', '(', 'i', 'am', 'an', 'education', 'major', ')', '.', 'it', 'went', 'awful', '.', 'they', 'told', 'me', 'i', 'either', 'had', 'to', 'take', 'a', 'course', 'i', 'already', 'took', ',', 'or', 'a', 'kindergarten', 'experience', 'class', '.', 'in', 'order', 'to', 'gain', 'more', 'experience', 'socially', '.', 'there', 'is', 'a', 'catch', ',', 'they', 'want', 'to', 'disclose', 'i', 'am', 'on', 'med', '##s', 'to', 'staff', 'members', '.', 'can', 'they', 'even', 'do', 'that', '?', 'i', 'am', 'on', '40', 'mg', 'of', 'v', '##y', '##van', '##se', ',', 'i', 'am', 'adjusting', 'it', 'to', 'lower', 'to', 'see', 'if', 'that', 'helps', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'you', 'think', '?']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'you', 'think', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 've', 'been', 'on', 'st', '##rat', '##tera', 'for', 'ten', 'days', '.', 'pleasantly', 'surprised', '!']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 've', 'been', 'on', 'st', '##rat', '##tera', 'for', 'ten', 'days', '.', 'pleasantly', 'surprised', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'tried', 'a', 'focusing', 'technique', 'so', 'someone', 'make', 'the', 'suggestion', 'to', 'make', 'a', 'tally', 'every', 'time', 'i', 'catch', 'myself', 'drifting', 'off', '.', '.', 'at', 'the', 'end', 'of', 'the', 'class', ',', 'i', 'had', '10', 'tall', '##ies', 'but', 'felt', 'like', 'it', 'would', 'have', 'been', 'more', 'productive', 'to', 'just', 'tally', 'the', 'times', 'i', 'paid', 'attention', '.', '.', 'when', 'noticing', 'my', 'internal', 'dial', '##og', '.', '.', 'i', \"'\", 've', 'always', 'noticed', 'it', 'was', 'consistent', '.', '.', 'particularly', 'around', 'people', '/', 'when', 'i', \"'\", 'm', 'not', 'alone', 'and', 'there', 'is', 'a', 'contrast', 'to', 'my', 'own', 'thinking', '.', '.', 'today', '.', '.', 'it', 'was', 'also', 'loud', '.', '.', 'i', 'couldn', \"'\", 't', 'hear', 'my', 'teacher', 'over', 'my', 'own', 'inner', 'ram', '##bling', '##s', 'and', 'wandering', 'off', 'about', 'everything', 'else', '.']\n",
      "INFO:__main__:Number of tokens: 123\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'tried', 'a', 'focusing', 'technique', 'so', 'someone', 'make', 'the', 'suggestion', 'to', 'make', 'a', 'tally', 'every', 'time', 'i', 'catch', 'myself', 'drifting', 'off', '.', '.', 'at', 'the', 'end', 'of', 'the', 'class', ',', 'i', 'had', '10', 'tall', '##ies', 'but', 'felt', 'like', 'it', 'would', 'have', 'been', 'more', 'productive', 'to', 'just', 'tally', 'the', 'times', 'i', 'paid', 'attention', '.', '.', 'when', 'noticing', 'my', 'internal', 'dial', '##og', '.', '.', 'i', \"'\", 've', 'always', 'noticed', 'it', 'was', 'consistent', '.', '.', 'particularly', 'around', 'people', '/', 'when', 'i', \"'\", 'm', 'not', 'alone', 'and', 'there', 'is', 'a', 'contrast', 'to', 'my', 'own', 'thinking', '.', '.', 'today', '.', '.', 'it', 'was', 'also', 'loud', '.', '.', 'i', 'couldn', \"'\", 't', 'hear', 'my', 'teacher', 'over', 'my', 'own', 'inner', 'ram', '##bling', '##s', 'and', 'wandering', 'off', 'about', 'everything', 'else', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['if', 'i', 'loose', 'attention', '/', 'focus', 'when', 'i', \"'\", 'm', 'being', 'talked', 'or', 'do', 'something', 'it', 'can', 'be', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['if', 'i', 'loose', 'attention', '/', 'focus', 'when', 'i', \"'\", 'm', 'being', 'talked', 'or', 'do', 'something', 'it', 'can', 'be', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['working', 'memory', 'i', \"'\", 've', 'been', 'reading', 'a', 'bit', 'today', 'and', 'i', 'think', 'i', \"'\", 've', 'narrowed', 'my', 'primary', 'problems', 'down', 'to', 'working', 'memory', '.', 'i', 'have', 'most', 'of', 'the', 'symptoms', 'of', 'd', '##ys', '##cal', '##cu', '##lia', '.', 'it', \"'\", 's', 'next', 'to', 'impossible', 'for', 'me', 'to', 'do', 'math', 'in', 'my', 'head', '.', 'i', 'often', 'confuse', 'left', 'and', 'right', ',', 'if', 'i', 'give', 'myself', 'a', 'few', 'seconds', 'i', \"'\", 'll', 'get', 'it', ',', 'but', 'if', 'i', 'have', 'to', 'respond', 'quickly', 'i', \"'\", 'll', 'often', 'confuse', 'them', '.', 'same', 'with', 'analog', 'clocks', ',', 'it', 'takes', 'me', 'a', 'few', 'seconds', 'to', 'get', 'it', 'and', 'i', 'often', 'find', 'myself', 'looking', 'at', 'my', 'watch', 'a', 'few', 'times', 'to', 'make', 'sure', 'i', 'read', 'it', 'correctly', '.', 'i', 'also', 'have', 'trouble', 'matching', 'names', 'to', 'faces', ',', 'there', 'were', 'even', 'instances', 'of', 'not', 'being', 'able', 'to', 'recall', 'immediate', 'family', 'members', 'names', '.', 'however', ',', 'i', 'have', 'been', 'programming', 'for', 'years', '(', 'since', 'i', 'was', '9', 'so', 'almost', '20', 'years', ')', '.', 'i', 'can', 'organize', 'a', 'programs', 'work', 'flow', 'in', 'my', 'head', 'and', 'it', 'is', 'sometimes', 'complex', ',', 'and', 'if', 'i', 'work', 'fast', ',', 'type', 'it', 'all', 'out', 'before', 'i', 'forget', 'any', 'of', 'it', '.', 'if', 'i', 'have', 'to', 'write', 'it', 'down', ',', 'i', 'can', 'only', 'describe', 'it', 'as', 'having', 'to', 'remember', 'the', 'programs', 'methods', 'and', 'work', 'flow', 'as', 'symbols', '.', 'its', 'very', 'difficult', 'to', 'describe', '.', 'my', 'question', 'is', 'if', 'my', 'primary', 'issues', 'boil', 'down', 'to', 'a', 'working', 'memory', 'deficit', ',', 'is', 'this', 'manage', '##able', '?', 'could', 'it', 'be', 'possible', 'that', 'i', 'work', 'out', 'a', 'different', 'way', 'of', 'doing', 'math', 'in', 'my', 'head', 'like', 'i', 'do', 'for', 'programming', '?', 'if', 'so', ',', 'perhaps', 'applying', 'that', 'to', 'other', 'areas', 'i', 'have', 'trouble', 'with', '?']\n",
      "INFO:__main__:Number of tokens: 286\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['working', 'memory', 'i', \"'\", 've', 'been', 'reading', 'a', 'bit', 'today', 'and', 'i', 'think', 'i', \"'\", 've', 'narrowed', 'my', 'primary', 'problems', 'down', 'to', 'working', 'memory', '.', 'i', 'have', 'most', 'of', 'the', 'symptoms', 'of', 'd', '##ys', '##cal', '##cu', '##lia', '.', 'it', \"'\", 's', 'next', 'to', 'impossible', 'for', 'me', 'to', 'do', 'math', 'in', 'my', 'head', '.', 'i', 'often', 'confuse', 'left', 'and', 'right', ',', 'if', 'i', 'give', 'myself', 'a', 'few', 'seconds', 'i', \"'\", 'll', 'get', 'it', ',', 'but', 'if', 'i', 'have', 'to', 'respond', 'quickly', 'i', \"'\", 'll', 'often', 'confuse', 'them', '.', 'same', 'with', 'analog', 'clocks', ',', 'it', 'takes', 'me', 'a', 'few', 'seconds', 'to', 'get', 'it', 'and', 'i', 'often', 'find', 'myself', 'looking', 'at', 'my', 'watch', 'a', 'few', 'times', 'to', 'make', 'sure', 'i', 'read', 'it', 'correctly', '.', 'i', 'also', 'have', 'trouble', 'matching', 'names', 'to', 'faces', ',', 'there', 'were', 'even', 'instances', 'of', 'not', 'being', 'able', 'to', 'recall', 'immediate', 'family', 'members', 'names', '.', 'however', ',', 'i', 'have', 'been', 'programming', 'for', 'years', '(', 'since', 'i', 'was', '9', 'so', 'almost', '20', 'years', ')', '.', 'i', 'can', 'organize', 'a', 'programs', 'work', 'flow', 'in', 'my', 'head', 'and', 'it', 'is', 'sometimes', 'complex', ',', 'and', 'if', 'i', 'work', 'fast', ',', 'type', 'it', 'all', 'out', 'before', 'i', 'forget', 'any', 'of', 'it', '.', 'if', 'i', 'have', 'to', 'write', 'it', 'down', ',', 'i', 'can', 'only', 'describe', 'it', 'as', 'having', 'to', 'remember', 'the', 'programs', 'methods', 'and', 'work', 'flow', 'as', 'symbols', '.', 'its', 'very', 'difficult', 'to', 'describe', '.', 'my', 'question', 'is', 'if', 'my', 'primary', 'issues', 'boil', 'down', 'to', 'a', 'working', 'memory', 'deficit', ',', 'is', 'this', 'manage', '##able', '?', 'could', 'it', 'be', 'possible', 'that', 'i', 'work', 'out', 'a', 'different', 'way', 'of', 'doing', 'math', 'in', 'my', 'head', 'like', 'i', 'do', 'for', 'programming', '?', 'if', 'so', ',', 'perhaps', 'applying', 'that', 'to', 'other', 'areas', 'i', 'have', 'trouble', 'with', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['was', 'on', 'well', '##bu', '##tri', '##n', 'but', 'got', 'sick', 'of', 'it', 'killing', 'my', 'appetite', ',', 'card', '##io', ',', 'and', 'extra', 'sweat', '.', 'i', 'can', 'live', 'with', 'the', 'extra', 'sweat', ',', 'but', 'i', 'do', 'brazilian', 'ji', '##u', '-', 'ji', '##tsu', '3', '-', '4', 'times', 'a', 'week', 'and', 'it', \"'\", 's', 'killing', 'my', 'card', '##io', '.', 'my', 'card', '##io', 'might', 'be', 'sucking', 'because', 'i', 'eat', 'a', 'lot', 'less', 'when', 'i', \"'\", 'm', 'on', 'it', ',', 'so', 'i', \"'\", 'm', 'possibly', 'not', 'getting', 'enough', 'energy', 'from', 'my', 'food', '.', 'has', 'anyone', 'here', 'been', 'on', 'well', '##bu', '##tri', '##n', 'and', 'switched', 'to', 'something', 'else', '?', 'any', 'suggestions', '?', 'i', \"'\", 'm', 'also', 'curious', 'if', 'st', '##im', '##ula', '##nts', 'would', 'effect', 'my', 'training', '/', 'fighting', 'in', 'any', 'negative', 'or', 'positive', 'ways', '.']\n",
      "INFO:__main__:Number of tokens: 127\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['was', 'on', 'well', '##bu', '##tri', '##n', 'but', 'got', 'sick', 'of', 'it', 'killing', 'my', 'appetite', ',', 'card', '##io', ',', 'and', 'extra', 'sweat', '.', 'i', 'can', 'live', 'with', 'the', 'extra', 'sweat', ',', 'but', 'i', 'do', 'brazilian', 'ji', '##u', '-', 'ji', '##tsu', '3', '-', '4', 'times', 'a', 'week', 'and', 'it', \"'\", 's', 'killing', 'my', 'card', '##io', '.', 'my', 'card', '##io', 'might', 'be', 'sucking', 'because', 'i', 'eat', 'a', 'lot', 'less', 'when', 'i', \"'\", 'm', 'on', 'it', ',', 'so', 'i', \"'\", 'm', 'possibly', 'not', 'getting', 'enough', 'energy', 'from', 'my', 'food', '.', 'has', 'anyone', 'here', 'been', 'on', 'well', '##bu', '##tri', '##n', 'and', 'switched', 'to', 'something', 'else', '?', 'any', 'suggestions', '?', 'i', \"'\", 'm', 'also', 'curious', 'if', 'st', '##im', '##ula', '##nts', 'would', 'effect', 'my', 'training', '/', 'fighting', 'in', 'any', 'negative', 'or', 'positive', 'ways', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['made', 'myself', 'lunch', ',', 'then', 'rita', '##lin', 'kicked', 'in', 'and', 'i', 'couldn', \"'\", 't', 'eat', 'more', 'than', 'two', 'bites', '.', 'happen', 'to', 'anyone', 'else', '?']\n",
      "INFO:__main__:Number of tokens: 25\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['made', 'myself', 'lunch', ',', 'then', 'rita', '##lin', 'kicked', 'in', 'and', 'i', 'couldn', \"'\", 't', 'eat', 'more', 'than', 'two', 'bites', '.', 'happen', 'to', 'anyone', 'else', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['psychiatrist', 'vs', 'psychologists']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['psychiatrist', 'vs', 'psychologists']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['having', 'a', 'very', 'bad', 'day', '.', 'tried', 'to', 'take', 'my', 'mid', '##ter', '##ms', 'today', 'and', 'had', 'a', 'panic', 'attack', '/', 'spaced', 'on', 'everything', 'i', \"'\", 've', 'been', 'studying', 'for', 'the', 'past', 'month', '.', 'couldn', \"'\", 't', 'even', 'finish', '4', 'questions', '.', 'i', 'had', 'to', 'bail', ',', 'and', 'i', 'was', 'supposed', 'to', 'take', 'another', 'one', 'right', 'afterwards', '.', 'i', 'live', 'on', 'my', 'own', 'right', 'now', 'so', 'i', 'have', 'to', 'work', ',', 'but', 'it', 'means', 'that', 'i', 'don', \"'\", 't', 'have', 'enough', 'time', 'to', 'study', ',', 'and', 'stuff', 'like', 'this', 'happens', '.', 'my', 'advisors', 'have', 'told', 'me', 'that', 'i', 'should', 'apply', 'for', 'a', 'medical', 'withdrawal', 'from', 'my', 'classes', 'to', 'try', 'to', 'get', 'my', 'medication', 'and', 'life', 'straightened', 'out', '.', 'this', 'will', 'be', 'the', 'third', 'quarter', 'that', 'i', 'have', 'failed', '/', 'withdrawn', 'from', 'the', 'same', 'two', 'classes', '.', 'my', 'doctor', 'is', 'either', 'hiding', 'from', 'me', 'or', 'doesn', \"'\", 't', 'give', 'enough', 'fuck', '##s', 'to', 'call', 'me', '.', 'i', \"'\", 've', 'been', 'trying', 'to', 'get', 'hold', 'of', 'him', 'for', 'a', 'month', 'and', 'a', 'half', '.', 'what', 'should', 'i', 'do', '?', 'edit', ':', 'i', 'should', 'note', 'that', 'if', 'i', 'take', 'a', 'medical', 'withdrawal', 'there', 'are', 'two', 'outcomes', ':', 'a', '.', 'they', 'give', 'me', 'a', 'full', 'ref', '##und', 'of', 'the', 'money', 'i', 'spent', 'on', 'tuition', '(', 'in', 'my', 'case', 'it', 'would', 'go', 'back', 'to', 'financial', 'aid', ',', 'and', 'my', 'left', 'over', 'money', 'from', 'my', 'check', 'would', 'go', 'back', 'to', 'them', 'as', 'well', ',', 'even', 'the', 'money', 'i', 'already', 'spent', 'on', 'books', ')', 'and', 'b', '.', 'they', 'deny', 'me', 'and', 'make', 'me', 'pay', 'back', 'every', 'cent', 'that', 'i', 'got', 'for', 'this', 'quarter', ',', 'which', 'would', 'ruin', 'me', 'financially', 'because', 'my', 'classes', 'cost', 'more', 'than', 'i', 'have', 'at', 'any', 'time', ',', 'and', 'i', \"'\", 'd', 'get', 'a', '0', '.', '0', 'in', 'my', 'classes', ',', 'ruining', 'my', 'gp', '##a', '.']\n",
      "INFO:__main__:Number of tokens: 301\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['having', 'a', 'very', 'bad', 'day', '.', 'tried', 'to', 'take', 'my', 'mid', '##ter', '##ms', 'today', 'and', 'had', 'a', 'panic', 'attack', '/', 'spaced', 'on', 'everything', 'i', \"'\", 've', 'been', 'studying', 'for', 'the', 'past', 'month', '.', 'couldn', \"'\", 't', 'even', 'finish', '4', 'questions', '.', 'i', 'had', 'to', 'bail', ',', 'and', 'i', 'was', 'supposed', 'to', 'take', 'another', 'one', 'right', 'afterwards', '.', 'i', 'live', 'on', 'my', 'own', 'right', 'now', 'so', 'i', 'have', 'to', 'work', ',', 'but', 'it', 'means', 'that', 'i', 'don', \"'\", 't', 'have', 'enough', 'time', 'to', 'study', ',', 'and', 'stuff', 'like', 'this', 'happens', '.', 'my', 'advisors', 'have', 'told', 'me', 'that', 'i', 'should', 'apply', 'for', 'a', 'medical', 'withdrawal', 'from', 'my', 'classes', 'to', 'try', 'to', 'get', 'my', 'medication', 'and', 'life', 'straightened', 'out', '.', 'this', 'will', 'be', 'the', 'third', 'quarter', 'that', 'i', 'have', 'failed', '/', 'withdrawn', 'from', 'the', 'same', 'two', 'classes', '.', 'my', 'doctor', 'is', 'either', 'hiding', 'from', 'me', 'or', 'doesn', \"'\", 't', 'give', 'enough', 'fuck', '##s', 'to', 'call', 'me', '.', 'i', \"'\", 've', 'been', 'trying', 'to', 'get', 'hold', 'of', 'him', 'for', 'a', 'month', 'and', 'a', 'half', '.', 'what', 'should', 'i', 'do', '?', 'edit', ':', 'i', 'should', 'note', 'that', 'if', 'i', 'take', 'a', 'medical', 'withdrawal', 'there', 'are', 'two', 'outcomes', ':', 'a', '.', 'they', 'give', 'me', 'a', 'full', 'ref', '##und', 'of', 'the', 'money', 'i', 'spent', 'on', 'tuition', '(', 'in', 'my', 'case', 'it', 'would', 'go', 'back', 'to', 'financial', 'aid', ',', 'and', 'my', 'left', 'over', 'money', 'from', 'my', 'check', 'would', 'go', 'back', 'to', 'them', 'as', 'well', ',', 'even', 'the', 'money', 'i', 'already', 'spent', 'on', 'books', ')', 'and', 'b', '.', 'they', 'deny', 'me', 'and', 'make', 'me', 'pay', 'back', 'every', 'cent', 'that', 'i', 'got', 'for', 'this', 'quarter', ',', 'which', 'would', 'ruin', 'me', 'financially', 'because', 'my', 'classes', 'cost', 'more', 'than', 'i', 'have', 'at', 'any', 'time', ',', 'and', 'i', \"'\", 'd', 'get', 'a', '0', '.', '0', 'in', 'my', 'classes', ',', 'ruining', 'my', 'gp', '##a', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'feel', 'awkward', 'in', 'social', 'situations', '?', 'i', 'don', \"'\", 't', 'really', 'like', 'going', 'anywhere', 'that', 'will', 'be', 'busy', ',', 'or', 'parties', 'sometimes', '.', 'like', 'when', 'you', 'go', 'to', 'the', 'grocery', 'store', 'and', 'stop', 'and', 'wait', 'on', 'another', 'person', 'to', 'move', '.', 'normally', 'if', 'i', \"'\", 'm', 'not', 'thinking', 'about', 'anything', 'important', 'i', 'always', 'let', 'the', 'other', 'person', 'go', 'through', ',', 'but', 'if', 'i', \"'\", 'm', 'focused', 'on', 'something', 'and', 'we', 'stop', 'and', 'the', 'other', 'person', 'doesn', \"'\", 't', 'move', 'right', 'away', 'i', \"'\", 'll', 'just', 'go', '.', 'then', 'after', 'i', 'realize', 'what', 'i', 'did', 'and', 'i', 'feel', 'like', 'an', 'ass', '.', 'or', 'sometimes', 'when', 'people', 'talk', 'to', 'me', 'and', 'i', \"'\", 'm', 'not', 'ready', 'for', 'it', 'i', \"'\", 'll', 'just', 'automatically', 'agree', 'with', 'what', 'they', 'said', 'as', 'a', 'reflex', '.', 'then', 'think', 'about', 'what', 'they', 'said', 'but', 'feel', 'awkward', 'when', 'i', 'finally', 'realize', 'what', 'i', 'agreed', 'to', '.']\n",
      "INFO:__main__:Number of tokens: 150\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'feel', 'awkward', 'in', 'social', 'situations', '?', 'i', 'don', \"'\", 't', 'really', 'like', 'going', 'anywhere', 'that', 'will', 'be', 'busy', ',', 'or', 'parties', 'sometimes', '.', 'like', 'when', 'you', 'go', 'to', 'the', 'grocery', 'store', 'and', 'stop', 'and', 'wait', 'on', 'another', 'person', 'to', 'move', '.', 'normally', 'if', 'i', \"'\", 'm', 'not', 'thinking', 'about', 'anything', 'important', 'i', 'always', 'let', 'the', 'other', 'person', 'go', 'through', ',', 'but', 'if', 'i', \"'\", 'm', 'focused', 'on', 'something', 'and', 'we', 'stop', 'and', 'the', 'other', 'person', 'doesn', \"'\", 't', 'move', 'right', 'away', 'i', \"'\", 'll', 'just', 'go', '.', 'then', 'after', 'i', 'realize', 'what', 'i', 'did', 'and', 'i', 'feel', 'like', 'an', 'ass', '.', 'or', 'sometimes', 'when', 'people', 'talk', 'to', 'me', 'and', 'i', \"'\", 'm', 'not', 'ready', 'for', 'it', 'i', \"'\", 'll', 'just', 'automatically', 'agree', 'with', 'what', 'they', 'said', 'as', 'a', 'reflex', '.', 'then', 'think', 'about', 'what', 'they', 'said', 'but', 'feel', 'awkward', 'when', 'i', 'finally', 'realize', 'what', 'i', 'agreed', 'to', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'from', 'singapore', 'here', '?', 'please', 'tell', 'me', 'how', 'you', 'got', 'diagnosed', 'and', 'med', '##icated', '?', 'i', 'just', 'cut', '4', 'out', 'of', '6', 'finals', 'because', 'i', 'couldn', \"'\", 't', 'study', 'all', 'semester', '.', 'i', 'tried', 'going', 'to', 'my', 'school', \"'\", 's', 'counseling', 'centre', 'and', 'they', 'were', 'more', 'interested', 'in', 'telling', 'me', 'to', 'work', 'harder', 'and', 'think', 'about', 'my', 'future', 'and', 'my', 'career', 'and', 'to', 'stop', 'wasting', 'time', 'and', 'stop', 'pro', '##cr', '##ast', '##inating', '(', 'yeah', 'no', 'shit', '.', 'like', 'i', 'need', 'you', 'to', 'tell', 'me', ')', '.', 'any', 'concerns', 'of', 'an', 'attention', 'problem', ',', 'they', 'brushed', 'them', 'off', 'and', 'seemed', 'to', 'con', '##st', '##ru', '##e', 'things', 'i', 'said', 'as', 'depression', ',', 'anxiety', '-', 'i', 'tried', 'telling', 'them', 'i', 'am', 'like', 'that', '*', 'because', '*', 'i', 'can', \"'\", 't', 'focus', '-', 'no', 'acknowledge', '##ment', 'of', 'that', '.', 'the', 'most', 'frustrating', 'and', 'humiliating', 'moment', 'of', 'my', 'life', '-', 'and', 'trust', 'me', ',', 'all', 'my', 'life', 'i', \"'\", 've', 'had', 'people', 'tell', 'me', 'i', \"'\", 'm', 'inc', '##osis', '##ten', '##t', ',', 'lazy', ',', 'not', 'achieving', 'her', 'full', 'potential', '.', 'i', 'read', 'this', 'link', '-', 'http', ':', '/', '/', 'www', '.', 'adult', '##ad', '##df', '##act', '##s', '.', 'com', '/', '-', 'posted', 'by', 'another', 'user', '-', 'most', 'of', 'the', 'symptoms', 'except', 'inability', 'to', 'sit', 'still', '.', 'i', \"'\", 've', 'never', 'been', 'hyper', '##active', '.', 'so', '.', 'has', 'anyone', 'from', 'singapore', 'gotten', 'diagnosed', '?', 'i', \"'\", 'm', 'most', 'interested', 'in', 'knowing', 'the', 'cost', '-', 'consultation', ',', 'diagnostic', 'test', 'cost', ',', 'medication', 'cost', '-', 'where', 'i', 'can', 'actually', 'get', 'it', 'done', '(', 'if', 'im', '##h', ',', 'i', \"'\", 've', 'heard', 'too', 'many', 'horror', 'stories', 'and', 'personal', 'an', '##ec', '##dote', '##s', 'but', 'i', 'can', \"'\", 't', 'let', 'this', 'go', 'on', 'much', 'longer', ')', '.']\n",
      "INFO:__main__:Number of tokens: 286\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'from', 'singapore', 'here', '?', 'please', 'tell', 'me', 'how', 'you', 'got', 'diagnosed', 'and', 'med', '##icated', '?', 'i', 'just', 'cut', '4', 'out', 'of', '6', 'finals', 'because', 'i', 'couldn', \"'\", 't', 'study', 'all', 'semester', '.', 'i', 'tried', 'going', 'to', 'my', 'school', \"'\", 's', 'counseling', 'centre', 'and', 'they', 'were', 'more', 'interested', 'in', 'telling', 'me', 'to', 'work', 'harder', 'and', 'think', 'about', 'my', 'future', 'and', 'my', 'career', 'and', 'to', 'stop', 'wasting', 'time', 'and', 'stop', 'pro', '##cr', '##ast', '##inating', '(', 'yeah', 'no', 'shit', '.', 'like', 'i', 'need', 'you', 'to', 'tell', 'me', ')', '.', 'any', 'concerns', 'of', 'an', 'attention', 'problem', ',', 'they', 'brushed', 'them', 'off', 'and', 'seemed', 'to', 'con', '##st', '##ru', '##e', 'things', 'i', 'said', 'as', 'depression', ',', 'anxiety', '-', 'i', 'tried', 'telling', 'them', 'i', 'am', 'like', 'that', '*', 'because', '*', 'i', 'can', \"'\", 't', 'focus', '-', 'no', 'acknowledge', '##ment', 'of', 'that', '.', 'the', 'most', 'frustrating', 'and', 'humiliating', 'moment', 'of', 'my', 'life', '-', 'and', 'trust', 'me', ',', 'all', 'my', 'life', 'i', \"'\", 've', 'had', 'people', 'tell', 'me', 'i', \"'\", 'm', 'inc', '##osis', '##ten', '##t', ',', 'lazy', ',', 'not', 'achieving', 'her', 'full', 'potential', '.', 'i', 'read', 'this', 'link', '-', 'http', ':', '/', '/', 'www', '.', 'adult', '##ad', '##df', '##act', '##s', '.', 'com', '/', '-', 'posted', 'by', 'another', 'user', '-', 'most', 'of', 'the', 'symptoms', 'except', 'inability', 'to', 'sit', 'still', '.', 'i', \"'\", 've', 'never', 'been', 'hyper', '##active', '.', 'so', '.', 'has', 'anyone', 'from', 'singapore', 'gotten', 'diagnosed', '?', 'i', \"'\", 'm', 'most', 'interested', 'in', 'knowing', 'the', 'cost', '-', 'consultation', ',', 'diagnostic', 'test', 'cost', ',', 'medication', 'cost', '-', 'where', 'i', 'can', 'actually', 'get', 'it', 'done', '(', 'if', 'im', '##h', ',', 'i', \"'\", 've', 'heard', 'too', 'many', 'horror', 'stories', 'and', 'personal', 'an', '##ec', '##dote', '##s', 'but', 'i', 'can', \"'\", 't', 'let', 'this', 'go', 'on', 'much', 'longer', ')', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['loud', '!', 'does', 'anyone', 'else', 'ever', 'feel', 'like', 'the', 'world', 'is', 'yelling', 'at', 'you', '?', 'you', \"'\", 're', 'sitting', 'in', ',', 'what', 'we', 'would', 'call', 'silence', ',', 'and', 'then', 'it', 'feels', 'like', 'you', \"'\", 're', 'being', 'swallowed', 'up', 'by', 'static', 'and', 'just', 'loud', '##ness', '.', 'like', 'you', 'stuck', 'your', 'finger', 'in', 'a', 'light', 'socket', '?', 'or', '.', '.', '.', 'am', 'i', 'just', 'a', 'sc', '##hi', '##zo', '?']\n",
      "INFO:__main__:Number of tokens: 67\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['loud', '!', 'does', 'anyone', 'else', 'ever', 'feel', 'like', 'the', 'world', 'is', 'yelling', 'at', 'you', '?', 'you', \"'\", 're', 'sitting', 'in', ',', 'what', 'we', 'would', 'call', 'silence', ',', 'and', 'then', 'it', 'feels', 'like', 'you', \"'\", 're', 'being', 'swallowed', 'up', 'by', 'static', 'and', 'just', 'loud', '##ness', '.', 'like', 'you', 'stuck', 'your', 'finger', 'in', 'a', 'light', 'socket', '?', 'or', '.', '.', '.', 'am', 'i', 'just', 'a', 'sc', '##hi', '##zo', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'am', 'not', 'sure', 'what', 'to', 'do']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'am', 'not', 'sure', 'what', 'to', 'do']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'am', 'new']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'am', 'new']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['2', ',', '600', 'strong', ',', 'yo', '!', '-', '-', 'i', 'think', 'it', 'is', 'time', 'to', 'have', 'a', 'community', 'discussion', '.']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['2', ',', '600', 'strong', ',', 'yo', '!', '-', '-', 'i', 'think', 'it', 'is', 'time', 'to', 'have', 'a', 'community', 'discussion', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'got', 'diagnosed', '.', 'do', 'i', 'have', 'a', 'right', 'to', 'choose', 'my', 'prescription', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'got', 'diagnosed', '.', 'do', 'i', 'have', 'a', 'right', 'to', 'choose', 'my', 'prescription', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['\"', 'autism', 'spectrum', 'disorder', '(', 'as', '##d', ')', 'and', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', '(', 'ad', '##hd', ')', 'share', 'about', '50', '-', '72', '%', 'of', 'their', 'genetic', 'factors', '\"']\n",
      "INFO:__main__:Number of tokens: 32\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['\"', 'autism', 'spectrum', 'disorder', '(', 'as', '##d', ')', 'and', 'attention', '-', 'deficit', '/', 'hyper', '##act', '##ivity', 'disorder', '(', 'ad', '##hd', ')', 'share', 'about', '50', '-', '72', '%', 'of', 'their', 'genetic', 'factors', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'have', 'trouble', 'with', 'group', 'conversations', '?', 'if', 'so', ',', 'what', 'are', 'your', 'strategies', 'to', 'overcome', 'it', '?']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'have', 'trouble', 'with', 'group', 'conversations', '?', 'if', 'so', ',', 'what', 'are', 'your', 'strategies', 'to', 'overcome', 'it', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'know', 'it', \"'\", 's', 'not', 'my', 'fault', ',', 'but', 'i', 'feel', 'like', 'i', 'have', 'let', 'down', 'a', 'lot', 'of', 'people', 'because', 'of', 'my', 'und', '##ia', '##gno', '##sed', 'add', '.', 'r', '/', 'ad', '##hd', ',', 'what', 'are', 'your', 'stories', 'of', 'not', 'meeting', 'expectations', '?']\n",
      "INFO:__main__:Number of tokens: 44\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'know', 'it', \"'\", 's', 'not', 'my', 'fault', ',', 'but', 'i', 'feel', 'like', 'i', 'have', 'let', 'down', 'a', 'lot', 'of', 'people', 'because', 'of', 'my', 'und', '##ia', '##gno', '##sed', 'add', '.', 'r', '/', 'ad', '##hd', ',', 'what', 'are', 'your', 'stories', 'of', 'not', 'meeting', 'expectations', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['fellow', 'add', '##ers', 'in', 'college', '/', 'university', '.', 'this', 'might', 'strike', 'a', 'cord', 'with', 'you', '.', 'made', 'by', 'some', 'friends', 'of', 'mine', '.', 'very', 'good', 'stuff', '.']\n",
      "INFO:__main__:Number of tokens: 27\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['fellow', 'add', '##ers', 'in', 'college', '/', 'university', '.', 'this', 'might', 'strike', 'a', 'cord', 'with', 'you', '.', 'made', 'by', 'some', 'friends', 'of', 'mine', '.', 'very', 'good', 'stuff', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['keep', 'your', 'phone', 'and', 'credit', 'cards', 'together', '-', 'for', 'cheap', '!', 'i', 'use', 'it', 'for', 'my', 'student', 'id', ',', 'driver', \"'\", 's', 'license', ',', 'and', 'credit', 'card', '.']\n",
      "INFO:__main__:Number of tokens: 28\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['keep', 'your', 'phone', 'and', 'credit', 'cards', 'together', '-', 'for', 'cheap', '!', 'i', 'use', 'it', 'for', 'my', 'student', 'id', ',', 'driver', \"'\", 's', 'license', ',', 'and', 'credit', 'card', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'think', 'this', 'place', 'might', 'be', 'relevant', 'to', 'some', 'of', 'us', ',', 'especially', 'with', 'imp', '##ulsive', '##ness', 'issues']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'think', 'this', 'place', 'might', 'be', 'relevant', 'to', 'some', 'of', 'us', ',', 'especially', 'with', 'imp', '##ulsive', '##ness', 'issues']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'was', 'recently', 'told', 'by', 'a', 'doctor', 'i', 'have', 'add', ',', 'but', '.', '.', 'little', 'background', ':', 'i', 'am', 'currently', 'a', 'sophomore', 'in', 'college', 'and', 'have', 'two', 'days', 'till', 'my', 'finals', 'begin', '.', 'i', 'am', 'studying', 'aerospace', 'engineering', '.', 'last', 'year', ',', 'i', 'had', 'been', 'extremely', 'down', '.', 'i', 'had', 'gone', 'to', 'college', 'and', 'move', 'into', 'the', 'dorm', '.', 'i', 'had', 'been', 'feeling', 'really', 'depressed', 'and', 'not', 'understanding', 'school', '.', 'i', 'have', 'always', 'had', 'these', 'phases', 'of', 'where', 'i', 'would', 'be', 'depressed', 'for', 'weeks', 'at', 'a', 'time', ',', 'then', 'feel', 'happy', 'for', 'a', 'few', 'days', '.', 'i', 'had', 'no', 'idea', 'what', 'it', 'was', '.', 'i', 'began', 'to', 'think', 'more', 'and', 'more', 'about', 'it', 'and', 'noticed', 'that', 'this', 'had', 'been', 'going', 'on', 'for', 'several', 'years', '.', 'i', 'went', 'to', 'the', 'doctor', 'and', 'they', 'said', 'i', 'had', 'depression', ',', 'so', 'i', 'began', 'to', 'take', 'medication', 'for', 'that', '.', 'for', 'a', 'few', 'months', 'i', 'felt', 'okay', ',', 'but', 'i', 'felt', 'as', 'if', 'the', 'pills', 'weren', '##t', 'really', 'doing', 'anything', 'and', 'any', 'cure', 'is', 'just', 'my', 'head', 'saying', 'the', 'pills', 'were', 'doing', 'something', '.', 'i', 'began', 'to', 'stop', 'taking', 'them', '.', 'school', 'had', 'began', 'pi', '##ling', 'up', 'more', 'and', 'more', 'and', 'i', 'felt', 'as', 'if', 'i', 'wasn', '##t', 'going', 'to', 'make', 'it', 'thru', 'the', 'semester', '(', 'this', 'was', 'last', 'semester', ')', '.', 'one', 'night', ',', 'i', 'was', 'just', 'overwhelmed', 'by', 'everything', '.', 'i', 'had', 'lost', 'my', 'girlfriend', 'at', 'the', 'time', ',', 'my', 'best', 'friend', 'had', 'found', 'someone', 'and', 'was', 'with', 'her', ',', 'and', 'my', 'grades', 'were', 'dropping', '.', 'that', 'night', ',', 'i', 'felt', 'extremely', 'suicidal', '.', 'i', 'fought', 'thru', 'the', 'night', 'and', 'went', 'back', 'to', 'my', 'doctor', 'the', 'next', 'day', '.', 'i', 'told', 'her', 'the', 'depression', 'was', 'getting', 'worse', 'and', 'i', 'wasn', '##t', 'focusing', 'at', 'all', 'in', 'school', '.', 'i', 'am', 'an', 'engineering', 'major', 'and', 'wasn', \"'\", 't', 'able', 'to', 'focus', 'reading', 'questions', 'on', 'a', 'economics', 'test', '.', 'i', 'would', 'read', 'the', 'question', 'several', 'times', 'and', 'still', 'have', 'no', 'idea', 'what', 'it', 'was', 'saying', '.', 'i', 'told', 'her', 'that', 'and', 'she', 'told', 'me', 'that', 'my', 'depression', 'could', 'just', 'be', 'because', 'i', 'wasn', \"'\", 't', 'able', 'to', 'focus', ',', 'so', 'she', 'told', 'me', 'that', 'i', 'could', 'have', 'add', '.', 'she', 'prescribed', 'me', 'add', '##eral', '##l', '(', '5', 'mg', 'a', 'day', ')', '.', 'things', 'got', 'better', ',', 'and', 'i', 'felt', 'as', 'if', 'i', 'was', 'focused', 'more', 'in', 'school', '.', 'several', 'months', 'have', 'passed', 'and', 'i', 'am', 'now', 'prescribed', '20', '##mg', 'in', 'the', 'morning', 'and', '5', '##mg', 'in', 'the', 'afternoon', '.', 'i', 'feel', 'like', 'sometimes', 'i', 'focus', 'extremely', 'well', ',', 'but', 'i', 'feel', 'like', 'i', 'am', 'still', 'having', 'trouble', '.', 'in', 'conversations', ',', 'i', 'can', 'try', 'as', 'hard', 'as', 'i', 'can', 'to', 'what', 'they', 'are', 'saying', ',', 'and', 'i', 'cannot', 'remember', 'anything', 'they', 'said', 'as', 'soon', 'as', 'they', 'finish', '.', 'i', 'focus', 'better', 'on', 'school', ',', 'but', 'when', 'it', 'comes', 'to', 'tests', ',', 'i', 'freak', 'out', 'and', 'forget', 'everything', '.', 'my', 'diet', 'is', 'not', 'the', 'best', '.', 'im', 'drinking', 'about', '80', '-', '100', 'oz', 'of', 'soda', 'a', 'day', '.', 'maybe', 'about', 'a', 'bottle', 'of', 'water', 'or', 'two', 'a', 'day', '.', 'i', 'eat', 'fast', 'food', 'all', 'day', '(', 'always', 'in', 'a', 'hurry', 'and', 'don', \"'\", 't', 'have', 'much', 'time', 'to', 'make', 'a', 'healthy', 'meal', ')', '.', 'during', 'the', 'week', 'my', 'sleep', 'is', 'very', 'light', '(', '5', '-', '6', 'hours', 'week', '##night', '##s', ',', 'about', '10', '-', '12', 'hours', 'on', 'weekends', ')', '.', 'my', 'question', 'is', ':', 'what', 'is', 'going', 'on', '?', 'i', 'feel', 'like', 'i', 'am', 'taking', 'more', 'and', 'more', 'add', '##eral', '##l', 'and', 'im', 'not', 'getting', 'much', 'in', 'return', '.', 'my', 'finals', 'are', 'coming', 'up', 'and', 'im', 'beginning', 'to', 'panic', 'because', 'i', 'will', 'not', 'be', 'able', 'to', 'remember', 'anything', 'i', 'studied', 'when', 'test', 'time', 'comes', '.', 'should', 'i', 'go', 'back', 'to', 'the', 'doctor', 'and', 'ask', 'to', 'be', 'prescribed', 'more', 'or', 'should', 'i', 'ask', 'for', 'a', 'different', 'approach', '?', 'can', 'my', 'diet', 'also', 'affect', 'add', '?', '*', '*', 'summary', ':', 'told', 'has', 'add', ',', 'taking', '20', '##mg', 'add', '##eral', '##l', 'and', 'still', 'having', 'trouble', 'on', 'tests', 'and', 'focusing', 'when', 'people', 'talk', 'to', 'me', 'in', 'conversation', '.', 'take', 'more', 'add', '##eral', '##l', 'or', 'try', 'something', 'else', '?', '*', '*']\n",
      "INFO:__main__:Number of tokens: 690\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['i', 'was', 'recently', 'told', 'by', 'a', 'doctor', 'i', 'have', 'add', ',', 'but', '.', '.', 'little', 'background', ':', 'i', 'am', 'currently', 'a', 'sophomore', 'in', 'college', 'and', 'have', 'two', 'days', 'till', 'my', 'finals', 'begin', '.', 'i', 'am', 'studying', 'aerospace', 'engineering', '.', 'last', 'year', ',', 'i', 'had', 'been', 'extremely', 'down', '.', 'i', 'had', 'gone', 'to', 'college', 'and', 'move', 'into', 'the', 'dorm', '.', 'i', 'had', 'been', 'feeling', 'really', 'depressed', 'and', 'not', 'understanding', 'school', '.', 'i', 'have', 'always', 'had', 'these', 'phases', 'of', 'where', 'i', 'would', 'be', 'depressed', 'for', 'weeks', 'at', 'a', 'time', ',', 'then', 'feel', 'happy', 'for', 'a', 'few', 'days', '.', 'i', 'had', 'no', 'idea', 'what', 'it', 'was', '.', 'i', 'began', 'to', 'think', 'more', 'and', 'more', 'about', 'it', 'and', 'noticed', 'that', 'this', 'had', 'been', 'going', 'on', 'for', 'several', 'years', '.', 'i', 'went', 'to', 'the', 'doctor', 'and', 'they', 'said', 'i', 'had', 'depression', ',', 'so', 'i', 'began', 'to', 'take', 'medication', 'for', 'that', '.', 'for', 'a', 'few', 'months', 'i', 'felt', 'okay', ',', 'but', 'i', 'felt', 'as', 'if', 'the', 'pills', 'weren', '##t', 'really', 'doing', 'anything', 'and', 'any', 'cure', 'is', 'just', 'my', 'head', 'saying', 'the', 'pills', 'were', 'doing', 'something', '.', 'i', 'began', 'to', 'stop', 'taking', 'them', '.', 'school', 'had', 'began', 'pi', '##ling', 'up', 'more', 'and', 'more', 'and', 'i', 'felt', 'as', 'if', 'i', 'wasn', '##t', 'going', 'to', 'make', 'it', 'thru', 'the', 'semester', '(', 'this', 'was', 'last', 'semester', ')', '.', 'one', 'night', ',', 'i', 'was', 'just', 'overwhelmed', 'by', 'everything', '.', 'i', 'had', 'lost', 'my', 'girlfriend', 'at', 'the', 'time', ',', 'my', 'best', 'friend', 'had', 'found', 'someone', 'and', 'was', 'with', 'her', ',', 'and', 'my', 'grades', 'were', 'dropping', '.', 'that', 'night', ',', 'i', 'felt', 'extremely', 'suicidal', '.', 'i', 'fought', 'thru', 'the', 'night', 'and', 'went', 'back', 'to', 'my', 'doctor', 'the', 'next', 'day', '.', 'i', 'told', 'her', 'the', 'depression', 'was', 'getting', 'worse', 'and', 'i', 'wasn', '##t', 'focusing', 'at', 'all', 'in', 'school', '.', 'i', 'am', 'an', 'engineering', 'major', 'and', 'wasn', \"'\", 't', 'able', 'to', 'focus', 'reading', 'questions', 'on', 'a', 'economics', 'test', '.', 'i', 'would', 'read', 'the', 'question', 'several', 'times', 'and', 'still', 'have', 'no', 'idea', 'what', 'it', 'was', 'saying', '.', 'i', 'told', 'her', 'that', 'and', 'she', 'told', 'me', 'that', 'my', 'depression', 'could', 'just', 'be', 'because', 'i', 'wasn', \"'\", 't', 'able', 'to', 'focus', ',', 'so', 'she', 'told', 'me', 'that', 'i', 'could', 'have', 'add', '.', 'she', 'prescribed', 'me', 'add', '##eral', '##l', '(', '5', 'mg', 'a', 'day', ')', '.', 'things', 'got', 'better', ',', 'and', 'i', 'felt', 'as', 'if', 'i', 'was', 'focused', 'more', 'in', 'school', '.', 'several', 'months', 'have', 'passed', 'and', 'i', 'am', 'now', 'prescribed', '20', '##mg', 'in', 'the', 'morning', 'and', '5', '##mg', 'in', 'the', 'afternoon', '.', 'i', 'feel', 'like', 'sometimes', 'i', 'focus', 'extremely', 'well', ',', 'but', 'i', 'feel', 'like', 'i', 'am', 'still', 'having', 'trouble', '.', 'in', 'conversations', ',', 'i', 'can', 'try', 'as', 'hard', 'as', 'i', 'can', 'to', 'what', 'they', 'are', 'saying', ',', 'and', 'i', 'cannot', 'remember', 'anything', 'they', 'said', 'as', 'soon', 'as', 'they', 'finish', '.', 'i', 'focus', 'better', 'on', 'school', ',', 'but', 'when', 'it', 'comes', 'to', 'tests', ',', 'i', 'freak', 'out', 'and', 'forget', 'everything', '.', 'my', 'diet', 'is', 'not', 'the', 'best', '.', 'im', 'drinking', 'about', '80', '-', '100', 'oz', 'of', 'soda', 'a', 'day', '.', 'maybe', 'about', 'a', 'bottle', 'of', 'water', 'or', 'two', 'a', 'day'], ['.', 'i', 'eat', 'fast', 'food', 'all', 'day', '(', 'always', 'in', 'a', 'hurry', 'and', 'don', \"'\", 't', 'have', 'much', 'time', 'to', 'make', 'a', 'healthy', 'meal', ')', '.', 'during', 'the', 'week', 'my', 'sleep', 'is', 'very', 'light', '(', '5', '-', '6', 'hours', 'week', '##night', '##s', ',', 'about', '10', '-', '12', 'hours', 'on', 'weekends', ')', '.', 'my', 'question', 'is', ':', 'what', 'is', 'going', 'on', '?', 'i', 'feel', 'like', 'i', 'am', 'taking', 'more', 'and', 'more', 'add', '##eral', '##l', 'and', 'im', 'not', 'getting', 'much', 'in', 'return', '.', 'my', 'finals', 'are', 'coming', 'up', 'and', 'im', 'beginning', 'to', 'panic', 'because', 'i', 'will', 'not', 'be', 'able', 'to', 'remember', 'anything', 'i', 'studied', 'when', 'test', 'time', 'comes', '.', 'should', 'i', 'go', 'back', 'to', 'the', 'doctor', 'and', 'ask', 'to', 'be', 'prescribed', 'more', 'or', 'should', 'i', 'ask', 'for', 'a', 'different', 'approach', '?', 'can', 'my', 'diet', 'also', 'affect', 'add', '?', '*', '*', 'summary', ':', 'told', 'has', 'add', ',', 'taking', '20', '##mg', 'add', '##eral', '##l', 'and', 'still', 'having', 'trouble', 'on', 'tests', 'and', 'focusing', 'when', 'people', 'talk', 'to', 'me', 'in', 'conversation', '.', 'take', 'more', 'add', '##eral', '##l', 'or', 'try', 'something', 'else', '?', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'hate', 'when', 'th', '##i', '.', '.', '.', 'hey', 'a', 'cater', '##pi', '##llar', '!']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'hate', 'when', 'th', '##i', '.', '.', '.', 'hey', 'a', 'cater', '##pi', '##llar', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'know', 'of', 'a', 'good', 'app', 'that', 'can', 'help', 'organize', 'or', 'track', 'tasks', 'or', 'projects', '?']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'know', 'of', 'a', 'good', 'app', 'that', 'can', 'help', 'organize', 'or', 'track', 'tasks', 'or', 'projects', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['not', 'getting', 'things', 'done', ',', 'job', 'at', 'risk', 'just', 'need', 'to', 'write', 'some', 'things', 'down', ';', ')', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'as', '##d', 'about', '15', 'years', 'ago', ',', 'but', 'i', 'do', 'not', 'have', 'an', 'official', 'diagnosis', 'of', 'add', '.', 'my', 'psychologist', 'told', 'me', 'it', \"'\", 's', 'likely', 'i', 'have', 'add', 'and', 'i', \"'\", 've', 'read', 'that', 'as', '##d', 'with', 'como', '##rb', '##idi', '##ty', 'ad', '##hd', 'is', 'quite', 'common', '.', 'i', \"'\", 've', 'been', 'kicked', 'out', 'of', 'highs', '##cho', '##ol', '7', 'years', 'ago', ',', 'because', 'i', 'refused', 'to', 'do', 'the', 'things', 'school', 'asked', 'of', 'me', 'and', 'i', 'only', 'did', 'the', 'things', 'i', 'liked', '.', '(', 'eternal', 'regrets', 'about', 'this', 'now', ')', 'after', 'this', 'i', 'completed', 'some', 'it', 'certificates', 'and', 'got', 'to', 'work', 'for', 'my', 'current', 'employer', '.', 'i', \"'\", 've', 'been', 'using', 'methyl', '##ph', '##eni', '##date', '/', 'concert', '##a', 'for', '2', 'years', 'now', '(', '27', '##mg', ')', 'and', 'it', 'helped', 'me', 'a', 'lot', 'especially', 'at', 'work', '.', 'instead', 'of', 'crashing', 'after', 'a', 'few', 'hours', 'in', 'the', 'morning', ',', 'i', 'am', 'now', 'able', 'to', 'work', 'th', '##rou', '##go', '##ut', 'the', 'whole', 'day', '.', 'also', 'focus', 'improved', 'a', 'lot', 'and', 'it', 'seems', 'to', 'shield', 'me', 'from', 'stimuli', 'in', 'my', 'environment', '.', 'i', 'didn', \"'\", 't', 'try', 'any', 'higher', 'dose', 'because', 'i', 'was', 'afraid', 'of', 'getting', 'dependent', '.', 'however', 'there', 'are', 'still', 'some', 'serious', 'problems', 'that', 'are', 'putting', 'my', 'job', 'at', 'risk', '.', 'i', \"'\", 've', 'have', 'more', 'responsibilities', 'now', 'and', 'as', 'a', 'result', 'a', 'lot', 'more', 'work', 'that', 'has', 'to', 'be', 'processed', '.', 'biggest', 'problems', 'at', 'work', ':', '*', 'when', 'i', 'am', 'not', 'motivated', 'to', 'do', 'a', 'certain', 'task', 'it', \"'\", 's', 'real', '##y', 'hard', 'to', 'yield', 'the', 'will', '##power', 'to', 'get', 'started', '.', 'often', 'i', 'end', 'up', '\"', 'para', '##ly', '##sed', '\"', 'doing', 'nothing', 'or', 'brows', '##ing', 'the', 'internet', '*', 'unable', 'to', 'resume', 'work', 'i', 'was', 'unable', 'to', 'finish', 'in', 'one', 'go', '*', 'avoiding', '\"', 'routine', '\"', 'tasks', '*', 'forget', 'to', 'fill', 'my', 'times', '##hee', '##ts', '*', 'not', 'sticking', 'to', 'my', 'schedule', ',', 'especially', 'when', 'out', 'of', 'sync', ':', 'which', 'is', 'often', 'my', 'own', 'fault', 'most', 'of', 'the', 'times', 'because', 'something', 'else', 'caught', 'my', 'attention', 'and', 'i', 'forgot', 'time', '.', '*', 'pro', '##cr', '##ast', '##ination', '*', 'not', 'getting', 'at', 'work', 'on', 'time', '*', 'managing', 'time', 'i', 'work', 'for', 'an', 'it', 'service', 'provider', 'and', 'most', 'of', 'the', 'time', 'i', 'manage', 'our', 'own', 'it', 'infrastructure', 'and', 'do', 'r', '&', 'd', '.', 'sometimes', 'i', 'have', 'external', 'customers', 'cases', 'if', 'they', 'require', 'very', 'specific', 'or', 'extensive', 'troubles', '##hoot', '##ing', 'in', 'my', 'area', \"'\", 's', 'of', 'expertise', '.', 'my', 'employer', 'knows', 'about', 'my', 'as', '##d', ',', 'that', \"'\", 's', 'one', 'of', 'the', 'reasons', 'i', 'primary', 'work', 'for', 'our', 'internal', 'organisation', '(', 'not', 'good', 'with', 'people', ')', 'and', 'still', 'have', 'my', 'job', '.', 'i', 'consider', 'myself', 'lucky', 'i', 'have', 'this', 'job', 'and', 'don', \"'\", 't', 'want', 'to', 'lose', 'it', '.', 'a', 'nice', 'girl', 'from', 'our', 'finance', 'department', 'helps', 'me', 'with', 'my', 'schedule', 'and', 'serves', 'as', 'a', 'proxy', 'for', 'my', 'tasks', '.', 'i', 'am', 'thankful', '##l', 'for', 'this', 'and', 'people', 'disturb', 'me', 'less', 'often', 'now', '.', 'however', 'the', 'problems', 'described', 'above', 'stil', 'persist', '##s', '.', 'i', 'weekly', 'have', 'a', 'meeting', 'with', 'her', 'and', 'my', 'boss', 'and', 'she', 'is', 'caught', 'in', 'the', 'cross', '##fire', 'for', 'the', 'fuck', '##ups', 'i', \"'\", 've', 'made', ',', 'when', 'my', 'boss', 'gets', 'irritated', '/', 'angry', '.', 'the', 'irritation', 'is', 'directed', 'at', 'me', ',', 'but', 'she', 'sometimes', 'gets', 'emotional', '.', 'i', 'also', 'feel', 'guilty', 'about', 'that', 'since', 'my', 'actions', 'are', 'the', 'cause', '.', 'everyone', 'is', 'telling', 'me', 'how', 'lazy', 'i', 'am', 'for', 'years', 'now', '.', 'i', 'believed', 'for', 'a', 'long', 'time', 'that', 'this', 'was', 'actually', 'the', 'case', ',', 'but', 'after', 'doing', 'some', 'more', 'research', 'on', 'ad', '##hd', 'it', 'doesn', \"'\", 't', 'seem', 'to', 'be', 'will', '##power', '.', 'how', 'do', 'you', 'distinguish', 'lazy', '##ness', 'and', 'ad', '##hd', 'impaired', 'motivation', '?', 'it', \"'\", 's', 'very', 'frustrating', ',', 'there', 'are', 'lots', 'of', 'things', 'i', 'want', 'to', 'do', ',', 'but', 'nothing', 'gets', 'finished', '.', 'some', 'things', 'i', \"'\", 've', 'stopped', 'doing', 'at', 'all', 'to', 'avoid', 'the', 'fr', '##ust', '##ation', 'of', 'skipping', '/', 'not', 'finishing', '.', '(', 'for', 'instance', 'i', 'no', 'longer', 'cook', 'food', 'anymore', '.', ')', 'this', 'will', 'not', 'work', 'much', 'longer', '.', 'i', 'no', 'longer', 'care', 'about', 'adi', '##cit', '##ion', 'risks', ',', 'if', 'a', 'higher', 'dose', 'can', 'reduce', 'the', 'problems', 'described', '.', 'what', 'can', 'be', 'realistic', 'expectations', 'of', 'increasing', 'medication', '?', 't', '##l', ';', 'dr', ':', 'not', 'finishing', 'things', 'sucks', ',', 'ad', '##hd', 'seems', 'no', 'will', '##power', 'issue', ',', 'what', 'to', 'expect', 'of', 'increasing', 'st', '##im', '##ula', '##nt', 'dos', '##age', '?']\n",
      "INFO:__main__:Number of tokens: 754\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['not', 'getting', 'things', 'done', ',', 'job', 'at', 'risk', 'just', 'need', 'to', 'write', 'some', 'things', 'down', ';', ')', 'i', \"'\", 've', 'been', 'diagnosed', 'with', 'as', '##d', 'about', '15', 'years', 'ago', ',', 'but', 'i', 'do', 'not', 'have', 'an', 'official', 'diagnosis', 'of', 'add', '.', 'my', 'psychologist', 'told', 'me', 'it', \"'\", 's', 'likely', 'i', 'have', 'add', 'and', 'i', \"'\", 've', 'read', 'that', 'as', '##d', 'with', 'como', '##rb', '##idi', '##ty', 'ad', '##hd', 'is', 'quite', 'common', '.', 'i', \"'\", 've', 'been', 'kicked', 'out', 'of', 'highs', '##cho', '##ol', '7', 'years', 'ago', ',', 'because', 'i', 'refused', 'to', 'do', 'the', 'things', 'school', 'asked', 'of', 'me', 'and', 'i', 'only', 'did', 'the', 'things', 'i', 'liked', '.', '(', 'eternal', 'regrets', 'about', 'this', 'now', ')', 'after', 'this', 'i', 'completed', 'some', 'it', 'certificates', 'and', 'got', 'to', 'work', 'for', 'my', 'current', 'employer', '.', 'i', \"'\", 've', 'been', 'using', 'methyl', '##ph', '##eni', '##date', '/', 'concert', '##a', 'for', '2', 'years', 'now', '(', '27', '##mg', ')', 'and', 'it', 'helped', 'me', 'a', 'lot', 'especially', 'at', 'work', '.', 'instead', 'of', 'crashing', 'after', 'a', 'few', 'hours', 'in', 'the', 'morning', ',', 'i', 'am', 'now', 'able', 'to', 'work', 'th', '##rou', '##go', '##ut', 'the', 'whole', 'day', '.', 'also', 'focus', 'improved', 'a', 'lot', 'and', 'it', 'seems', 'to', 'shield', 'me', 'from', 'stimuli', 'in', 'my', 'environment', '.', 'i', 'didn', \"'\", 't', 'try', 'any', 'higher', 'dose', 'because', 'i', 'was', 'afraid', 'of', 'getting', 'dependent', '.', 'however', 'there', 'are', 'still', 'some', 'serious', 'problems', 'that', 'are', 'putting', 'my', 'job', 'at', 'risk', '.', 'i', \"'\", 've', 'have', 'more', 'responsibilities', 'now', 'and', 'as', 'a', 'result', 'a', 'lot', 'more', 'work', 'that', 'has', 'to', 'be', 'processed', '.', 'biggest', 'problems', 'at', 'work', ':', '*', 'when', 'i', 'am', 'not', 'motivated', 'to', 'do', 'a', 'certain', 'task', 'it', \"'\", 's', 'real', '##y', 'hard', 'to', 'yield', 'the', 'will', '##power', 'to', 'get', 'started', '.', 'often', 'i', 'end', 'up', '\"', 'para', '##ly', '##sed', '\"', 'doing', 'nothing', 'or', 'brows', '##ing', 'the', 'internet', '*', 'unable', 'to', 'resume', 'work', 'i', 'was', 'unable', 'to', 'finish', 'in', 'one', 'go', '*', 'avoiding', '\"', 'routine', '\"', 'tasks', '*', 'forget', 'to', 'fill', 'my', 'times', '##hee', '##ts', '*', 'not', 'sticking', 'to', 'my', 'schedule', ',', 'especially', 'when', 'out', 'of', 'sync', ':', 'which', 'is', 'often', 'my', 'own', 'fault', 'most', 'of', 'the', 'times', 'because', 'something', 'else', 'caught', 'my', 'attention', 'and', 'i', 'forgot', 'time', '.', '*', 'pro', '##cr', '##ast', '##ination', '*', 'not', 'getting', 'at', 'work', 'on', 'time', '*', 'managing', 'time', 'i', 'work', 'for', 'an', 'it', 'service', 'provider', 'and', 'most', 'of', 'the', 'time', 'i', 'manage', 'our', 'own', 'it', 'infrastructure', 'and', 'do', 'r', '&', 'd', '.', 'sometimes', 'i', 'have', 'external', 'customers', 'cases', 'if', 'they', 'require', 'very', 'specific', 'or', 'extensive', 'troubles', '##hoot', '##ing', 'in', 'my', 'area', \"'\", 's', 'of', 'expertise', '.', 'my', 'employer', 'knows', 'about', 'my', 'as', '##d', ',', 'that', \"'\", 's', 'one', 'of', 'the', 'reasons', 'i', 'primary', 'work', 'for', 'our', 'internal', 'organisation', '(', 'not', 'good', 'with', 'people', ')', 'and', 'still', 'have', 'my', 'job', '.', 'i', 'consider', 'myself', 'lucky', 'i', 'have', 'this', 'job', 'and', 'don', \"'\", 't', 'want', 'to', 'lose', 'it', '.', 'a', 'nice', 'girl', 'from', 'our', 'finance', 'department', 'helps', 'me', 'with', 'my', 'schedule', 'and', 'serves', 'as', 'a', 'proxy', 'for', 'my', 'tasks', '.', 'i', 'am', 'thankful', '##l', 'for', 'this', 'and', 'people', 'disturb', 'me', 'less', 'often', 'now', '.', 'however', 'the', 'problems'], ['described', 'above', 'stil', 'persist', '##s', '.', 'i', 'weekly', 'have', 'a', 'meeting', 'with', 'her', 'and', 'my', 'boss', 'and', 'she', 'is', 'caught', 'in', 'the', 'cross', '##fire', 'for', 'the', 'fuck', '##ups', 'i', \"'\", 've', 'made', ',', 'when', 'my', 'boss', 'gets', 'irritated', '/', 'angry', '.', 'the', 'irritation', 'is', 'directed', 'at', 'me', ',', 'but', 'she', 'sometimes', 'gets', 'emotional', '.', 'i', 'also', 'feel', 'guilty', 'about', 'that', 'since', 'my', 'actions', 'are', 'the', 'cause', '.', 'everyone', 'is', 'telling', 'me', 'how', 'lazy', 'i', 'am', 'for', 'years', 'now', '.', 'i', 'believed', 'for', 'a', 'long', 'time', 'that', 'this', 'was', 'actually', 'the', 'case', ',', 'but', 'after', 'doing', 'some', 'more', 'research', 'on', 'ad', '##hd', 'it', 'doesn', \"'\", 't', 'seem', 'to', 'be', 'will', '##power', '.', 'how', 'do', 'you', 'distinguish', 'lazy', '##ness', 'and', 'ad', '##hd', 'impaired', 'motivation', '?', 'it', \"'\", 's', 'very', 'frustrating', ',', 'there', 'are', 'lots', 'of', 'things', 'i', 'want', 'to', 'do', ',', 'but', 'nothing', 'gets', 'finished', '.', 'some', 'things', 'i', \"'\", 've', 'stopped', 'doing', 'at', 'all', 'to', 'avoid', 'the', 'fr', '##ust', '##ation', 'of', 'skipping', '/', 'not', 'finishing', '.', '(', 'for', 'instance', 'i', 'no', 'longer', 'cook', 'food', 'anymore', '.', ')', 'this', 'will', 'not', 'work', 'much', 'longer', '.', 'i', 'no', 'longer', 'care', 'about', 'adi', '##cit', '##ion', 'risks', ',', 'if', 'a', 'higher', 'dose', 'can', 'reduce', 'the', 'problems', 'described', '.', 'what', 'can', 'be', 'realistic', 'expectations', 'of', 'increasing', 'medication', '?', 't', '##l', ';', 'dr', ':', 'not', 'finishing', 'things', 'sucks', ',', 'ad', '##hd', 'seems', 'no', 'will', '##power', 'issue', ',', 'what', 'to', 'expect', 'of', 'increasing', 'st', '##im', '##ula', '##nt', 'dos', '##age', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'are', 'the', 'dangers', 'of', 'mixing', 'ad', '##hd', 'medication', 'and', 'alcohol', '?', 'i', 'am', 'going', 'to', 'be', 'going', 'to', 'prom', 'soon', ',', 'and', 'as', 'such', ',', 'the', 'after', '-', 'prom', 'party', 'where', 'i', \"'\", 'm', 'going', 'to', 'be', 'expected', 'to', 'drink', '.', 'i', \"'\", 'm', 'thinking', 'of', 'taking', 'my', 'medication', 'for', 'prom', 'because', 'it', 'helps', 'me', 'keep', 'me', 'under', 'control', 'of', 'my', 'emotions', 'so', 'i', 'won', \"'\", 't', 'be', 'a', 'crazy', 'person', 'during', 'the', 'dance', '.', 'but', 'what', 'would', 'happen', 'if', 'i', 'drank', 'while', 'i', 'was', 'on', 'my', 'medication', '?', 'would', 'it', 'screw', 'me', 'up', 'medical', '##ly', '?', 'anybody', 'have', 'an', 'idea', '?', 'info', ':', '5', '\"', '8', \"'\", ',', '150', '##lb', '##s', ',', 'v', '##y', '##vance', '60', '##mg']\n",
      "INFO:__main__:Number of tokens: 119\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'are', 'the', 'dangers', 'of', 'mixing', 'ad', '##hd', 'medication', 'and', 'alcohol', '?', 'i', 'am', 'going', 'to', 'be', 'going', 'to', 'prom', 'soon', ',', 'and', 'as', 'such', ',', 'the', 'after', '-', 'prom', 'party', 'where', 'i', \"'\", 'm', 'going', 'to', 'be', 'expected', 'to', 'drink', '.', 'i', \"'\", 'm', 'thinking', 'of', 'taking', 'my', 'medication', 'for', 'prom', 'because', 'it', 'helps', 'me', 'keep', 'me', 'under', 'control', 'of', 'my', 'emotions', 'so', 'i', 'won', \"'\", 't', 'be', 'a', 'crazy', 'person', 'during', 'the', 'dance', '.', 'but', 'what', 'would', 'happen', 'if', 'i', 'drank', 'while', 'i', 'was', 'on', 'my', 'medication', '?', 'would', 'it', 'screw', 'me', 'up', 'medical', '##ly', '?', 'anybody', 'have', 'an', 'idea', '?', 'info', ':', '5', '\"', '8', \"'\", ',', '150', '##lb', '##s', ',', 'v', '##y', '##vance', '60', '##mg']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['now', 'that', 'i', \"'\", 'm', 'on', 'v', '##y', '##van', '##se', ',', 'i', 'keep', 'forgetting', 'to', 'eat', '.', 'what', 'should', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['now', 'that', 'i', \"'\", 'm', 'on', 'v', '##y', '##van', '##se', ',', 'i', 'keep', 'forgetting', 'to', 'eat', '.', 'what', 'should', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'there', 'any', 'research', 'on', 'the', 'long', 'term', 'effects', 'of', 'ad', '##hd', 'medication', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'there', 'any', 'research', 'on', 'the', 'long', 'term', 'effects', 'of', 'ad', '##hd', 'medication', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['party', '##ing', '.', '.', '.', 'so', 'i', 'am', '30', 'y', '##rs', 'old', 'and', 'am', 'not', 'diagnosed', '(', 'school', 'psychologist', 'ref', '##fer', '##ed', 'my', 'to', 'a', 'psychiatrist', 'because', 'she', 'bel', '##ives', 'heavily', 'i', 'am', 'add', ')', '.', 'anyone', 'ever', 'was', 'a', 'hard', 'part', '##ier', '.', 'i', 'was', 'always', 'party', '##ing', '(', 'stopped', 'cu', '##ase', 'of', 'legal', 'problems', ')', ',', 'i', 'found', 'that', 'i', 'almost', 'felt', 'calm', 'at', 'clubs', 'and', 'what', 'not', '.', 'it', 'was', 'an', 'environment', 'where', 'my', 'com', '##pl', '##us', '##ive', '##ness', 'shine', '##d', 'and', 'that', 'my', 'lack', 'of', 'being', 'able', 'to', 'focus', 'in', 'on', 'things', 'allowed', 'me', 'to', 'bounce', 'around', 'and', 'talk', 'to', 'lots', 'of', 'people', '.', 'i', 'was', 'usually', 'the', 'center', 'of', 'whatever', 'was', 'going', 'on', ',', 'which', 'was', 'odd', 'cause', 'i', 'am', 'usually', 'not', 'very', 'social', '.', 'any', 'sim', '##mi', '##lia', '##r', 'experiences', '?']\n",
      "INFO:__main__:Number of tokens: 137\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['party', '##ing', '.', '.', '.', 'so', 'i', 'am', '30', 'y', '##rs', 'old', 'and', 'am', 'not', 'diagnosed', '(', 'school', 'psychologist', 'ref', '##fer', '##ed', 'my', 'to', 'a', 'psychiatrist', 'because', 'she', 'bel', '##ives', 'heavily', 'i', 'am', 'add', ')', '.', 'anyone', 'ever', 'was', 'a', 'hard', 'part', '##ier', '.', 'i', 'was', 'always', 'party', '##ing', '(', 'stopped', 'cu', '##ase', 'of', 'legal', 'problems', ')', ',', 'i', 'found', 'that', 'i', 'almost', 'felt', 'calm', 'at', 'clubs', 'and', 'what', 'not', '.', 'it', 'was', 'an', 'environment', 'where', 'my', 'com', '##pl', '##us', '##ive', '##ness', 'shine', '##d', 'and', 'that', 'my', 'lack', 'of', 'being', 'able', 'to', 'focus', 'in', 'on', 'things', 'allowed', 'me', 'to', 'bounce', 'around', 'and', 'talk', 'to', 'lots', 'of', 'people', '.', 'i', 'was', 'usually', 'the', 'center', 'of', 'whatever', 'was', 'going', 'on', ',', 'which', 'was', 'odd', 'cause', 'i', 'am', 'usually', 'not', 'very', 'social', '.', 'any', 'sim', '##mi', '##lia', '##r', 'experiences', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'explain', 'ad', '##hd', 'to', 'others', '?', 'i', 'know', 'we', 'all', 'have', 'trouble', 'expressing', 'our', 'thoughts', 'in', 'words', ',', 'but', 'does', 'anyone', 'have', 'a', 'template', 'of', 'how', 'they', 'explain', 'ad', '##hd', 'to', 'people', 'that', 'want', 'to', 'know', '?', 'i', 'don', \"'\", 't', 'mean', 'asshole', '##s', 'that', 'make', 'qui', '##ps', 'about', 'squirrels', 'or', 'butterflies', '.', 'i', 'mean', 'when', 'someone', 'asks', 'you', '\"', 'what', \"'\", 's', 'it', 'like', '?', 'what', 'do', 'you', 'think', 'about', '?', '\"', 'and', 'actually', 'wants', 'to', 'know', '.', '*', '*', 'my', 'best', 'explanation', 'is', 'this', ':', '*', '*', '\"', 'think', 'of', 'the', 'stock', 'market', 'tick', '##er', 'where', 'prices', 'are', 'constantly', 'flowing', 'by', 'and', 'often', 'changing', 'as', 'they', 'do', '.', 'when', 'i', 'want', 'to', 'focus', 'on', 'something', 'i', 'have', 'to', 'apply', 'effort', 'to', 'mentally', 'pull', 'it', 'out', 'of', 'there', 'and', 'ignore', 'everything', 'else', ',', 'including', 'physical', 'stimuli', '.', 'sometimes', 'i', 'fail', 'and', 'continue', 'to', 'just', 'watch', 'thoughts', 'go', 'by', '.', 'if', 'i', 'am', 'focusing', 'and', 'something', 'breaks', 'that', 'focus', ',', 'i', 'have', 'to', 'start', 'all', 'over', 'trying', 'to', 'get', 'back', 'to', 'that', 'level', 'of', 'focus', '.', '\"', 'edit', ':', 'thought', 'of', 'another', '-', 'a', 'radio', 'or', 'tv', 'on', 'scan', '.', '.', '.', 'each', 'station', 'lasts', 'a', 'couple', 'seconds', 'and', 'then', 'it', 'changes', 'without', 'me', 'trying', 'to', '.']\n",
      "INFO:__main__:Number of tokens: 210\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'explain', 'ad', '##hd', 'to', 'others', '?', 'i', 'know', 'we', 'all', 'have', 'trouble', 'expressing', 'our', 'thoughts', 'in', 'words', ',', 'but', 'does', 'anyone', 'have', 'a', 'template', 'of', 'how', 'they', 'explain', 'ad', '##hd', 'to', 'people', 'that', 'want', 'to', 'know', '?', 'i', 'don', \"'\", 't', 'mean', 'asshole', '##s', 'that', 'make', 'qui', '##ps', 'about', 'squirrels', 'or', 'butterflies', '.', 'i', 'mean', 'when', 'someone', 'asks', 'you', '\"', 'what', \"'\", 's', 'it', 'like', '?', 'what', 'do', 'you', 'think', 'about', '?', '\"', 'and', 'actually', 'wants', 'to', 'know', '.', '*', '*', 'my', 'best', 'explanation', 'is', 'this', ':', '*', '*', '\"', 'think', 'of', 'the', 'stock', 'market', 'tick', '##er', 'where', 'prices', 'are', 'constantly', 'flowing', 'by', 'and', 'often', 'changing', 'as', 'they', 'do', '.', 'when', 'i', 'want', 'to', 'focus', 'on', 'something', 'i', 'have', 'to', 'apply', 'effort', 'to', 'mentally', 'pull', 'it', 'out', 'of', 'there', 'and', 'ignore', 'everything', 'else', ',', 'including', 'physical', 'stimuli', '.', 'sometimes', 'i', 'fail', 'and', 'continue', 'to', 'just', 'watch', 'thoughts', 'go', 'by', '.', 'if', 'i', 'am', 'focusing', 'and', 'something', 'breaks', 'that', 'focus', ',', 'i', 'have', 'to', 'start', 'all', 'over', 'trying', 'to', 'get', 'back', 'to', 'that', 'level', 'of', 'focus', '.', '\"', 'edit', ':', 'thought', 'of', 'another', '-', 'a', 'radio', 'or', 'tv', 'on', 'scan', '.', '.', '.', 'each', 'station', 'lasts', 'a', 'couple', 'seconds', 'and', 'then', 'it', 'changes', 'without', 'me', 'trying', 'to', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['let', \"'\", 's', 'get', 'this', 'straight', ',', 'r', '/', 'ad', '##hd', '-', 'drinking', 'caf', '##fe', '##ine', 'while', 'on', 'med', '##s', ',', 'helping', 'or', 'hurting', '?', 'i', \"'\", 'm', 'trying', 'to', 'figure', 'out', 'how', 'to', 'best', 'help', 'myself', 'to', 'pay', 'attention', 'and', 'stay', 'motivated', 'to', 'work', 'for', 'long', 'stretches', 'at', 'a', 'time', '.', 'i', 'want', 'to', 'help', 'my', 'med', '##s', ',', 'not', 'hurt', 'them', '.', 'i', 'just', 'started', 'on', 'add', '##eral', '##l', ',', 'and', 'now', 'that', 'i', \"'\", 'm', 'used', 'to', 'it', 'enough', ',', 'i', 'can', 'drink', 'caf', '##fe', '##ine', 'again', 'without', 'feeling', 'sick', '.', 'but', 'the', 'questions', 'is', '.', '.', '.', 'should', 'i', '?', 'is', 'the', 'caf', '##fe', '##ine', 'helping', '?']\n",
      "INFO:__main__:Number of tokens: 111\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['let', \"'\", 's', 'get', 'this', 'straight', ',', 'r', '/', 'ad', '##hd', '-', 'drinking', 'caf', '##fe', '##ine', 'while', 'on', 'med', '##s', ',', 'helping', 'or', 'hurting', '?', 'i', \"'\", 'm', 'trying', 'to', 'figure', 'out', 'how', 'to', 'best', 'help', 'myself', 'to', 'pay', 'attention', 'and', 'stay', 'motivated', 'to', 'work', 'for', 'long', 'stretches', 'at', 'a', 'time', '.', 'i', 'want', 'to', 'help', 'my', 'med', '##s', ',', 'not', 'hurt', 'them', '.', 'i', 'just', 'started', 'on', 'add', '##eral', '##l', ',', 'and', 'now', 'that', 'i', \"'\", 'm', 'used', 'to', 'it', 'enough', ',', 'i', 'can', 'drink', 'caf', '##fe', '##ine', 'again', 'without', 'feeling', 'sick', '.', 'but', 'the', 'questions', 'is', '.', '.', '.', 'should', 'i', '?', 'is', 'the', 'caf', '##fe', '##ine', 'helping', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'just', 'been', 'diagnosed', 'and', 'wondering', 'what', 'happens', 'next', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'just', 'been', 'diagnosed', 'and', 'wondering', 'what', 'happens', 'next', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'i', 'convince', 'my', 'parents', 'i', 'have', 'add', ',', 'and', 'change', 'their', 'minds', 'about', 'medication', '.']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'i', 'convince', 'my', 'parents', 'i', 'have', 'add', ',', 'and', 'change', 'their', 'minds', 'about', 'medication', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'couldn', \"'\", 't', 'focus', 'on', 'my', 'school', '##work', 'today', 'even', 'with', 'medication', '(', 'v', '##y', '##van', '##se', ')', '.', 'can', 'someone', 'explain', 'this', '?', '*', 'normally', 'i', 'function', 'quite', 'well', 'when', 'i', \"'\", 'm', 'on', 'v', '##y', '##van', '##se', '.', 'like', 'i', 'stated', 'in', 'a', 'previous', 'post', ',', 'i', 'tend', 'to', 'get', 'distracted', 'but', 'i', 'can', 'get', 'back', 'to', 'work', '.', 'but', 'today', ',', 'for', 'some', 'odd', 'reason', ',', 'i', 'just', 'couldn', \"'\", 't', 'get', 'my', 'head', 'on', 'the', 'work', ',', 'even', 'with', 'no', 'distraction', '##s', 'whatsoever', '.', '*', 'it', 'really', 'bother', '##s', 'me', 'and', 'my', 'mom', 'that', 'i', 'wasn', \"'\", 't', 'able', 'to', 'accomplish', 'much', 'today', ',', 'so', 'i', 'was', 'wondering', 'if', 'any', 'of', 'you', 'have', 'ever', 'had', 'days', 'where', 'you', 'weren', \"'\", 't', 'able', 'to', 'focus', 'even', 'with', 'medication', '.', '*', 'if', 'this', 'hasn', \"'\", 't', 'happened', 'to', 'anyone', 'before', ',', 'could', 'someone', 'offer', 'me', 'advise', 'as', 'to', 'how', 'i', 'should', 'go', 'about', 'this', '?', 'should', 'i', 'be', 'concerned', '?']\n",
      "INFO:__main__:Number of tokens: 163\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'couldn', \"'\", 't', 'focus', 'on', 'my', 'school', '##work', 'today', 'even', 'with', 'medication', '(', 'v', '##y', '##van', '##se', ')', '.', 'can', 'someone', 'explain', 'this', '?', '*', 'normally', 'i', 'function', 'quite', 'well', 'when', 'i', \"'\", 'm', 'on', 'v', '##y', '##van', '##se', '.', 'like', 'i', 'stated', 'in', 'a', 'previous', 'post', ',', 'i', 'tend', 'to', 'get', 'distracted', 'but', 'i', 'can', 'get', 'back', 'to', 'work', '.', 'but', 'today', ',', 'for', 'some', 'odd', 'reason', ',', 'i', 'just', 'couldn', \"'\", 't', 'get', 'my', 'head', 'on', 'the', 'work', ',', 'even', 'with', 'no', 'distraction', '##s', 'whatsoever', '.', '*', 'it', 'really', 'bother', '##s', 'me', 'and', 'my', 'mom', 'that', 'i', 'wasn', \"'\", 't', 'able', 'to', 'accomplish', 'much', 'today', ',', 'so', 'i', 'was', 'wondering', 'if', 'any', 'of', 'you', 'have', 'ever', 'had', 'days', 'where', 'you', 'weren', \"'\", 't', 'able', 'to', 'focus', 'even', 'with', 'medication', '.', '*', 'if', 'this', 'hasn', \"'\", 't', 'happened', 'to', 'anyone', 'before', ',', 'could', 'someone', 'offer', 'me', 'advise', 'as', 'to', 'how', 'i', 'should', 'go', 'about', 'this', '?', 'should', 'i', 'be', 'concerned', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'else', 'ever', 'feel', 'like', 'some', 'kind', 'of', 'metaphor', '-', 'analysis', '-', 'robot', '-', 'genius', 'sometimes', '?', 'on', 'good', 'days', '.', 'on', 'good', 'days', 'i', 'feel', 'like', 'i', 'can', 'cut', 'straight', 'to', 'the', 'essence', 'of', 'a', 'concept', 'and', 'give', 'the', 'perfect', 'metaphor', 'for', 'communicating', 'it', '(', 'relative', 'to', 'the', 'given', 'situation', ')', '.', 'i', 'can', 'see', 'how', 'my', 'ad', '##hd', 'plays', 'a', 'role', 'in', 'this', '(', 'in', 'that', 'i', 'have', 'more', 'metaphor', '##s', 'in', 'my', 'head', 'than', 'i', \"'\", 'll', 'ever', 'need', ')', 'and', 'was', 'curious', 'to', 'see', 'how', 'you', 'all', 'perceive', 'it', '.']\n",
      "INFO:__main__:Number of tokens: 94\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'else', 'ever', 'feel', 'like', 'some', 'kind', 'of', 'metaphor', '-', 'analysis', '-', 'robot', '-', 'genius', 'sometimes', '?', 'on', 'good', 'days', '.', 'on', 'good', 'days', 'i', 'feel', 'like', 'i', 'can', 'cut', 'straight', 'to', 'the', 'essence', 'of', 'a', 'concept', 'and', 'give', 'the', 'perfect', 'metaphor', 'for', 'communicating', 'it', '(', 'relative', 'to', 'the', 'given', 'situation', ')', '.', 'i', 'can', 'see', 'how', 'my', 'ad', '##hd', 'plays', 'a', 'role', 'in', 'this', '(', 'in', 'that', 'i', 'have', 'more', 'metaphor', '##s', 'in', 'my', 'head', 'than', 'i', \"'\", 'll', 'ever', 'need', ')', 'and', 'was', 'curious', 'to', 'see', 'how', 'you', 'all', 'perceive', 'it', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['cr', '##ip', '##pling', 'add', 'is', 'destroying', 'my', 'life', 'but', 'i', \"'\", 'm', 'too', 'afraid', 'of', 'what', 'my', 'friends', 'and', 'family', 'will', 'think', 'of', 'me', 'to', 'seek', 'treatment', '.']\n",
      "INFO:__main__:Number of tokens: 28\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['cr', '##ip', '##pling', 'add', 'is', 'destroying', 'my', 'life', 'but', 'i', \"'\", 'm', 'too', 'afraid', 'of', 'what', 'my', 'friends', 'and', 'family', 'will', 'think', 'of', 'me', 'to', 'seek', 'treatment', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'teachers', 'look', 'for', 'to', 'flag', 'a', 'potential', 'case', 'of', 'ad', '##hd', '?', 'how', 'long', 'have', 'they', 'been', 'doing', 'this', '?', 'i', \"'\", 'm', 'just', 'trying', 'to', 'figure', 'out', 'how', 'no', 'teacher', 'ever', 'suggested', 'i', 'might', 'have', 'ad', '##hd', 'even', 'though', 'i', 'had', 'a', 'really', 'bad', 'case', 'and', 'showing', 'all', 'the', 'symptoms', 'very', 'clearly', 'since', 'the', 'beginning', 'of', 'school', 'through', 'high', 'school', 'and', 'beyond', '.', 'i', 'was', 'constantly', 'getting', 'in', 'trouble', 'for', 'my', 'bad', 'grades', ',', 'so', 'the', 'fact', 'i', 'was', 'having', 'problems', 'was', 'acknowledged', 'by', 'all', 'my', 'teachers', '.', 'did', 'teachers', 'not', 'know', 'what', 'to', 'look', 'for', 'in', 'the', '80s', 'and', '90s', '?']\n",
      "INFO:__main__:Number of tokens: 105\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'teachers', 'look', 'for', 'to', 'flag', 'a', 'potential', 'case', 'of', 'ad', '##hd', '?', 'how', 'long', 'have', 'they', 'been', 'doing', 'this', '?', 'i', \"'\", 'm', 'just', 'trying', 'to', 'figure', 'out', 'how', 'no', 'teacher', 'ever', 'suggested', 'i', 'might', 'have', 'ad', '##hd', 'even', 'though', 'i', 'had', 'a', 'really', 'bad', 'case', 'and', 'showing', 'all', 'the', 'symptoms', 'very', 'clearly', 'since', 'the', 'beginning', 'of', 'school', 'through', 'high', 'school', 'and', 'beyond', '.', 'i', 'was', 'constantly', 'getting', 'in', 'trouble', 'for', 'my', 'bad', 'grades', ',', 'so', 'the', 'fact', 'i', 'was', 'having', 'problems', 'was', 'acknowledged', 'by', 'all', 'my', 'teachers', '.', 'did', 'teachers', 'not', 'know', 'what', 'to', 'look', 'for', 'in', 'the', '80s', 'and', '90s', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'researcher', 'here', '.', 'ama', 'about', 'the', 'science', 'of', 'ad', '##hd', 'i', \"'\", 'd', 'be', 'happy', 'to', 'answer', 'any', 'questions', 'people', 'have', 'concerning', 'what', 'we', 'know', 'about', 'ad', '##hd', '.', 'my', 'areas', 'of', 'expertise', 'are', 'cognitive', 'functioning', 'in', 'adults', 'with', 'ad', '##hd', 'and', ',', 'to', 'a', 'lesser', 'extent', ',', 'psycho', '##pha', '##rma', '##cology', '.', 'i', 'am', 'con', '##vers', '##ant', 'in', 'most', 'things', 'ad', '##hd', ',', 'however', ',', 'so', 'questions', 'don', \"'\", 't', 'have', 'to', 'be', 'limited', 'to', 'this', 'topic', '.', 'i', 'can', \"'\", 't', 'answer', 'questions', 'you', 'may', 'have', 'about', 'treating', 'your', 'own', 'symptoms', '(', 'ethical', 'grey', 'area', ')', ',', 'but', 'i', \"'\", 'd', 'be', 'happy', 'to', 'answer', 'general', 'questions', 'about', 'treatment', 'methods', '.', 'edit', '*', 'excellent', 'questions', 'so', 'far', '!', 'i', 'have', 'a', 'thing', 'i', 'need', 'to', 'do', 'for', 'a', 'couple', 'of', 'hours', '.', 'i', \"'\", 'll', 'be', 'back', 'to', 'answer', 'more', 'questions', 'afterwards', '.', 'edit', '2', '*', 'i', 'anti', '##ci', '##pate', 'that', 'there', 'will', 'be', 'some', 'overlapping', 'questions', '.', 'do', 'a', 'ct', '##rl', '+', 'f', 'before', 'posting', 'if', 'you', 'don', \"'\", 't', 'mind', '.', 'it', 'will', 'help', 'to', 'avoid', 'cl', '##utter', '.', 'edit', '3', '*', 'i', 'need', 'to', 'get', 'some', 'work', 'done', 'before', 'i', 'go', 'home', 'for', 'the', 'day', '.', 'i', \"'\", 'll', 'continue', 'to', 'answer', 'questions', 'for', 'a', 'few', 'hours', 'after', 'work', '.', 'edit', '4', '*', 'had', 'some', 'family', 'stuff', 'come', 'up', 'after', 'getting', 'home', 'from', 'the', 'office', ',', 'and', 'i', \"'\", 've', 'been', 'tied', 'up', 'dealing', 'with', 'that', '.', 'i', \"'\", 'll', 'answer', 'the', 'top', '3', 'una', '##ns', '##wer', '##ed', 'questions', 'tomorrow', 'morning', ',', 'and', 'then', 'i', \"'\", 'll', 'have', 'to', 'get', 'back', 'to', 'the', 'grind', '.', 'thanks', 'for', 'the', 'questions', 'everyone', '!', 'i', 'have', 'enjoyed', 'the', 'discussion', '.', 'i', 'lu', '##rk', 'around', 'the', 'sub', '##red', '##dit', 'and', 'post', 'from', 'time', 'to', 'time', ',', 'so', 'feel', 'free', 'to', 'post', 'questions', 'as', 'self', 'posts', 'and', 'i', \"'\", 'll', 'do', 'my', 'best', 'to', 'answer', 'when', 'i', 'have', 'time', '.']\n",
      "INFO:__main__:Number of tokens: 322\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'researcher', 'here', '.', 'ama', 'about', 'the', 'science', 'of', 'ad', '##hd', 'i', \"'\", 'd', 'be', 'happy', 'to', 'answer', 'any', 'questions', 'people', 'have', 'concerning', 'what', 'we', 'know', 'about', 'ad', '##hd', '.', 'my', 'areas', 'of', 'expertise', 'are', 'cognitive', 'functioning', 'in', 'adults', 'with', 'ad', '##hd', 'and', ',', 'to', 'a', 'lesser', 'extent', ',', 'psycho', '##pha', '##rma', '##cology', '.', 'i', 'am', 'con', '##vers', '##ant', 'in', 'most', 'things', 'ad', '##hd', ',', 'however', ',', 'so', 'questions', 'don', \"'\", 't', 'have', 'to', 'be', 'limited', 'to', 'this', 'topic', '.', 'i', 'can', \"'\", 't', 'answer', 'questions', 'you', 'may', 'have', 'about', 'treating', 'your', 'own', 'symptoms', '(', 'ethical', 'grey', 'area', ')', ',', 'but', 'i', \"'\", 'd', 'be', 'happy', 'to', 'answer', 'general', 'questions', 'about', 'treatment', 'methods', '.', 'edit', '*', 'excellent', 'questions', 'so', 'far', '!', 'i', 'have', 'a', 'thing', 'i', 'need', 'to', 'do', 'for', 'a', 'couple', 'of', 'hours', '.', 'i', \"'\", 'll', 'be', 'back', 'to', 'answer', 'more', 'questions', 'afterwards', '.', 'edit', '2', '*', 'i', 'anti', '##ci', '##pate', 'that', 'there', 'will', 'be', 'some', 'overlapping', 'questions', '.', 'do', 'a', 'ct', '##rl', '+', 'f', 'before', 'posting', 'if', 'you', 'don', \"'\", 't', 'mind', '.', 'it', 'will', 'help', 'to', 'avoid', 'cl', '##utter', '.', 'edit', '3', '*', 'i', 'need', 'to', 'get', 'some', 'work', 'done', 'before', 'i', 'go', 'home', 'for', 'the', 'day', '.', 'i', \"'\", 'll', 'continue', 'to', 'answer', 'questions', 'for', 'a', 'few', 'hours', 'after', 'work', '.', 'edit', '4', '*', 'had', 'some', 'family', 'stuff', 'come', 'up', 'after', 'getting', 'home', 'from', 'the', 'office', ',', 'and', 'i', \"'\", 've', 'been', 'tied', 'up', 'dealing', 'with', 'that', '.', 'i', \"'\", 'll', 'answer', 'the', 'top', '3', 'una', '##ns', '##wer', '##ed', 'questions', 'tomorrow', 'morning', ',', 'and', 'then', 'i', \"'\", 'll', 'have', 'to', 'get', 'back', 'to', 'the', 'grind', '.', 'thanks', 'for', 'the', 'questions', 'everyone', '!', 'i', 'have', 'enjoyed', 'the', 'discussion', '.', 'i', 'lu', '##rk', 'around', 'the', 'sub', '##red', '##dit', 'and', 'post', 'from', 'time', 'to', 'time', ',', 'so', 'feel', 'free', 'to', 'post', 'questions', 'as', 'self', 'posts', 'and', 'i', \"'\", 'll', 'do', 'my', 'best', 'to', 'answer', 'when', 'i', 'have', 'time', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'mind', '##fulness', 'a', 'valid', 'treatment', 'for', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'mind', '##fulness', 'a', 'valid', 'treatment', 'for', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'without', 'medication']\n",
      "INFO:__main__:Number of tokens: 4\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'without', 'medication']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'momentum', 'monday', ']', 'is', 'where', '/', 'r', '/', 'ad', '##hd', 'can', 'share', 'the', 'goals', ',', 'tasks', ',', 'or', 'habits', 'we', 'want', 'to', 'accomplish', 'this', 'week', ',', 'and', 'be', 'held', 'gently', 'accountable', '.', 'momentum', 'monday', 'is', 'where', '/', 'r', '/', 'ad', '##hd', 'can', 'come', 'to', 'share', 'the', 'goals', ',', 'tasks', ',', 'or', 'habits', 'we', 'all', 'want', 'to', 'accomplish', 'this', 'week', '.', 'by', 'sharing', 'here', 'you', 'will', 'be', 'held', 'accountable', '(', 'which', 'is', 'a', 'good', 'thing', '!', ')', 'and', 'we', 'can', 'help', 'check', 'in', 'with', 'you', '.', '.', '.', 'helping', 'you', 'get', 'things', 'done', '.', '#', '#', 'here', 'are', 'a', 'couple', 'of', 'the', 'ways', 'you', 'can', 'participate', ':', '*', '*', '*', 'most', 'important', 'task', '*', '*', '-', 'share', 'one', '(', 'or', 'a', 'couple', ')', 'high', '-', 'priority', 'tasks', 'that', 'you', 'really', 'would', 'like', 'to', 'finally', 'finish', '.', 'ex', '.', 'set', 'a', 'doctors', 'appointment', ',', 'finish', 'assignment', ',', 'clean', 'the', 'kitchen', '.', '.', '.', 'whatever', 'you', 'think', 'is', 'most', 'important', '.', '*', '*', '*', 'make', 'a', 'list', '*', '*', '-', 'post', 'a', 'list', 'of', 'all', 'the', 'things', 'you', 'want', 'to', 'accomplish', 'this', 'week', '.', '(', 'i', 'highly', 'recommend', 'identifying', 'the', 'top', '3', 'priorities', '.', ')', '*', '*', '*', 'feed', 'your', 'soul', '*', '*', '-', 'sometimes', 'by', 'relaxing', 'and', 'having', 'fun', 'we', 'have', 'more', 'energy', 'to', 'get', 'more', 'done', '.', '*', '*', 'list', 'a', 'couple', 'fun', 'items', 'to', 'make', 'your', 'list', 'more', 'exciting', '.', '*', '*', '*', '*', 'need', '/', 'want', 'to', 'copy', 'last', 'week', \"'\", 's', 'list', '*', 'verb', '##ati', '##m', '*', '?', 'absolutely', 'do', 'so', '!', '*', '*', '*', 'those', 'might', 'be', 'your', 'keystone', 'habits', '.', 'once', 'you', 'start', 'doing', 'those', 'regularly', 'you', 'are', 'able', 'to', 'do', 'more', '.', 'soon', 'they', 'will', 'be', 'automatic', 'and', 'you', 'can', 'put', 'your', 'focus', 'elsewhere', '.', '*', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'publicly', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '♥', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '#', '#', 'suggested', 'guidelines', '*', '*', '*', 'prior', '##iti', '##ze', 'your', 'list', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to', 'do', 'that', 'too', '!', ')', '*', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'committing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '*', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '*', '*', '*', 'small', 'start', '*', '*', '-', 'share', 'what', 'small', '(', 'under', '10', 'minutes', ')', 'commitment', 'you', 'want', 'to', 'make', 'towards', 'starting', 'your', 'goal', '.', 'i', 'found', 'when', 'my', 'clients', 'didn', '’', 't', 'reach', 'their', 'goals', '90', '%', 'of', 'the', 'time', 'they', 'never', 'started', '.', '.', '.', 'this', 'will', 'help', 'you', 'break', 'through', 'the', 'initial', 'barrier', '.', '*', '*', '*', '#', '#', 'tips', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '.', '.', '.', 'build', 'up', 'slowly', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', '30', 'minutes', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '*', '*', '*', '#', '#', 'examples', 'from', 'previous', 'weeks', ':', '*', 'un', '##load', 'the', 'dish', '##wash', '##er', '*', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '*', 'make', 'an', 'appointment', 'with', 'doctor', '*', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '*', '*', '*', 'start', '*', '*', '*', '[', 'something', ']', '*', '.', 'spend', '5', 'minutes', 'on', 'writing', 'my', 'paper', '.', '*', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '#', '#', '*', 'i', 'made', 'it', 'easier', 'this', 'week', 'to', 'respond', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '*', '#', '#', 'daily', 'habit', '/', 'pro', '##cr', '##ast', '##inated', 'project', '/', 'soul', '-', 'feeding', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?', 'when', '?', '*', '*', '-', '*', '*', '*', 'small', 'start', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-', '*', '*', '*', 'something', 'fun', 'you', 'want', 'to', 'do', 'this', 'week', '*', '*', '-', '*', '*', '*', 'daily', 'habit', '*', '*', '-', '*', '*', '*', 'hopefully', 'most', 'of', 'you', 'can', 'also', 'participate', 'in', 'win', 'wednesday', 'on', 'wednesday', '.', 'we', 'had', 'a', 'drop', 'in', 'participation', 'in', 'both', 'these', 'threads', '(', 'and', 'i', 'didn', '’', 't', 'even', 'get', 'around', 'to', 'posting', 'it', 'last', 'week', ')', '.', '*', '*', 'celebrating', '/', 'acknowledging', 'your', 'wins', 'is', 'just', 'as', 'important', 'as', 'actually', 'getting', 'stuff', 'done', '.', '*', '*', 'if', 'you', 'have', 'any', 'other', 'suggestions', 'for', 'weekly', 'threads', 'or', 'improvements', 'to', 'these', 'ones', 'let', 'me', 'know', '!']\n",
      "INFO:__main__:Number of tokens: 1062\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['[', 'momentum', 'monday', ']', 'is', 'where', '/', 'r', '/', 'ad', '##hd', 'can', 'share', 'the', 'goals', ',', 'tasks', ',', 'or', 'habits', 'we', 'want', 'to', 'accomplish', 'this', 'week', ',', 'and', 'be', 'held', 'gently', 'accountable', '.', 'momentum', 'monday', 'is', 'where', '/', 'r', '/', 'ad', '##hd', 'can', 'come', 'to', 'share', 'the', 'goals', ',', 'tasks', ',', 'or', 'habits', 'we', 'all', 'want', 'to', 'accomplish', 'this', 'week', '.', 'by', 'sharing', 'here', 'you', 'will', 'be', 'held', 'accountable', '(', 'which', 'is', 'a', 'good', 'thing', '!', ')', 'and', 'we', 'can', 'help', 'check', 'in', 'with', 'you', '.', '.', '.', 'helping', 'you', 'get', 'things', 'done', '.', '#', '#', 'here', 'are', 'a', 'couple', 'of', 'the', 'ways', 'you', 'can', 'participate', ':', '*', '*', '*', 'most', 'important', 'task', '*', '*', '-', 'share', 'one', '(', 'or', 'a', 'couple', ')', 'high', '-', 'priority', 'tasks', 'that', 'you', 'really', 'would', 'like', 'to', 'finally', 'finish', '.', 'ex', '.', 'set', 'a', 'doctors', 'appointment', ',', 'finish', 'assignment', ',', 'clean', 'the', 'kitchen', '.', '.', '.', 'whatever', 'you', 'think', 'is', 'most', 'important', '.', '*', '*', '*', 'make', 'a', 'list', '*', '*', '-', 'post', 'a', 'list', 'of', 'all', 'the', 'things', 'you', 'want', 'to', 'accomplish', 'this', 'week', '.', '(', 'i', 'highly', 'recommend', 'identifying', 'the', 'top', '3', 'priorities', '.', ')', '*', '*', '*', 'feed', 'your', 'soul', '*', '*', '-', 'sometimes', 'by', 'relaxing', 'and', 'having', 'fun', 'we', 'have', 'more', 'energy', 'to', 'get', 'more', 'done', '.', '*', '*', 'list', 'a', 'couple', 'fun', 'items', 'to', 'make', 'your', 'list', 'more', 'exciting', '.', '*', '*', '*', '*', 'need', '/', 'want', 'to', 'copy', 'last', 'week', \"'\", 's', 'list', '*', 'verb', '##ati', '##m', '*', '?', 'absolutely', 'do', 'so', '!', '*', '*', '*', 'those', 'might', 'be', 'your', 'keystone', 'habits', '.', 'once', 'you', 'start', 'doing', 'those', 'regularly', 'you', 'are', 'able', 'to', 'do', 'more', '.', 'soon', 'they', 'will', 'be', 'automatic', 'and', 'you', 'can', 'put', 'your', 'focus', 'elsewhere', '.', '*', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'publicly', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '♥', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '#', '#', 'suggested', 'guidelines', '*', '*', '*', 'prior', '##iti', '##ze', 'your', 'list', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to', 'do', 'that', 'too', '!', ')', '*', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'committing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '*', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')'], ['.', '*', '*', '*', 'small', 'start', '*', '*', '-', 'share', 'what', 'small', '(', 'under', '10', 'minutes', ')', 'commitment', 'you', 'want', 'to', 'make', 'towards', 'starting', 'your', 'goal', '.', 'i', 'found', 'when', 'my', 'clients', 'didn', '’', 't', 'reach', 'their', 'goals', '90', '%', 'of', 'the', 'time', 'they', 'never', 'started', '.', '.', '.', 'this', 'will', 'help', 'you', 'break', 'through', 'the', 'initial', 'barrier', '.', '*', '*', '*', '#', '#', 'tips', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 'smart', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '.', '.', '.', 'build', 'up', 'slowly', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', '30', 'minutes', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '*', '*', '*', '#', '#', 'examples', 'from', 'previous', 'weeks', ':', '*', 'un', '##load', 'the', 'dish', '##wash', '##er', '*', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '*', 'make', 'an', 'appointment', 'with', 'doctor', '*', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '*', '*', '*', 'start', '*', '*', '*', '[', 'something', ']', '*', '.', 'spend', '5', 'minutes', 'on', 'writing', 'my', 'paper', '.', '*', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '#', '#', '*', 'i', 'made', 'it', 'easier', 'this', 'week', 'to', 'respond', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '*', '#', '#', 'daily', 'habit', '/', 'pro', '##cr', '##ast', '##inated', 'project', '/', 'soul', '-', 'feeding', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?', 'when', '?', '*', '*', '-', '*', '*', '*', 'small', 'start', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-', '*', '*', '*', 'something', 'fun', 'you', 'want', 'to', 'do', 'this', 'week', '*', '*', '-', '*', '*', '*', 'daily', 'habit', '*', '*', '-', '*', '*', '*', 'hopefully', 'most', 'of', 'you', 'can', 'also', 'participate', 'in', 'win', 'wednesday', 'on', 'wednesday', '.', 'we', 'had', 'a', 'drop', 'in', 'participation', 'in', 'both', 'these', 'threads', '(', 'and', 'i', 'didn', '’', 't', 'even', 'get', 'around', 'to', 'posting', 'it', 'last', 'week', ')'], ['.', '*', '*', 'celebrating', '/', 'acknowledging', 'your', 'wins', 'is', 'just', 'as', 'important', 'as', 'actually', 'getting', 'stuff', 'done', '.', '*', '*', 'if', 'you', 'have', 'any', 'other', 'suggestions', 'for', 'weekly', 'threads', 'or', 'improvements', 'to', 'these', 'ones', 'let', 'me', 'know', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'to', 'structure', 'myself', 'when', 'i', 'lack', 'any', 'external', 'structure', 'at', 'the', 'moment', '?']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'to', 'structure', 'myself', 'when', 'i', 'lack', 'any', 'external', 'structure', 'at', 'the', 'moment', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'you', 'find', 'yourself', 'getting', 'distracted', 'by', 'the', 'strange', '##st', 'things', 'even', 'on', 'med', '##s', '?', 'right', 'now', 'i', 'should', 'be', 'studying', 'for', 'my', 'final', 'that', \"'\", 's', 'tomorrow', ',', 'but', 'there', \"'\", 's', 'a', 'brown', '##ie', 'smell', 'in', 'my', 'home', 'that', 'completely', 'der', '##ailed', 'my', 'train', 'of', 'commitment', 'to', 'studying', '.', 'i', \"'\", 've', 'noticed', 'that', 'this', 'seems', 'to', 'be', 'a', 'common', 'thing', 'for', 'me', '.', 'if', 'i', 'smell', 'something', 'that', \"'\", 's', 'at', '##yp', '##ical', 'to', 'my', 'environment', 'it', 'completely', 'distract', '##s', 'me', 'from', 'whatever', 'i', \"'\", 'm', 'doing', 'and', 'it', \"'\", 's', 'difficult', 'to', 'go', 'back', 'to', 'what', 'i', 'should', 'be', 'doing', '.', 'i', 'have', 'no', 'clue', 'how', 'to', 'stop', 'this', 'from', 'happening', '!']\n",
      "INFO:__main__:Number of tokens: 117\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'you', 'find', 'yourself', 'getting', 'distracted', 'by', 'the', 'strange', '##st', 'things', 'even', 'on', 'med', '##s', '?', 'right', 'now', 'i', 'should', 'be', 'studying', 'for', 'my', 'final', 'that', \"'\", 's', 'tomorrow', ',', 'but', 'there', \"'\", 's', 'a', 'brown', '##ie', 'smell', 'in', 'my', 'home', 'that', 'completely', 'der', '##ailed', 'my', 'train', 'of', 'commitment', 'to', 'studying', '.', 'i', \"'\", 've', 'noticed', 'that', 'this', 'seems', 'to', 'be', 'a', 'common', 'thing', 'for', 'me', '.', 'if', 'i', 'smell', 'something', 'that', \"'\", 's', 'at', '##yp', '##ical', 'to', 'my', 'environment', 'it', 'completely', 'distract', '##s', 'me', 'from', 'whatever', 'i', \"'\", 'm', 'doing', 'and', 'it', \"'\", 's', 'difficult', 'to', 'go', 'back', 'to', 'what', 'i', 'should', 'be', 'doing', '.', 'i', 'have', 'no', 'clue', 'how', 'to', 'stop', 'this', 'from', 'happening', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', \"'\", 't', 'turn', 'off', 'my', 'mind', 'at', 'night', 'i', \"'\", 'm', 'on', '70', '##mg', 'v', '##y', '##van', '##se', 'and', 'have', 'really', 'bad', 'add', '.', 'the', 'v', '##y', '##van', '##se', 'is', 'out', 'of', 'my', 'system', 'by', '10', 'at', 'night', '.', 'if', 'i', 'get', 'in', 'bed', 'at', '10', ',', 'i', 'can', \"'\", 't', 'fall', 'asleep', 'until', 'at', 'least', '1', 'in', 'the', 'morning', '.', 'i', 'don', \"'\", 't', 'do', 'anything', 'but', 'lie', 'in', 'my', 'bed', 'with', 'my', 'eyes', 'closed', '.', 'since', 'the', 'v', '##y', '##van', '##se', 'is', 'out', 'of', 'my', 'system', ',', 'i', 'can', \"'\", 't', 'concentrate', 'at', 'all', 'and', 'i', 'have', 'so', 'many', 'thoughts', 'going', 'through', 'my', 'head', 'that', 'its', 'impossible', 'for', 'me', 'to', 'fall', 'asleep', 'easily', '.', 'is', 'there', 'any', 'way', 'to', 'like', '(', 'for', 'lack', 'of', 'a', 'better', 'way', 'to', 'say', 'it', ')', 'turn', 'off', 'my', 'mind', 'so', 'its', 'easier', 'to', 'fall', 'asleep', '?', 'does', 'anyone', 'else', 'have', 'this', 'problem', '?', 'i', 'tried', 'reading', 'before', 'bed', 'but', 'it', 'never', 'worked', 'out', 'because', 'i', 'always', 'have', 'to', 'finish', 'the', 'book', 'before', 'i', 'fall', 'asleep', 'so', 'i', 'end', 'up', 'going', 'to', 'sleep', 'later', 'than', 'i', 'would', 'if', 'i', 'just', 'sat', 'there', '.', 't', '##ld', '##r', '-', 'i', 'can', \"'\", 't', \"'\", '\"', 'turn', 'off', 'my', 'mind', '\"', 'at', 'night', 'so', 'i', 'can', '##t', 'fall', 'asleep', '.', 'any', 'tips', '?', 'edit', '-', 'i', 'don', \"'\", 't', 'do', 'any', 'drugs', '(', 'aka', 'weed', 'and', 'stuff', 'like', 'that', ')', '.', 'edit', '#', '2', '-', 'thanks', 'for', 'all', 'your', 'suggestions', 'and', 'i', 'will', 'try', 'as', 'many', 'of', 'them', 'as', 'possible', '.', 'you', 'guys', 'have', 'no', 'idea', 'how', 'much', 'this', 'helps', '.']\n",
      "INFO:__main__:Number of tokens: 266\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', \"'\", 't', 'turn', 'off', 'my', 'mind', 'at', 'night', 'i', \"'\", 'm', 'on', '70', '##mg', 'v', '##y', '##van', '##se', 'and', 'have', 'really', 'bad', 'add', '.', 'the', 'v', '##y', '##van', '##se', 'is', 'out', 'of', 'my', 'system', 'by', '10', 'at', 'night', '.', 'if', 'i', 'get', 'in', 'bed', 'at', '10', ',', 'i', 'can', \"'\", 't', 'fall', 'asleep', 'until', 'at', 'least', '1', 'in', 'the', 'morning', '.', 'i', 'don', \"'\", 't', 'do', 'anything', 'but', 'lie', 'in', 'my', 'bed', 'with', 'my', 'eyes', 'closed', '.', 'since', 'the', 'v', '##y', '##van', '##se', 'is', 'out', 'of', 'my', 'system', ',', 'i', 'can', \"'\", 't', 'concentrate', 'at', 'all', 'and', 'i', 'have', 'so', 'many', 'thoughts', 'going', 'through', 'my', 'head', 'that', 'its', 'impossible', 'for', 'me', 'to', 'fall', 'asleep', 'easily', '.', 'is', 'there', 'any', 'way', 'to', 'like', '(', 'for', 'lack', 'of', 'a', 'better', 'way', 'to', 'say', 'it', ')', 'turn', 'off', 'my', 'mind', 'so', 'its', 'easier', 'to', 'fall', 'asleep', '?', 'does', 'anyone', 'else', 'have', 'this', 'problem', '?', 'i', 'tried', 'reading', 'before', 'bed', 'but', 'it', 'never', 'worked', 'out', 'because', 'i', 'always', 'have', 'to', 'finish', 'the', 'book', 'before', 'i', 'fall', 'asleep', 'so', 'i', 'end', 'up', 'going', 'to', 'sleep', 'later', 'than', 'i', 'would', 'if', 'i', 'just', 'sat', 'there', '.', 't', '##ld', '##r', '-', 'i', 'can', \"'\", 't', \"'\", '\"', 'turn', 'off', 'my', 'mind', '\"', 'at', 'night', 'so', 'i', 'can', '##t', 'fall', 'asleep', '.', 'any', 'tips', '?', 'edit', '-', 'i', 'don', \"'\", 't', 'do', 'any', 'drugs', '(', 'aka', 'weed', 'and', 'stuff', 'like', 'that', ')', '.', 'edit', '#', '2', '-', 'thanks', 'for', 'all', 'your', 'suggestions', 'and', 'i', 'will', 'try', 'as', 'many', 'of', 'them', 'as', 'possible', '.', 'you', 'guys', 'have', 'no', 'idea', 'how', 'much', 'this', 'helps', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'is', 'not', 'a', 'gift', 'by', 'dr', '.', 'russell', 'bark', '##ley', '.', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'is', 'not', 'a', 'gift', 'by', 'dr', '.', 'russell', 'bark', '##ley', '.', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'new', 'here', 'so', 'i', 'don', \"'\", 't', 'know', 'it', 'this', 'fits', ',', 'but', 'this', 'is', 'what', 'i', \"'\", 've', 'learned', 'so', 'far', '.']\n",
      "INFO:__main__:Number of tokens: 26\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'new', 'here', 'so', 'i', 'don', \"'\", 't', 'know', 'it', 'this', 'fits', ',', 'but', 'this', 'is', 'what', 'i', \"'\", 've', 'learned', 'so', 'far', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['red', '##dit', '+', 'ad', '##hd', '=', 'late', 'grades']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['red', '##dit', '+', 'ad', '##hd', '=', 'late', 'grades']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['simpsons', 'fans', 'will', 'appreciate', 'this', '!', 'from', 'season', '11', '-', '\"', 'brother', \"'\", 's', 'little', 'help', '##er', '\"']\n",
      "INFO:__main__:Number of tokens: 18\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['simpsons', 'fans', 'will', 'appreciate', 'this', '!', 'from', 'season', '11', '-', '\"', 'brother', \"'\", 's', 'little', 'help', '##er', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['id', '##k', 'about', 'the', 'rest', 'of', 'you', ',', 'but', 'this', 'is', 'how', 'i', 'feel', 'when', 'i', 'set', 'a', 'goal', '.']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['id', '##k', 'about', 'the', 'rest', 'of', 'you', ',', 'but', 'this', 'is', 'how', 'i', 'feel', 'when', 'i', 'set', 'a', 'goal', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['because', 'non', 'of', 'us', 'need', 'sleep']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['because', 'non', 'of', 'us', 'need', 'sleep']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['love', 'my', 'rings', '<', '3', '…', 'and', 'fake', 'nails', '.', 'so', 'much', 'fun']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['love', 'my', 'rings', '<', '3', '…', 'and', 'fake', 'nails', '.', 'so', 'much', 'fun']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['tip', 'for', 'not', 'getting', 'distracted', 'by', 'internet', 'during', 'study', '/', 'work', 'this', 'might', 'be', 'a', 'small', 'thing', ',', 'but', 'this', 'tiny', 'chrome', '-', 'extension', 'is', 'probably', 'the', 'reason', 'i', \"'\", 'm', 'keeping', 'my', 'job', 'and', 'am', 'actually', 'being', 'praised', 'for', 'the', 'great', 'job', 'i', 'do', ':', '[', 'nanny', 'for', 'chrome', ']', '(', 'https', ':', '/', '/', 'chrome', '.', 'google', '.', 'com', '/', 'web', '##stor', '##e', '/', 'detail', '/', 'cl', '##j', '##c', '##gc', '##h', '##bn', '##ol', '##he', '##gg', '##d', '##ga', '##ec', '##lf', '##fe', '##ag', '##nn', '##m', '##hn', '##o', ')', 'i', 'actually', 'blocked', 'websites', '(', 'like', 'red', '##dit', ')', 'with', 'this', 'tiny', 'program', 'that', 'just', 'asks', 'me', '\"', 'if', 'i', 'shouldn', \"'\", 't', 'be', 'working', '.', '\"', 'there', 'probably', 'is', 'a', 'version', 'for', 'fire', '##fo', '##x', 'as', 'well', '.']\n",
      "INFO:__main__:Number of tokens: 127\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['tip', 'for', 'not', 'getting', 'distracted', 'by', 'internet', 'during', 'study', '/', 'work', 'this', 'might', 'be', 'a', 'small', 'thing', ',', 'but', 'this', 'tiny', 'chrome', '-', 'extension', 'is', 'probably', 'the', 'reason', 'i', \"'\", 'm', 'keeping', 'my', 'job', 'and', 'am', 'actually', 'being', 'praised', 'for', 'the', 'great', 'job', 'i', 'do', ':', '[', 'nanny', 'for', 'chrome', ']', '(', 'https', ':', '/', '/', 'chrome', '.', 'google', '.', 'com', '/', 'web', '##stor', '##e', '/', 'detail', '/', 'cl', '##j', '##c', '##gc', '##h', '##bn', '##ol', '##he', '##gg', '##d', '##ga', '##ec', '##lf', '##fe', '##ag', '##nn', '##m', '##hn', '##o', ')', 'i', 'actually', 'blocked', 'websites', '(', 'like', 'red', '##dit', ')', 'with', 'this', 'tiny', 'program', 'that', 'just', 'asks', 'me', '\"', 'if', 'i', 'shouldn', \"'\", 't', 'be', 'working', '.', '\"', 'there', 'probably', 'is', 'a', 'version', 'for', 'fire', '##fo', '##x', 'as', 'well', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['medications', 'don', \"'\", 't', 'work', '.', 'it', 'feels', 'hopeless', '.', 'i', 'was', 'diagnosed', '2', 'years', 'ago', 'by', 'a', 'psychiatrist', 'and', 'we', \"'\", 've', 'tried', 'a', 'number', 'of', 'therapy', 'and', 'medications', '(', 'bi', '##ph', '##ent', '##in', ',', 'dex', '##ed', '##rine', 'spans', '##ules', ',', 'add', '##eral', '##l', ',', 'st', '##rat', '##tera', '/', 'atom', '##ox', '##eti', '##ne', ')', 'none', 'of', 'which', 'seem', 'to', 'have', 'any', 'beneficial', 'effect', '.', 'at', 'this', 'point', 'we', \"'\", 've', 'tried', 'everything', '.', 'it', 'feels', 'hopeless', '.', 'sometimes', 'i', 'want', 'to', 'just', 'fucking', 'kill', 'myself', '.', 'any', 'ideas', '?', '(', 'edit', ':', 'af', '##ai', '##k', ',', 'bi', '##ph', '##ent', '##in', 'is', 'the', 'same', 'as', 'rita', '##lin', ')']\n",
      "INFO:__main__:Number of tokens: 108\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['medications', 'don', \"'\", 't', 'work', '.', 'it', 'feels', 'hopeless', '.', 'i', 'was', 'diagnosed', '2', 'years', 'ago', 'by', 'a', 'psychiatrist', 'and', 'we', \"'\", 've', 'tried', 'a', 'number', 'of', 'therapy', 'and', 'medications', '(', 'bi', '##ph', '##ent', '##in', ',', 'dex', '##ed', '##rine', 'spans', '##ules', ',', 'add', '##eral', '##l', ',', 'st', '##rat', '##tera', '/', 'atom', '##ox', '##eti', '##ne', ')', 'none', 'of', 'which', 'seem', 'to', 'have', 'any', 'beneficial', 'effect', '.', 'at', 'this', 'point', 'we', \"'\", 've', 'tried', 'everything', '.', 'it', 'feels', 'hopeless', '.', 'sometimes', 'i', 'want', 'to', 'just', 'fucking', 'kill', 'myself', '.', 'any', 'ideas', '?', '(', 'edit', ':', 'af', '##ai', '##k', ',', 'bi', '##ph', '##ent', '##in', 'is', 'the', 'same', 'as', 'rita', '##lin', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['trick', 'for', 'getting', 'through', 'insane', '##ly', 'boring', 'classes', '.', '[', '\"', 'the', 'countdown', '\"', ']', '*', '*', 'technique', 'bold', '##ed', 'below', 'for', 't', '##l', ';', 'dr', '*', '*', 'half', 'my', 'classes', 'keep', 'me', 'interested', 'or', 'busy', 'enough', 'during', 'class', 'time', 'to', 'not', 'get', 'bored', '.', 'the', 'other', 'half', 'drive', 'me', 'crazy', '.', 'i', \"'\", 'll', 'be', 'sitting', 'there', 'as', 'my', 'mind', 'is', 'screaming', 'in', 'my', 'head', 'how', 'bored', 'i', 'am', ',', 'looking', 'for', 'anything', 'to', 'occupy', 'itself', '.', 'i', 'thought', 'up', 'this', 'trick', 'that', \"'\", 's', 'actually', 'been', 'working', 'for', 'me', ',', 'by', 'actually', '*', 'paying', 'attention', 'to', 'the', 'clock', '.', '*', '*', '*', 'what', 'i', 'do', ',', 'is', 'write', 'down', 'how', 'many', 'minutes', 'are', 'left', 'of', 'class', 'in', 'the', 'margin', 'of', 'my', 'notes', '(', 'or', 'any', 'sheet', 'of', 'paper', 'i', 'can', 'find', ')', 'every', '5', 'minutes', '.', 'for', 'example', ',', 'for', 'an', 'hour', 'class', ',', 'i', 'sit', 'down', ',', 'and', 'write', 'on', 'my', 'paper', '\"', '60', 'minutes', '!', '\"', '(', 'ex', '##cl', '##ama', '##tion', 'points', 'make', 'it', 'exciting', ')', ',', 'then', 'when', 'i', 'see', '5', 'minutes', 'have', 'passed', ',', 'i', 'write', '\"', '55', 'minutes', '!', '\"', 'i', 'recommend', 'using', 'a', 'digital', 'clock', 'for', 'this', 'to', 'avoid', '\"', 'cheating', '\"', '.', '*', '*', 'what', 'this', 'does', 'is', 'it', 'takes', 'your', 'mind', 'off', 'the', 'clock', ',', 'and', 'shows', 'you', 'in', 'written', 'form', 'that', 'time', 'is', 'actually', 'passing', '.', 'you', \"'\", 'd', 'be', 'surprised', 'how', 'many', 'times', 'you', 'might', 'even', 'miss', 'your', 'five', '-', 'minute', 'mark', 'with', 'less', 'attention', 'on', 'the', 'time', '(', 'still', 'write', 'it', 'down', '!', 'you', \"'\", 'll', 'just', 'have', 'shorter', 'to', 'go', 'until', 'the', 'next', 'one', '.', ')', 'it', 'is', 'nice', 'halfway', 'through', 'class', ',', 'looking', 'back', 'at', 'the', '6', 'lines', 'i', \"'\", 've', 'written', 'in', '\"', 'the', 'countdown', '\"', 'and', 'think', '\"', 'wow', '!', 'i', \"'\", 've', 'already', 'made', 'it', 'through', 'all', 'that', '.', '\"', 'it', 'also', 'breaks', 'up', 'the', 'class', 'into', 'smaller', 'chunks', ',', 'so', 'i', 'don', \"'\", 't', 'need', 'to', 'worry', 'about', 'making', 'it', 'through', 'the', 'next', '60', 'minutes', ',', 'just', 'the', 'next', '5', ',', 'and', 'helps', 'me', 'to', 'focus', 'more', 'on', 'the', 'class', '.', 'id', '##k', 'if', 'this', 'is', 'for', 'everyone', ',', 'but', 'it', \"'\", 's', 'working', 'great', 'for', 'me', '!', 'if', 'you', 'try', 'it', ',', 'let', 'me', 'know', 'how', 'it', 'goes', '!']\n",
      "INFO:__main__:Number of tokens: 379\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['trick', 'for', 'getting', 'through', 'insane', '##ly', 'boring', 'classes', '.', '[', '\"', 'the', 'countdown', '\"', ']', '*', '*', 'technique', 'bold', '##ed', 'below', 'for', 't', '##l', ';', 'dr', '*', '*', 'half', 'my', 'classes', 'keep', 'me', 'interested', 'or', 'busy', 'enough', 'during', 'class', 'time', 'to', 'not', 'get', 'bored', '.', 'the', 'other', 'half', 'drive', 'me', 'crazy', '.', 'i', \"'\", 'll', 'be', 'sitting', 'there', 'as', 'my', 'mind', 'is', 'screaming', 'in', 'my', 'head', 'how', 'bored', 'i', 'am', ',', 'looking', 'for', 'anything', 'to', 'occupy', 'itself', '.', 'i', 'thought', 'up', 'this', 'trick', 'that', \"'\", 's', 'actually', 'been', 'working', 'for', 'me', ',', 'by', 'actually', '*', 'paying', 'attention', 'to', 'the', 'clock', '.', '*', '*', '*', 'what', 'i', 'do', ',', 'is', 'write', 'down', 'how', 'many', 'minutes', 'are', 'left', 'of', 'class', 'in', 'the', 'margin', 'of', 'my', 'notes', '(', 'or', 'any', 'sheet', 'of', 'paper', 'i', 'can', 'find', ')', 'every', '5', 'minutes', '.', 'for', 'example', ',', 'for', 'an', 'hour', 'class', ',', 'i', 'sit', 'down', ',', 'and', 'write', 'on', 'my', 'paper', '\"', '60', 'minutes', '!', '\"', '(', 'ex', '##cl', '##ama', '##tion', 'points', 'make', 'it', 'exciting', ')', ',', 'then', 'when', 'i', 'see', '5', 'minutes', 'have', 'passed', ',', 'i', 'write', '\"', '55', 'minutes', '!', '\"', 'i', 'recommend', 'using', 'a', 'digital', 'clock', 'for', 'this', 'to', 'avoid', '\"', 'cheating', '\"', '.', '*', '*', 'what', 'this', 'does', 'is', 'it', 'takes', 'your', 'mind', 'off', 'the', 'clock', ',', 'and', 'shows', 'you', 'in', 'written', 'form', 'that', 'time', 'is', 'actually', 'passing', '.', 'you', \"'\", 'd', 'be', 'surprised', 'how', 'many', 'times', 'you', 'might', 'even', 'miss', 'your', 'five', '-', 'minute', 'mark', 'with', 'less', 'attention', 'on', 'the', 'time', '(', 'still', 'write', 'it', 'down', '!', 'you', \"'\", 'll', 'just', 'have', 'shorter', 'to', 'go', 'until', 'the', 'next', 'one', '.', ')', 'it', 'is', 'nice', 'halfway', 'through', 'class', ',', 'looking', 'back', 'at', 'the', '6', 'lines', 'i', \"'\", 've', 'written', 'in', '\"', 'the', 'countdown', '\"', 'and', 'think', '\"', 'wow', '!', 'i', \"'\", 've', 'already', 'made', 'it', 'through', 'all', 'that', '.', '\"', 'it', 'also', 'breaks', 'up', 'the', 'class', 'into', 'smaller', 'chunks', ',', 'so', 'i', 'don', \"'\", 't', 'need', 'to', 'worry', 'about', 'making', 'it', 'through', 'the', 'next', '60', 'minutes', ',', 'just', 'the', 'next', '5', ',', 'and', 'helps', 'me', 'to', 'focus', 'more', 'on', 'the', 'class', '.', 'id', '##k', 'if', 'this', 'is', 'for', 'everyone', ',', 'but', 'it', \"'\", 's', 'working', 'great', 'for', 'me', '!', 'if', 'you', 'try', 'it', ',', 'let', 'me', 'know', 'how', 'it', 'goes', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['trick', 'for', 'getting', 'through', 'insane', '##ly', 'boring', 'classes', '.', '[', '\"', 'the', 'countdown', '\"', ']']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['trick', 'for', 'getting', 'through', 'insane', '##ly', 'boring', 'classes', '.', '[', '\"', 'the', 'countdown', '\"', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['have', 'any', 'of', 'you', 'been', 'tested', 'for', 'celia', '##c', 'disease', 'or', 'a', 'g', '##lu', '##ten', 'into', '##ler', '##ance', '?', 'i', 'ask', 'because', 'my', 'little', 'guy', 'was', 'recently', 'diagnosed', 'with', 'ad', '##hd', '-', 'pi', ',', 'but', 'he', \"'\", 's', 'also', 'having', 'some', 'testing', 'done', 'because', 'he', \"'\", 's', 'su', '##u', '##u', '##up', '##er', 'teen', '##y', 'for', 'his', 'age', '.', 'while', 'reading', 'before', 'his', 'upcoming', 'exam', ',', 'i', 'keep', 'coming', 'across', 'articles', 'linking', 'celia', '##c', 'disease', 'to', 'ad', '##hd', '.', 'it', 'seems', 'that', 'there', 'is', 'a', 'correlation', 'between', 'g', '##lu', '##ten', 'into', '##ler', '##ance', 'and', 'symptoms', 'of', 'ad', '##hd', '.', 'have', 'any', 'of', 'you', 'been', 'diagnosed', 'with', 'both', '?', 'have', 'your', 'symptoms', 'improved', 'on', 'a', 'g', '##lu', '##ten', 'free', 'diet', '?']\n",
      "INFO:__main__:Number of tokens: 119\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['have', 'any', 'of', 'you', 'been', 'tested', 'for', 'celia', '##c', 'disease', 'or', 'a', 'g', '##lu', '##ten', 'into', '##ler', '##ance', '?', 'i', 'ask', 'because', 'my', 'little', 'guy', 'was', 'recently', 'diagnosed', 'with', 'ad', '##hd', '-', 'pi', ',', 'but', 'he', \"'\", 's', 'also', 'having', 'some', 'testing', 'done', 'because', 'he', \"'\", 's', 'su', '##u', '##u', '##up', '##er', 'teen', '##y', 'for', 'his', 'age', '.', 'while', 'reading', 'before', 'his', 'upcoming', 'exam', ',', 'i', 'keep', 'coming', 'across', 'articles', 'linking', 'celia', '##c', 'disease', 'to', 'ad', '##hd', '.', 'it', 'seems', 'that', 'there', 'is', 'a', 'correlation', 'between', 'g', '##lu', '##ten', 'into', '##ler', '##ance', 'and', 'symptoms', 'of', 'ad', '##hd', '.', 'have', 'any', 'of', 'you', 'been', 'diagnosed', 'with', 'both', '?', 'have', 'your', 'symptoms', 'improved', 'on', 'a', 'g', '##lu', '##ten', 'free', 'diet', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['dex', '##ed', '##rine', 'vs', '.', 'add', '##eral', '##l', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['dex', '##ed', '##rine', 'vs', '.', 'add', '##eral', '##l', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['iq', 'testing', '.', '.', '.', 'so', 'i', 'saw', 'a', 'psychiatrist', 'today', 'and', 'was', 'ref', '##fer', '##ed', 'to', 'a', 'place', 'to', 'get', 'add', 'testing', 'done', '.', 'along', 'with', 'this', 'she', 'wanted', 'me', 'to', 'get', 'an', 'iq', 'test', ',', 'but', 'the', 'other', 'place', 'she', 'gave', 'me', 'is', 'backed', 'up', 'for', 'months', 'and', 'the', 'place', 'im', 'going', 'for', 'the', 'add', 'testing', 'does', 'not', 'do', 'iq', 'test', ',', 'any', 'advice', 'on', 'where', 'to', 'go', 'to', 'get', 'an', 'iq', 'test', '?', 'im', 'from', 'a', 'suburb', 'of', 'saint', 'louis', 'missouri', '.']\n",
      "INFO:__main__:Number of tokens: 85\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['iq', 'testing', '.', '.', '.', 'so', 'i', 'saw', 'a', 'psychiatrist', 'today', 'and', 'was', 'ref', '##fer', '##ed', 'to', 'a', 'place', 'to', 'get', 'add', 'testing', 'done', '.', 'along', 'with', 'this', 'she', 'wanted', 'me', 'to', 'get', 'an', 'iq', 'test', ',', 'but', 'the', 'other', 'place', 'she', 'gave', 'me', 'is', 'backed', 'up', 'for', 'months', 'and', 'the', 'place', 'im', 'going', 'for', 'the', 'add', 'testing', 'does', 'not', 'do', 'iq', 'test', ',', 'any', 'advice', 'on', 'where', 'to', 'go', 'to', 'get', 'an', 'iq', 'test', '?', 'im', 'from', 'a', 'suburb', 'of', 'saint', 'louis', 'missouri', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['cutting', 'out', 'medication', 'and', 'all', 'other', 'drugs', 'after', 'being', 'on', 'something', 'for', 'twelve', 'years', '.', 'what', 'should', 'i', 'expect', '?', 'any', 'advice', '?', 'i', \"'\", 've', 'been', 'med', '##icated', 'for', 'add', 'without', 'hyper', '##act', '##ivity', 'since', 'i', 'was', 'in', 'the', 'fourth', 'grade', '.', 'i', \"'\", 'm', 'graduating', 'college', 'this', 'year', 'and', 'i', \"'\", 'm', 'doing', 'pretty', 'well', '.', 'while', 'i', 'experimented', 'with', 'fun', 'things', 'like', 'l', '##sd', ',', 'md', '##ma', ',', 'etc', ',', 'etc', ',', 'i', 'haven', \"'\", 't', 'touched', 'any', 'drugs', 'aside', 'from', 'cannabis', ',', 'the', 'occasional', 'drink', ',', 'and', 'my', 'prescribed', 'med', '##s', 'for', 'three', 'years', 'now', '.', 'i', \"'\", 'm', 'a', 'bit', 'dependent', 'on', 'cannabis', 'but', 'i', \"'\", 've', 'quit', 'for', 'weeks', '-', 'months', 'at', 'a', 'time', 'before', 'and', 'i', \"'\", 'm', 'not', 'terribly', 'concerned', 'about', 'making', 'myself', 'quit', 'again', '.', 'that', 'said', ',', 'i', \"'\", 'm', 'concerned', 'about', 'quit', '##ting', 'cannabis', 'and', 'my', 'prescribed', 'med', '##s', 'at', 'the', 'same', 'time', '.', 'but', 'i', 'feel', 'like', 'there', \"'\", 's', 'something', 'to', 'be', 'gained', 'by', 'doing', 'this', '.', 'i', 'don', \"'\", 't', 'feel', 'like', 'i', 'understand', 'how', 'my', 'brain', 'works', 'off', 'drugs', '.', 'i', 'feel', 'like', 'i', \"'\", 've', 'lived', 'my', 'whole', 'life', 'with', 'chemicals', 'forcing', 'me', 'into', 'a', 'certain', 'box', '.', 'i', 'feel', 'like', 'it', \"'\", 's', 'been', 'a', 'brute', 'force', 'push', 'on', 'my', 'brain', 'to', 'do', 'something', 'rather', 'than', 'a', 'genuine', 'negotiation', '.', 'so', 'i', \"'\", 've', 'told', 'myself', ':', 'after', 'i', 'graduate', 'college', ',', 'i', \"'\", 'm', 'going', 'to', 'take', 'a', 'few', 'months', 'off', 'of', 'all', 'drugs', 'and', 'try', 'to', 'better', 'understand', 'myself', '.', 'has', 'anyone', 'else', 'done', 'this', 'after', 'being', 'med', '##icated', 'for', 'so', 'long', '?', 'find', 'anything', 'out', '?', 'how', 'was', 'the', 'transition', '?', 't', '##l', ';', 'dr', ':', 'i', \"'\", 've', 'been', 'med', '##icated', 'my', 'whole', 'life', 'while', 'simultaneously', 'self', '-', 'med', '##ica', '##ting', 'myself', 'with', 'cannabis', '.', 'i', 'want', 'to', 'try', 'my', 'life', 'without', 'any', 'drugs', 'and', 'i', \"'\", 'm', 'not', 'sure', 'if', 'i', \"'\", 'm', 'capable', 'of', 'doing', 'it', 'successfully', '.', 'advice', '?']\n",
      "INFO:__main__:Number of tokens: 333\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['cutting', 'out', 'medication', 'and', 'all', 'other', 'drugs', 'after', 'being', 'on', 'something', 'for', 'twelve', 'years', '.', 'what', 'should', 'i', 'expect', '?', 'any', 'advice', '?', 'i', \"'\", 've', 'been', 'med', '##icated', 'for', 'add', 'without', 'hyper', '##act', '##ivity', 'since', 'i', 'was', 'in', 'the', 'fourth', 'grade', '.', 'i', \"'\", 'm', 'graduating', 'college', 'this', 'year', 'and', 'i', \"'\", 'm', 'doing', 'pretty', 'well', '.', 'while', 'i', 'experimented', 'with', 'fun', 'things', 'like', 'l', '##sd', ',', 'md', '##ma', ',', 'etc', ',', 'etc', ',', 'i', 'haven', \"'\", 't', 'touched', 'any', 'drugs', 'aside', 'from', 'cannabis', ',', 'the', 'occasional', 'drink', ',', 'and', 'my', 'prescribed', 'med', '##s', 'for', 'three', 'years', 'now', '.', 'i', \"'\", 'm', 'a', 'bit', 'dependent', 'on', 'cannabis', 'but', 'i', \"'\", 've', 'quit', 'for', 'weeks', '-', 'months', 'at', 'a', 'time', 'before', 'and', 'i', \"'\", 'm', 'not', 'terribly', 'concerned', 'about', 'making', 'myself', 'quit', 'again', '.', 'that', 'said', ',', 'i', \"'\", 'm', 'concerned', 'about', 'quit', '##ting', 'cannabis', 'and', 'my', 'prescribed', 'med', '##s', 'at', 'the', 'same', 'time', '.', 'but', 'i', 'feel', 'like', 'there', \"'\", 's', 'something', 'to', 'be', 'gained', 'by', 'doing', 'this', '.', 'i', 'don', \"'\", 't', 'feel', 'like', 'i', 'understand', 'how', 'my', 'brain', 'works', 'off', 'drugs', '.', 'i', 'feel', 'like', 'i', \"'\", 've', 'lived', 'my', 'whole', 'life', 'with', 'chemicals', 'forcing', 'me', 'into', 'a', 'certain', 'box', '.', 'i', 'feel', 'like', 'it', \"'\", 's', 'been', 'a', 'brute', 'force', 'push', 'on', 'my', 'brain', 'to', 'do', 'something', 'rather', 'than', 'a', 'genuine', 'negotiation', '.', 'so', 'i', \"'\", 've', 'told', 'myself', ':', 'after', 'i', 'graduate', 'college', ',', 'i', \"'\", 'm', 'going', 'to', 'take', 'a', 'few', 'months', 'off', 'of', 'all', 'drugs', 'and', 'try', 'to', 'better', 'understand', 'myself', '.', 'has', 'anyone', 'else', 'done', 'this', 'after', 'being', 'med', '##icated', 'for', 'so', 'long', '?', 'find', 'anything', 'out', '?', 'how', 'was', 'the', 'transition', '?', 't', '##l', ';', 'dr', ':', 'i', \"'\", 've', 'been', 'med', '##icated', 'my', 'whole', 'life', 'while', 'simultaneously', 'self', '-', 'med', '##ica', '##ting', 'myself', 'with', 'cannabis', '.', 'i', 'want', 'to', 'try', 'my', 'life', 'without', 'any', 'drugs', 'and', 'i', \"'\", 'm', 'not', 'sure', 'if', 'i', \"'\", 'm', 'capable', 'of', 'doing', 'it', 'successfully', '.', 'advice', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'rita', '##lin', 'going', 'to', 'change', 'my', 'life', '?']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'rita', '##lin', 'going', 'to', 'change', 'my', 'life', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'i', 'really', 'want', 'to', 'know', 'if', 'this', 'has', 'happened', 'to', 'my', 'fellow', 'ad', '##hd', 'brethren', '.', '.', '.', 'i', 'was', 'having', 'my', 'daily', 'shower', '-', 'brains', '##torm', 'tonight', 'and', 'i', 'came', 'up', 'with', 'an', 'awesome', 'idea', 'for', 'a', 'story', '.', 'now', ',', 'my', 'main', 'outlet', 'is', 'visual', 'art', ',', 'so', 'the', 'idea', 'to', 'write', 'seemed', 'to', 'be', 'an', 'interesting', 'adventure', '.', 'soon', ',', 'i', 'began', 'writing', 'out', 'some', 'outlines', ',', 'basic', 'ideas', ',', 'but', 'when', 'i', 'began', 'researching', 'some', 'physical', 'disorders', '(', 'the', 'main', 'char', '##ect', '##er', 'was', 'supposed', 'to', 'be', 'a', '\"', 'monster', '\"', ',', 'but', 'i', 'dig', '##ress', ')', 'i', 'just', 'decided', 'i', 'wasn', \"'\", 't', 'interested', 'in', 'writing', 'anymore', '.', 'now', ',', 'this', 'isn', \"'\", 't', 'the', 'first', 'occur', '##ence', 'of', 'this', 'sort', 'of', 'thing', ',', 'it', 'reaches', 'into', 'my', 'art', ',', 'my', 'studies', ',', 'my', 'recreation', '##s', ',', 'basically', 'everything', '.', 'do', 'any', 'of', 'you', 'have', 'this', 'problem', '?', 'or', 'a', 'way', 'of', 'dealing', 'with', 'it', '?', 'also', ',', 'i', \"'\", 'm', 'a', '17', 'year', 'old', 'girl', ',', 'and', 'have', 'been', 'on', 'concert', '##a', 'since', 'i', 'was', '6', ',', 'if', 'that', 'provides', 'any', 'insight', '.']\n",
      "INFO:__main__:Number of tokens: 190\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', 'i', 'really', 'want', 'to', 'know', 'if', 'this', 'has', 'happened', 'to', 'my', 'fellow', 'ad', '##hd', 'brethren', '.', '.', '.', 'i', 'was', 'having', 'my', 'daily', 'shower', '-', 'brains', '##torm', 'tonight', 'and', 'i', 'came', 'up', 'with', 'an', 'awesome', 'idea', 'for', 'a', 'story', '.', 'now', ',', 'my', 'main', 'outlet', 'is', 'visual', 'art', ',', 'so', 'the', 'idea', 'to', 'write', 'seemed', 'to', 'be', 'an', 'interesting', 'adventure', '.', 'soon', ',', 'i', 'began', 'writing', 'out', 'some', 'outlines', ',', 'basic', 'ideas', ',', 'but', 'when', 'i', 'began', 'researching', 'some', 'physical', 'disorders', '(', 'the', 'main', 'char', '##ect', '##er', 'was', 'supposed', 'to', 'be', 'a', '\"', 'monster', '\"', ',', 'but', 'i', 'dig', '##ress', ')', 'i', 'just', 'decided', 'i', 'wasn', \"'\", 't', 'interested', 'in', 'writing', 'anymore', '.', 'now', ',', 'this', 'isn', \"'\", 't', 'the', 'first', 'occur', '##ence', 'of', 'this', 'sort', 'of', 'thing', ',', 'it', 'reaches', 'into', 'my', 'art', ',', 'my', 'studies', ',', 'my', 'recreation', '##s', ',', 'basically', 'everything', '.', 'do', 'any', 'of', 'you', 'have', 'this', 'problem', '?', 'or', 'a', 'way', 'of', 'dealing', 'with', 'it', '?', 'also', ',', 'i', \"'\", 'm', 'a', '17', 'year', 'old', 'girl', ',', 'and', 'have', 'been', 'on', 'concert', '##a', 'since', 'i', 'was', '6', ',', 'if', 'that', 'provides', 'any', 'insight', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['have', 'any', 'of', 'you', 'experimented', 'with', 'marijuana', 'as', 'a', 'form', 'of', 'medication', '?', 'there', 'was', 'a', 'usc', 'study', 'a', 'while', 'back', 'that', 'claimed', 'that', 'marijuana', 'acted', 'as', 'a', 'perfect', 'substitute', 'for', 'rita', '##lin', ',', 'so', 'i', 'was', 'wondering', 'if', 'anyone', 'has', 'had', 'success', 'with', 'using', 'marijuana', 'as', 'a', 'treatment', '.', 'i', 'couldn', \"'\", 't', 'find', 'the', 'original', 'study', ',', 'but', 'there', 'are', 'many', 'references', 'to', 'it', ',', 'as', 'well', 'as', 'a', 'video', 'here', ':', '[', 'http', ':', '/', '/', 'www', '.', 'oregon', '##med', '##ical', '##mar', '##ij', '##uan', '##ap', '##ro', '##gram', '.', 'com', '/', 'add', '##ad', '##hd', ']', '(', 'http', ':', '/', '/', 'www', '.', 'oregon', '##med', '##ical', '##mar', '##ij', '##uan', '##ap', '##ro', '##gram', '.', 'com', '/', 'add', '##ad', '##hd', ')', 'edit', ':', 'this', 'is', 'a', 'legitimate', 'question', 'about', 'whether', 'or', 'not', 'people', 'have', 'had', 'positive', 'experiences', 'with', 'it', '.', 'i', 'would', 'appreciate', 'it', 'if', 'you', 'didn', \"'\", 't', 'down', '##vot', '##e', 'just', 'because', 'you', 'are', 'against', 'marijuana', '.']\n",
      "INFO:__main__:Number of tokens: 158\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['have', 'any', 'of', 'you', 'experimented', 'with', 'marijuana', 'as', 'a', 'form', 'of', 'medication', '?', 'there', 'was', 'a', 'usc', 'study', 'a', 'while', 'back', 'that', 'claimed', 'that', 'marijuana', 'acted', 'as', 'a', 'perfect', 'substitute', 'for', 'rita', '##lin', ',', 'so', 'i', 'was', 'wondering', 'if', 'anyone', 'has', 'had', 'success', 'with', 'using', 'marijuana', 'as', 'a', 'treatment', '.', 'i', 'couldn', \"'\", 't', 'find', 'the', 'original', 'study', ',', 'but', 'there', 'are', 'many', 'references', 'to', 'it', ',', 'as', 'well', 'as', 'a', 'video', 'here', ':', '[', 'http', ':', '/', '/', 'www', '.', 'oregon', '##med', '##ical', '##mar', '##ij', '##uan', '##ap', '##ro', '##gram', '.', 'com', '/', 'add', '##ad', '##hd', ']', '(', 'http', ':', '/', '/', 'www', '.', 'oregon', '##med', '##ical', '##mar', '##ij', '##uan', '##ap', '##ro', '##gram', '.', 'com', '/', 'add', '##ad', '##hd', ')', 'edit', ':', 'this', 'is', 'a', 'legitimate', 'question', 'about', 'whether', 'or', 'not', 'people', 'have', 'had', 'positive', 'experiences', 'with', 'it', '.', 'i', 'would', 'appreciate', 'it', 'if', 'you', 'didn', \"'\", 't', 'down', '##vot', '##e', 'just', 'because', 'you', 'are', 'against', 'marijuana', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ra', '##dd', '##it', ',', 'will', 'pills', 'help', 'me', 'focus', 'on', 'long', 'term', 'goals', '?', 'this', 'is', 'the', 'thing', 'that', 'i', 'dislike', 'most', 'about', 'my', 'ad', '##hd', '.', 'sure', ',', 'i', \"'\", 'm', 'a', 'little', 'un', '##fo', '##cus', '##ed', 'and', 'confused', 'at', 'times', ',', 'and', 'i', 'do', 'pro', '##cr', '##ast', '##inate', ',', 'but', 'those', 'are', 'tri', '##fles', 'compared', 'to', 'my', 'seeming', 'inability', 'to', 'focus', 'and', 'follow', 'through', 'on', 'a', 'goal', 'that', 'is', 'longer', 'than', 'a', 'week', 'away', '.', 'especially', 'one', 'that', 'requires', 'daily', 'practice', 'such', 'as', 'guitar', 'and', 'tennis', '.', 'i', \"'\", 'm', 'going', 'find', 'the', 'right', 'pill', 'for', 'me', 'this', 'summer', ',', 'but', 'i', 'want', 'to', 'know', 'if', 'it', 'will', 'help', 'me', 'make', 'the', 'mental', 'connection', 'between', 'what', 'i', 'do', 'today', ',', 'and', 'what', 'i', 'want', 'for', 'the', 'next', '3', 'months', ',', 'or', 'is', 'this', 'something', 'i', \"'\", 'm', 'going', 'to', 'have', 'to', 'figure', 'out', 'how', 'to', 'develop', 'a', 'coping', 'mechanism', 'for', '?']\n",
      "INFO:__main__:Number of tokens: 153\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ra', '##dd', '##it', ',', 'will', 'pills', 'help', 'me', 'focus', 'on', 'long', 'term', 'goals', '?', 'this', 'is', 'the', 'thing', 'that', 'i', 'dislike', 'most', 'about', 'my', 'ad', '##hd', '.', 'sure', ',', 'i', \"'\", 'm', 'a', 'little', 'un', '##fo', '##cus', '##ed', 'and', 'confused', 'at', 'times', ',', 'and', 'i', 'do', 'pro', '##cr', '##ast', '##inate', ',', 'but', 'those', 'are', 'tri', '##fles', 'compared', 'to', 'my', 'seeming', 'inability', 'to', 'focus', 'and', 'follow', 'through', 'on', 'a', 'goal', 'that', 'is', 'longer', 'than', 'a', 'week', 'away', '.', 'especially', 'one', 'that', 'requires', 'daily', 'practice', 'such', 'as', 'guitar', 'and', 'tennis', '.', 'i', \"'\", 'm', 'going', 'find', 'the', 'right', 'pill', 'for', 'me', 'this', 'summer', ',', 'but', 'i', 'want', 'to', 'know', 'if', 'it', 'will', 'help', 'me', 'make', 'the', 'mental', 'connection', 'between', 'what', 'i', 'do', 'today', ',', 'and', 'what', 'i', 'want', 'for', 'the', 'next', '3', 'months', ',', 'or', 'is', 'this', 'something', 'i', \"'\", 'm', 'going', 'to', 'have', 'to', 'figure', 'out', 'how', 'to', 'develop', 'a', 'coping', 'mechanism', 'for', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'you', 'guys', 'deal', 'with', 'add', '##eral', '##l', 'come', '##down', '?']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'you', 'guys', 'deal', 'with', 'add', '##eral', '##l', 'come', '##down', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['never', '##mind', '!', '!', '!', 'i', 'was', 'about', 'to', 'post', 'a', '\"', 'does', 'anyone', 'else', '\"', 'type', 'question', 'here', 'about', 'my', 'ad', '##hd', '.', 'however', ',', 'i', 'should', 'be', 'writing', 'a', 'paper', '.', 'so', 'never', '##mind', '.', 'i', \"'\", 'm', 'not', 'the', 'only', 'one', ',', 'though', '.', '.', '.', 'you', '!', 'yes', ',', 'you', '.', 'i', 'saw', 'your', 'eye', 'twitch', '.', 'you', 'took', 'your', 'med', '##s', 'to', 'get', 'that', 'paper', '/', 'project', 'done', 'and', 'now', 'your', 'uber', '-', 'brows', '##ing', 'red', '##dit', '.', 'stop', '.', 'alright', 'i', \"'\", 'm', 'going', 'now', '.']\n",
      "INFO:__main__:Number of tokens: 92\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['never', '##mind', '!', '!', '!', 'i', 'was', 'about', 'to', 'post', 'a', '\"', 'does', 'anyone', 'else', '\"', 'type', 'question', 'here', 'about', 'my', 'ad', '##hd', '.', 'however', ',', 'i', 'should', 'be', 'writing', 'a', 'paper', '.', 'so', 'never', '##mind', '.', 'i', \"'\", 'm', 'not', 'the', 'only', 'one', ',', 'though', '.', '.', '.', 'you', '!', 'yes', ',', 'you', '.', 'i', 'saw', 'your', 'eye', 'twitch', '.', 'you', 'took', 'your', 'med', '##s', 'to', 'get', 'that', 'paper', '/', 'project', 'done', 'and', 'now', 'your', 'uber', '-', 'brows', '##ing', 'red', '##dit', '.', 'stop', '.', 'alright', 'i', \"'\", 'm', 'going', 'now', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['that', 'moment', 'when', 'you', 'get', 'all', 'the', 'way', 'to', 'work', '/', 'school', 'and', 'realize', 'you', 'left', 'your', 'medicine', 'at', 'home', 'and', 'you', 'are', 'like', 'that', \"'\", 's', 'why', 'i', \"'\", 'm', 'so', 'hungry', '.', '#', 'fail']\n",
      "INFO:__main__:Number of tokens: 36\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['that', 'moment', 'when', 'you', 'get', 'all', 'the', 'way', 'to', 'work', '/', 'school', 'and', 'realize', 'you', 'left', 'your', 'medicine', 'at', 'home', 'and', 'you', 'are', 'like', 'that', \"'\", 's', 'why', 'i', \"'\", 'm', 'so', 'hungry', '.', '#', 'fail']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['control', 'technique', ':', '\"', 'reverse', 'distraction', '\"', 'this', 'is', 'one', 'of', 'the', 'tricks', 'i', 'use', 'to', 'help', 'control', 'my', 'ad', '##hd', 'i', 'hope', 'it', 'helps', 'someone', 'else', 'too', '.', 'when', 'you', 'are', 'doing', 'something', 'that', 'you', 'know', 'you', 'will', 'lose', 'attention', 'and', 'get', 'distracted', 'try', 'setting', 'up', 'your', 'own', 'distraction', 'and', 'concentrating', 'on', 'that', '.', 'what', 'i', 'find', 'happens', 'is', 'that', 'my', 'attention', 'wander', '##s', 'from', 'the', 'distraction', 'towards', 'the', 'thing', 'i', 'really', 'am', 'supposed', 'to', 'concentrate', 'on', '.', 'test', 'it', 'with', 'a', 'friend', ',', 'have', 'them', 'talk', 'to', 'you', 'about', 'so', '##em', '##thing', 'that', 'normally', 'spaces', 'you', 'out', 'and', 'see', 'how', 'long', 'you', 'can', 'actually', 'pay', 'attention', 'to', 'them', 'while', 'keeping', 'something', 'else', 'as', 'your', 'su', '##pp', '##osa', '##d', 'main', 'focus', '.']\n",
      "INFO:__main__:Number of tokens: 123\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['control', 'technique', ':', '\"', 'reverse', 'distraction', '\"', 'this', 'is', 'one', 'of', 'the', 'tricks', 'i', 'use', 'to', 'help', 'control', 'my', 'ad', '##hd', 'i', 'hope', 'it', 'helps', 'someone', 'else', 'too', '.', 'when', 'you', 'are', 'doing', 'something', 'that', 'you', 'know', 'you', 'will', 'lose', 'attention', 'and', 'get', 'distracted', 'try', 'setting', 'up', 'your', 'own', 'distraction', 'and', 'concentrating', 'on', 'that', '.', 'what', 'i', 'find', 'happens', 'is', 'that', 'my', 'attention', 'wander', '##s', 'from', 'the', 'distraction', 'towards', 'the', 'thing', 'i', 'really', 'am', 'supposed', 'to', 'concentrate', 'on', '.', 'test', 'it', 'with', 'a', 'friend', ',', 'have', 'them', 'talk', 'to', 'you', 'about', 'so', '##em', '##thing', 'that', 'normally', 'spaces', 'you', 'out', 'and', 'see', 'how', 'long', 'you', 'can', 'actually', 'pay', 'attention', 'to', 'them', 'while', 'keeping', 'something', 'else', 'as', 'your', 'su', '##pp', '##osa', '##d', 'main', 'focus', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', \"'\", 're', 'proud', 'of', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '!', 'remember', 'the', 'good', '!', '[', 'week', '6', ']', '*', '*', 'welcome', 'to', 'the', '6th', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', 'so', 'here', 'is', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', '*', 'if', 'you', 'can', \"'\", 't', 'think', 'of', 'any', '\"', 'wins', '\"', 'you', 'may', 'put', 'something', 'you', 'are', 'grateful', 'for', '.', 'we', 'all', 'can', 'express', 'some', 'gratitude', '.', '*', '*', '*', '*', '*', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', '*', '*', '*', 'some', 'examples', 'from', 'past', 'weeks', '[UNK]', '*', '*', 'started', 'taking', 'ad', '##hd', 'medication', '*', '*', 'x', '##2', '[UNK]', '*', '*', 'called', 'doctor', '*', '*', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '[UNK]', '*', '*', 'working', 'out', '*', '*', 'and', 'eating', 'a', 'healthy', 'diet', '[UNK]', 'got', 'to', 'work', '.', '.', '.', '*', '*', 'on', 'time', '*', '*', '!', '[UNK]', 'went', 'to', '*', '*', 'sleep', 'by', '2a', '##m', '*', '*', 'for', '6', 'nights', '[UNK]', 'finally', 'got', 'a', '*', '*', 'diagnosis', '*', '*', '(', 'a', 'few', 'people', ')', '*', '*', 'very', 'awesome', '*', '*', '[UNK]', 'started', '*', '*', 'working', 'out', '*', '*', 'again', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', '*', '*', '*', 'my', 'wins', '*', 'posted', 'this', 'thread', 'before', 'work', 'despite', '2', 'hours', 'of', 'sleep', '!', '(', 'i', 'say', 'this', 'every', 'week', 'but', 'i', 'missed', 'last', 'week', '.', ')', '*', 'cleaned', ',', 'vacuum', '##ed', ',', 'and', 'dust', '##ed', 'my', 'entire', 'desk', 'area', '.', 'removed', 'everything', '.', '*', 'even', 'though', 'i', 'could', 'not', 'sleep', 'last', 'night', 'i', 'av', '##ode', '##d', 'negative', 'self', '-', 'talk', 'and', 'beating', 'myself', 'up', '.', 'accepting', 'a', 'sleep', '##less', 'night', 'is', 'so', 'much', 'less', 'draining', '!', '*', 'i', 'made', 'a', 'few', 'cs', '##s', 'changes', 'in', '/', 'r', '/', 'ad', '##hd', 'this', 'week', '.', 'added', 'colored', 'flair', '.', 'added', 'side', '##bar', '.', 'created', 'some', 'ways', 'for', 'the', 'mod', '##s', 'to', 'communicate', 'more', 'efficiently', '.', '*', 'i', 'noticed', 'that', 'drinking', 'a', 'small', 'amount', 'of', 'coffee', 'seems', 'to', 'really', 'help', 'my', 'add', '##eral', '##l', 'work', '.', 'at', 'least', 'it', 'has', 'the', 'past', 'couple', 'days', '.']\n",
      "INFO:__main__:Number of tokens: 500\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', \"'\", 're', 'proud', 'of', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '!', 'remember', 'the', 'good', '!', '[', 'week', '6', ']', '*', '*', 'welcome', 'to', 'the', '6th', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', 'so', 'here', 'is', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', '*', 'if', 'you', 'can', \"'\", 't', 'think', 'of', 'any', '\"', 'wins', '\"', 'you', 'may', 'put', 'something', 'you', 'are', 'grateful', 'for', '.', 'we', 'all', 'can', 'express', 'some', 'gratitude', '.', '*', '*', '*', '*', '*', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', '*', '*', '*', 'some', 'examples', 'from', 'past', 'weeks', '[UNK]', '*', '*', 'started', 'taking', 'ad', '##hd', 'medication', '*', '*', 'x', '##2', '[UNK]', '*', '*', 'called', 'doctor', '*', '*', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '[UNK]', '*', '*', 'working', 'out', '*', '*', 'and', 'eating', 'a', 'healthy', 'diet', '[UNK]', 'got', 'to', 'work', '.', '.', '.', '*', '*', 'on', 'time', '*', '*', '!', '[UNK]', 'went', 'to', '*', '*', 'sleep', 'by', '2a', '##m', '*', '*', 'for', '6', 'nights', '[UNK]', 'finally', 'got', 'a', '*', '*', 'diagnosis', '*', '*', '(', 'a', 'few', 'people', ')', '*', '*', 'very', 'awesome', '*', '*', '[UNK]', 'started', '*', '*', 'working', 'out', '*', '*', 'again', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', '*', '*', '*', 'my', 'wins', '*', 'posted', 'this', 'thread', 'before', 'work', 'despite', '2', 'hours', 'of', 'sleep', '!', '(', 'i', 'say', 'this', 'every', 'week', 'but', 'i', 'missed', 'last', 'week', '.', ')', '*', 'cleaned', ',', 'vacuum', '##ed', ',', 'and', 'dust', '##ed', 'my', 'entire', 'desk', 'area', '.', 'removed', 'everything', '.', '*', 'even', 'though', 'i', 'could', 'not', 'sleep', 'last', 'night', 'i', 'av', '##ode', '##d', 'negative', 'self', '-', 'talk', 'and', 'beating', 'myself', 'up', '.', 'accepting', 'a', 'sleep', '##less', 'night', 'is', 'so', 'much', 'less', 'draining', '!', '*', 'i', 'made', 'a', 'few', 'cs', '##s', 'changes', 'in', '/', 'r', '/', 'ad', '##hd', 'this', 'week', '.', 'added', 'colored', 'flair', '.', 'added', 'side', '##bar', '.', 'created', 'some', 'ways', 'for', 'the', 'mod', '##s', 'to', 'communicate', 'more', 'efficiently', '.', '*', 'i', 'noticed', 'that', 'drinking', 'a', 'small', 'amount', 'of', 'coffee', 'seems', 'to', 'really', 'help', 'my', 'add', '##eral', '##l', 'work', '.', 'at', 'least', 'it', 'has', 'the', 'past', 'couple', 'days', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', \"'\", 'm', 'not', 'even', 'motivated', 'by', 'last', 'minute', 'pressure', 'anymore', '.', 'so', 'now', 'i', 'just', 'do', 'no', 'work', '.']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', \"'\", 'm', 'not', 'even', 'motivated', 'by', 'last', 'minute', 'pressure', 'anymore', '.', 'so', 'now', 'i', 'just', 'do', 'no', 'work', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'i', 'just', 'started', 'with', 'an', 'ad', '##hd', 'coach', 'yesterday', '.', '.', '.', 'so', 'i', \"'\", 'm', 'not', 'really', 'sure', 'what', 'to', 'expect', '.', 'the', 'first', 'session', 'was', 'just', 'getting', 'to', 'know', 'each', 'other', 'i', 'guess', ',', 'and', 'she', 'seems', 'very', 'nice', ',', 'understanding', ',', 'and', 'informed', '.', 'what', 'should', 'i', 'expect', '?', 'anyone', 'else', 'out', 'there', 'seeing', 'a', 'coach', '?', 'what', 'should', 'i', 'bring', 'to', 'the', 'table', '?', 'i', \"'\", 've', 'been', 'taking', 'notes', 'on', 'my', 'day', 'to', 'day', 'life', 'for', 'her', '.', '.', '.', '.', 'i', 'guess', 'that', \"'\", 's', 'okay', '.', '.', '.', '.', 't', '##l', ';', 'dr', 'ad', '##hd', 'coaching', ',', 'what', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 108\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', 'i', 'just', 'started', 'with', 'an', 'ad', '##hd', 'coach', 'yesterday', '.', '.', '.', 'so', 'i', \"'\", 'm', 'not', 'really', 'sure', 'what', 'to', 'expect', '.', 'the', 'first', 'session', 'was', 'just', 'getting', 'to', 'know', 'each', 'other', 'i', 'guess', ',', 'and', 'she', 'seems', 'very', 'nice', ',', 'understanding', ',', 'and', 'informed', '.', 'what', 'should', 'i', 'expect', '?', 'anyone', 'else', 'out', 'there', 'seeing', 'a', 'coach', '?', 'what', 'should', 'i', 'bring', 'to', 'the', 'table', '?', 'i', \"'\", 've', 'been', 'taking', 'notes', 'on', 'my', 'day', 'to', 'day', 'life', 'for', 'her', '.', '.', '.', '.', 'i', 'guess', 'that', \"'\", 's', 'okay', '.', '.', '.', '.', 't', '##l', ';', 'dr', 'ad', '##hd', 'coaching', ',', 'what', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['returning', 'to', 'add', '##eral', '##l', 'after', 'only', 'very', 'in', '##fr', '##e', '##quent', 'use', 'since', '2009', '.', 'had', 'to', 'go', 'to', '5', 'different', 'ph', '##arm', '##acies', 'to', 'find', 'any', 'in', 'stock', '.', 'back', 'on', 'add', '##eral', '##l', '.', 'in', 'the', 'last', 'four', 'days', 'i', 'have', ':', 'pro', '##s', ':', '*', 'created', 'a', 'budget', 'for', 'my', 'shop', '.', '*', 'created', 'a', 'budget', 'for', 'my', 'home', '.', '*', 'set', 'up', 'payment', 'schedules', 'for', 'past', 'due', 'debts', '.', '*', 'scratched', 'out', '8', 'things', 'off', 'a', 'list', 'almost', 'a', 'month', 'old', '.', '*', 'began', 'using', 'my', 'actual', 'calendar', 'again', 'for', 'scheduling', 'app', '##ts', '.', '*', '~', '~', 'quit', 'drinking', 'alcohol', '.', '~', '~', 'no', ',', 'quit', 'craving', 'alcohol', '.', '*', 'quit', 'over', '##ea', '##ting', '.', 'eat', 'till', 'i', \"'\", 'm', 'full', ',', 'then', 'i', 'stop', '.', '*', 'quit', 'caf', '##fe', '##ine', '.', '*', 'haven', \"'\", 't', 'had', 'one', 'single', 'impulse', 'purchase', '.', 'example', ':', 'oh', 'look', ',', 'the', 'new', 'o', '##cz', 'vertex', '4', 'ss', '##d', 'is', 'out', 'and', 'only', '$', '10', 'more', 'expensive', 'then', 'the', 'vertex', '3', '.', 'i', 'must', 'have', 'it', '!', 'no', '.', '*', 'i', \"'\", 've', 'begun', 'writing', 'letters', 'to', 'my', 'not', 'yet', '2', 'year', 'old', 'son', '.', '*', 'went', 'to', 'the', 'doctor', 'and', 'got', 'a', 'physical', '.', 'need', 'to', 'lose', '50', 'pounds', 'and', 'get', 'in', 'shape', '.', '*', 'learned', 'how', 'to', 'use', 'this', 'ni', '##ft', '##y', 'bullet', 'feature', 'on', 'red', '##dit', '!', 'con', '##s', ':', '*', 'haven', \"'\", 't', 'gone', 'to', 'sleep', 'before', '3a', '##m', 'yet', '.', '*', 'stress', 'level', 'has', 'escalated', 'since', 'now', 'i', 'can', 'see', 'all', 'the', 'stuff', 'i', \"'\", 've', 'been', 'putting', 'off', 'and', 'have', 'become', 'overwhelmed', 'by', 'how', 'much', 'stuff', 'i', 'have', 'still', 'yet', 'to', 'do', '.', '*', 'for', 'some', 'reason', ',', 'i', \"'\", 'm', 'really', 'horn', '##y', '.', 'w', '##tf', 'brain', '?', 'other', ':', '*', 'dove', 'back', 'into', 'world', 'of', 'war', '##craft', 'with', 'the', 'hyper', '##fo', '##cus', 'only', 'someone', 'with', 'ad', '##hd', 'can', 'achieve', '.', '(', 'pro', '?', 'con', '?', 'time', 'spent', 'enjoying', 'one', \"'\", 's', 'self', 'is', 'never', 'time', 'wasted', '.', ')', '*', 'i', 'get', 'really', 'chat', '##ty', 'when', 'i', \"'\", 'm', 'on', 'my', 'med', '##s', '.', 'suddenly', 'all', 'my', 'thoughts', 'are', 'ducks', 'in', 'a', 'row', 'and', 'i', 'just', 'want', 'to', 'get', 'them', 'all', 'out', '.', 'this', 'can', 'also', 'be', 'considered', 'a', 'pro', '.', 'started', 'a', 'conversation', 'with', 'a', 'random', 'person', 'while', 'waiting', 'to', 'purchase', 'ammunition', 'at', 'wal', '##mart', 'and', 'wound', 'up', 'getting', 'a', 'new', 'customer', 'at', 'my', 'shop', 'because', 'of', 'it', '.', 'what', 'sorts', 'of', 'things', 'has', 'your', 'medication', 'done', 'for', 'you', '?']\n",
      "INFO:__main__:Number of tokens: 419\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['returning', 'to', 'add', '##eral', '##l', 'after', 'only', 'very', 'in', '##fr', '##e', '##quent', 'use', 'since', '2009', '.', 'had', 'to', 'go', 'to', '5', 'different', 'ph', '##arm', '##acies', 'to', 'find', 'any', 'in', 'stock', '.', 'back', 'on', 'add', '##eral', '##l', '.', 'in', 'the', 'last', 'four', 'days', 'i', 'have', ':', 'pro', '##s', ':', '*', 'created', 'a', 'budget', 'for', 'my', 'shop', '.', '*', 'created', 'a', 'budget', 'for', 'my', 'home', '.', '*', 'set', 'up', 'payment', 'schedules', 'for', 'past', 'due', 'debts', '.', '*', 'scratched', 'out', '8', 'things', 'off', 'a', 'list', 'almost', 'a', 'month', 'old', '.', '*', 'began', 'using', 'my', 'actual', 'calendar', 'again', 'for', 'scheduling', 'app', '##ts', '.', '*', '~', '~', 'quit', 'drinking', 'alcohol', '.', '~', '~', 'no', ',', 'quit', 'craving', 'alcohol', '.', '*', 'quit', 'over', '##ea', '##ting', '.', 'eat', 'till', 'i', \"'\", 'm', 'full', ',', 'then', 'i', 'stop', '.', '*', 'quit', 'caf', '##fe', '##ine', '.', '*', 'haven', \"'\", 't', 'had', 'one', 'single', 'impulse', 'purchase', '.', 'example', ':', 'oh', 'look', ',', 'the', 'new', 'o', '##cz', 'vertex', '4', 'ss', '##d', 'is', 'out', 'and', 'only', '$', '10', 'more', 'expensive', 'then', 'the', 'vertex', '3', '.', 'i', 'must', 'have', 'it', '!', 'no', '.', '*', 'i', \"'\", 've', 'begun', 'writing', 'letters', 'to', 'my', 'not', 'yet', '2', 'year', 'old', 'son', '.', '*', 'went', 'to', 'the', 'doctor', 'and', 'got', 'a', 'physical', '.', 'need', 'to', 'lose', '50', 'pounds', 'and', 'get', 'in', 'shape', '.', '*', 'learned', 'how', 'to', 'use', 'this', 'ni', '##ft', '##y', 'bullet', 'feature', 'on', 'red', '##dit', '!', 'con', '##s', ':', '*', 'haven', \"'\", 't', 'gone', 'to', 'sleep', 'before', '3a', '##m', 'yet', '.', '*', 'stress', 'level', 'has', 'escalated', 'since', 'now', 'i', 'can', 'see', 'all', 'the', 'stuff', 'i', \"'\", 've', 'been', 'putting', 'off', 'and', 'have', 'become', 'overwhelmed', 'by', 'how', 'much', 'stuff', 'i', 'have', 'still', 'yet', 'to', 'do', '.', '*', 'for', 'some', 'reason', ',', 'i', \"'\", 'm', 'really', 'horn', '##y', '.', 'w', '##tf', 'brain', '?', 'other', ':', '*', 'dove', 'back', 'into', 'world', 'of', 'war', '##craft', 'with', 'the', 'hyper', '##fo', '##cus', 'only', 'someone', 'with', 'ad', '##hd', 'can', 'achieve', '.', '(', 'pro', '?', 'con', '?', 'time', 'spent', 'enjoying', 'one', \"'\", 's', 'self', 'is', 'never', 'time', 'wasted', '.', ')', '*', 'i', 'get', 'really', 'chat', '##ty', 'when', 'i', \"'\", 'm', 'on', 'my', 'med', '##s', '.', 'suddenly', 'all', 'my', 'thoughts', 'are', 'ducks', 'in', 'a', 'row', 'and', 'i', 'just', 'want', 'to', 'get', 'them', 'all', 'out', '.', 'this', 'can', 'also', 'be', 'considered', 'a', 'pro', '.', 'started', 'a', 'conversation', 'with', 'a', 'random', 'person', 'while', 'waiting', 'to', 'purchase', 'ammunition', 'at', 'wal', '##mart', 'and', 'wound', 'up', 'getting', 'a', 'new', 'customer', 'at', 'my', 'shop', 'because', 'of', 'it', '.', 'what', 'sorts', 'of', 'things', 'has', 'your', 'medication', 'done', 'for', 'you', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['saw', 'this', 'one', 'on', 'advice', 'animals', ',', 'thought', 'it', 'belonged', 'here', '.']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['saw', 'this', 'one', 'on', 'advice', 'animals', ',', 'thought', 'it', 'belonged', 'here', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'two', 'days', 'on', 'add', '##eral', '##l', ':', 'the', 'experience']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', 'two', 'days', 'on', 'add', '##eral', '##l', ':', 'the', 'experience']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['has', 'anyone', 'taken', 'v', '##y', '##van', '##se', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['has', 'anyone', 'taken', 'v', '##y', '##van', '##se', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'i', 'started', 'taking', 'concert', '##a', 'again', 'and', 'my', 'eyes', 'feel', 'like', 'they', \"'\", 're', 'covered', 'in', 'desert', 'sand', '.', 'sound', 'familiar', '?', 'should', 'i', 'use', 'them', 'eye', '-', 'watering', 'thing', '##ies', 'one', 'uses', 'when', 'wearing', 'contacts', '?', 'also', ',', 'i', \"'\", 've', 'used', 'concert', '##a', 'regularly', 'until', 'about', 'two', 'years', 'ago', '.', 'in', 'that', 'regular', 'period', 'i', 'formed', 'a', 'little', 'red', 'bulge', 'next', 'to', 'my', 'iris', '.', 'it', \"'\", 's', 'kind', 'of', 'annoying', 'and', 'feels', 'really', 'crap', '##py', 'now', '.', 'does', 'anyone', 'else', 'have', 'that', 'and', 'does', 'anyone', 'know', 'if', 'something', 'like', 'that', 'can', 'manifest', 'because', 'of', 'long', 'term', 'dry', 'eyes', '?']\n",
      "INFO:__main__:Number of tokens: 103\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', 'i', 'started', 'taking', 'concert', '##a', 'again', 'and', 'my', 'eyes', 'feel', 'like', 'they', \"'\", 're', 'covered', 'in', 'desert', 'sand', '.', 'sound', 'familiar', '?', 'should', 'i', 'use', 'them', 'eye', '-', 'watering', 'thing', '##ies', 'one', 'uses', 'when', 'wearing', 'contacts', '?', 'also', ',', 'i', \"'\", 've', 'used', 'concert', '##a', 'regularly', 'until', 'about', 'two', 'years', 'ago', '.', 'in', 'that', 'regular', 'period', 'i', 'formed', 'a', 'little', 'red', 'bulge', 'next', 'to', 'my', 'iris', '.', 'it', \"'\", 's', 'kind', 'of', 'annoying', 'and', 'feels', 'really', 'crap', '##py', 'now', '.', 'does', 'anyone', 'else', 'have', 'that', 'and', 'does', 'anyone', 'know', 'if', 'something', 'like', 'that', 'can', 'manifest', 'because', 'of', 'long', 'term', 'dry', 'eyes', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'advice', 'for', 'test', 'taking', ',', 'particularly', 'multiple', 'choice', 'i', 'am', 'good', 'at', 'taking', 'tests', 'that', 'you', 'can', 'explain', 'your', 'self', 'in', 'a', 'rational', 'manner', 'and', 'show', 'reasoning', 'but', 'for', 'some', 'reason', 'when', 'it', 'comes', 'to', 'multiple', 'choice', 'i', 'freeze', 'up', 'and', 'get', 'really', 'bad', 'test', 'anxiety', 'and', 'fail', 'horribly', '.', 'i', 'have', 'an', 'accounting', 'final', 'next', 'thursday', 'that', 'is', 'all', 'multiple', 'choice', ',', 'we', 'are', 'given', 'a', 'note', 'card', 'and', '2', 'hours', 'to', 'complete', 'the', '11', 'chapter', 'comprehensive', 'final', '.', 'any', 'suggestions', 'or', 'advice', 'that', 'other', 'ad', '##hd', \"'\", 'er', '##s', 'have', 'for', 'taking', 'multiple', 'choice', 'tests', '?', 'edit', ':', 'had', 'a', 'multiple', 'choice', 'test', 'today', 'and', 'it', 'went', 'well', 'thanks', 'to', 'your', 'advice', '.', 'i', 'took', 'a', 'little', 'bit', 'of', 'everyone', '##s', 'advice', 'and', 'combined', 'it', 'together', 'like', 'a', 'true', 'ad', '##hd', 'person', 'would', '.']\n",
      "INFO:__main__:Number of tokens: 137\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'advice', 'for', 'test', 'taking', ',', 'particularly', 'multiple', 'choice', 'i', 'am', 'good', 'at', 'taking', 'tests', 'that', 'you', 'can', 'explain', 'your', 'self', 'in', 'a', 'rational', 'manner', 'and', 'show', 'reasoning', 'but', 'for', 'some', 'reason', 'when', 'it', 'comes', 'to', 'multiple', 'choice', 'i', 'freeze', 'up', 'and', 'get', 'really', 'bad', 'test', 'anxiety', 'and', 'fail', 'horribly', '.', 'i', 'have', 'an', 'accounting', 'final', 'next', 'thursday', 'that', 'is', 'all', 'multiple', 'choice', ',', 'we', 'are', 'given', 'a', 'note', 'card', 'and', '2', 'hours', 'to', 'complete', 'the', '11', 'chapter', 'comprehensive', 'final', '.', 'any', 'suggestions', 'or', 'advice', 'that', 'other', 'ad', '##hd', \"'\", 'er', '##s', 'have', 'for', 'taking', 'multiple', 'choice', 'tests', '?', 'edit', ':', 'had', 'a', 'multiple', 'choice', 'test', 'today', 'and', 'it', 'went', 'well', 'thanks', 'to', 'your', 'advice', '.', 'i', 'took', 'a', 'little', 'bit', 'of', 'everyone', '##s', 'advice', 'and', 'combined', 'it', 'together', 'like', 'a', 'true', 'ad', '##hd', 'person', 'would', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'question', ']', 'does', 'st', '##im', '##ula', '##nt', 'medication', 'decrease', 'athletic', 'performance', '?', '*', 'i', 'remember', 'reading', 'somewhere', 'here', 'that', 'a', 'person', 'was', 'having', 'trouble', 'working', 'out', 'because', 'of', 'their', 'medication', '.', '*', 'i', \"'\", 'm', 'on', 'v', '##y', '##van', '##se', 'and', 'i', 'plan', 'on', 'working', 'out', 'after', 'school', 'lets', 'out', '(', 'i', 'have', 'tons', 'of', 'work', 'in', 'this', 'last', 'quarter', ')', 'and', 'maybe', 'when', 'i', 'have', 'the', 'time', '.', '*', 'should', 'i', 'stop', 'taking', 'my', 'medication', 'if', 'i', 'need', 'to', 'exercise', '?', 'will', 'my', 'medication', 'make', 'it', 'harder', 'if', 'i', 'do', '?']\n",
      "INFO:__main__:Number of tokens: 93\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['[', 'question', ']', 'does', 'st', '##im', '##ula', '##nt', 'medication', 'decrease', 'athletic', 'performance', '?', '*', 'i', 'remember', 'reading', 'somewhere', 'here', 'that', 'a', 'person', 'was', 'having', 'trouble', 'working', 'out', 'because', 'of', 'their', 'medication', '.', '*', 'i', \"'\", 'm', 'on', 'v', '##y', '##van', '##se', 'and', 'i', 'plan', 'on', 'working', 'out', 'after', 'school', 'lets', 'out', '(', 'i', 'have', 'tons', 'of', 'work', 'in', 'this', 'last', 'quarter', ')', 'and', 'maybe', 'when', 'i', 'have', 'the', 'time', '.', '*', 'should', 'i', 'stop', 'taking', 'my', 'medication', 'if', 'i', 'need', 'to', 'exercise', '?', 'will', 'my', 'medication', 'make', 'it', 'harder', 'if', 'i', 'do', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['brain', \"'\", 's', 'do', '##pa', '##mine', 'levels', 'may', 'explain', 'why', 'some', 'are', '\"', 'slack', '##ers', '\"', 'and', 'others', '\"', 'go', '-', 'get', '##ters', '\"', '[', 'x', '-', 'post', 'from', 'r', '/', 'science', ']']\n",
      "INFO:__main__:Number of tokens: 33\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['brain', \"'\", 's', 'do', '##pa', '##mine', 'levels', 'may', 'explain', 'why', 'some', 'are', '\"', 'slack', '##ers', '\"', 'and', 'others', '\"', 'go', '-', 'get', '##ters', '\"', '[', 'x', '-', 'post', 'from', 'r', '/', 'science', ']']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'just', 'wanted', 'to', 'introduce', 'myself', ',', 'i', 'think', 'i', 'will', 'be', 'hanging', 'out', 'here', 'a', 'lot', 'in', 'the', 'future', '.']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'just', 'wanted', 'to', 'introduce', 'myself', ',', 'i', 'think', 'i', 'will', 'be', 'hanging', 'out', 'here', 'a', 'lot', 'in', 'the', 'future', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['did', 'any', 'one', 'else', \"'\", 's', 'teacher', 'every', 'say', 'this', '?', 'i', 'was', 'wonder', 'when', 'you', 'were', 'in', 'high', 'school', 'and', 'middle', 'school', 'da', '##e', 'have', 'their', 'teachers', 'constantly', 'tell', 'you', 'and', 'your', 'parents', 'how', '\"', 'he', 'is', 'very', 'smart', ',', 'but', 'is', 'lazy', 'and', 'refuses', 'to', 'pay', 'attention', '\"', '.', 'i', 'was', 'wondering', 'is', 'that', 'a', 'common', 'comment', 'made', 'by', 'teachers', 'of', 'und', '##ia', '##gno', '##sed', 'ad', '##hd', 'suffer', '##ers', '?']\n",
      "INFO:__main__:Number of tokens: 72\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['did', 'any', 'one', 'else', \"'\", 's', 'teacher', 'every', 'say', 'this', '?', 'i', 'was', 'wonder', 'when', 'you', 'were', 'in', 'high', 'school', 'and', 'middle', 'school', 'da', '##e', 'have', 'their', 'teachers', 'constantly', 'tell', 'you', 'and', 'your', 'parents', 'how', '\"', 'he', 'is', 'very', 'smart', ',', 'but', 'is', 'lazy', 'and', 'refuses', 'to', 'pay', 'attention', '\"', '.', 'i', 'was', 'wondering', 'is', 'that', 'a', 'common', 'comment', 'made', 'by', 'teachers', 'of', 'und', '##ia', '##gno', '##sed', 'ad', '##hd', 'suffer', '##ers', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hey', 'guys', ',', 'i', 'need', 'some', 'serious', 'help', 'with', 'motivation', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hey', 'guys', ',', 'i', 'need', 'some', 'serious', 'help', 'with', 'motivation', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'do', 'i', 'say', 'when', 'i', 'call', 'a', 'ps', '##ych', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'do', 'i', 'say', 'when', 'i', 'call', 'a', 'ps', '##ych', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'and', 'resume', 'building', '?', 'so', 'i', 'have', 'finished', 'high', 'school', '10', 'years', 'ago', ',', 'i', \"'\", 've', 'dropped', 'out', 'of', 'university', 'twice', 'and', 'college', 'once', ',', 'and', 'i', 'have', 'an', 'eclectic', 'list', 'of', 'jobs', 'as', 'long', 'as', 'your', 'arm', '.', 'currently', 'i', \"'\", 'm', 'trying', 'to', 'save', 'up', 'money', 'for', 'a', 'house', 'with', 'my', 's', '.', 'o', '.', 'but', 'my', 'job', 'doesn', \"'\", 't', 'pay', 'enough', 'and', 'i', \"'\", 'm', 'trying', 'to', 'build', 'a', 'decent', 'looking', 'resume', 'to', 'get', 'hired', 'in', 'a', '9', 'to', '5', 'monday', 'to', 'friday', 'sort', 'of', 'deal', '.', '.', '.', 'i', 'have', 'some', 'office', 'experience', ',', 'i', \"'\", 've', 'done', 'taxes', 'at', 'h', '&', 'r', 'block', ',', 'as', 'well', 'as', 'worked', 'in', 'retail', ',', 'but', 'predominantly', 'i', \"'\", 'm', 'a', 'waitress', '-', 'and', 'a', 'med', '##io', '##cre', 'one', 'at', 'that', '.', 'this', 'leads', 'me', 'to', 'my', 'problem', ',', 'when', 'i', \"'\", 'm', 'writing', 'my', 'resume', 'i', 'feel', 'rather', '.', '.', '.', 'un', '##qual', '##ified', 'and', 'slightly', 'inferior', '.', 'the', 'resume', 'ends', 'up', 'deleted', 'and', 'reworked', 'and', 'deleted', 'and', 'reworked', 'time', 'after', 'time', ',', 'leaving', 'me', 'with', 'nothing', 'to', 'pass', 'around', 'to', 'potential', 'employees', '.', 'i', 'need', 'to', 'get', 'this', 'done', 'so', 'i', 'can', 'pay', 'off', 'my', 'student', 'loans', 'and', 'help', 'my', 'man', 'get', 'that', 'house', 'we', 'want', '!', 'i', 'feel', 'like', 'nothing', 'i', 'put', 'down', 'on', 'paper', 'about', 'my', 'employment', 'history', 'is', 'good', 'enough', '.', '.', '.', '.', 'any', 'sources', 'out', 'there', 'for', 'resume', 'writing', 'that', 'are', 'ad', '##hd', 'friendly', '?', '*', '*', 't', '##l', ';', 'dr', '*', '*', '*', '*', 'ad', '##hd', 'leaves', 'my', 'resume', 'un', '##written', 'due', 'to', 'feelings', 'of', 'inferior', '##ity', 'and', 'high', 'levels', 'of', 'perfection', '##ism', '.', 'need', 'help', ',', 'sources', 'or', 'advice', '.', '*', '*', '*', '*', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '*', '*', '*', '*', 'update', '*', '*', ':', 'thanks', 'to', 'everyone', 'for', 'the', 'advice', ',', 'i', 'got', 'some', 'great', 'perspective', 'and', '2', 'really', 'promising', 'websites', '.', 'i', \"'\", 've', 'been', 'working', 'day', 'and', 'night', 'shifts', 'for', 'the', 'past', 'few', 'days', ',', 'and', 'now', 'i', \"'\", 'm', 'off', 'for', 'the', 'weekend', '.', 'that', 'means', 'that', 'no', 'matter', 'what', ',', 'it', \"'\", 's', 'resume', 'crunch', 'time', '!', 'i', \"'\", 'll', 'let', 'ya', \"'\", 'll', 'know', 'how', 'it', 'goes', '!']\n",
      "INFO:__main__:Number of tokens: 493\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'and', 'resume', 'building', '?', 'so', 'i', 'have', 'finished', 'high', 'school', '10', 'years', 'ago', ',', 'i', \"'\", 've', 'dropped', 'out', 'of', 'university', 'twice', 'and', 'college', 'once', ',', 'and', 'i', 'have', 'an', 'eclectic', 'list', 'of', 'jobs', 'as', 'long', 'as', 'your', 'arm', '.', 'currently', 'i', \"'\", 'm', 'trying', 'to', 'save', 'up', 'money', 'for', 'a', 'house', 'with', 'my', 's', '.', 'o', '.', 'but', 'my', 'job', 'doesn', \"'\", 't', 'pay', 'enough', 'and', 'i', \"'\", 'm', 'trying', 'to', 'build', 'a', 'decent', 'looking', 'resume', 'to', 'get', 'hired', 'in', 'a', '9', 'to', '5', 'monday', 'to', 'friday', 'sort', 'of', 'deal', '.', '.', '.', 'i', 'have', 'some', 'office', 'experience', ',', 'i', \"'\", 've', 'done', 'taxes', 'at', 'h', '&', 'r', 'block', ',', 'as', 'well', 'as', 'worked', 'in', 'retail', ',', 'but', 'predominantly', 'i', \"'\", 'm', 'a', 'waitress', '-', 'and', 'a', 'med', '##io', '##cre', 'one', 'at', 'that', '.', 'this', 'leads', 'me', 'to', 'my', 'problem', ',', 'when', 'i', \"'\", 'm', 'writing', 'my', 'resume', 'i', 'feel', 'rather', '.', '.', '.', 'un', '##qual', '##ified', 'and', 'slightly', 'inferior', '.', 'the', 'resume', 'ends', 'up', 'deleted', 'and', 'reworked', 'and', 'deleted', 'and', 'reworked', 'time', 'after', 'time', ',', 'leaving', 'me', 'with', 'nothing', 'to', 'pass', 'around', 'to', 'potential', 'employees', '.', 'i', 'need', 'to', 'get', 'this', 'done', 'so', 'i', 'can', 'pay', 'off', 'my', 'student', 'loans', 'and', 'help', 'my', 'man', 'get', 'that', 'house', 'we', 'want', '!', 'i', 'feel', 'like', 'nothing', 'i', 'put', 'down', 'on', 'paper', 'about', 'my', 'employment', 'history', 'is', 'good', 'enough', '.', '.', '.', '.', 'any', 'sources', 'out', 'there', 'for', 'resume', 'writing', 'that', 'are', 'ad', '##hd', 'friendly', '?', '*', '*', 't', '##l', ';', 'dr', '*', '*', '*', '*', 'ad', '##hd', 'leaves', 'my', 'resume', 'un', '##written', 'due', 'to', 'feelings', 'of', 'inferior', '##ity', 'and', 'high', 'levels', 'of', 'perfection', '##ism', '.', 'need', 'help', ',', 'sources', 'or', 'advice', '.', '*', '*', '*', '*', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '*', '*', '*', '*', 'update', '*', '*', ':', 'thanks', 'to', 'everyone', 'for', 'the', 'advice', ',', 'i', 'got', 'some', 'great', 'perspective', 'and', '2', 'really', 'promising', 'websites', '.', 'i', \"'\", 've', 'been', 'working', 'day', 'and', 'night', 'shifts', 'for', 'the', 'past', 'few', 'days', ',', 'and', 'now', 'i', \"'\", 'm', 'off', 'for', 'the', 'weekend', '.', 'that', 'means', 'that', 'no', 'matter', 'what', ',', 'it', \"'\", 's', 'resume', 'crunch', 'time', '!', 'i', \"'\", 'll', 'let', 'ya', \"'\", 'll', 'know', 'how', 'it', 'goes', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', \"'\", 't', 'get', 'to', 'work', 'anymore', 'i', 'promised', 'myself', 'i', 'would', 'do', 'at', 'least', 'an', 'hour', 'of', 'studying', 'every', 'day', ',', 'but', 'this', 'weekend', 'i', 'went', 'on', 'a', 'holiday', 'and', 'since', 'i', 'came', 'back', 'i', 'didn', \"'\", 't', 'do', 'a', 'single', 'thing', '.', 'it', 'broke', 'my', 'rhythm', 'and', 'i', 'can', \"'\", 't', 'seem', 'to', 'get', 'back', '.', 'i', 'keep', 'telling', 'myself', \"'\", 'i', \"'\", 'm', 'going', 'to', 'study', 'tomorrow', \"'\", ',', 'but', 'i', 'know', 'i', 'won', \"'\", 't', '.', 'i', 'was', 'just', 'keeping', 'up', 'and', 'now', 'i', \"'\", 'm', 'fucking', 'up', 'again', ',', 'it', \"'\", 's', 'so', 'annoying', '!', 'i', 'hope', 'i', 'can', 'get', 'myself', 'to', 'study', 'some', 'time', 'this', 'day', ',', 'even', 'when', 'it', \"'\", 's', '11', 'pm', '.', 'but', 'for', 'the', 'time', 'being', 'i', \"'\", 'd', 'rather', 'watch', 'doctor', 'who', '.', 'edit', ':', 'ya', '##y', '!', 'i', \"'\", 've', 'already', 'worked', 'for', 'one', 'hour', 'today', ',', 'if', 'i', 'work', 'two', 'i', 'can', 'watch', 'doctor', 'who', '.', 'it', \"'\", 's', 'hard', 'for', 'me', 'to', 'concentrate', ',', 'but', 'at', 'least', 'i', \"'\", 'm', 'doing', 'something', 'useful', '.']\n",
      "INFO:__main__:Number of tokens: 178\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', \"'\", 't', 'get', 'to', 'work', 'anymore', 'i', 'promised', 'myself', 'i', 'would', 'do', 'at', 'least', 'an', 'hour', 'of', 'studying', 'every', 'day', ',', 'but', 'this', 'weekend', 'i', 'went', 'on', 'a', 'holiday', 'and', 'since', 'i', 'came', 'back', 'i', 'didn', \"'\", 't', 'do', 'a', 'single', 'thing', '.', 'it', 'broke', 'my', 'rhythm', 'and', 'i', 'can', \"'\", 't', 'seem', 'to', 'get', 'back', '.', 'i', 'keep', 'telling', 'myself', \"'\", 'i', \"'\", 'm', 'going', 'to', 'study', 'tomorrow', \"'\", ',', 'but', 'i', 'know', 'i', 'won', \"'\", 't', '.', 'i', 'was', 'just', 'keeping', 'up', 'and', 'now', 'i', \"'\", 'm', 'fucking', 'up', 'again', ',', 'it', \"'\", 's', 'so', 'annoying', '!', 'i', 'hope', 'i', 'can', 'get', 'myself', 'to', 'study', 'some', 'time', 'this', 'day', ',', 'even', 'when', 'it', \"'\", 's', '11', 'pm', '.', 'but', 'for', 'the', 'time', 'being', 'i', \"'\", 'd', 'rather', 'watch', 'doctor', 'who', '.', 'edit', ':', 'ya', '##y', '!', 'i', \"'\", 've', 'already', 'worked', 'for', 'one', 'hour', 'today', ',', 'if', 'i', 'work', 'two', 'i', 'can', 'watch', 'doctor', 'who', '.', 'it', \"'\", 's', 'hard', 'for', 'me', 'to', 'concentrate', ',', 'but', 'at', 'least', 'i', \"'\", 'm', 'doing', 'something', 'useful', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['going', 'to', 'go', 'get', 'tested', 'today', ',', 'not', 'sure', 'what', 'to', 'expect', ',', 'freaking', 'me', 'out', '.', 'help', '?', 'hey', 'guys', ',', 'so', ',', 'i', \"'\", 'm', 'going', 'to', 'finally', 'get', 'tested', 'through', 'my', 'school', 'today', '.', 'i', \"'\", 'm', 'not', 'sure', 'exactly', 'what', 'to', 'expect', 'and', 'its', 'kind', 'of', 'making', 'me', 'worry', '.', 'anything', 'that', 'you', 'can', 'tell', 'me', 'about', 'what', 'might', 'happen', 'today', '?']\n",
      "INFO:__main__:Number of tokens: 66\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['going', 'to', 'go', 'get', 'tested', 'today', ',', 'not', 'sure', 'what', 'to', 'expect', ',', 'freaking', 'me', 'out', '.', 'help', '?', 'hey', 'guys', ',', 'so', ',', 'i', \"'\", 'm', 'going', 'to', 'finally', 'get', 'tested', 'through', 'my', 'school', 'today', '.', 'i', \"'\", 'm', 'not', 'sure', 'exactly', 'what', 'to', 'expect', 'and', 'its', 'kind', 'of', 'making', 'me', 'worry', '.', 'anything', 'that', 'you', 'can', 'tell', 'me', 'about', 'what', 'might', 'happen', 'today', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['emotional', 'intelligence', 'and', 'how', 'it', 'relates', 'to', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 9\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['emotional', 'intelligence', 'and', 'how', 'it', 'relates', 'to', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'is', 'the', 'average', 'price', 'of', 'ad', '##hd', 'medicine', 'for', 'month']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'is', 'the', 'average', 'price', 'of', 'ad', '##hd', 'medicine', 'for', 'month']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'just', 'watched', 'the', 'bark', '##ley', 'video', 'for', 'the', 'first', 'time', 'when', 'he', 'got', 'to', 'the', 'part', 'about', 'grief', 'and', 'anger', ',', 'missing', 'out', 'on', 'opportunity', 'and', 'college', ',', 'i', 'completely', 'broke', 'down', ',', 'which', 'very', 'very', 'rarely', 'happens', 'to', 'me', ',', 'usually', 'only', 'out', 'of', 'anger', '.', 'the', 'video', 'was', 'everything', 'i', 'go', 'through', 'to', 'a', 't', ',', 'that', 'i', 'thought', 'was', 'sort', 'of', 'my', 'specific', 'version', 'of', 'add', ',', 'the', 'way', 'it', 'affected', 'me', '.', 'the', 'way', 'he', 'described', 'was', 'a', 'realization', 'i', 'honestly', 'did', 'not', 'see', 'coming', '.', 'why', 'don', \"'\", 't', 'psychologists', 'say', 'the', 'things', 'he', 'says', '?', 'why', 'isn', \"'\", 't', 'this', 'information', 'being', 'shoved', 'down', 'their', 'throat', '##s', '?', 'i', 'feel', 'like', 'i', \"'\", 'm', 'in', 'a', 'complete', 'mess', 'right', 'now', '.', 'my', 'last', 'two', 'college', 'finals', 'are', 'tomorrow', ',', 'i', \"'\", 've', 'been', 'up', 'for', '24', 'hours', 'doing', 'work', 'and', 'have', 'hardly', 'accomplished', '/', 'completed', 'any', 'tasks', '.', 'i', 'completely', 'failed', 'to', 'complete', 'a', 'history', 'assignment', 'on', 'time', ',', 'in', 'which', 'the', 'professor', 'isn', \"'\", 't', 'really', 'aware', 'of', 'my', 'situation', 'because', 'i', 'kept', 'telling', 'myself', 'i', 'would', 'get', 'it', 'done', 'and', 'now', 'i', \"'\", 'm', 'going', 'to', 'have', 'to', 'e', '-', 'mail', 'with', 'the', 'whole', 'add', 'sc', '##ht', '##ick', 'that', 'i', \"'\", 've', 'had', 'to', 'do', 'with', 'my', 'other', 'professors', '(', 'who', 'all', 'have', 'been', 'very', 'understanding', ')', '.', 'it', 'sucks', 'when', 'my', 'professor', 'has', 'sy', '##mp', '##athi', '##zed', 'with', 'me', 'all', 'year', 'and', 'i', 'still', 'feel', 'like', 'i', \"'\", 'm', 'about', 'to', 'let', 'her', 'down', '.', 'and', 'i', \"'\", 'm', 'exhausted', '.', 'i', 'probably', 'need', 'to', 'get', 'a', 'few', 'hours', 'of', 'sleep', 'but', 'i', 'don', \"'\", 't', 'know', 'if', 'that', 'will', 'leave', 'me', 'enough', 'time', 'to', 'study', '.', 'i', \"'\", 'm', 'almost', 'tempted', 'to', 'pop', 'another', 'v', '##y', '##van', '##se', 'and', 'ride', 'this', 'thing', 'out', 'to', 'the', 'bitter', 'end', '.', 'any', 'advice', 'or', 'encouraging', 'thoughts', 'are', 'much', 'appreciated', '.', 'i', \"'\", 'll', 'make', 'it', ',', 'i', \"'\", 'm', 'just', 'having', 'a', 'rough', 'night', '/', 'emotional', 'breakdown', 'after', 'wat', '##hing', 'that', 'video', '.', 'on', 'a', 'more', 'positive', 'note', ',', 'both', 'the', 'classes', 'that', 'are', 'completed', 'for', 'this', 'semester', 'i', \"'\", 've', 'gotten', 'a', 'b', 'and', 'an', 'a', 'or', 'b', 'on', '.']\n",
      "INFO:__main__:Number of tokens: 369\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'just', 'watched', 'the', 'bark', '##ley', 'video', 'for', 'the', 'first', 'time', 'when', 'he', 'got', 'to', 'the', 'part', 'about', 'grief', 'and', 'anger', ',', 'missing', 'out', 'on', 'opportunity', 'and', 'college', ',', 'i', 'completely', 'broke', 'down', ',', 'which', 'very', 'very', 'rarely', 'happens', 'to', 'me', ',', 'usually', 'only', 'out', 'of', 'anger', '.', 'the', 'video', 'was', 'everything', 'i', 'go', 'through', 'to', 'a', 't', ',', 'that', 'i', 'thought', 'was', 'sort', 'of', 'my', 'specific', 'version', 'of', 'add', ',', 'the', 'way', 'it', 'affected', 'me', '.', 'the', 'way', 'he', 'described', 'was', 'a', 'realization', 'i', 'honestly', 'did', 'not', 'see', 'coming', '.', 'why', 'don', \"'\", 't', 'psychologists', 'say', 'the', 'things', 'he', 'says', '?', 'why', 'isn', \"'\", 't', 'this', 'information', 'being', 'shoved', 'down', 'their', 'throat', '##s', '?', 'i', 'feel', 'like', 'i', \"'\", 'm', 'in', 'a', 'complete', 'mess', 'right', 'now', '.', 'my', 'last', 'two', 'college', 'finals', 'are', 'tomorrow', ',', 'i', \"'\", 've', 'been', 'up', 'for', '24', 'hours', 'doing', 'work', 'and', 'have', 'hardly', 'accomplished', '/', 'completed', 'any', 'tasks', '.', 'i', 'completely', 'failed', 'to', 'complete', 'a', 'history', 'assignment', 'on', 'time', ',', 'in', 'which', 'the', 'professor', 'isn', \"'\", 't', 'really', 'aware', 'of', 'my', 'situation', 'because', 'i', 'kept', 'telling', 'myself', 'i', 'would', 'get', 'it', 'done', 'and', 'now', 'i', \"'\", 'm', 'going', 'to', 'have', 'to', 'e', '-', 'mail', 'with', 'the', 'whole', 'add', 'sc', '##ht', '##ick', 'that', 'i', \"'\", 've', 'had', 'to', 'do', 'with', 'my', 'other', 'professors', '(', 'who', 'all', 'have', 'been', 'very', 'understanding', ')', '.', 'it', 'sucks', 'when', 'my', 'professor', 'has', 'sy', '##mp', '##athi', '##zed', 'with', 'me', 'all', 'year', 'and', 'i', 'still', 'feel', 'like', 'i', \"'\", 'm', 'about', 'to', 'let', 'her', 'down', '.', 'and', 'i', \"'\", 'm', 'exhausted', '.', 'i', 'probably', 'need', 'to', 'get', 'a', 'few', 'hours', 'of', 'sleep', 'but', 'i', 'don', \"'\", 't', 'know', 'if', 'that', 'will', 'leave', 'me', 'enough', 'time', 'to', 'study', '.', 'i', \"'\", 'm', 'almost', 'tempted', 'to', 'pop', 'another', 'v', '##y', '##van', '##se', 'and', 'ride', 'this', 'thing', 'out', 'to', 'the', 'bitter', 'end', '.', 'any', 'advice', 'or', 'encouraging', 'thoughts', 'are', 'much', 'appreciated', '.', 'i', \"'\", 'll', 'make', 'it', ',', 'i', \"'\", 'm', 'just', 'having', 'a', 'rough', 'night', '/', 'emotional', 'breakdown', 'after', 'wat', '##hing', 'that', 'video', '.', 'on', 'a', 'more', 'positive', 'note', ',', 'both', 'the', 'classes', 'that', 'are', 'completed', 'for', 'this', 'semester', 'i', \"'\", 've', 'gotten', 'a', 'b', 'and', 'an', 'a', 'or', 'b', 'on', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'x', '##r', 'vs', '.', 'v', '##y', '##van', '##se', '.', 'could', 'use', 'your', 'experience', 'and', 'advice', '.', 'hello', 'all', ',', 'i', 'was', 'diagnosed', 'by', 'with', 'add', 'a', 'few', 'months', 'ago', ',', 'and', 'have', 'seen', 'a', 'drastic', 'improvement', 'in', 'my', 'life', 'since', 'being', 'prescribed', 'add', '##eral', '##l', 'x', '##r', 'accompanying', 'a', 'lot', 'of', 'lifestyle', 'changes', '.', 'i', 'just', 'recently', 'finished', 'a', 'psychiatric', 'assessment', 'which', 'resulted', 'with', 'me', 'being', 'ad', '##hd', 'with', 'very', 'minor', 'hyper', '##active', ',', 'as', 'well', 'as', 'having', 'a', 'generalized', 'anxiety', '.', 'when', 'i', 'asked', 'the', 'doctor', 'about', 'the', 'possibility', 'of', 'a', 'longer', 'lasting', 'medication', ',', 'he', 'told', 'me', 'that', 'he', \"'\", 'll', 'be', 'recommend', '##ing', 'v', '##y', '##vance', '30', '##mg', 'to', 'my', 'gp', 'in', 'place', 'of', 'add', '##eral', '##l', 'x', '##r', '15', '##mg', '.', 'i', 'have', 'been', 'reading', 'up', 'on', 'this', 'subject', ',', 'and', 'was', 'hoping', 'anyone', 'who', 'has', 'had', 'a', 'run', 'with', 'both', 'of', 'these', 'drugs', 'could', 'offer', 'me', 'some', 'advice', 'in', 'regards', 'to', 'your', 'compared', 'experience', '.', 'my', 'only', 'major', 'issues', 'that', 'i', 'have', 'with', 'add', '##eral', '##l', 'at', 'the', 'moment', 'is', 'the', 'seemingly', 'progressive', 'loss', 'in', 'appetite', ',', 'unpredictable', 'up', 'time', ',', 'as', 'well', 'as', 'the', 'unpredictable', ',', 'and', 'sometimes', 'rather', 'steep', 'come', 'down', '.', 'mind', 'you', ',', 'i', 'have', 'found', 'add', '##eral', '##l', 'to', 'be', 'effective', 'in', 'parts', 'of', 'my', 'life', 'other', 'than', 'add', ',', 'namely', 'a', 'decrease', 'in', 'social', 'anxiety', 'and', 'depression', 'and', 'i', 'would', 'like', 'to', 'see', 'that', 'remain', 'if', 'possible', '.', 'thank', '-', 'you', 'so', 'much', 'in', 'advance', ',', 'this', 'sub', 'has', 'been', 'a', 'great', 'source', 'of', 'knowledge', 'for', 'me', ',', 'and', 'i', '’', 'm', 'hoping', 'to', 'learn', 'just', 'a', 'little', 'more', 'with', 'this', 'post', '.', 'i', 'understand', 'your', 'experience', 'and', 'mine', 'may', 'differ', ',', 'but', 'i', '’', 'd', 'like', 'to', 'at', 'least', 'know', 'a', 'few', 'things', 'i', 'may', 'experience', 'from', 'the', 'switch', '.', 'grey']\n",
      "INFO:__main__:Number of tokens: 306\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'x', '##r', 'vs', '.', 'v', '##y', '##van', '##se', '.', 'could', 'use', 'your', 'experience', 'and', 'advice', '.', 'hello', 'all', ',', 'i', 'was', 'diagnosed', 'by', 'with', 'add', 'a', 'few', 'months', 'ago', ',', 'and', 'have', 'seen', 'a', 'drastic', 'improvement', 'in', 'my', 'life', 'since', 'being', 'prescribed', 'add', '##eral', '##l', 'x', '##r', 'accompanying', 'a', 'lot', 'of', 'lifestyle', 'changes', '.', 'i', 'just', 'recently', 'finished', 'a', 'psychiatric', 'assessment', 'which', 'resulted', 'with', 'me', 'being', 'ad', '##hd', 'with', 'very', 'minor', 'hyper', '##active', ',', 'as', 'well', 'as', 'having', 'a', 'generalized', 'anxiety', '.', 'when', 'i', 'asked', 'the', 'doctor', 'about', 'the', 'possibility', 'of', 'a', 'longer', 'lasting', 'medication', ',', 'he', 'told', 'me', 'that', 'he', \"'\", 'll', 'be', 'recommend', '##ing', 'v', '##y', '##vance', '30', '##mg', 'to', 'my', 'gp', 'in', 'place', 'of', 'add', '##eral', '##l', 'x', '##r', '15', '##mg', '.', 'i', 'have', 'been', 'reading', 'up', 'on', 'this', 'subject', ',', 'and', 'was', 'hoping', 'anyone', 'who', 'has', 'had', 'a', 'run', 'with', 'both', 'of', 'these', 'drugs', 'could', 'offer', 'me', 'some', 'advice', 'in', 'regards', 'to', 'your', 'compared', 'experience', '.', 'my', 'only', 'major', 'issues', 'that', 'i', 'have', 'with', 'add', '##eral', '##l', 'at', 'the', 'moment', 'is', 'the', 'seemingly', 'progressive', 'loss', 'in', 'appetite', ',', 'unpredictable', 'up', 'time', ',', 'as', 'well', 'as', 'the', 'unpredictable', ',', 'and', 'sometimes', 'rather', 'steep', 'come', 'down', '.', 'mind', 'you', ',', 'i', 'have', 'found', 'add', '##eral', '##l', 'to', 'be', 'effective', 'in', 'parts', 'of', 'my', 'life', 'other', 'than', 'add', ',', 'namely', 'a', 'decrease', 'in', 'social', 'anxiety', 'and', 'depression', 'and', 'i', 'would', 'like', 'to', 'see', 'that', 'remain', 'if', 'possible', '.', 'thank', '-', 'you', 'so', 'much', 'in', 'advance', ',', 'this', 'sub', 'has', 'been', 'a', 'great', 'source', 'of', 'knowledge', 'for', 'me', ',', 'and', 'i', '’', 'm', 'hoping', 'to', 'learn', 'just', 'a', 'little', 'more', 'with', 'this', 'post', '.', 'i', 'understand', 'your', 'experience', 'and', 'mine', 'may', 'differ', ',', 'but', 'i', '’', 'd', 'like', 'to', 'at', 'least', 'know', 'a', 'few', 'things', 'i', 'may', 'experience', 'from', 'the', 'switch', '.', 'grey']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['recently', 'diagnosed', ',', 'feel', 'more', 'like', 'shit', 'now', 'about', 'my', 'lack', 'of', 'focus', ',', 'despite', 'having', 'something', 'to', 'blame']\n",
      "INFO:__main__:Number of tokens: 19\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['recently', 'diagnosed', ',', 'feel', 'more', 'like', 'shit', 'now', 'about', 'my', 'lack', 'of', 'focus', ',', 'despite', 'having', 'something', 'to', 'blame']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['other', 'v', '##y', '##van', '##se', 'users', ':', 'did', 'you', 'experience', 'a', 'significant', 'dry', '##ness', '?']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['other', 'v', '##y', '##van', '##se', 'users', ':', 'did', 'you', 'experience', 'a', 'significant', 'dry', '##ness', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'get', 'mig', '##raine', '##s', '/', 'headache', '##s', '?', 'it', \"'\", 's', 'not', 'my', 'medication', 'i', \"'\", 've', 'always', 'had', 'headache', '##s', 'and', 'occasional', 'mig', '##raine', '##s', '.', 'i', 'do', 'have', 'pretty', 'bad', 'eyes', 'to', '(', 'one', 'near', 'sighted', 'one', 'far', 'sighted', ')', 'and', 'i', 'know', 'that', 'does', 'cause', 'al', '##ot', 'of', 'them', ',', 'but', 'i', \"'\", 'm', 'curious', 'if', 'anyone', 'has', 'headache', '##s', 'from', 'ad', '##hd', 'and', 'if', 'so', 'any', 'ways', 'of', 'dealing', 'with', 'the', 'pain', 'in', 'the', 'brain', '.']\n",
      "INFO:__main__:Number of tokens: 82\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'get', 'mig', '##raine', '##s', '/', 'headache', '##s', '?', 'it', \"'\", 's', 'not', 'my', 'medication', 'i', \"'\", 've', 'always', 'had', 'headache', '##s', 'and', 'occasional', 'mig', '##raine', '##s', '.', 'i', 'do', 'have', 'pretty', 'bad', 'eyes', 'to', '(', 'one', 'near', 'sighted', 'one', 'far', 'sighted', ')', 'and', 'i', 'know', 'that', 'does', 'cause', 'al', '##ot', 'of', 'them', ',', 'but', 'i', \"'\", 'm', 'curious', 'if', 'anyone', 'has', 'headache', '##s', 'from', 'ad', '##hd', 'and', 'if', 'so', 'any', 'ways', 'of', 'dealing', 'with', 'the', 'pain', 'in', 'the', 'brain', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'much', 'clothes', 'should', 'one', 'have', '?', 'i', 'have', 'been', 'getting', 'increasingly', 'di', '##sor', '##gan', '##ized', 'over', 'the', 'past', 'month', 'and', 'i', 'am', 'taking', 'tonight', 'to', 'get', 'myself', 'together', '.', 'i', 'cleaned', 'everything', 'and', 'put', 'it', 'into', 'drawers', '/', 'closet', '.', 'looks', 'like', 'i', 'am', 'going', 'to', 'have', 'to', 'throw', 'a', 'lot', 'of', 'stuff', 'out', ',', 'and', 'i', 'have', 'decided', 'to', 'try', 'to', 'come', 'up', 'with', 'a', 'list', 'of', 'articles', 'of', 'clothing', 'i', 'need', 'before', 'getting', 'to', 'it', '.', 'what', 'do', 'you', 'guys', 'think', '?', 'how', 'many', 'of', 'everything', 'should', 'i', 'have', '?', 'i', 'am', 'a', 'male', 'college', 'student', 'from', 'nj', 'with', 'no', 'interest', 'in', 'being', 'fashionable', '.', 'so', 'no', 'need', 'for', 'bra', '##s', 'or', 'vest', '##s', 'or', 'anything', 'silly', '.', 'i', 'just', 'want', 'to', 'be', 'organized', 'and', 'functional', '.', 'i', 'figure', 'if', 'i', 'can', 'make', 'my', 'environment', 'less', 'cl', '##uttered', ',', 'i', 'can', 'make', 'my', 'life', 'less', 'cl', '##uttered', 'and', 'get', 'my', 'ad', '##hd', 'under', 'control', '.']\n",
      "INFO:__main__:Number of tokens: 158\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'much', 'clothes', 'should', 'one', 'have', '?', 'i', 'have', 'been', 'getting', 'increasingly', 'di', '##sor', '##gan', '##ized', 'over', 'the', 'past', 'month', 'and', 'i', 'am', 'taking', 'tonight', 'to', 'get', 'myself', 'together', '.', 'i', 'cleaned', 'everything', 'and', 'put', 'it', 'into', 'drawers', '/', 'closet', '.', 'looks', 'like', 'i', 'am', 'going', 'to', 'have', 'to', 'throw', 'a', 'lot', 'of', 'stuff', 'out', ',', 'and', 'i', 'have', 'decided', 'to', 'try', 'to', 'come', 'up', 'with', 'a', 'list', 'of', 'articles', 'of', 'clothing', 'i', 'need', 'before', 'getting', 'to', 'it', '.', 'what', 'do', 'you', 'guys', 'think', '?', 'how', 'many', 'of', 'everything', 'should', 'i', 'have', '?', 'i', 'am', 'a', 'male', 'college', 'student', 'from', 'nj', 'with', 'no', 'interest', 'in', 'being', 'fashionable', '.', 'so', 'no', 'need', 'for', 'bra', '##s', 'or', 'vest', '##s', 'or', 'anything', 'silly', '.', 'i', 'just', 'want', 'to', 'be', 'organized', 'and', 'functional', '.', 'i', 'figure', 'if', 'i', 'can', 'make', 'my', 'environment', 'less', 'cl', '##uttered', ',', 'i', 'can', 'make', 'my', 'life', 'less', 'cl', '##uttered', 'and', 'get', 'my', 'ad', '##hd', 'under', 'control', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sustained', 'thought', 'as', 'a', 'balancing', 'act', 'some', 'people', 'describe', 'ad', '##hd', 'as', 'feeling', 'like', 'having', 'a', 'whole', 'bunch', 'of', 'tv', 'channels', 'on', 'at', 'the', 'same', 'time', ',', 'but', 'that', 'doesn', \"'\", 't', 'quite', 'seem', 'to', 'fit', 'for', 'me', '.', 'so', 'i', 'wanted', 'to', 'try', 'one', 'on', 'you', 'guys', 'and', 'see', 'if', 'it', 'fits', ',', 'or', 'you', 'can', 'find', 'a', 'better', 'mental', 'image', '.', 'it', \"'\", 's', 'like', 'balancing', 'plates', 'on', 'sticks', '.', 'sustained', 'thought', 'or', 'effort', 'towards', 'a', 'goal', 'is', 'like', 'keeping', 'that', 'plate', 'on', 'the', 'stick', ',', 'the', 'dar', '##n', 'thing', 'just', 'falling', 'off', 'over', 'and', 'over', 'again', '.', 'can', \"'\", 't', 'really', 'effort', 'your', 'way', 'to', 'it', ',', 'just', 'kind', 'of', 'has', 'to', 'stay', 'there', 'through', 'the', 'right', 'spin', 'or', 'what', 'have', 'you', '.', 'i', \"'\", 'm', 'not', 'entirely', 'satisfied', 'with', 'this', 'mental', 'image', ',', 'doesn', \"'\", 't', 'quite', 'nail', 'it', 'for', 'me', ',', 'but', 'am', 'wondering', 'if', 'it', \"'\", 's', 'a', 'better', 'depiction', 'for', 'any', 'of', 'your', 'ad', '##hd', '##s', 'than', 'the', 'multi', 'tv', 'thing', '.', 'i', 'dislike', 'talking', 'about', 'focus', '/', 'concentration', 'because', 'that', 'implies', 'blocking', 'out', 'everything', 'else', 'as', 'desirable', ',', 'which', 'doesn', \"'\", 't', 'really', 'fit', 'for', 'me', '.', 'medication', 'does', 'not', 'help', 'me', 'to', 'focus', ',', 'so', 'much', 'as', 'to', 'balance', '.', 'magical', '##ly', ',', 'the', 'plate', 'stays', 'on', 'the', 'stick', '.', 'similarly', 'i', 'dislike', 'talking', 'about', 'distraction', 'because', 'for', 'me', ',', 'it', \"'\", 's', 'less', '\"', 'o', '##oh', 'squirrel', '!', '\"', 'and', 'more', '\"', 'huh', ',', 'what', '?', 'where', 'was', 'i', '?', '\"']\n",
      "INFO:__main__:Number of tokens: 251\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sustained', 'thought', 'as', 'a', 'balancing', 'act', 'some', 'people', 'describe', 'ad', '##hd', 'as', 'feeling', 'like', 'having', 'a', 'whole', 'bunch', 'of', 'tv', 'channels', 'on', 'at', 'the', 'same', 'time', ',', 'but', 'that', 'doesn', \"'\", 't', 'quite', 'seem', 'to', 'fit', 'for', 'me', '.', 'so', 'i', 'wanted', 'to', 'try', 'one', 'on', 'you', 'guys', 'and', 'see', 'if', 'it', 'fits', ',', 'or', 'you', 'can', 'find', 'a', 'better', 'mental', 'image', '.', 'it', \"'\", 's', 'like', 'balancing', 'plates', 'on', 'sticks', '.', 'sustained', 'thought', 'or', 'effort', 'towards', 'a', 'goal', 'is', 'like', 'keeping', 'that', 'plate', 'on', 'the', 'stick', ',', 'the', 'dar', '##n', 'thing', 'just', 'falling', 'off', 'over', 'and', 'over', 'again', '.', 'can', \"'\", 't', 'really', 'effort', 'your', 'way', 'to', 'it', ',', 'just', 'kind', 'of', 'has', 'to', 'stay', 'there', 'through', 'the', 'right', 'spin', 'or', 'what', 'have', 'you', '.', 'i', \"'\", 'm', 'not', 'entirely', 'satisfied', 'with', 'this', 'mental', 'image', ',', 'doesn', \"'\", 't', 'quite', 'nail', 'it', 'for', 'me', ',', 'but', 'am', 'wondering', 'if', 'it', \"'\", 's', 'a', 'better', 'depiction', 'for', 'any', 'of', 'your', 'ad', '##hd', '##s', 'than', 'the', 'multi', 'tv', 'thing', '.', 'i', 'dislike', 'talking', 'about', 'focus', '/', 'concentration', 'because', 'that', 'implies', 'blocking', 'out', 'everything', 'else', 'as', 'desirable', ',', 'which', 'doesn', \"'\", 't', 'really', 'fit', 'for', 'me', '.', 'medication', 'does', 'not', 'help', 'me', 'to', 'focus', ',', 'so', 'much', 'as', 'to', 'balance', '.', 'magical', '##ly', ',', 'the', 'plate', 'stays', 'on', 'the', 'stick', '.', 'similarly', 'i', 'dislike', 'talking', 'about', 'distraction', 'because', 'for', 'me', ',', 'it', \"'\", 's', 'less', '\"', 'o', '##oh', 'squirrel', '!', '\"', 'and', 'more', '\"', 'huh', ',', 'what', '?', 'where', 'was', 'i', '?', '\"']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['sustained', 'thought', 'as', 'a', 'balancing', 'act']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['sustained', 'thought', 'as', 'a', 'balancing', 'act']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anyone', 'know', 'a', 'guy', 'named', 'thom', 'hartman', '##n', 'and', 'his', 'theories', 'on', 'ad', '##hd', '?', 'so', 'i', 'wrote', 'a', 'paper', 'about', 'thom', 'hartman', '##n', \"'\", 's', 'theory', 'on', 'ad', '##hd', 'and', 'how', 'they', 'we', 'were', 'built', 'for', 'a', 'pre', '-', 'agricultural', 'state', 'of', 'society', 'and', 'how', 'ad', '##hd', 'could', 'be', 'considered', 'not', 'a', 'disorder', 'because', 'common', 'ad', '##hd', 'symptoms', 'could', 'have', 'helped', 'a', 'hunter', 'in', 'his', 'state', 'of', 'society', '.', 'thoughts', '?', '?']\n",
      "INFO:__main__:Number of tokens: 73\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anyone', 'know', 'a', 'guy', 'named', 'thom', 'hartman', '##n', 'and', 'his', 'theories', 'on', 'ad', '##hd', '?', 'so', 'i', 'wrote', 'a', 'paper', 'about', 'thom', 'hartman', '##n', \"'\", 's', 'theory', 'on', 'ad', '##hd', 'and', 'how', 'they', 'we', 'were', 'built', 'for', 'a', 'pre', '-', 'agricultural', 'state', 'of', 'society', 'and', 'how', 'ad', '##hd', 'could', 'be', 'considered', 'not', 'a', 'disorder', 'because', 'common', 'ad', '##hd', 'symptoms', 'could', 'have', 'helped', 'a', 'hunter', 'in', 'his', 'state', 'of', 'society', '.', 'thoughts', '?', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'tips', 'do', 'you', 'have', 'for', 'dealing', 'with', 'ad', '##hd', '?']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'tips', 'do', 'you', 'have', 'for', 'dealing', 'with', 'ad', '##hd', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['forgot', 'to', 'take', 'my', 'add', '##eral', '##l', 'today', '.', '.', '.', 'spent', 'the', 'last', 'hour', 'at', 'work', 'researching', 'how', 'to', 'build', 'underground', 'bunker', '##s', '.', 'forgot', 'i', 'have', 'lesson', 'planning', 'to', 'do', '.']\n",
      "INFO:__main__:Number of tokens: 33\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['forgot', 'to', 'take', 'my', 'add', '##eral', '##l', 'today', '.', '.', '.', 'spent', 'the', 'last', 'hour', 'at', 'work', 'researching', 'how', 'to', 'build', 'underground', 'bunker', '##s', '.', 'forgot', 'i', 'have', 'lesson', 'planning', 'to', 'do', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['probably', 'a', 're', '-', 'post', 'but', 'still', 'one', 'of', 'my', 'favorite', 'me', '##me', \"'\", 's']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['probably', 'a', 're', '-', 'post', 'but', 'still', 'one', 'of', 'my', 'favorite', 'me', '##me', \"'\", 's']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['some', 'tips', 'for', 'eating', 'on', 'v', '##y', '##vance', '(', 'and', 'other', 'such', 'drugs', ')', 'i', 'started', 'v', '##y', '##vance', 'recently', ',', 'and', 'so', 'i', 'sc', '##oured', 'the', 'land', 'for', 'tips', 'on', 'managing', 'my', 'new', 'non', '-', 'existent', 'appetite', '.', 'i', \"'\", 'd', 'like', 'to', 'share', 'with', 'you', 'the', 'things', 'i', \"'\", 've', 'found', ',', 'in', 'the', 'hopes', 'that', 'you', \"'\", 'll', 'get', 'some', 'help', 'out', 'of', 'it', 'too', '.', 'even', 'on', 'other', 'med', '##s', ',', 'these', 'tips', 'will', 'help', 'you', 'eat', 'better', 'and', 'in', 'turn', 'will', 'actually', 'help', 'manage', 'some', 'of', 'your', 'attention', 'troubles', ',', 'i', \"'\", 've', 'found', '.', 'i', \"'\", 'd', 'like', 'this', 'to', 'be', 'sort', 'of', 'a', 'comprehensive', 'list', ',', 'so', 'that', 'people', 'don', \"'\", 't', 'have', 'to', 'do', 'what', 'i', 'did', '.', 'i', \"'\", 've', 'love', 'more', 'suggestions', '.', 'i', \"'\", 've', 'tried', 'to', 'make', 'a', 'simple', ',', 'easy', '-', 'to', '-', 'read', 'layout', '.', 'i', 'know', 'it', \"'\", 's', 'long', '!', 'without', 'further', 'ad', '##o', ':', '*', '*', '*', 'eating', 'and', 'eating', 'healthy', '*', '*', '*', '*', '*', '*', 'eat', 'a', 'decent', 'amount', 'before', 'your', 'pill', '*', '*', '.', 'you', 'need', 'a', 'healthy', ',', 'high', 'protein', 'breakfast', 'before', 'your', 'med', '##s', '.', 'some', 'say', 'to', 'eat', 'as', 'much', 'as', 'possible', ',', 'but', 'i', \"'\", 'd', 'argue', 'against', 'that', '.', 'you', 'need', 'to', 'be', 'eating', 'throughout', 'the', 'day', ',', 'especially', 'on', 'v', '##y', '##van', '##se', ',', 'as', 'food', 'helps', 'to', 'release', 'it', '.', 'eating', 'until', 'you', \"'\", 're', 'stuffed', 'will', 'get', 'you', 'through', 'part', 'of', 'the', 'day', ',', 'but', 'it', 'only', 'makes', 'it', 'harder', 'to', 'keep', 'up', 'a', 'regular', 'eating', 'habit', '.', '*', '*', '*', 'liquid', 'cal', '##ories', '.', '*', '*', 'if', 'you', 'really', 'can', \"'\", 't', 'make', 'yourself', 'eat', ',', 'but', 'you', 'know', 'you', 'should', ',', 'try', 'for', 'some', 'liquid', 'cal', '##ories', '.', 'a', 'protein', 'smooth', '##ie', 'or', 'something', 'like', '[', 'ensure', ']', '(', 'http', ':', '/', '/', 'abbott', '##nut', '##rit', '##ion', '.', 'com', '/', 'products', '/', 'ensure', '-', 'plus', ')', 'is', 'a', 'great', 'way', '.', 'i', 'especially', 'love', 'ensure', ',', 'because', 'it', 'is', 'lac', '##tose', 'safe', '.', 'i', \"'\", 've', 'since', 'grown', 'out', 'of', 'my', 'lac', '##tose', 'into', '##ler', '##ance', ',', 'but', 'these', 'things', 'go', 'down', 'really', 'smooth', '.', 'heavy', 'dairy', 'isn', \"'\", 't', 'the', 'greatest', 'for', 'upset', 'stomach', '##s', ',', 'so', 'i', 'highly', ',', 'highly', 'recommend', 'picking', 'up', 'a', 'few', '.', 'tastes', 'like', 'chocolate', 'milk', '!', '*', '*', '*', 'drink', 'lots', 'of', 'water', '*', '*', '!', 'even', 'if', 'you', 'don', \"'\", 't', 'feel', 'thirsty', '.', 'you', 'should', 'be', 'drinking', '1', '1', '/', '2', 'to', '3', 'litres', 'a', 'day', '*', 'without', '*', 'medicine', '.', 'i', 'scrape', 'at', 'least', 'one', 'and', 'a', 'half', ',', 'and', 'it', \"'\", 's', 'almost', 'guaranteed', 'that', 'i', \"'\", 'm', 'small', 'than', 'you', '.', 'you', 'can', 'do', 'it', '!', 'if', 'you', 'get', 'that', 'tired', 'of', 'water', ',', 'a', 'glass', 'of', '[', 'crystal', 'light', ']', '(', 'http', ':', '/', '/', 'www', '.', 'kraft', '##cana', '##da', '.', 'com', '/', 'en', '/', 'products', '/', 'a', '-', 'c', '/', 'crystal', '##light', '##2', '.', 'as', '##p', '##x', ')', 'is', 'a', 'nice', 'break', '.', 'i', 'especially', 'love', 'the', 'iced', 'tea', '.', 'it', \"'\", 's', 'better', 'than', 'nest', '##le', 'in', 'mode', '##ration', '.', '*', '*', '*', 'peanut', 'butter', '*', '*', 'is', 'great', 'for', 'upset', 'stomach', '##s', '.', 'some', 'peanut', 'butter', 'and', 'toast', 'twenty', 'minutes', 'before', 'your', 'pill', 'helps', 'a', 'lot', ',', 'especially', 'with', 'st', '##rat', '##tera', '.', '*', '*', '*', 'multi', '-', 'vitamin', '##s', '*', '*', '.', 'it', \"'\", 's', 'widely', 'accepted', 'that', 'your', 'eating', 'goes', 'to', 'crap', 'on', 'these', 'medications', '.', 'a', '[', 'one', 'a', 'day', ']', '(', 'http', ':', '/', '/', 'www', '.', 'one', '##aday', '.', 'com', '/', ')', 'or', 'something', 'like', 'that', 'should', 'be', 'good', '.', 'i', \"'\", 've', 'heard', 'to', 'stay', 'away', 'from', 'too', 'much', 'vitamin', 'c', 'though', ',', 'as', 'it', 'can', 'affect', 'the', 'absorption', 'of', 'v', '##y', '##van', '##se', ',', 'at', 'least', '.', '*', '*', '*', 'eat', 'a', 'high', '-', 'protein', ',', 'high', 'cal', '##ori', '##e', 'snack', 'every', 'three', 'hours', 'or', 'so', '.', '*', '*', 'i', 'went', 'to', 'the', 'bulk', 'barn', 'and', 'made', 'up', 'my', 'own', 'trail', 'mix', ':', 'peanuts', ',', 'cash', '##ew', '##s', ',', 'pre', '##tze', '##ls', ',', 'dried', 'bananas', ',', 'and', 'almond', '##s', '.', 'definitely', 'have', 'some', 'fruit', '.', 'gran', '##ola', '/', 'cl', '##if', 'bars', 'etc', '.', 'are', 'great', 'too', '.', '(', 'thanks', 'someone', '##far', '##ted', ')', 'for', 'people', 'with', 'all', '##er', '##gies', ',', 'i', \"'\", 've', 'heard', 'good', 'cheese', 'and', 'fruit', 'are', 'excellent', 'as', 'well', '.', 'v', '##y', '##van', '##se', 'depends', 'on', 'food', 'to', 'work', 'properly', ',', 'so', 'a', 'small', 'snack', 'every', 'few', 'hours', 'will', 'keep', 'your', 'medicine', 'working', 'well', '.', '*', '*', '*', 'incense', '.', '*', '*', 'so', 'this', 'is', 'really', 'weird', ',', 'but', 'incense', 'helps', 'with', 'my', 'appetite', 'immensely', '.', 'i', 'discovered', 'it', 'by', 'accident', 'today', '.', 'wasn', \"'\", 't', 'hungry', 'at', 'all', ',', 'then', 'decided', 'i', 'would', 'burn', 'some', 'of', 'mandarin', '[', 'orange', ']', 'incense', 'oil', '.', 'about', 'ten', 'minutes', 'in', ',', 'i', 'was', 'starving', '!', 'ate', 'a', 'large', 'snack', 'after', 'that', 'one', '.', '*', '*', '*', 'cut', 'out', 'the', 'junk', 'food', '.', '*', '*', 'i', 'stopped', 'drinking', 'pop', '(', 'i', 'was', 'at', 'two', 'or', 'three', 'a', 'day', ')', 'cold', 'turkey', ',', 'and', 'after', 'a', 'few', 'days', 'the', 'caf', '##fe', '##ine', 'headache', '##s', 'went', 'away', ',', 'and', 'i', \"'\", 'm', 'feeling', 'much', 'better', 'overall', '.', 'i', 'cut', 'out', 'chips', ',', 'ram', '##en', ',', 'and', '[', 'for', 'the', 'most', 'part', ']', 'things', 'like', 'bag', '##el', 'bites', '.', 'if', 'you', \"'\", 're', 'trying', 'to', 'gain', 'weight', ',', 'do', 'it', 'healthy', '.', 'you', \"'\", 'll', 'naturally', 'cr', '##ave', 'fatty', 'foods', 'to', 'make', 'up', 'for', 'it', ',', 'but', 'don', \"'\", 't', 'ind', '##ul', '##ge', 'too', 'often', '.', 'eating', 'rarely', '*', '*', 'and', '*', '*', 'poorly', 'will', 'make', 'both', 'your', 'physical', 'and', 'mental', 'health', 'worse', '.', 'google', '\"', 'high', 'cal', '##ori', '##e', 'foods', '\"', 'to', 'get', 'some', 'great', 'ideas', 'for', 'healthy', 'meals', '.', '*', '*', '*', 'make', 'your', 'food', 'app', '##eti', '##zing', '*', '*', '.', '(', 'courtesy', 'sc', '##hmi', '##n', '!', ')', '[', '\\\\', '(', 'my', 'start', 'on', 'a', 'collection', 'of', 'easy', ',', 'healthy', 'go', '##ur', '##met', 'recipes', '.', '\\\\', ')', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 'rq', '##57', '##t', '/', 'what', '_', 'should', '_', 'we', '_', 'add', '_', 'to', '_', 'the', '_', 'fa', '##q', '_', 'or', '_', 'im', '_', 'tired', '_', 'of', '/', 'c', '##48', '##3', '##md', '##4', ')', '[', '6', 'fruit', '-', 'packed', 'flavors', 'of', 'overnight', ',', 'no', '-', 'cook', 'refrigerator', 'o', '##at', '##me', '##al', ']', '(', 'http', ':', '/', '/', 'www', '.', 'they', '##um', '##my', '##life', '.', 'com', '/', 'blog', '/', '2012', '/', '03', '/', '293', '/', 'overnight', '%', '2', '##c', '+', 'no', '-', 'cook', '+', 'refrigerator', '+', 'o', '##at', '##me', '##al', '+', '-', '-', '+', 'a', '+', 'healthy', '+', 'breakfast', '+', 'made', '+', 'in', '+', 'mason', '+', 'jars', '+', 'in', '+', 'six', '+', 'different', '+', 'flavors', '!', ')', '[', '6', 'fruit', '-', 'packed', 'flavors', 'of', 'make', '-', 'ahead', 'o', '##at', '##me', '##al', 'smooth', '##ies', ']', '(', 'http', ':', '/', '/', 'www', '.', 'they', '##um', '##my', '##life', '.', 'com', '/', 'blog', '/', '2012', '/', '04', '/', '299', '/', 'make', '-', 'ahead', '+', 'o', '##at', '##me', '##al', '+', 'smooth', '##ies', '+', '-', '-', '+', 'healthy', '+', '%', '26', '+', 'delicious', '+', 'with', '+', 'grab', '-', 'and', '-', 'go', '+', 'convenience', '%', '3', '##b', '+', '6', '+', 'varieties', '%', '2', '##c', '+', 'plus', '+', 'how', '+', 'to', '+', 'in', '##vent', '+', 'your', '+', 'own', ')', '.', '*', 'pepper', '##min', '##t', 'and', 'cinnamon', 'can', 'also', 'pi', '##que', 'your', 'appetite', ',', 'as', 'does', 'orange', '.', '*', '*', '*', '*', 'exercising', '*', '*', '*', '*', '*', '*', 'daily', 'exercise', 'is', 'highly', 'recommended', '.', '*', '*', 'my', 'workout', 'is', 'upper', 'body', 'and', 'core', 'on', 'mondays', ',', 'wednesday', '##s', ',', 'and', 'fridays', '.', 'lower', 'body', 'and', 'card', '##io', 'is', 'on', 'tuesday', '##s', 'and', 'thursday', '##s', '.', 'i', 'average', 'about', 'forty', 'minutes', 'a', 'day', '.', 'now', ',', 'i', 'don', \"'\", 't', 'pretend', 'to', 'be', 'any', 'sort', 'of', 'expert', 'on', 'this', ',', 'so', 'here', 'i', \"'\", 'm', 'only', 'really', 'recommend', '##ing', 'what', 'to', 'look', 'up', '.', '/', 'r', '/', 'fitness', 'is', 'a', 'wonderful', 'resource', '.', 'i', \"'\", 'm', 'not', 'naturally', 'inclined', 'to', 'athletic', '##ism', ',', 'thus', 'i', 'find', 'it', 'very', 'hard', 'to', 'mo', '##tiv', '##ate', 'myself', '.', 'so', ':', '*', '*', '*', 'get', 'a', 'buddy', '!', '*', '*', 'find', 'a', 'friend', 'to', 'work', 'out', 'with', '.', 'it', 'mo', '##tiv', '##ates', 'you', 'to', 'go', 'when', 'you', 'don', \"'\", 't', 'want', 'to', '.', 'and', 'it', \"'\", 's', 'always', 'easier', '(', 'and', 'safer', '!', ')', 'with', 'a', 'friend', '.', '*', '*', '*', 'schedules', '.', '*', '*', 'i', 'know', ',', 'easily', 'one', 'of', 'our', 'worst', 'enemies', '.', 'i', 'have', 'trouble', 'following', 'set', 'times', ',', 'so', 'what', 'i', 'do', 'is', 'pick', 'a', 'time', 'slot', 'of', 'about', 'half', 'an', 'hour', '.', 'i', 'have', 'to', 'start', 'within', 'that', 'half', 'hour', '.', 'it', 'gives', 'me', 'a', 'little', 'lee', '##way', 'if', 'there', \"'\", 's', 'something', 'i', 'want', 'to', 'finish', 'first', '.', 'nothing', 'is', 'worse', 'than', 'being', 'interrupted', '!', 'as', 'well', ',', 'a', 'good', 'buddy', 'will', 'help', 'you', 'stay', 'on', 'track', '.', '*', '*', '*', 'a', 'quick', 'note', 'of', 'concern', '.', '*', '*', 'it', 'is', 'risky', 'to', 'exercise', 'if', 'you', \"'\", 're', 'not', 'eating', 'properly', '.', 'you', '*', '*', 'must', '*', '*', 'have', 'something', 'to', 'eat', 'and', 'let', 'it', 'sit', 'before', 'you', 'workout', '.', 'and', 'use', 'some', 'discretion', '/', 'common', 'sense', ';', 'if', 'that', \"'\", 's', 'the', 'first', 'time', 'you', \"'\", 've', 'eaten', 'in', 'two', 'days', ',', 'it', \"'\", 's', 'not', 'enough', '.', 'keep', 'eating', ',', 'then', 'get', 'back', 'to', 'it', 'later', '.', '*', '*', 'it', \"'\", 's', 'better', 'to', 'skip', 'a', 'workout', 'than', 'to', 'do', 'it', 'on', 'an', 'empty', 'stomach', '.', '*', '*', 'if', 'you', 'have', 'a', 'buddy', ',', 'let', 'them', 'know', 'to', 'watch', 'for', 'your', 'eating', '.', 'they', 'may', 'need', 'to', 'remind', 'you', '.', 'just', 'because', 'you', 'don', \"'\", 't', 'feel', 'hungry', ',', 'doesn', \"'\", 't', 'mean', 'you', 'aren', \"'\", 't', '.', 'make', 'sure', 'to', 'eat', 'after', 'too', '!', 'even', 'just', 'a', 'good', 'protein', 'shake', '.', '*', '*', '*', 'water', '.', '*', '*', 'you', 'better', 'be', 'drinking', 'lots', '!', 'i', 'drink', 'just', 'under', 'a', 'litre', 'every', 'time', 'i', 'work', 'out', '.', 'and', 'i', 'am', 'small', '!', 'you', 'bigger', 'lads', 'best', 'be', 'drinking', 'more', 'than', 'that', '.', '*', '*', '*', 'final', 'notes', '*', '*', '*', 'i', 'am', 'running', 'out', 'of', 'word', 'space', '!', 'i', 'really', 'hope', 'this', 'helps', '.', 'i', 'have', 'depression', 'as', 'well', 'and', 'these', 'tips', 'are', 'great', 'for', 'your', 'overall', 'happiness', '.', 'it', \"'\", 's', 'amazing', 'how', 'such', 'little', 'things', 'help', 'so', 'much', '.', 'it', 'fix', '##es', 'sleeping', 'problems', 'too', '.', 'if', 'trees', 'aren', \"'\", 't', 'your', 'thing', 'these', 'tips', 'are', 'especially', 'helpful', '.', 'i', \"'\", 've', 'cut', 'down', 'a', 'ton', '##ne', 'on', 'my', 'smoking', 'since', 'starting', 'this', '.', '<', '3', 'edit', ':', 'woo', '##ho', '##o', ',', 'i', 'got', 'a', 'label', '!']\n",
      "INFO:__main__:Number of tokens: 1801\n",
      "INFO:__main__:Number of chunks: 4\n",
      "INFO:__main__:Chunks: [['some', 'tips', 'for', 'eating', 'on', 'v', '##y', '##vance', '(', 'and', 'other', 'such', 'drugs', ')', 'i', 'started', 'v', '##y', '##vance', 'recently', ',', 'and', 'so', 'i', 'sc', '##oured', 'the', 'land', 'for', 'tips', 'on', 'managing', 'my', 'new', 'non', '-', 'existent', 'appetite', '.', 'i', \"'\", 'd', 'like', 'to', 'share', 'with', 'you', 'the', 'things', 'i', \"'\", 've', 'found', ',', 'in', 'the', 'hopes', 'that', 'you', \"'\", 'll', 'get', 'some', 'help', 'out', 'of', 'it', 'too', '.', 'even', 'on', 'other', 'med', '##s', ',', 'these', 'tips', 'will', 'help', 'you', 'eat', 'better', 'and', 'in', 'turn', 'will', 'actually', 'help', 'manage', 'some', 'of', 'your', 'attention', 'troubles', ',', 'i', \"'\", 've', 'found', '.', 'i', \"'\", 'd', 'like', 'this', 'to', 'be', 'sort', 'of', 'a', 'comprehensive', 'list', ',', 'so', 'that', 'people', 'don', \"'\", 't', 'have', 'to', 'do', 'what', 'i', 'did', '.', 'i', \"'\", 've', 'love', 'more', 'suggestions', '.', 'i', \"'\", 've', 'tried', 'to', 'make', 'a', 'simple', ',', 'easy', '-', 'to', '-', 'read', 'layout', '.', 'i', 'know', 'it', \"'\", 's', 'long', '!', 'without', 'further', 'ad', '##o', ':', '*', '*', '*', 'eating', 'and', 'eating', 'healthy', '*', '*', '*', '*', '*', '*', 'eat', 'a', 'decent', 'amount', 'before', 'your', 'pill', '*', '*', '.', 'you', 'need', 'a', 'healthy', ',', 'high', 'protein', 'breakfast', 'before', 'your', 'med', '##s', '.', 'some', 'say', 'to', 'eat', 'as', 'much', 'as', 'possible', ',', 'but', 'i', \"'\", 'd', 'argue', 'against', 'that', '.', 'you', 'need', 'to', 'be', 'eating', 'throughout', 'the', 'day', ',', 'especially', 'on', 'v', '##y', '##van', '##se', ',', 'as', 'food', 'helps', 'to', 'release', 'it', '.', 'eating', 'until', 'you', \"'\", 're', 'stuffed', 'will', 'get', 'you', 'through', 'part', 'of', 'the', 'day', ',', 'but', 'it', 'only', 'makes', 'it', 'harder', 'to', 'keep', 'up', 'a', 'regular', 'eating', 'habit', '.', '*', '*', '*', 'liquid', 'cal', '##ories', '.', '*', '*', 'if', 'you', 'really', 'can', \"'\", 't', 'make', 'yourself', 'eat', ',', 'but', 'you', 'know', 'you', 'should', ',', 'try', 'for', 'some', 'liquid', 'cal', '##ories', '.', 'a', 'protein', 'smooth', '##ie', 'or', 'something', 'like', '[', 'ensure', ']', '(', 'http', ':', '/', '/', 'abbott', '##nut', '##rit', '##ion', '.', 'com', '/', 'products', '/', 'ensure', '-', 'plus', ')', 'is', 'a', 'great', 'way', '.', 'i', 'especially', 'love', 'ensure', ',', 'because', 'it', 'is', 'lac', '##tose', 'safe', '.', 'i', \"'\", 've', 'since', 'grown', 'out', 'of', 'my', 'lac', '##tose', 'into', '##ler', '##ance', ',', 'but', 'these', 'things', 'go', 'down', 'really', 'smooth', '.', 'heavy', 'dairy', 'isn', \"'\", 't', 'the', 'greatest', 'for', 'upset', 'stomach', '##s', ',', 'so', 'i', 'highly', ',', 'highly', 'recommend', 'picking', 'up', 'a', 'few', '.', 'tastes', 'like', 'chocolate', 'milk', '!', '*', '*', '*', 'drink', 'lots', 'of', 'water', '*', '*', '!', 'even', 'if', 'you', 'don', \"'\", 't', 'feel', 'thirsty', '.', 'you', 'should', 'be', 'drinking', '1', '1', '/', '2', 'to', '3', 'litres', 'a', 'day', '*', 'without', '*', 'medicine', '.', 'i', 'scrape', 'at', 'least', 'one', 'and', 'a', 'half', ',', 'and', 'it', \"'\", 's', 'almost', 'guaranteed', 'that', 'i', \"'\", 'm', 'small', 'than', 'you', '.', 'you', 'can', 'do', 'it', '!', 'if', 'you', 'get', 'that', 'tired', 'of', 'water', ',', 'a', 'glass', 'of', '[', 'crystal', 'light', ']', '(', 'http', ':', '/', '/', 'www', '.', 'kraft', '##cana', '##da', '.', 'com', '/', 'en', '/', 'products', '/', 'a', '-', 'c', '/', 'crystal', '##light', '##2', '.', 'as', '##p', '##x', ')', 'is', 'a', 'nice', 'break', '.', 'i', 'especially', 'love', 'the', 'iced'], ['tea', '.', 'it', \"'\", 's', 'better', 'than', 'nest', '##le', 'in', 'mode', '##ration', '.', '*', '*', '*', 'peanut', 'butter', '*', '*', 'is', 'great', 'for', 'upset', 'stomach', '##s', '.', 'some', 'peanut', 'butter', 'and', 'toast', 'twenty', 'minutes', 'before', 'your', 'pill', 'helps', 'a', 'lot', ',', 'especially', 'with', 'st', '##rat', '##tera', '.', '*', '*', '*', 'multi', '-', 'vitamin', '##s', '*', '*', '.', 'it', \"'\", 's', 'widely', 'accepted', 'that', 'your', 'eating', 'goes', 'to', 'crap', 'on', 'these', 'medications', '.', 'a', '[', 'one', 'a', 'day', ']', '(', 'http', ':', '/', '/', 'www', '.', 'one', '##aday', '.', 'com', '/', ')', 'or', 'something', 'like', 'that', 'should', 'be', 'good', '.', 'i', \"'\", 've', 'heard', 'to', 'stay', 'away', 'from', 'too', 'much', 'vitamin', 'c', 'though', ',', 'as', 'it', 'can', 'affect', 'the', 'absorption', 'of', 'v', '##y', '##van', '##se', ',', 'at', 'least', '.', '*', '*', '*', 'eat', 'a', 'high', '-', 'protein', ',', 'high', 'cal', '##ori', '##e', 'snack', 'every', 'three', 'hours', 'or', 'so', '.', '*', '*', 'i', 'went', 'to', 'the', 'bulk', 'barn', 'and', 'made', 'up', 'my', 'own', 'trail', 'mix', ':', 'peanuts', ',', 'cash', '##ew', '##s', ',', 'pre', '##tze', '##ls', ',', 'dried', 'bananas', ',', 'and', 'almond', '##s', '.', 'definitely', 'have', 'some', 'fruit', '.', 'gran', '##ola', '/', 'cl', '##if', 'bars', 'etc', '.', 'are', 'great', 'too', '.', '(', 'thanks', 'someone', '##far', '##ted', ')', 'for', 'people', 'with', 'all', '##er', '##gies', ',', 'i', \"'\", 've', 'heard', 'good', 'cheese', 'and', 'fruit', 'are', 'excellent', 'as', 'well', '.', 'v', '##y', '##van', '##se', 'depends', 'on', 'food', 'to', 'work', 'properly', ',', 'so', 'a', 'small', 'snack', 'every', 'few', 'hours', 'will', 'keep', 'your', 'medicine', 'working', 'well', '.', '*', '*', '*', 'incense', '.', '*', '*', 'so', 'this', 'is', 'really', 'weird', ',', 'but', 'incense', 'helps', 'with', 'my', 'appetite', 'immensely', '.', 'i', 'discovered', 'it', 'by', 'accident', 'today', '.', 'wasn', \"'\", 't', 'hungry', 'at', 'all', ',', 'then', 'decided', 'i', 'would', 'burn', 'some', 'of', 'mandarin', '[', 'orange', ']', 'incense', 'oil', '.', 'about', 'ten', 'minutes', 'in', ',', 'i', 'was', 'starving', '!', 'ate', 'a', 'large', 'snack', 'after', 'that', 'one', '.', '*', '*', '*', 'cut', 'out', 'the', 'junk', 'food', '.', '*', '*', 'i', 'stopped', 'drinking', 'pop', '(', 'i', 'was', 'at', 'two', 'or', 'three', 'a', 'day', ')', 'cold', 'turkey', ',', 'and', 'after', 'a', 'few', 'days', 'the', 'caf', '##fe', '##ine', 'headache', '##s', 'went', 'away', ',', 'and', 'i', \"'\", 'm', 'feeling', 'much', 'better', 'overall', '.', 'i', 'cut', 'out', 'chips', ',', 'ram', '##en', ',', 'and', '[', 'for', 'the', 'most', 'part', ']', 'things', 'like', 'bag', '##el', 'bites', '.', 'if', 'you', \"'\", 're', 'trying', 'to', 'gain', 'weight', ',', 'do', 'it', 'healthy', '.', 'you', \"'\", 'll', 'naturally', 'cr', '##ave', 'fatty', 'foods', 'to', 'make', 'up', 'for', 'it', ',', 'but', 'don', \"'\", 't', 'ind', '##ul', '##ge', 'too', 'often', '.', 'eating', 'rarely', '*', '*', 'and', '*', '*', 'poorly', 'will', 'make', 'both', 'your', 'physical', 'and', 'mental', 'health', 'worse', '.', 'google', '\"', 'high', 'cal', '##ori', '##e', 'foods', '\"', 'to', 'get', 'some', 'great', 'ideas', 'for', 'healthy', 'meals', '.', '*', '*', '*', 'make', 'your', 'food', 'app', '##eti', '##zing', '*', '*', '.', '(', 'courtesy', 'sc', '##hmi', '##n', '!', ')', '[', '\\\\', '(', 'my', 'start', 'on', 'a', 'collection', 'of', 'easy', ',', 'healthy', 'go', '##ur', '##met', 'recipes', '.', '\\\\', ')', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/'], ['ad', '##hd', '/', 'comments', '/', 'rq', '##57', '##t', '/', 'what', '_', 'should', '_', 'we', '_', 'add', '_', 'to', '_', 'the', '_', 'fa', '##q', '_', 'or', '_', 'im', '_', 'tired', '_', 'of', '/', 'c', '##48', '##3', '##md', '##4', ')', '[', '6', 'fruit', '-', 'packed', 'flavors', 'of', 'overnight', ',', 'no', '-', 'cook', 'refrigerator', 'o', '##at', '##me', '##al', ']', '(', 'http', ':', '/', '/', 'www', '.', 'they', '##um', '##my', '##life', '.', 'com', '/', 'blog', '/', '2012', '/', '03', '/', '293', '/', 'overnight', '%', '2', '##c', '+', 'no', '-', 'cook', '+', 'refrigerator', '+', 'o', '##at', '##me', '##al', '+', '-', '-', '+', 'a', '+', 'healthy', '+', 'breakfast', '+', 'made', '+', 'in', '+', 'mason', '+', 'jars', '+', 'in', '+', 'six', '+', 'different', '+', 'flavors', '!', ')', '[', '6', 'fruit', '-', 'packed', 'flavors', 'of', 'make', '-', 'ahead', 'o', '##at', '##me', '##al', 'smooth', '##ies', ']', '(', 'http', ':', '/', '/', 'www', '.', 'they', '##um', '##my', '##life', '.', 'com', '/', 'blog', '/', '2012', '/', '04', '/', '299', '/', 'make', '-', 'ahead', '+', 'o', '##at', '##me', '##al', '+', 'smooth', '##ies', '+', '-', '-', '+', 'healthy', '+', '%', '26', '+', 'delicious', '+', 'with', '+', 'grab', '-', 'and', '-', 'go', '+', 'convenience', '%', '3', '##b', '+', '6', '+', 'varieties', '%', '2', '##c', '+', 'plus', '+', 'how', '+', 'to', '+', 'in', '##vent', '+', 'your', '+', 'own', ')', '.', '*', 'pepper', '##min', '##t', 'and', 'cinnamon', 'can', 'also', 'pi', '##que', 'your', 'appetite', ',', 'as', 'does', 'orange', '.', '*', '*', '*', '*', 'exercising', '*', '*', '*', '*', '*', '*', 'daily', 'exercise', 'is', 'highly', 'recommended', '.', '*', '*', 'my', 'workout', 'is', 'upper', 'body', 'and', 'core', 'on', 'mondays', ',', 'wednesday', '##s', ',', 'and', 'fridays', '.', 'lower', 'body', 'and', 'card', '##io', 'is', 'on', 'tuesday', '##s', 'and', 'thursday', '##s', '.', 'i', 'average', 'about', 'forty', 'minutes', 'a', 'day', '.', 'now', ',', 'i', 'don', \"'\", 't', 'pretend', 'to', 'be', 'any', 'sort', 'of', 'expert', 'on', 'this', ',', 'so', 'here', 'i', \"'\", 'm', 'only', 'really', 'recommend', '##ing', 'what', 'to', 'look', 'up', '.', '/', 'r', '/', 'fitness', 'is', 'a', 'wonderful', 'resource', '.', 'i', \"'\", 'm', 'not', 'naturally', 'inclined', 'to', 'athletic', '##ism', ',', 'thus', 'i', 'find', 'it', 'very', 'hard', 'to', 'mo', '##tiv', '##ate', 'myself', '.', 'so', ':', '*', '*', '*', 'get', 'a', 'buddy', '!', '*', '*', 'find', 'a', 'friend', 'to', 'work', 'out', 'with', '.', 'it', 'mo', '##tiv', '##ates', 'you', 'to', 'go', 'when', 'you', 'don', \"'\", 't', 'want', 'to', '.', 'and', 'it', \"'\", 's', 'always', 'easier', '(', 'and', 'safer', '!', ')', 'with', 'a', 'friend', '.', '*', '*', '*', 'schedules', '.', '*', '*', 'i', 'know', ',', 'easily', 'one', 'of', 'our', 'worst', 'enemies', '.', 'i', 'have', 'trouble', 'following', 'set', 'times', ',', 'so', 'what', 'i', 'do', 'is', 'pick', 'a', 'time', 'slot', 'of', 'about', 'half', 'an', 'hour', '.', 'i', 'have', 'to', 'start', 'within', 'that', 'half', 'hour', '.', 'it', 'gives', 'me', 'a', 'little', 'lee', '##way', 'if', 'there', \"'\", 's', 'something', 'i', 'want', 'to', 'finish', 'first', '.', 'nothing', 'is', 'worse', 'than', 'being', 'interrupted', '!', 'as', 'well', ',', 'a', 'good', 'buddy', 'will', 'help', 'you', 'stay', 'on', 'track', '.', '*', '*', '*', 'a', 'quick', 'note', 'of', 'concern', '.', '*', '*', 'it', 'is', 'risky', 'to', 'exercise', 'if', 'you', \"'\", 're', 'not', 'eating', 'properly', '.', 'you', '*', '*', 'must'], ['*', '*', 'have', 'something', 'to', 'eat', 'and', 'let', 'it', 'sit', 'before', 'you', 'workout', '.', 'and', 'use', 'some', 'discretion', '/', 'common', 'sense', ';', 'if', 'that', \"'\", 's', 'the', 'first', 'time', 'you', \"'\", 've', 'eaten', 'in', 'two', 'days', ',', 'it', \"'\", 's', 'not', 'enough', '.', 'keep', 'eating', ',', 'then', 'get', 'back', 'to', 'it', 'later', '.', '*', '*', 'it', \"'\", 's', 'better', 'to', 'skip', 'a', 'workout', 'than', 'to', 'do', 'it', 'on', 'an', 'empty', 'stomach', '.', '*', '*', 'if', 'you', 'have', 'a', 'buddy', ',', 'let', 'them', 'know', 'to', 'watch', 'for', 'your', 'eating', '.', 'they', 'may', 'need', 'to', 'remind', 'you', '.', 'just', 'because', 'you', 'don', \"'\", 't', 'feel', 'hungry', ',', 'doesn', \"'\", 't', 'mean', 'you', 'aren', \"'\", 't', '.', 'make', 'sure', 'to', 'eat', 'after', 'too', '!', 'even', 'just', 'a', 'good', 'protein', 'shake', '.', '*', '*', '*', 'water', '.', '*', '*', 'you', 'better', 'be', 'drinking', 'lots', '!', 'i', 'drink', 'just', 'under', 'a', 'litre', 'every', 'time', 'i', 'work', 'out', '.', 'and', 'i', 'am', 'small', '!', 'you', 'bigger', 'lads', 'best', 'be', 'drinking', 'more', 'than', 'that', '.', '*', '*', '*', 'final', 'notes', '*', '*', '*', 'i', 'am', 'running', 'out', 'of', 'word', 'space', '!', 'i', 'really', 'hope', 'this', 'helps', '.', 'i', 'have', 'depression', 'as', 'well', 'and', 'these', 'tips', 'are', 'great', 'for', 'your', 'overall', 'happiness', '.', 'it', \"'\", 's', 'amazing', 'how', 'such', 'little', 'things', 'help', 'so', 'much', '.', 'it', 'fix', '##es', 'sleeping', 'problems', 'too', '.', 'if', 'trees', 'aren', \"'\", 't', 'your', 'thing', 'these', 'tips', 'are', 'especially', 'helpful', '.', 'i', \"'\", 've', 'cut', 'down', 'a', 'ton', '##ne', 'on', 'my', 'smoking', 'since', 'starting', 'this', '.', '<', '3', 'edit', ':', 'woo', '##ho', '##o', ',', 'i', 'got', 'a', 'label', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['#', 'what', '##sho', '##uld', '##we', '##cal', '##lm', '##e', ':', 'top', '10', 'gi', '##fs', 'and', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['#', 'what', '##sho', '##uld', '##we', '##cal', '##lm', '##e', ':', 'top', '10', 'gi', '##fs', 'and', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['this', '.', 'gi', '##f', 'hilarious', '##ly', 'and', 'sadly', 'sum', '##mar', '##izes', 'life', 'with', 'ad', '##hd', 'to', 'me', '.', '.', '.', 'so', 'much', 'promise']\n",
      "INFO:__main__:Number of tokens: 23\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['this', '.', 'gi', '##f', 'hilarious', '##ly', 'and', 'sadly', 'sum', '##mar', '##izes', 'life', 'with', 'ad', '##hd', 'to', 'me', '.', '.', '.', 'so', 'much', 'promise']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['took', 'my', 'ne', '##uro', '##psy', '##ch', 'evaluation', 'a', 'few', 'days', 'ago', '.', '.']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['took', 'my', 'ne', '##uro', '##psy', '##ch', 'evaluation', 'a', 'few', 'days', 'ago', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['using', 'co', '##gm', '##ed', 'to', 'improve', 'working', 'memory', 'my', 'dr', 'suggested', 'i', 'use', 'http', ':', '/', '/', 'www', '.', 'co', '##gm', '##ed', '.', 'com', '/', 'for', 'their', 'training', 'tool', '.', 'i', 'have', 'no', 'idea', 'how', 'much', 'it', 'costs', '.', 'has', 'anyone', 'on', 'here', 'used', 'it', 'before', '?', 'how', 'much', 'did', 'it', 'cost', '?', 'did', 'you', 'find', 'it', 'effective', '?', 'i', \"'\", 'm', 'a', 'bit', 'concerned', 'because', 'i', 'worry', 'that', 'my', 'dr', 'suggests', 'this', 'for', 'his', 'own', 'financial', 'gain', '.', 'are', 'there', 'any', 'free', 'ad', 'effective', 'versions', 'of', 'this', '?']\n",
      "INFO:__main__:Number of tokens: 89\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['using', 'co', '##gm', '##ed', 'to', 'improve', 'working', 'memory', 'my', 'dr', 'suggested', 'i', 'use', 'http', ':', '/', '/', 'www', '.', 'co', '##gm', '##ed', '.', 'com', '/', 'for', 'their', 'training', 'tool', '.', 'i', 'have', 'no', 'idea', 'how', 'much', 'it', 'costs', '.', 'has', 'anyone', 'on', 'here', 'used', 'it', 'before', '?', 'how', 'much', 'did', 'it', 'cost', '?', 'did', 'you', 'find', 'it', 'effective', '?', 'i', \"'\", 'm', 'a', 'bit', 'concerned', 'because', 'i', 'worry', 'that', 'my', 'dr', 'suggests', 'this', 'for', 'his', 'own', 'financial', 'gain', '.', 'are', 'there', 'any', 'free', 'ad', 'effective', 'versions', 'of', 'this', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ha', '##e', 'switched', 'from', 'add', '##eral', '##l', 'sr', 'to', 'add', '##eral', '##l', 'x', '##r', 'my', 'doctor', ',', 'at', 'my', 'request', 'switched', 'me', 'from', '20', 'mg', 'sr', 'to', '40', 'mg', 'x', '##r', '.', 'i', 'tried', '20', 'mg', 'x', '##r', ',', 'but', 'that', 'didn', \"'\", 't', 'seem', 'to', 'touch', 'me', '.', 'i', \"'\", 'm', 'now', 'on', '40', 'mg', 'of', 'x', '##r', ',', 'and', 'so', 'far', 'so', 'good', '.', 'anyone', 'else', 'have', 'experience', 'switching', 'from', 'sr', 'to', 'x', '##r', '?', 'how', 'long', 'does', 'it', 'last', 'for', 'you', '?']\n",
      "INFO:__main__:Number of tokens: 85\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ha', '##e', 'switched', 'from', 'add', '##eral', '##l', 'sr', 'to', 'add', '##eral', '##l', 'x', '##r', 'my', 'doctor', ',', 'at', 'my', 'request', 'switched', 'me', 'from', '20', 'mg', 'sr', 'to', '40', 'mg', 'x', '##r', '.', 'i', 'tried', '20', 'mg', 'x', '##r', ',', 'but', 'that', 'didn', \"'\", 't', 'seem', 'to', 'touch', 'me', '.', 'i', \"'\", 'm', 'now', 'on', '40', 'mg', 'of', 'x', '##r', ',', 'and', 'so', 'far', 'so', 'good', '.', 'anyone', 'else', 'have', 'experience', 'switching', 'from', 'sr', 'to', 'x', '##r', '?', 'how', 'long', 'does', 'it', 'last', 'for', 'you', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['da', '##e', 'find', 'that', 'taking', 'add', '##eral', '##l', 'didn', \"'\", 't', 'help', 'the', 'ad', '##hd', 'but', 'instead', 'just', 'added', 'o', '##cd', 'i', 'spent', '4', 'hours', 'organizing', 'my', 'pictures', 'and', 'got', 'no', 'actual', 'work', 'done', ',', 'i', 'can', \"'\", 't', 'direct', 'my', 'actions', 'still', ',', 'also', 'i', 'tried', 'st', '##rate', '##rra', ',', 'no', 'help', ',', 'no', 'idea', 'what', 'med', '##s', 'i', \"'\", 'm', 'trying', 'next']\n",
      "INFO:__main__:Number of tokens: 64\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['da', '##e', 'find', 'that', 'taking', 'add', '##eral', '##l', 'didn', \"'\", 't', 'help', 'the', 'ad', '##hd', 'but', 'instead', 'just', 'added', 'o', '##cd', 'i', 'spent', '4', 'hours', 'organizing', 'my', 'pictures', 'and', 'got', 'no', 'actual', 'work', 'done', ',', 'i', 'can', \"'\", 't', 'direct', 'my', 'actions', 'still', ',', 'also', 'i', 'tried', 'st', '##rate', '##rra', ',', 'no', 'help', ',', 'no', 'idea', 'what', 'med', '##s', 'i', \"'\", 'm', 'trying', 'next']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'my', 'symptoms', 'seem', 'most', 'typical', 'of', 'ad', '##hd', ',', 'anxiety', ',', 'depression', ',', 'or', 'something', 'else', '?', '*', '*', '*', '*', 'apologies', 'for', 'the', 'crazy', 'length', '*', '*', '*', '*', 'i', \"'\", 'm', '19', 'years', 'old', 'and', 'was', 'diagnosed', 'with', 'depression', 'when', 'i', 'was', '16', '.', 'since', 'then', 'i', \"'\", 've', 'been', 'on', 'and', 'off', '3', 'different', 'anti', '-', 'de', '##press', '##ant', '/', 'anti', '-', 'anxiety', 'med', '##s', '.', 'none', 'have', 'helped', 'at', 'all', ',', 'and', 'i', 'last', 'stopped', 'taking', 'them', 'in', 'january', 'of', 'this', 'year', '.', '(', 'med', '##s', 'i', \"'\", 've', 'been', 'on', ':', 'pro', '##za', '##c', ',', 'cy', '##pr', '##ale', '##x', ',', 'pri', '##sti', '##q', '.', ')', 'here', 'are', 'my', 'symptoms', ':', '-', 'i', 'find', 'it', 'very', 'difficult', 'to', 'communicate', 'verbal', '##ly', '.', 'this', 'is', 'the', 'most', 'major', 'issue', 'i', 'have', 'in', 'my', 'life', '.', 'when', 'i', 'talk', ',', 'it', 'comes', 'out', 'as', 'an', 'inc', '##oh', '##ere', '##nt', ',', 'mixed', 'ju', '##mble', '.', 'i', 'can', \"'\", 't', 'explain', 'myself', 'properly', '.', 'i', 'say', 'things', 'that', 'i', 'don', \"'\", 't', 'mean', '.', 'as', 'i', \"'\", 'm', 'explaining', 'something', 'to', 'someone', ',', '*', 'i', '*', 'even', 'realize', 'what', 'i', \"'\", 'm', 'saying', 'doesn', \"'\", 't', 'make', 'sense', '.', 'i', 'can', 'tell', 'they', \"'\", 're', 'confused', 'and', 'i', 'can', \"'\", 't', 'blame', 'them', 'for', 'it', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'explain', 'myself', 'better', '.', 'despite', 'this', ',', 'i', 'seem', 'to', 'be', 'fine', 'with', 'written', 'communication', '.', '-', 'ties', 'into', 'the', 'above', ',', 'but', 'i', 'can', \"'\", 't', 'pass', 'messages', 'along', '.', 'if', 'i', \"'\", 'm', 'talking', 'to', 'someone', 'on', 'the', 'phone', 'and', 'they', 'want', 'me', 'to', 'pass', 'a', 'brief', 'message', 'to', 'a', 'cow', '##or', '##ker', ',', 'i', 'have', 'to', 'either', 'write', 'or', 'type', 'down', 'everything', 'they', 'say', ',', 'no', 'matter', 'how', 'short', 'the', 'message', '.', 'writing', 'and', 'typing', 'help', ',', 'but', 'sometimes', 'they', 'aren', \"'\", 't', 'practical', '.', '-', 'i', 'can', \"'\", 't', 'watch', 'a', 'full', 'movie', 'or', 'tv', 'show', '.', 'i', 'get', 'bored', 'and', 'leave', 'ten', 'minutes', 'in', '.', 'i', 'like', 'websites', ',', 'though', '.', 'red', '##dit', 'and', 'stumble', '##up', '##on', 'are', 'great', ',', 'and', 'i', 'can', 'spend', 'forever', 'on', 'each', '.', 'i', 'haven', \"'\", 't', 'watched', 'a', 'movie', 'for', 'probably', '7', 'months', ',', 'and', 'a', 'show', 'in', 'even', 'longer', '.', '-', 'i', 'neglect', 'details', 'that', ',', 'when', 'pointed', 'out', 'to', 'me', ',', 'were', 'so', 'glaring', '##ly', 'obvious', 'and', 'un', '##mis', '##sable', '.', 'this', 'happens', 'a', 'lot', 'of', 'the', 'time', 'at', 'work', '-', '-', 'ex', ':', 'someone', 'asks', 'me', 'to', 'do', 'x', ',', 'y', 'and', 'z', ',', 'but', 'i', 'just', 'do', 'y', 'and', 'z', ',', 'and', 'forget', 'all', 'about', 'x', '.', '-', 'i', 'have', 'tried', 'and', 'failed', 'at', 'learning', 'the', 'guitar', ',', 'piano', ',', 'getting', 'my', 'driver', \"'\", 's', 'license', '(', 'haven', \"'\", 't', 'even', 'gotten', 'my', 'g', '##1', 'yet', ')', ',', 'any', 'sports', 'that', 'i', 'try', '(', 'at', 'the', 'gym', 'i', 'get', 'overwhelming', 'anxiety', 'from', 'the', 'thought', 'of', 'not', 'being', 'able', 'to', 'find', 'a', 'free', 'machine', '.', '.', '.', ')', 'other', 'per', '##tine', '##nt', 'info', ':', '-', 'i', \"'\", 've', 'never', 'been', 'diagnosed', 'with', 'a', 'learning', 'disability', ',', 'but', 'do', 'believe', 'i', 'have', 'd', '##ys', '##cal', '##cu', '##lia', '.', 'numbers', 'simply', 'do', 'not', 'make', 'sense', 'to', 'me', '.', 'adding', ',', 'sub', '##tra', '##cting', ',', 'multi', '##ply', '##ing', 'and', 'dividing', 'do', 'not', 'come', 'naturally', 'to', 'me', ',', 'especially', 'the', 'last', 'two', '.', '-', 'my', 'dad', 'has', 'chronic', 'depression', 'which', 'he', \"'\", 's', 'on', 'med', '##s', 'for', ',', 'and', 'my', 'grandpa', 'was', 'severely', 'bipolar', '.', '-', 'i', 'have', 'a', 'summer', 'job', 'right', 'now', 'and', 'will', 'be', 'going', 'into', 'my', 'third', 'year', 'of', 'university', 'in', 'the', 'fall', '.', '-', 'i', 'have', 'never', 'been', 'hyper', '##active', ',', 'talk', '##ative', 'or', 'social', ',', 'even', 'in', 'elementary', 'school', '.', 'i', 'am', 'a', 'very', 'quiet', 'person', 'in', 'general', ',', 'to', 'the', 'point', 'of', 'being', 'pretty', 'anti', '##so', '##cial', '.', 'i', 'want', 'to', 'be', 'with', 'others', ',', 'but', 'i', 'find', 'it', 'difficult', 'to', 'open', 'up', 'to', 'them', 'since', 'i', 'can', \"'\", 't', 'seem', 'to', 'communicate', 'my', 'thoughts', 'or', 'feelings', 'very', 'accurately', '.', 'that', \"'\", 's', 'about', 'it', ',', 'i', 'think', '.', 'i', \"'\", 've', 'gone', 'from', 'being', 'convinced', 'it', 'was', 'depression', ',', 'to', 'being', 'sure', 'it', 'was', 'anxiety', ',', 'and', 'now', 'suspect', '##ing', 'it', \"'\", 's', 'ad', '##hd', '.', 'problem', 'is', 'that', 'when', 'i', 'told', 'my', 'mom', 'about', 'my', 'new', 'suspicions', ',', 'she', 'told', 'me', 'she', 'thinks', 'i', \"'\", 'm', 'over', '-', 'dia', '##gno', '##sing', 'myself', ',', 'and', 'that', 'most', 'of', 'these', '\"', 'symptoms', '\"', 'are', 'normal', '.', 'while', 'i', 'agree', 'with', 'her', 'about', 'me', 'maybe', 'path', '##olo', '##gizing', 'behaviour', '##s', 'that', 'aren', \"'\", 't', 'necessarily', 'indicative', 'of', 'a', 'mental', 'disorder', ',', 'together', 'these', 'symptoms', 'make', 'my', 'life', 'a', 'living', 'hell', '.', 'it', \"'\", 's', 'like', 'i', \"'\", 'm', 'at', 'war', 'with', 'myself', '.', 'i', 'seem', 'to', 'sabotage', 'myself', 'in', 'the', 'most', 'ins', '##idi', '##ous', 'ways', '.', '*', '*', 't', '##ld', '##r', ':', 'i', 'think', 'my', 'anxiety', 'and', 'depression', 'stem', 'from', 'verbal', 'com', '##mun', '##icative', 'issues', 'and', 'i', \"'\", 'm', 'not', 'sure', 'whether', 'i', \"'\", 'm', 'interpreting', 'normal', 'behaviour', '##s', 'as', 'being', 'ad', '##hd', ',', 'mis', '##con', '##st', '##ru', '##ing', 'my', 'anxiety', 'or', 'depression', 'as', 'being', 'ad', '##hd', ',', 'or', 'whether', 'i', 'truly', 'do', 'have', 'ad', '##hd', '*', '*']\n",
      "INFO:__main__:Number of tokens: 868\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['do', 'my', 'symptoms', 'seem', 'most', 'typical', 'of', 'ad', '##hd', ',', 'anxiety', ',', 'depression', ',', 'or', 'something', 'else', '?', '*', '*', '*', '*', 'apologies', 'for', 'the', 'crazy', 'length', '*', '*', '*', '*', 'i', \"'\", 'm', '19', 'years', 'old', 'and', 'was', 'diagnosed', 'with', 'depression', 'when', 'i', 'was', '16', '.', 'since', 'then', 'i', \"'\", 've', 'been', 'on', 'and', 'off', '3', 'different', 'anti', '-', 'de', '##press', '##ant', '/', 'anti', '-', 'anxiety', 'med', '##s', '.', 'none', 'have', 'helped', 'at', 'all', ',', 'and', 'i', 'last', 'stopped', 'taking', 'them', 'in', 'january', 'of', 'this', 'year', '.', '(', 'med', '##s', 'i', \"'\", 've', 'been', 'on', ':', 'pro', '##za', '##c', ',', 'cy', '##pr', '##ale', '##x', ',', 'pri', '##sti', '##q', '.', ')', 'here', 'are', 'my', 'symptoms', ':', '-', 'i', 'find', 'it', 'very', 'difficult', 'to', 'communicate', 'verbal', '##ly', '.', 'this', 'is', 'the', 'most', 'major', 'issue', 'i', 'have', 'in', 'my', 'life', '.', 'when', 'i', 'talk', ',', 'it', 'comes', 'out', 'as', 'an', 'inc', '##oh', '##ere', '##nt', ',', 'mixed', 'ju', '##mble', '.', 'i', 'can', \"'\", 't', 'explain', 'myself', 'properly', '.', 'i', 'say', 'things', 'that', 'i', 'don', \"'\", 't', 'mean', '.', 'as', 'i', \"'\", 'm', 'explaining', 'something', 'to', 'someone', ',', '*', 'i', '*', 'even', 'realize', 'what', 'i', \"'\", 'm', 'saying', 'doesn', \"'\", 't', 'make', 'sense', '.', 'i', 'can', 'tell', 'they', \"'\", 're', 'confused', 'and', 'i', 'can', \"'\", 't', 'blame', 'them', 'for', 'it', ',', 'but', 'i', 'don', \"'\", 't', 'know', 'how', 'to', 'explain', 'myself', 'better', '.', 'despite', 'this', ',', 'i', 'seem', 'to', 'be', 'fine', 'with', 'written', 'communication', '.', '-', 'ties', 'into', 'the', 'above', ',', 'but', 'i', 'can', \"'\", 't', 'pass', 'messages', 'along', '.', 'if', 'i', \"'\", 'm', 'talking', 'to', 'someone', 'on', 'the', 'phone', 'and', 'they', 'want', 'me', 'to', 'pass', 'a', 'brief', 'message', 'to', 'a', 'cow', '##or', '##ker', ',', 'i', 'have', 'to', 'either', 'write', 'or', 'type', 'down', 'everything', 'they', 'say', ',', 'no', 'matter', 'how', 'short', 'the', 'message', '.', 'writing', 'and', 'typing', 'help', ',', 'but', 'sometimes', 'they', 'aren', \"'\", 't', 'practical', '.', '-', 'i', 'can', \"'\", 't', 'watch', 'a', 'full', 'movie', 'or', 'tv', 'show', '.', 'i', 'get', 'bored', 'and', 'leave', 'ten', 'minutes', 'in', '.', 'i', 'like', 'websites', ',', 'though', '.', 'red', '##dit', 'and', 'stumble', '##up', '##on', 'are', 'great', ',', 'and', 'i', 'can', 'spend', 'forever', 'on', 'each', '.', 'i', 'haven', \"'\", 't', 'watched', 'a', 'movie', 'for', 'probably', '7', 'months', ',', 'and', 'a', 'show', 'in', 'even', 'longer', '.', '-', 'i', 'neglect', 'details', 'that', ',', 'when', 'pointed', 'out', 'to', 'me', ',', 'were', 'so', 'glaring', '##ly', 'obvious', 'and', 'un', '##mis', '##sable', '.', 'this', 'happens', 'a', 'lot', 'of', 'the', 'time', 'at', 'work', '-', '-', 'ex', ':', 'someone', 'asks', 'me', 'to', 'do', 'x', ',', 'y', 'and', 'z', ',', 'but', 'i', 'just', 'do', 'y', 'and', 'z', ',', 'and', 'forget', 'all', 'about', 'x', '.', '-', 'i', 'have', 'tried', 'and', 'failed', 'at', 'learning', 'the', 'guitar', ',', 'piano', ',', 'getting', 'my', 'driver', \"'\", 's', 'license', '(', 'haven', \"'\", 't', 'even', 'gotten', 'my', 'g', '##1', 'yet', ')', ',', 'any', 'sports', 'that', 'i', 'try', '(', 'at', 'the', 'gym', 'i', 'get', 'overwhelming', 'anxiety', 'from', 'the', 'thought', 'of', 'not', 'being', 'able', 'to', 'find', 'a', 'free', 'machine', '.', '.', '.', ')', 'other', 'per', '##tine', '##nt', 'info', ':', '-', 'i', \"'\", 've', 'never', 'been', 'diagnosed', 'with', 'a'], ['learning', 'disability', ',', 'but', 'do', 'believe', 'i', 'have', 'd', '##ys', '##cal', '##cu', '##lia', '.', 'numbers', 'simply', 'do', 'not', 'make', 'sense', 'to', 'me', '.', 'adding', ',', 'sub', '##tra', '##cting', ',', 'multi', '##ply', '##ing', 'and', 'dividing', 'do', 'not', 'come', 'naturally', 'to', 'me', ',', 'especially', 'the', 'last', 'two', '.', '-', 'my', 'dad', 'has', 'chronic', 'depression', 'which', 'he', \"'\", 's', 'on', 'med', '##s', 'for', ',', 'and', 'my', 'grandpa', 'was', 'severely', 'bipolar', '.', '-', 'i', 'have', 'a', 'summer', 'job', 'right', 'now', 'and', 'will', 'be', 'going', 'into', 'my', 'third', 'year', 'of', 'university', 'in', 'the', 'fall', '.', '-', 'i', 'have', 'never', 'been', 'hyper', '##active', ',', 'talk', '##ative', 'or', 'social', ',', 'even', 'in', 'elementary', 'school', '.', 'i', 'am', 'a', 'very', 'quiet', 'person', 'in', 'general', ',', 'to', 'the', 'point', 'of', 'being', 'pretty', 'anti', '##so', '##cial', '.', 'i', 'want', 'to', 'be', 'with', 'others', ',', 'but', 'i', 'find', 'it', 'difficult', 'to', 'open', 'up', 'to', 'them', 'since', 'i', 'can', \"'\", 't', 'seem', 'to', 'communicate', 'my', 'thoughts', 'or', 'feelings', 'very', 'accurately', '.', 'that', \"'\", 's', 'about', 'it', ',', 'i', 'think', '.', 'i', \"'\", 've', 'gone', 'from', 'being', 'convinced', 'it', 'was', 'depression', ',', 'to', 'being', 'sure', 'it', 'was', 'anxiety', ',', 'and', 'now', 'suspect', '##ing', 'it', \"'\", 's', 'ad', '##hd', '.', 'problem', 'is', 'that', 'when', 'i', 'told', 'my', 'mom', 'about', 'my', 'new', 'suspicions', ',', 'she', 'told', 'me', 'she', 'thinks', 'i', \"'\", 'm', 'over', '-', 'dia', '##gno', '##sing', 'myself', ',', 'and', 'that', 'most', 'of', 'these', '\"', 'symptoms', '\"', 'are', 'normal', '.', 'while', 'i', 'agree', 'with', 'her', 'about', 'me', 'maybe', 'path', '##olo', '##gizing', 'behaviour', '##s', 'that', 'aren', \"'\", 't', 'necessarily', 'indicative', 'of', 'a', 'mental', 'disorder', ',', 'together', 'these', 'symptoms', 'make', 'my', 'life', 'a', 'living', 'hell', '.', 'it', \"'\", 's', 'like', 'i', \"'\", 'm', 'at', 'war', 'with', 'myself', '.', 'i', 'seem', 'to', 'sabotage', 'myself', 'in', 'the', 'most', 'ins', '##idi', '##ous', 'ways', '.', '*', '*', 't', '##ld', '##r', ':', 'i', 'think', 'my', 'anxiety', 'and', 'depression', 'stem', 'from', 'verbal', 'com', '##mun', '##icative', 'issues', 'and', 'i', \"'\", 'm', 'not', 'sure', 'whether', 'i', \"'\", 'm', 'interpreting', 'normal', 'behaviour', '##s', 'as', 'being', 'ad', '##hd', ',', 'mis', '##con', '##st', '##ru', '##ing', 'my', 'anxiety', 'or', 'depression', 'as', 'being', 'ad', '##hd', ',', 'or', 'whether', 'i', 'truly', 'do', 'have', 'ad', '##hd', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'worse', 'with', 'med', '##s', '?', 'i', 'recently', 'started', 'taking', 'add', '##eral', '##l', ',', 'and', 'thus', 'far', 'it', \"'\", 's', 'been', 'a', 'huge', 'help', '.', 'this', 'morning', 'i', 'took', '30', 'mg', 'with', 'breakfast', ',', 'and', 'though', 'i', 'had', 'a', 'to', '-', 'do', 'list', 'written', 'out', ',', 'i', 'couldn', \"'\", 't', 'pry', 'myself', 'from', 'my', 'computer', '.', 'i', 'barely', 'even', 'ate', '.', 'i', 'did', 'feel', '*', 'different', '*', 'from', 'my', 'regular', 'symptoms', 'though', ',', 'in', 'a', 'sense', 'that', 'instead', 'of', 'jumping', 'from', 'place', 'to', 'place', ',', 'my', 'brain', 'wanted', 'to', 'read', 'everything', 'before', 'moving', 'on', 'to', 'anything', 'else', '.', 'it', \"'\", 's', 'like', 'i', 'was', 'super', '##fo', '##cus', '##ed', 'with', 'no', 'control', '.', 'my', 'doc', 'is', 'still', 'fi', '##ddling', 'with', 'my', 'dos', '##age', ',', 'so', 'i', 'suspect', 'this', 'may', 'be', 'the', 'cu', '##lp', '##rit', ',', 'but', 'i', 'have', 'been', 'taking', 'the', 'exact', 'same', 'dos', '##age', 'for', 'the', 'past', 'two', 'weeks', 'and', 'this', 'is', 'the', 'first', 'time', 'this', 'has', 'happened', '.', 'anyone', 'have', 'a', 'similar', 'experience', '/', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 168\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'worse', 'with', 'med', '##s', '?', 'i', 'recently', 'started', 'taking', 'add', '##eral', '##l', ',', 'and', 'thus', 'far', 'it', \"'\", 's', 'been', 'a', 'huge', 'help', '.', 'this', 'morning', 'i', 'took', '30', 'mg', 'with', 'breakfast', ',', 'and', 'though', 'i', 'had', 'a', 'to', '-', 'do', 'list', 'written', 'out', ',', 'i', 'couldn', \"'\", 't', 'pry', 'myself', 'from', 'my', 'computer', '.', 'i', 'barely', 'even', 'ate', '.', 'i', 'did', 'feel', '*', 'different', '*', 'from', 'my', 'regular', 'symptoms', 'though', ',', 'in', 'a', 'sense', 'that', 'instead', 'of', 'jumping', 'from', 'place', 'to', 'place', ',', 'my', 'brain', 'wanted', 'to', 'read', 'everything', 'before', 'moving', 'on', 'to', 'anything', 'else', '.', 'it', \"'\", 's', 'like', 'i', 'was', 'super', '##fo', '##cus', '##ed', 'with', 'no', 'control', '.', 'my', 'doc', 'is', 'still', 'fi', '##ddling', 'with', 'my', 'dos', '##age', ',', 'so', 'i', 'suspect', 'this', 'may', 'be', 'the', 'cu', '##lp', '##rit', ',', 'but', 'i', 'have', 'been', 'taking', 'the', 'exact', 'same', 'dos', '##age', 'for', 'the', 'past', 'two', 'weeks', 'and', 'this', 'is', 'the', 'first', 'time', 'this', 'has', 'happened', '.', 'anyone', 'have', 'a', 'similar', 'experience', '/', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['how', 'do', 'i', 'handle', 'a', 'pre', '-', 'existing', 'condition', 'investigation', '?', 'after', 'getting', 'a', 'good', 'job', 'with', 'insurance', 'in', 'the', 'new', 'year', ',', 'i', 'was', 'finally', 'able', 'to', 'get', 'diagnosed', 'with', 'ad', '##hd', 'and', 'start', 'treatment', '.', 'it', \"'\", 's', 'been', 'amazing', 'and', 'life', '-', 'changing', '.', 'i', \"'\", 've', 'excelled', 'at', 'work', 'and', 'escaped', 'a', 'lot', 'of', 'fog', '##gy', 'years', 'where', 'i', 'felt', 'like', 'a', 'failure', '.', 'i', 'just', 'got', 'a', 'voice', '##mail', 'from', 'my', 'doctor', \"'\", 's', 'office', 'saying', 'they', 'got', 'a', 'heads', 'up', 'that', 'coverage', 'was', 'being', 'suspended', 'due', 'to', 'an', 'investigation', 'of', 'a', 'pre', '-', 'existing', 'condition', '.', 'the', 'message', 'said', 'this', 'wasn', \"'\", 't', 'unusual', ',', 'but', 'that', 'i', 'needed', 'to', 'respond', 'to', 'a', 'letter', 'i', 'would', 'get', '.', 'later', 'in', 'the', 'day', ',', 'i', 'got', 'two', 'calls', 'to', 'my', 'google', 'voice', 'from', 'what', 'i', 'think', 'is', 'blue', 'shield', ',', 'saying', 'to', 'contact', 'my', 'provider', 'at', 'their', '800', 'number', '.', 'before', 'i', 'contact', 'them', ',', 'i', \"'\", 'm', 'really', 'hoping', 'to', 'get', 'advice', 'on', 'how', 'to', 'proceed', '.', 'insurance', 'is', 'so', 'scary', 'and', 'i', 'feel', 'like', 'a', 'miss', 'step', 'could', 'screw', 'up', 'my', 'coverage', '.', 'i', 'just', 'started', 'getting', 'my', 'feet', 'back', 'on', 'the', 'ground', 'financially', 'and', 'i', 'won', \"'\", 't', 'be', 'able', 'to', 'cover', 'this', 'myself', '.', 'i', \"'\", 've', 'never', 'been', 'diagnosed', 'before', ',', 'but', 'over', 'five', 'years', 'ago', 'in', 'another', 'state', 'i', 'told', 'a', 'general', 'physician', 'that', 'i', 'was', 'struggling', 'with', 'focus', 'and', 'he', 'prescribed', 'a', 'st', '##im', '##ula', '##nt', 'to', 'try', '.', 'it', 'didn', \"'\", 't', 'work', 'for', 'me', 'and', 'i', 'didn', \"'\", 't', 'follow', 'up', 'on', 'it', '.', 'does', 'insurance', 'have', 'access', 'to', 'information', 'like', 'that', '?', 'i', 'don', \"'\", 't', 'even', 'remember', 'that', 'doctor', \"'\", 's', 'name', '.', 'would', 'they', 'consider', 'that', 'diagnosis', 'even', 'though', 'he', 'wasn', \"'\", 't', 'a', 'psychiatrist', 'and', 'there', 'was', 'no', 'measurement', 'of', 'any', 'kind', '?', 'has', 'anyone', 'dealt', 'with', 'this', 'before', '?', 'i', 'want', 'to', 'have', 'my', 'ducks', 'in', 'a', 'row', 'before', 'i', 'follow', '-', 'up', 'with', 'my', 'insurance', 'company', '.', 'i', 'feel', 'so', 'deeply', 'discouraged', '.']\n",
      "INFO:__main__:Number of tokens: 343\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['how', 'do', 'i', 'handle', 'a', 'pre', '-', 'existing', 'condition', 'investigation', '?', 'after', 'getting', 'a', 'good', 'job', 'with', 'insurance', 'in', 'the', 'new', 'year', ',', 'i', 'was', 'finally', 'able', 'to', 'get', 'diagnosed', 'with', 'ad', '##hd', 'and', 'start', 'treatment', '.', 'it', \"'\", 's', 'been', 'amazing', 'and', 'life', '-', 'changing', '.', 'i', \"'\", 've', 'excelled', 'at', 'work', 'and', 'escaped', 'a', 'lot', 'of', 'fog', '##gy', 'years', 'where', 'i', 'felt', 'like', 'a', 'failure', '.', 'i', 'just', 'got', 'a', 'voice', '##mail', 'from', 'my', 'doctor', \"'\", 's', 'office', 'saying', 'they', 'got', 'a', 'heads', 'up', 'that', 'coverage', 'was', 'being', 'suspended', 'due', 'to', 'an', 'investigation', 'of', 'a', 'pre', '-', 'existing', 'condition', '.', 'the', 'message', 'said', 'this', 'wasn', \"'\", 't', 'unusual', ',', 'but', 'that', 'i', 'needed', 'to', 'respond', 'to', 'a', 'letter', 'i', 'would', 'get', '.', 'later', 'in', 'the', 'day', ',', 'i', 'got', 'two', 'calls', 'to', 'my', 'google', 'voice', 'from', 'what', 'i', 'think', 'is', 'blue', 'shield', ',', 'saying', 'to', 'contact', 'my', 'provider', 'at', 'their', '800', 'number', '.', 'before', 'i', 'contact', 'them', ',', 'i', \"'\", 'm', 'really', 'hoping', 'to', 'get', 'advice', 'on', 'how', 'to', 'proceed', '.', 'insurance', 'is', 'so', 'scary', 'and', 'i', 'feel', 'like', 'a', 'miss', 'step', 'could', 'screw', 'up', 'my', 'coverage', '.', 'i', 'just', 'started', 'getting', 'my', 'feet', 'back', 'on', 'the', 'ground', 'financially', 'and', 'i', 'won', \"'\", 't', 'be', 'able', 'to', 'cover', 'this', 'myself', '.', 'i', \"'\", 've', 'never', 'been', 'diagnosed', 'before', ',', 'but', 'over', 'five', 'years', 'ago', 'in', 'another', 'state', 'i', 'told', 'a', 'general', 'physician', 'that', 'i', 'was', 'struggling', 'with', 'focus', 'and', 'he', 'prescribed', 'a', 'st', '##im', '##ula', '##nt', 'to', 'try', '.', 'it', 'didn', \"'\", 't', 'work', 'for', 'me', 'and', 'i', 'didn', \"'\", 't', 'follow', 'up', 'on', 'it', '.', 'does', 'insurance', 'have', 'access', 'to', 'information', 'like', 'that', '?', 'i', 'don', \"'\", 't', 'even', 'remember', 'that', 'doctor', \"'\", 's', 'name', '.', 'would', 'they', 'consider', 'that', 'diagnosis', 'even', 'though', 'he', 'wasn', \"'\", 't', 'a', 'psychiatrist', 'and', 'there', 'was', 'no', 'measurement', 'of', 'any', 'kind', '?', 'has', 'anyone', 'dealt', 'with', 'this', 'before', '?', 'i', 'want', 'to', 'have', 'my', 'ducks', 'in', 'a', 'row', 'before', 'i', 'follow', '-', 'up', 'with', 'my', 'insurance', 'company', '.', 'i', 'feel', 'so', 'deeply', 'discouraged', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['first', 'day', 'on', 'rita', '##lin', 'just', 'finished', 'my', '2nd', 'day', 'on', 'rita', '##lin', '.', 'the', 'day', 'started', 'out', 'wonderful', '.', 'i', 'was', 'focused', ',', 'motivated', 'and', 'could', 'think', 'clearly', '.', 'as', 'the', 'day', 'went', 'on', 'the', 'feeling', 'began', 'to', 'fade', 'and', 'i', 'actually', 'began', 'to', 'feel', 'sort', 'of', 'cr', '##um', '##my', '.', 'now', 'i', 'am', 'on', 'a', 'very', 'low', 'dose', '(', '10', '##mg', ')', 'just', 'to', 'gauge', 'my', 'reaction', 'to', 'it', 'until', 'i', 'meet', 'with', 'my', 'ps', '##ych', 'again', '.', 'is', 'it', 'a', 'normal', 'thing', 'to', 'have', 'the', 'down', 'feeling', 'as', 'the', 'drug', 'wears', 'off', '?', 'do', 'most', 'people', 'take', 'multiple', 'treatments', 'per', 'day', 'or', 'is', 'it', 'supposed', 'to', 'last', 'all', 'day', 'with', 'a', 'single', 'dose', 'in', 'the', 'am', '?']\n",
      "INFO:__main__:Number of tokens: 120\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['first', 'day', 'on', 'rita', '##lin', 'just', 'finished', 'my', '2nd', 'day', 'on', 'rita', '##lin', '.', 'the', 'day', 'started', 'out', 'wonderful', '.', 'i', 'was', 'focused', ',', 'motivated', 'and', 'could', 'think', 'clearly', '.', 'as', 'the', 'day', 'went', 'on', 'the', 'feeling', 'began', 'to', 'fade', 'and', 'i', 'actually', 'began', 'to', 'feel', 'sort', 'of', 'cr', '##um', '##my', '.', 'now', 'i', 'am', 'on', 'a', 'very', 'low', 'dose', '(', '10', '##mg', ')', 'just', 'to', 'gauge', 'my', 'reaction', 'to', 'it', 'until', 'i', 'meet', 'with', 'my', 'ps', '##ych', 'again', '.', 'is', 'it', 'a', 'normal', 'thing', 'to', 'have', 'the', 'down', 'feeling', 'as', 'the', 'drug', 'wears', 'off', '?', 'do', 'most', 'people', 'take', 'multiple', 'treatments', 'per', 'day', 'or', 'is', 'it', 'supposed', 'to', 'last', 'all', 'day', 'with', 'a', 'single', 'dose', 'in', 'the', 'am', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'recently', 'self', '-', 'diagnosed', 'and', 'need', 'some', 'tips', '!', 'so', ',', 'anyway', ',', 'this', 'is', 'a', 'throw', '##way', ',', 'since', 'my', 'friends', 'and', 'family', 'don', \"'\", 't', 'have', 'clue', 'what', 'i', \"'\", 'm', 'thinking', 'of', 'and', 'that', 'i', \"'\", 'm', 'seeking', 'help', 'for', 'a', 'condition', 'that', 'seems', 'to', 'be', 'making', 'my', 'life', 'as', 'shitty', 'as', 'possible', '.', 'anyway', ',', 'i', \"'\", 'm', 'in', 'college', ',', 'first', 'year', ',', 'history', ',', 'and', 'there', \"'\", 's', 'a', 'hell', 'of', 'a', 'lot', 'literature', 'that', '*', 'i', 'just', 'can', \"'\", 't', 'handle', '*', '.', 'no', 'concentration', ',', 'my', 'mind', \"'\", 's', 'a', 'mess', ',', 'everything', 'each', 'and', 'every', 'one', 'of', 'you', 'is', 'already', 'familiar', 'with', '.', 'the', 'thing', 'is', ',', 'i', 'never', 'had', 'to', 'study', ',', 'but', 'got', 'away', 'with', 'it', 'because', 'i', 'know', 'with', 'words', '.', 'it', \"'\", 's', 'not', 'that', 'i', '*', 'never', '*', 'studied', ',', 'just', 'never', 'as', 'much', 'as', 'i', 'should', 'have', 'to', 'have', 'straight', 'a', \"'\", 's', '.', 'i', 'went', 'for', 'the', 'pass', 'since', 'it', 'was', 'the', 'easy', 'way', 'out', ',', 'and', 'studying', 'was', 'torture', ',', 'no', 'matter', 'how', 'interesting', 'it', 'got', ';', 'the', 'only', 'thing', 'i', 'got', 'right', 'away', 'were', 'languages', 'and', 'history', '.', 'and', 'now', ',', 'college', '+', 'add', 'are', 'making', 'one', 'of', 'my', 'loves', 'hell', '.', 'i', 'have', 'no', 'idea', 'how', 'to', 'study', ',', 'since', 'no', 'method', 'i', 'ever', 'heard', 'from', 'my', 'friends', 'ever', 'worked', '(', 'which', 'i', 'can', 'now', 'at', '##rri', '##bu', '##te', 'to', 'add', ')', ',', 'and', 'now', 'i', '*', 'really', '*', 'have', 'to', 'study', 'if', 'i', 'want', 'this', ',', 'and', 'it', \"'\", 's', 'killing', 'me', 'that', 'i', 'just', 'can', \"'\", 't', '.', 'the', 'wall', 'is', 'more', 'appealing', 'that', 'the', 'books', 'they', 'give', 'us', '!', 'please', ',', 'if', 'you', 'have', 'any', 'tips', 'for', 'studying', 'and', 'actually', 'not', 'getting', 'consumed', 'by', 'fear', 'and', 'distraction', '##s', ',', 'share', '!', 'thank', 'you', 'in', 'advance', '!', ';', ')', '*', '*', 't', '##l', ';', 'dr', '-', 'newly', 'self', '-', 'diagnosed', 'add', '19', '##yo', 'needs', 'tips', 'for', 'studying', 'so', 'she', 'wouldn', \"'\", 't', 'flu', '##nk', 'first', 'year', 'of', 'college', 'and', 'have', 'to', 'go', 'home', 'and', 'give', 'up', 'com', '##ple', '##tly', '.', '*', '*', 'edit', ';', 'there', 'are', 'very', 'few', 'experts', 'on', 'the', 'matter', 'in', 'my', 'small', 'country', ',', 'and', 'all', '*', 'four', 'or', 'five', '*', 'of', 'them', 'are', 'in', 'the', 'capital', '.', 'there', 'is', 'only', 'one', 'institution', 'and', 'a', 'private', 'clinic', '.', 'so', 'i', 'turned', 'to', 'the', 'institution', 'for', 'a', 'testing', 'and', 'am', 'waiting', 'for', 'a', 'reply', '.', 'will', 'update', 'when', '/', 'if', 'i', 'get', 'a', 'response', '!', ':', ')']\n",
      "INFO:__main__:Number of tokens: 421\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'recently', 'self', '-', 'diagnosed', 'and', 'need', 'some', 'tips', '!', 'so', ',', 'anyway', ',', 'this', 'is', 'a', 'throw', '##way', ',', 'since', 'my', 'friends', 'and', 'family', 'don', \"'\", 't', 'have', 'clue', 'what', 'i', \"'\", 'm', 'thinking', 'of', 'and', 'that', 'i', \"'\", 'm', 'seeking', 'help', 'for', 'a', 'condition', 'that', 'seems', 'to', 'be', 'making', 'my', 'life', 'as', 'shitty', 'as', 'possible', '.', 'anyway', ',', 'i', \"'\", 'm', 'in', 'college', ',', 'first', 'year', ',', 'history', ',', 'and', 'there', \"'\", 's', 'a', 'hell', 'of', 'a', 'lot', 'literature', 'that', '*', 'i', 'just', 'can', \"'\", 't', 'handle', '*', '.', 'no', 'concentration', ',', 'my', 'mind', \"'\", 's', 'a', 'mess', ',', 'everything', 'each', 'and', 'every', 'one', 'of', 'you', 'is', 'already', 'familiar', 'with', '.', 'the', 'thing', 'is', ',', 'i', 'never', 'had', 'to', 'study', ',', 'but', 'got', 'away', 'with', 'it', 'because', 'i', 'know', 'with', 'words', '.', 'it', \"'\", 's', 'not', 'that', 'i', '*', 'never', '*', 'studied', ',', 'just', 'never', 'as', 'much', 'as', 'i', 'should', 'have', 'to', 'have', 'straight', 'a', \"'\", 's', '.', 'i', 'went', 'for', 'the', 'pass', 'since', 'it', 'was', 'the', 'easy', 'way', 'out', ',', 'and', 'studying', 'was', 'torture', ',', 'no', 'matter', 'how', 'interesting', 'it', 'got', ';', 'the', 'only', 'thing', 'i', 'got', 'right', 'away', 'were', 'languages', 'and', 'history', '.', 'and', 'now', ',', 'college', '+', 'add', 'are', 'making', 'one', 'of', 'my', 'loves', 'hell', '.', 'i', 'have', 'no', 'idea', 'how', 'to', 'study', ',', 'since', 'no', 'method', 'i', 'ever', 'heard', 'from', 'my', 'friends', 'ever', 'worked', '(', 'which', 'i', 'can', 'now', 'at', '##rri', '##bu', '##te', 'to', 'add', ')', ',', 'and', 'now', 'i', '*', 'really', '*', 'have', 'to', 'study', 'if', 'i', 'want', 'this', ',', 'and', 'it', \"'\", 's', 'killing', 'me', 'that', 'i', 'just', 'can', \"'\", 't', '.', 'the', 'wall', 'is', 'more', 'appealing', 'that', 'the', 'books', 'they', 'give', 'us', '!', 'please', ',', 'if', 'you', 'have', 'any', 'tips', 'for', 'studying', 'and', 'actually', 'not', 'getting', 'consumed', 'by', 'fear', 'and', 'distraction', '##s', ',', 'share', '!', 'thank', 'you', 'in', 'advance', '!', ';', ')', '*', '*', 't', '##l', ';', 'dr', '-', 'newly', 'self', '-', 'diagnosed', 'add', '19', '##yo', 'needs', 'tips', 'for', 'studying', 'so', 'she', 'wouldn', \"'\", 't', 'flu', '##nk', 'first', 'year', 'of', 'college', 'and', 'have', 'to', 'go', 'home', 'and', 'give', 'up', 'com', '##ple', '##tly', '.', '*', '*', 'edit', ';', 'there', 'are', 'very', 'few', 'experts', 'on', 'the', 'matter', 'in', 'my', 'small', 'country', ',', 'and', 'all', '*', 'four', 'or', 'five', '*', 'of', 'them', 'are', 'in', 'the', 'capital', '.', 'there', 'is', 'only', 'one', 'institution', 'and', 'a', 'private', 'clinic', '.', 'so', 'i', 'turned', 'to', 'the', 'institution', 'for', 'a', 'testing', 'and', 'am', 'waiting', 'for', 'a', 'reply', '.', 'will', 'update', 'when', '/', 'if', 'i', 'get', 'a', 'response', '!', ':', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['rita', '##lin', ',', 'lex', '##ap', '##ro', ',', 'therapy', 'don', \"'\", 't', 'work', '.', 'i', 'also', 'don', \"'\", 't', 'have', 'add', '.']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['rita', '##lin', ',', 'lex', '##ap', '##ro', ',', 'therapy', 'don', \"'\", 't', 'work', '.', 'i', 'also', 'don', \"'\", 't', 'have', 'add', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', 'x', '##r', 'i', 'have', 'a', 'question', 'about', 'add', '##eral', '##l', 'x', '##r', ',', 'i', 'switched', 'from', 'v', '##y', '##van', '##se', 'to', 'add', '##eral', '##l', 'because', 'i', 'was', 'using', 'v', '##y', '##van', '##se', 'for', 'over', 'a', 'year', 'and', 'it', 'was', 'great', 'at', 'first', 'but', 'then', 'the', 'medicine', 'started', 'wearing', 'off', 'quicker', 'and', 'quicker', 'so', 'my', 'psychiatrist', 'suggested', 'i', 'switch', '.', 'my', 'concern', 'is', 'though', ',', 'i', 'am', 'very', 'thin', 'and', 'i', 'was', 'wondering', 'if', 'i', 'should', 'use', 'add', '##eral', '##l', 'or', 'v', '##y', '##van', '##se', 'based', 'on', 'my', 'height', 'and', 'weight', '.', 'i', 'am', '5', \"'\", '7', 'and', 'i', 'weigh', 'about', '108', 'pounds', '(', 'it', 'runs', 'in', 'my', 'family', ',', 'my', 'dad', 'was', 'very', 'thin', 'himself', 'at', 'my', 'age', '.', ')']\n",
      "INFO:__main__:Number of tokens: 123\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', 'x', '##r', 'i', 'have', 'a', 'question', 'about', 'add', '##eral', '##l', 'x', '##r', ',', 'i', 'switched', 'from', 'v', '##y', '##van', '##se', 'to', 'add', '##eral', '##l', 'because', 'i', 'was', 'using', 'v', '##y', '##van', '##se', 'for', 'over', 'a', 'year', 'and', 'it', 'was', 'great', 'at', 'first', 'but', 'then', 'the', 'medicine', 'started', 'wearing', 'off', 'quicker', 'and', 'quicker', 'so', 'my', 'psychiatrist', 'suggested', 'i', 'switch', '.', 'my', 'concern', 'is', 'though', ',', 'i', 'am', 'very', 'thin', 'and', 'i', 'was', 'wondering', 'if', 'i', 'should', 'use', 'add', '##eral', '##l', 'or', 'v', '##y', '##van', '##se', 'based', 'on', 'my', 'height', 'and', 'weight', '.', 'i', 'am', '5', \"'\", '7', 'and', 'i', 'weigh', 'about', '108', 'pounds', '(', 'it', 'runs', 'in', 'my', 'family', ',', 'my', 'dad', 'was', 'very', 'thin', 'himself', 'at', 'my', 'age', '.', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['father', 'won', '##t', 'let', 'be', 'on', 'medication', ',', 'how', 'can', 'i', 'change', 'his', 'mind']\n",
      "INFO:__main__:Number of tokens: 14\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['father', 'won', '##t', 'let', 'be', 'on', 'medication', ',', 'how', 'can', 'i', 'change', 'his', 'mind']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'i', 'just', 'realized', ',', 'at', '10', 'pm', ',', 'that', 'i', 'only', 'took', 'half', 'my', 'med', '##s', 'today', '.', 'i', 'take', 'day', '##tra', '##na', 'which', 'is', 'a', 'patch', ',', 'and', 'i', 'just', 'now', 'realized', 'that', 'i', 'only', 'put', 'on', 'one', 'of', 'my', '2', 'patches', 'this', 'morning', '.', 'i', 'am', 'pleasantly', 'surprised', ',', 'but', 'mostly', 'annoyed', 'at', 'myself', '.', 'i', 'am', 'surprised', 'that', 'i', 'was', 'able', 'to', 'get', 'a', 'decent', 'amount', 'of', 'work', 'done', 'today', '(', 'i', \"'\", 'm', 'a', 'senior', 'in', 'engineering', 'and', 'i', 'have', '2', 'take', 'home', 'finals', 'due', 'on', 'mod', '##ay', ',', 'so', 'i', 'was', 'working', 'on', 'them', 'today', ')', '.', 'i', 'got', 'about', 'half', 'of', 'one', 'final', 'done', ',', 'and', '30', '%', 'of', 'the', 'other', '(', 'i', 'already', 'had', '60', '%', 'done', 'from', '##e', 'yesterday', ')', '.', 'so', 'that', 'is', 'good', 'that', 'i', 'did', 'something', '.', 'however', ',', 'i', 'also', 'was', 'very', 'distracted', 'through', 'the', 'day', ',', 'and', 'was', 'not', 'able', 'to', 'focus', 'even', 'doing', 'group', 'work', '(', 'we', 'are', 'allowed', 'minor', 'collaboration', 'on', 'one', 'of', 'the', 'finals', ')', '.', 'also', ',', 'i', 'had', 'a', 'large', 'project', 'due', 'tuesday', ',', 'and', 'a', 'massive', 'final', '(', 'pass', 'and', 'graduate', ',', 'or', 'fail', 'and', 'take', 'another', 'semester', 'big', ')', 'on', 'wed', '.', 'that', 'i', 'still', 'have', 'not', 'started', 'studying', 'for', '(', 'i', 'do', 'have', 'a', 'study', 'outline', 'but', 'no', 'real', 'studying', ')', '.', 'the', 'part', 'that', 'i', 'am', 'most', 'annoyed', 'with', 'my', 'self', 'about', 'is', 'the', 'fact', 'that', 'i', 'should', \"'\", 've', 'easily', 'gotten', '40', '%', 'more', 'work', 'done', 'today', ',', 'and', 'now', 'i', 'have', 'that', 'much', 'less', 'time', 'to', 'study', 'for', 'the', 'final', '.', 'also', ',', 'i', 'wish', 'i', 'had', 'thought', 'to', 'check', 'my', 'med', '##s', '.', 'i', 'remember', 'making', 'several', 'comments', 'that', 'i', 'am', 'particularly', 'distract', '##able', 'today', ',', 'but', 'i', 'never', 'made', 'the', 'med', '##s', 'connection', '.', 'i', 'am', 'done', 'studying', 'for', 'the', 'day', ',', 'and', 'i', \"'\", 'm', 'gonna', 'go', 'relax', 'for', 'a', 'while', '(', 'it', 'is', 'the', 'last', 'weekend', 'of', 'the', 'semester', ')', '.', 'tomorrow', 'i', 'will', 'hopefully', 'remember', 'my', 'med', '##s', '(', 'this', 'has', 'not', 'happened', 'for', 'over', 'a', 'year', ')', ',', 'so', 'i', 'can', 'put', 'in', 'a', 'solid', '12', '-', '13', 'hr', '##s', 'of', 'study', 'time', 'tomorrow', '.', 'thanks', 'for', 'listening', '!']\n",
      "INFO:__main__:Number of tokens: 370\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', 'i', 'just', 'realized', ',', 'at', '10', 'pm', ',', 'that', 'i', 'only', 'took', 'half', 'my', 'med', '##s', 'today', '.', 'i', 'take', 'day', '##tra', '##na', 'which', 'is', 'a', 'patch', ',', 'and', 'i', 'just', 'now', 'realized', 'that', 'i', 'only', 'put', 'on', 'one', 'of', 'my', '2', 'patches', 'this', 'morning', '.', 'i', 'am', 'pleasantly', 'surprised', ',', 'but', 'mostly', 'annoyed', 'at', 'myself', '.', 'i', 'am', 'surprised', 'that', 'i', 'was', 'able', 'to', 'get', 'a', 'decent', 'amount', 'of', 'work', 'done', 'today', '(', 'i', \"'\", 'm', 'a', 'senior', 'in', 'engineering', 'and', 'i', 'have', '2', 'take', 'home', 'finals', 'due', 'on', 'mod', '##ay', ',', 'so', 'i', 'was', 'working', 'on', 'them', 'today', ')', '.', 'i', 'got', 'about', 'half', 'of', 'one', 'final', 'done', ',', 'and', '30', '%', 'of', 'the', 'other', '(', 'i', 'already', 'had', '60', '%', 'done', 'from', '##e', 'yesterday', ')', '.', 'so', 'that', 'is', 'good', 'that', 'i', 'did', 'something', '.', 'however', ',', 'i', 'also', 'was', 'very', 'distracted', 'through', 'the', 'day', ',', 'and', 'was', 'not', 'able', 'to', 'focus', 'even', 'doing', 'group', 'work', '(', 'we', 'are', 'allowed', 'minor', 'collaboration', 'on', 'one', 'of', 'the', 'finals', ')', '.', 'also', ',', 'i', 'had', 'a', 'large', 'project', 'due', 'tuesday', ',', 'and', 'a', 'massive', 'final', '(', 'pass', 'and', 'graduate', ',', 'or', 'fail', 'and', 'take', 'another', 'semester', 'big', ')', 'on', 'wed', '.', 'that', 'i', 'still', 'have', 'not', 'started', 'studying', 'for', '(', 'i', 'do', 'have', 'a', 'study', 'outline', 'but', 'no', 'real', 'studying', ')', '.', 'the', 'part', 'that', 'i', 'am', 'most', 'annoyed', 'with', 'my', 'self', 'about', 'is', 'the', 'fact', 'that', 'i', 'should', \"'\", 've', 'easily', 'gotten', '40', '%', 'more', 'work', 'done', 'today', ',', 'and', 'now', 'i', 'have', 'that', 'much', 'less', 'time', 'to', 'study', 'for', 'the', 'final', '.', 'also', ',', 'i', 'wish', 'i', 'had', 'thought', 'to', 'check', 'my', 'med', '##s', '.', 'i', 'remember', 'making', 'several', 'comments', 'that', 'i', 'am', 'particularly', 'distract', '##able', 'today', ',', 'but', 'i', 'never', 'made', 'the', 'med', '##s', 'connection', '.', 'i', 'am', 'done', 'studying', 'for', 'the', 'day', ',', 'and', 'i', \"'\", 'm', 'gonna', 'go', 'relax', 'for', 'a', 'while', '(', 'it', 'is', 'the', 'last', 'weekend', 'of', 'the', 'semester', ')', '.', 'tomorrow', 'i', 'will', 'hopefully', 'remember', 'my', 'med', '##s', '(', 'this', 'has', 'not', 'happened', 'for', 'over', 'a', 'year', ')', ',', 'so', 'i', 'can', 'put', 'in', 'a', 'solid', '12', '-', '13', 'hr', '##s', 'of', 'study', 'time', 'tomorrow', '.', 'thanks', 'for', 'listening', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 2\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['college', 'supplies', 'list', ',', 'any', 'more', 'suggestions', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['college', 'supplies', 'list', ',', 'any', 'more', 'suggestions', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['restless', 'leg', 'syndrome', 'and', 'ad', '##hd']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['restless', 'leg', 'syndrome', 'and', 'ad', '##hd']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['fore', '##va', '?', 'fore', '##va', 'eva', '?']\n",
      "INFO:__main__:Number of tokens: 7\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['fore', '##va', '?', 'fore', '##va', 'eva', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['success', '!', 'i', 'just', 'might', 'not', 'fail', 'this', 'quarter', '!', 'thanks', 'to', 'my', 'medication', ',', 'i', \"'\", 'm', 'been', 'more', 'productive', 'than', 'i', \"'\", 've', 'ever', 'been', '.', 'earlier', 'i', 'submitted', 'a', 'comment', 'in', 'the', 'momentum', 'monday', 'post', 'about', 'how', 'i', 'was', 'several', 'weeks', 'behind', 'in', 'my', 'ap', 'us', 'history', 'notes', ',', 'and', 'how', 'i', 'set', 'out', 'a', 'plan', 'to', 'get', 'it', 'all', 'done', 'before', 'it', 'was', 'due', '.', 'although', 'i', \"'\", 'm', 'a', 'little', 'behind', ',', 'i', \"'\", 'm', 'almost', 'fully', 'caught', 'up', ',', 'and', 'with', 'a', 'week', 'to', 'spare', '!', 'had', 'this', 'been', 'not', 'even', 'two', 'months', 'ago', '(', 'i', 'was', 'diagnosed', 'a', 'little', 'more', 'than', 'a', 'month', 'ago', ')', 'i', 'probably', 'would', 'have', 'nothing', 'by', 'now', ',', 'even', 'though', 'it', \"'\", 's', 'due', 'this', 'week', '.', 'this', 'week', 'i', 'was', 'able', 'to', 'adhere', 'to', 'a', 'strict', 'plan', '(', 'except', 'two', 'days', ',', 'where', 'things', 'got', 'a', 'little', 'messed', 'up', ')', 'and', 'pull', 'through', '.', 'i', \"'\", 'm', 'being', 'distracted', 'by', 'making', 'this', 'post', '(', 'bad', ',', 'i', 'know', ')', 'so', 'i', \"'\", 'm', 'gonna', 'get', 'back', 'to', 'work', ',', 'but', 'i', 'just', 'thought', 'i', \"'\", 'd', 'share', 'with', 'you', 'guys', '!']\n",
      "INFO:__main__:Number of tokens: 194\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['success', '!', 'i', 'just', 'might', 'not', 'fail', 'this', 'quarter', '!', 'thanks', 'to', 'my', 'medication', ',', 'i', \"'\", 'm', 'been', 'more', 'productive', 'than', 'i', \"'\", 've', 'ever', 'been', '.', 'earlier', 'i', 'submitted', 'a', 'comment', 'in', 'the', 'momentum', 'monday', 'post', 'about', 'how', 'i', 'was', 'several', 'weeks', 'behind', 'in', 'my', 'ap', 'us', 'history', 'notes', ',', 'and', 'how', 'i', 'set', 'out', 'a', 'plan', 'to', 'get', 'it', 'all', 'done', 'before', 'it', 'was', 'due', '.', 'although', 'i', \"'\", 'm', 'a', 'little', 'behind', ',', 'i', \"'\", 'm', 'almost', 'fully', 'caught', 'up', ',', 'and', 'with', 'a', 'week', 'to', 'spare', '!', 'had', 'this', 'been', 'not', 'even', 'two', 'months', 'ago', '(', 'i', 'was', 'diagnosed', 'a', 'little', 'more', 'than', 'a', 'month', 'ago', ')', 'i', 'probably', 'would', 'have', 'nothing', 'by', 'now', ',', 'even', 'though', 'it', \"'\", 's', 'due', 'this', 'week', '.', 'this', 'week', 'i', 'was', 'able', 'to', 'adhere', 'to', 'a', 'strict', 'plan', '(', 'except', 'two', 'days', ',', 'where', 'things', 'got', 'a', 'little', 'messed', 'up', ')', 'and', 'pull', 'through', '.', 'i', \"'\", 'm', 'being', 'distracted', 'by', 'making', 'this', 'post', '(', 'bad', ',', 'i', 'know', ')', 'so', 'i', \"'\", 'm', 'gonna', 'get', 'back', 'to', 'work', ',', 'but', 'i', 'just', 'thought', 'i', \"'\", 'd', 'share', 'with', 'you', 'guys', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'sort', 'of', 'ratio', 'of', 'studying', 'to', 'break', 'time', 'do', 'you', 'use', '?', 'i', 'feel', 'like', 'i', \"'\", 've', 'never', 'gotten', 'a', 'good', 'balance']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'sort', 'of', 'ratio', 'of', 'studying', 'to', 'break', 'time', 'do', 'you', 'use', '?', 'i', 'feel', 'like', 'i', \"'\", 've', 'never', 'gotten', 'a', 'good', 'balance']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['so', 'who', 'wants', 'to', 'guess', 'when', 'i', 'started', 'medication', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['so', 'who', 'wants', 'to', 'guess', 'when', 'i', 'started', 'medication', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['focus', '.', 'p', '##y', ':', 'a', 'useful', 'tool', 'for', 'any', 'of', 'you', 'who', 'use', 'linux', '.', '(', 'xp', '##ost', 'from', '/', 'r', '/', 'python', ')']\n",
      "INFO:__main__:Number of tokens: 25\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['focus', '.', 'p', '##y', ':', 'a', 'useful', 'tool', 'for', 'any', 'of', 'you', 'who', 'use', 'linux', '.', '(', 'xp', '##ost', 'from', '/', 'r', '/', 'python', ')']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'just', 'got', 'a', 'hold', 'of', 'rita', '##lin', '.', 'i', \"'\", 'm', 'taking', 'a', 'final', 'tomorrow', 'at', '1', '##pm', ',', 'at', 'what', 'time', 'should', 'i', 'take', 'my', 'first', 'dose', '?']\n",
      "INFO:__main__:Number of tokens: 30\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'just', 'got', 'a', 'hold', 'of', 'rita', '##lin', '.', 'i', \"'\", 'm', 'taking', 'a', 'final', 'tomorrow', 'at', '1', '##pm', ',', 'at', 'what', 'time', 'should', 'i', 'take', 'my', 'first', 'dose', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'kick', '-', 'start', 'sunday', ']', 'go', 'for', 'the', 'easy', 'success', 'this', 'week', 'with', 'gentle', 'accountability', 'from', '/', 'r', '/', 'ad', '##hd', '!', '~', '~', 'to', 'accomplish', 'tuesday', '.', '.', '.', 'momentum', 'monday', '.', '.', '.', '~', '~', 'kick', '-', 'start', 'sunday', 'is', 'where', '/', 'r', '/', 'ad', '##hd', 'can', 'come', 'to', 'share', 'the', 'goals', ',', 'tasks', ',', 'or', 'habits', 'we', 'all', 'want', 'to', 'accomplish', 'this', 'week', '.', 'by', 'sharing', 'here', 'you', 'will', 'be', 'held', 'accountable', '(', 'which', 'is', 'a', 'good', 'thing', '!', ')', 'and', 'we', 'can', 'help', 'check', 'in', 'with', 'you', '.', '.', '.', '*', '*', 'helping', 'you', 'get', 'things', 'done', '.', '*', '*', '*', 'i', 'highly', 'recommend', 'reading', 'this', 'longer', 'post', '.', 'if', 'you', 'want', 'to', 'skip', 'it', 'and', 'just', 'list', 'what', 'you', 'want', 'to', 'do', 'this', 'week', '.', '.', '.', 'go', 'ahead', '.', 'but', ',', 'if', 'you', 'want', 'a', 'better', 'chance', 'to', 'succeed', 'and', 'are', 'willing', 'to', 'put', 'a', 'little', 'extra', 'initial', 'effort', 'in', 'now', '.', '.', '.', 'continue', 'reading', '.', '.', '.', '*', '*', '*', '*', '#', '#', 'suggested', 'ways', 'you', 'can', 'participate', ':', '*', '*', '*', 'most', 'important', 'task', '*', '*', '-', 'share', 'one', '(', 'or', 'a', 'couple', ')', '*', '*', 'high', '-', 'priority', '*', '*', 'tasks', 'that', 'you', 'really', 'would', 'like', 'to', 'finally', 'finish', '.', 'ex', '.', 'set', 'a', 'doctors', 'appointment', ',', 'finish', 'assignment', ',', 'clean', 'the', 'kitchen', '.', '.', '.', 'whatever', 'you', 'think', 'is', 'most', 'important', '.', '*', '*', '*', 'make', 'a', 'list', '*', '*', '-', 'post', 'a', 'list', 'of', 'all', 'the', 'things', 'you', 'want', 'to', 'accomplish', 'this', 'week', '.', '(', 'i', 'highly', 'recommend', 'identifying', 'the', 'top', '3', 'priorities', '.', ')', '*', '*', '*', 'feed', 'your', 'soul', '*', '*', '-', 'sometimes', 'by', 'relaxing', 'and', 'having', 'fun', 'we', 'have', 'more', 'energy', 'to', 'get', 'more', 'done', '.', '*', '*', 'list', 'a', 'couple', 'fun', 'items', 'to', 'make', 'your', 'list', 'more', 'exciting', '.', '*', '*', '*', '*', 'need', '/', 'want', 'to', 'copy', 'last', 'week', \"'\", 's', 'list', '*', 'verb', '##ati', '##m', '*', '?', 'absolutely', 'do', 'so', '!', '*', '*', '*', 'those', 'might', 'be', 'your', 'keystone', 'habits', '.', 'once', 'you', 'start', 'doing', 'those', 'regularly', 'you', 'are', 'able', 'to', 'do', 'more', '.', 'soon', 'they', 'will', 'be', 'automatic', 'and', 'you', 'can', 'put', 'your', 'focus', 'elsewhere', '.', '*', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'publicly', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '♥', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '#', '#', 'suggested', 'guidelines', '*', '*', '*', 'prior', '##iti', '##ze', 'your', 'list', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to', 'do', 'that', 'too', '!', ')', '*', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'committing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '*', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '*', '*', '*', 'small', 'start', '*', '*', '-', 'share', 'what', 'small', '(', 'under', '10', 'minutes', ')', 'commitment', 'you', 'want', 'to', 'make', 'towards', 'starting', 'your', 'goal', '.', 'i', 'found', 'when', 'my', 'clients', 'didn', '’', 't', 'reach', 'their', 'goals', '90', '%', 'of', 'the', 'time', 'they', 'never', 'started', '.', '.', '.', 'this', 'will', 'help', 'you', 'break', 'through', 'the', 'initial', 'barrier', '.', '*', '*', '*', '#', '#', 'tips', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 's', '.', 'm', '.', 'a', '.', 'r', '.', 't', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '.', '.', '.', 'build', 'up', 'slowly', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', '30', 'minutes', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '*', '*', '*', '#', '#', 'examples', 'from', 'previous', 'weeks', ':', '*', 'un', '##load', 'the', 'dish', '##wash', '##er', '*', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '*', 'make', 'an', 'appointment', 'with', 'doctor', '*', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '*', '*', '*', 'start', '*', '*', '*', '[', 'something', ']', '*', '.', 'spend', '5', 'minutes', 'on', 'writing', 'my', 'paper', '.', '*', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '#', '#', '*', 'it', '’', 's', 'now', 'easier', 'to', 'respond', '!', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '*', '#', '#', 'daily', 'habit', '/', 'pro', '##cr', '##ast', '##inated', 'project', '/', 'soul', '-', 'feeding', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?', 'when', '?', '*', '*', '-', '*', '*', '*', 'small', 'start', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-', '*', '*', '*', 'something', 'fun', 'you', 'want', 'to', 'do', 'this', 'week', '*', '*', '-', '*', '*', '*', 'daily', 'habit', '*', '*', '-', '*', '*', '*', 'i', 'just', 'came', 'up', 'with', 'a', 'new', 'option', 'that', 'might', 'work', 'better', 'for', 'some', 'people', '.', 'listing', 'the', 'things', 'they', 'want', 'to', 'do', 'each', 'day', '.', 'so', 'for', 'those', 'people', 'here', 'is', 'a', 'template', '.', '#', '#', '*', '*', 'monday', '*', '*', '-', '*', 'item', '#', '#', '*', '*', 'tuesday', '*', '*', '-', '#', '#', '*', '*', 'wednesday', '*', '*', '-', '#', '#', '*', '*', 'thursday', '*', '*', '-', '#', '#', '*', '*', 'friday', '*', '*', '-', '#', '#', '*', '*', 'saturday', '*', '*', '-', '#', '#', '*', '*', 'sunday', '*', '*', '-', '*', '*', '*', '*', '*', 'hopefully', 'most', 'of', 'you', 'can', 'also', 'participate', 'in', 'win', 'wednesday', '*', '*', '.', 'we', 'had', 'an', 'increase', 'in', 'participation', 'last', 'week', 'and', 'i', 'am', 'hoping', 'to', 'continue', 'to', 'build', 'on', 'that', 'this', 'week', '.', '*', '*', 'celebrating', '/', 'acknowledging', 'your', 'wins', 'is', 'just', 'as', 'important', 'as', 'actually', 'getting', 'stuff', 'done', '.', '*', '*', 'if', 'you', 'have', 'any', 'other', 'suggestions', 'for', 'weekly', 'threads', 'or', 'improvements', 'to', 'the', 'op', 'text', 'or', 'just', 'a', 'suggestion', 'on', 'how', 'we', 'can', 'make', '/', 'r', '/', 'ad', '##hd', 'better', 'then', 'message', 'me', '!', '*', '*', '*', '*', '*', 't', '##ld', '##r', ':', 'share', 'what', 'you', 'want', 'to', 'accomplish', 'this', 'week', 'and', 'how', 'you', 'will', 'do', 'it', '.', 'having', 'people', 'check', 'on', 'you', 'via', 'red', '##dit', 'is', 'extremely', 'helpful', '!', '*', '*']\n",
      "INFO:__main__:Number of tokens: 1292\n",
      "INFO:__main__:Number of chunks: 3\n",
      "INFO:__main__:Chunks: [['[', 'kick', '-', 'start', 'sunday', ']', 'go', 'for', 'the', 'easy', 'success', 'this', 'week', 'with', 'gentle', 'accountability', 'from', '/', 'r', '/', 'ad', '##hd', '!', '~', '~', 'to', 'accomplish', 'tuesday', '.', '.', '.', 'momentum', 'monday', '.', '.', '.', '~', '~', 'kick', '-', 'start', 'sunday', 'is', 'where', '/', 'r', '/', 'ad', '##hd', 'can', 'come', 'to', 'share', 'the', 'goals', ',', 'tasks', ',', 'or', 'habits', 'we', 'all', 'want', 'to', 'accomplish', 'this', 'week', '.', 'by', 'sharing', 'here', 'you', 'will', 'be', 'held', 'accountable', '(', 'which', 'is', 'a', 'good', 'thing', '!', ')', 'and', 'we', 'can', 'help', 'check', 'in', 'with', 'you', '.', '.', '.', '*', '*', 'helping', 'you', 'get', 'things', 'done', '.', '*', '*', '*', 'i', 'highly', 'recommend', 'reading', 'this', 'longer', 'post', '.', 'if', 'you', 'want', 'to', 'skip', 'it', 'and', 'just', 'list', 'what', 'you', 'want', 'to', 'do', 'this', 'week', '.', '.', '.', 'go', 'ahead', '.', 'but', ',', 'if', 'you', 'want', 'a', 'better', 'chance', 'to', 'succeed', 'and', 'are', 'willing', 'to', 'put', 'a', 'little', 'extra', 'initial', 'effort', 'in', 'now', '.', '.', '.', 'continue', 'reading', '.', '.', '.', '*', '*', '*', '*', '#', '#', 'suggested', 'ways', 'you', 'can', 'participate', ':', '*', '*', '*', 'most', 'important', 'task', '*', '*', '-', 'share', 'one', '(', 'or', 'a', 'couple', ')', '*', '*', 'high', '-', 'priority', '*', '*', 'tasks', 'that', 'you', 'really', 'would', 'like', 'to', 'finally', 'finish', '.', 'ex', '.', 'set', 'a', 'doctors', 'appointment', ',', 'finish', 'assignment', ',', 'clean', 'the', 'kitchen', '.', '.', '.', 'whatever', 'you', 'think', 'is', 'most', 'important', '.', '*', '*', '*', 'make', 'a', 'list', '*', '*', '-', 'post', 'a', 'list', 'of', 'all', 'the', 'things', 'you', 'want', 'to', 'accomplish', 'this', 'week', '.', '(', 'i', 'highly', 'recommend', 'identifying', 'the', 'top', '3', 'priorities', '.', ')', '*', '*', '*', 'feed', 'your', 'soul', '*', '*', '-', 'sometimes', 'by', 'relaxing', 'and', 'having', 'fun', 'we', 'have', 'more', 'energy', 'to', 'get', 'more', 'done', '.', '*', '*', 'list', 'a', 'couple', 'fun', 'items', 'to', 'make', 'your', 'list', 'more', 'exciting', '.', '*', '*', '*', '*', 'need', '/', 'want', 'to', 'copy', 'last', 'week', \"'\", 's', 'list', '*', 'verb', '##ati', '##m', '*', '?', 'absolutely', 'do', 'so', '!', '*', '*', '*', 'those', 'might', 'be', 'your', 'keystone', 'habits', '.', 'once', 'you', 'start', 'doing', 'those', 'regularly', 'you', 'are', 'able', 'to', 'do', 'more', '.', 'soon', 'they', 'will', 'be', 'automatic', 'and', 'you', 'can', 'put', 'your', 'focus', 'elsewhere', '.', '*', '*', '*', '*', 'by', 'sharing', 'your', 'goal', 'semi', '-', 'publicly', 'we', 'can', 'keep', 'each', 'other', 'and', '*', '*', 'accountable', '*', '*', '.', '.', '.', 'and', '*', '*', 'celebrate', 'in', 'our', 'small', 'successes', '!', '*', '*', 'by', 'getting', 'one', 'small', 'task', 'done', 'it', 'creates', '*', '*', 'momentum', '*', '*', ',', '*', '*', 'trust', '*', '*', 'in', 'yourself', ',', '*', '*', 'confidence', '*', '*', ',', 'and', 'you', 'will', 'feel', 'like', 'doing', 'more', '.', '♥', '♥', '♥', '*', '*', 'never', 'posted', 'on', 'red', '##dit', 'before', '?', '*', '*', 'i', 'welcome', 'you', 'to', 'make', 'this', 'your', 'first', '!', '[UNK]', '*', '*', '*', '#', '#', 'suggested', 'guidelines', '*', '*', '*', 'prior', '##iti', '##ze', 'your', 'list', '*', '*', 'one', 'thing', 'you', 'want', 'to', 'get', 'done', '(', 'or', 'start', ')', '.', 'what', 'is', 'the', 'most', 'important', 'thing', 'for', 'you', '?', '(', 'if', 'making', 'a', 'list', 'works', 'best', 'for', 'you', 'feel', 'free', 'to'], ['do', 'that', 'too', '!', ')', '*', '*', '*', 'when', 'would', 'you', 'like', 'to', 'finish', 'or', 'start', 'it', 'by', '?', '*', '*', 'by', 'committing', 'to', 'a', 'date', '/', 'time', ',', 'we', 'can', 'check', 'on', 'your', 'progress', '.', '*', '*', '*', 'edit', 'your', 'comment', 'when', 'done', '*', '*', '-', 'come', 'back', 'and', 'edit', 'your', 'comment', 'when', 'you', 'have', 'finished', 'your', 'goal', '.', 'we', 'can', 'celebrate', 'that', '(', 'and', 'then', 'people', 'won', \"'\", 't', 'check', 'on', 'you', ')', '.', '*', '*', '*', 'small', 'start', '*', '*', '-', 'share', 'what', 'small', '(', 'under', '10', 'minutes', ')', 'commitment', 'you', 'want', 'to', 'make', 'towards', 'starting', 'your', 'goal', '.', 'i', 'found', 'when', 'my', 'clients', 'didn', '’', 't', 'reach', 'their', 'goals', '90', '%', 'of', 'the', 'time', 'they', 'never', 'started', '.', '.', '.', 'this', 'will', 'help', 'you', 'break', 'through', 'the', 'initial', 'barrier', '.', '*', '*', '*', '#', '#', 'tips', '*', 'your', 'tasks', '/', 'goals', 'should', 'ideally', 'be', 's', '.', 'm', '.', 'a', '.', 'r', '.', 't', 'goals', ':', '*', '*', 'specific', ',', 'me', '##asurable', ',', 'attain', '##able', ',', 'relevant', ',', 'and', 'time', '-', 'bound', '*', '*', '*', '*', '*', 'me', '##asurable', '*', '*', '-', 'make', 'sure', 'you', 'can', 'be', 'able', 'to', 'say', 'yes', 'i', 'did', 'this', '.', '*', '*', 'study', 'enough', '*', '*', 'is', 'not', 'me', '##asurable', '.', 'study', '2', 'hours', 'is', '.', '*', '*', '*', 'break', 'it', 'down', '*', '*', '-', 'instead', 'of', 'saying', 'clean', 'my', 'room', '.', '.', '.', 'it', 'might', 'help', 'to', 'say', '.', 'clean', 'room', 'for', '10', 'minutes', 'or', 'pick', 'up', 'trash', 'in', 'room', '.', '*', '*', '*', 'write', 'it', 'down', 'elsewhere', '*', '*', '-', 'sticky', 'note', 'by', 'computer', '.', 'google', 'calendar', '.', 'agenda', '.', 'write', 'it', 'down', 'so', 'you', 'don', \"'\", 't', 'forget', '.', 'our', 'working', 'memories', 'are', 'fl', '##ak', '##y', '.', '.', '.', '*', '*', '*', 'start', 'small', '.', '.', '.', 'build', 'up', 'slowly', '*', '*', '-', 'we', 'want', 'to', 'build', 'on', 'small', 'successes', 'so', 'committing', 'to', 'cleaning', 'your', 'entire', 'house', 'might', 'be', 'a', 'bit', 'too', 'much', 'to', 'handle', '.', 'pick', 'a', 'room', 'instead', '.', '*', '*', 'want', 'to', 'study', 'for', '2', 'hours', 'a', 'day', '?', '*', '*', 'commit', 'to', 'studying', '30', 'minutes', 'to', 'make', 'it', 'easier', 'on', 'you', '.', '*', '*', '*', '#', '#', 'examples', 'from', 'previous', 'weeks', ':', '*', 'un', '##load', 'the', 'dish', '##wash', '##er', '*', 'study', 'x', 'hours', 'a', 'day', '(', 'though', 'i', 'suggest', 'starting', 'small', 'with', 'minutes', ')', '*', 'make', 'an', 'appointment', 'with', 'doctor', '*', 'clean', 'car', '(', 'get', 'trash', 'out', ')', '*', '*', '*', 'start', '*', '*', '*', '[', 'something', ']', '*', '.', 'spend', '5', 'minutes', 'on', 'writing', 'my', 'paper', '.', '*', 'many', 'more', '.', '.', '.', '.', '*', '*', '*', '#', '#', '*', 'it', '’', 's', 'now', 'easier', 'to', 'respond', '!', 'just', 'copy', '/', 'paste', 'the', 'below', 'into', 'your', 'comment', '.', '*', '#', '#', 'daily', 'habit', '/', 'pro', '##cr', '##ast', '##inated', 'project', '/', 'soul', '-', 'feeding', '*', '*', '*', 'what', '?', '*', '*', '-', '*', '*', '*', 'how', 'much', 'time', '?', '*', '*', '-', '*', '*', '*', 'when', '?', '*', '*', '-', '*', '*', '*', 'potential', 'obstacles', '?', '*', '*', '-', '*', '*', '*', 'check', 'in', 'on', 'me', '?'], ['when', '?', '*', '*', '-', '*', '*', '*', 'small', 'start', '*', '*', '-', '*', '*', '*', 'reward', '*', '*', '-', '*', '*', '*', 'something', 'fun', 'you', 'want', 'to', 'do', 'this', 'week', '*', '*', '-', '*', '*', '*', 'daily', 'habit', '*', '*', '-', '*', '*', '*', 'i', 'just', 'came', 'up', 'with', 'a', 'new', 'option', 'that', 'might', 'work', 'better', 'for', 'some', 'people', '.', 'listing', 'the', 'things', 'they', 'want', 'to', 'do', 'each', 'day', '.', 'so', 'for', 'those', 'people', 'here', 'is', 'a', 'template', '.', '#', '#', '*', '*', 'monday', '*', '*', '-', '*', 'item', '#', '#', '*', '*', 'tuesday', '*', '*', '-', '#', '#', '*', '*', 'wednesday', '*', '*', '-', '#', '#', '*', '*', 'thursday', '*', '*', '-', '#', '#', '*', '*', 'friday', '*', '*', '-', '#', '#', '*', '*', 'saturday', '*', '*', '-', '#', '#', '*', '*', 'sunday', '*', '*', '-', '*', '*', '*', '*', '*', 'hopefully', 'most', 'of', 'you', 'can', 'also', 'participate', 'in', 'win', 'wednesday', '*', '*', '.', 'we', 'had', 'an', 'increase', 'in', 'participation', 'last', 'week', 'and', 'i', 'am', 'hoping', 'to', 'continue', 'to', 'build', 'on', 'that', 'this', 'week', '.', '*', '*', 'celebrating', '/', 'acknowledging', 'your', 'wins', 'is', 'just', 'as', 'important', 'as', 'actually', 'getting', 'stuff', 'done', '.', '*', '*', 'if', 'you', 'have', 'any', 'other', 'suggestions', 'for', 'weekly', 'threads', 'or', 'improvements', 'to', 'the', 'op', 'text', 'or', 'just', 'a', 'suggestion', 'on', 'how', 'we', 'can', 'make', '/', 'r', '/', 'ad', '##hd', 'better', 'then', 'message', 'me', '!', '*', '*', '*', '*', '*', 't', '##ld', '##r', ':', 'share', 'what', 'you', 'want', 'to', 'accomplish', 'this', 'week', 'and', 'how', 'you', 'will', 'do', 'it', '.', 'having', 'people', 'check', 'on', 'you', 'via', 'red', '##dit', 'is', 'extremely', 'helpful', '!', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['question', 'about', 'med', 'experience', '.', '.']\n",
      "INFO:__main__:Number of tokens: 6\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['question', 'about', 'med', 'experience', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'tips', '/', 'tricks', 'for', 'handel', '##ing', 'the', \"'\", 'internal', 'critic', \"'\", 'without', 'med', '##s', '?']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'tips', '/', 'tricks', 'for', 'handel', '##ing', 'the', \"'\", 'internal', 'critic', \"'\", 'without', 'med', '##s', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['when', 'i', 'was', '7', 'i', 'was', 'put', 'on', 'med', '##s', '.', 'i', 'had', 'some', 'pretty', 'serious', 'side', 'effects', '.', 'now', 'that', 'i', \"'\", 'm', 'older', ',', 'i', 'think', 'i', 'might', 'want', 'to', 'try', 'again', ',', 'but', 'i', 'am', 'afraid', '.', 'where', 'do', 'i', 'start', '?', 'i', 'seemed', 'to', 'have', 'some', 'kind', 'of', 'allergic', 'or', 'compulsory', 'reaction', '(', 'it', 'was', 'so', 'long', 'ago', ',', 'i', 'just', 'remember', 'my', 'eyes', 'were', 'always', '*', 'really', '*', 'it', '##chy', ')', 'reaction', 'to', 'many', 'different', 'types', 'of', 'st', '##im', '##ula', '##nts', ':', 'dex', '##id', '##rain', ',', 'rita', '##lin', ',', 'concert', '##a', '.', '.', '.', 'i', 'don', \"'\", 't', 'even', 'remember', 'the', 'names', 'of', 'all', 'of', 'the', 'drugs', 'i', 'was', 'put', 'on', 'for', 'depression', 'and', 'ad', '##hd', '.', 'after', 'one', 'didn', \"'\", 't', 'work', 'or', 'gave', 'me', 'a', 'reaction', ',', 'they', \"'\", 'd', 'try', 'another', '.', 'i', 'used', 'to', 'hide', 'them', 'under', 'my', 'tongue', 'and', 'spit', 'them', 'out', 'when', 'nobody', 'was', 'looking', ',', 'because', 'they', 'made', 'me', 'feel', 'so', 'horrible', '.', 'my', 'eyes', 'swelled', 'up', 'painfully', 'every', 'time', 'i', 'took', 'the', 'medications', '.', 'as', 'a', 'result', ',', 'some', 'of', 'the', 'reactions', 'progressed', 'into', 'horrible', 'infections', '.', 'one', 'of', 'them', 'caused', 'some', 'damage', 'to', 'my', 'left', 'eye', '.', 'i', 'might', 'have', 'lost', 'my', 'vision', 'if', 'not', 'for', 'antibiotics', '.', 'i', 'was', 'always', 'sick', 'and', 'missing', 'school', '.', 'add', '##eral', '##l', 'was', 'the', 'only', 'drug', 'i', 'had', 'a', 'relatively', 'few', 'side', 'effects', 'with', ',', 'but', 'it', 'made', 'me', 'feel', 'very', 'distant', 'and', 'lose', 'my', 'appetite', '.', 'my', 'parents', 'thankfully', 'took', 'the', 'hint', 'and', 'stopped', 'making', 'me', 'take', 'anything', ',', 'but', 'my', 'grades', 'suffered', 'quite', 'a', 'bit', '.', 'now', ',', 'after', 'years', 'of', 'being', 'med', '-', 'free', ',', 'i', 'am', 'an', 'adult', ',', 'and', 'i', 'think', 'i', 'might', 'be', 'ready', 'to', 'explore', 'some', 'newer', 'medication', 'options', '.', 'where', 'should', 'i', 'start', '?', 'i', 'haven', \"'\", 't', 'seen', 'anyone', 'for', 'this', 'in', 'decades', ',', 'and', 'it', \"'\", 's', 'pretty', 'intimidating', '.']\n",
      "INFO:__main__:Number of tokens: 321\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['when', 'i', 'was', '7', 'i', 'was', 'put', 'on', 'med', '##s', '.', 'i', 'had', 'some', 'pretty', 'serious', 'side', 'effects', '.', 'now', 'that', 'i', \"'\", 'm', 'older', ',', 'i', 'think', 'i', 'might', 'want', 'to', 'try', 'again', ',', 'but', 'i', 'am', 'afraid', '.', 'where', 'do', 'i', 'start', '?', 'i', 'seemed', 'to', 'have', 'some', 'kind', 'of', 'allergic', 'or', 'compulsory', 'reaction', '(', 'it', 'was', 'so', 'long', 'ago', ',', 'i', 'just', 'remember', 'my', 'eyes', 'were', 'always', '*', 'really', '*', 'it', '##chy', ')', 'reaction', 'to', 'many', 'different', 'types', 'of', 'st', '##im', '##ula', '##nts', ':', 'dex', '##id', '##rain', ',', 'rita', '##lin', ',', 'concert', '##a', '.', '.', '.', 'i', 'don', \"'\", 't', 'even', 'remember', 'the', 'names', 'of', 'all', 'of', 'the', 'drugs', 'i', 'was', 'put', 'on', 'for', 'depression', 'and', 'ad', '##hd', '.', 'after', 'one', 'didn', \"'\", 't', 'work', 'or', 'gave', 'me', 'a', 'reaction', ',', 'they', \"'\", 'd', 'try', 'another', '.', 'i', 'used', 'to', 'hide', 'them', 'under', 'my', 'tongue', 'and', 'spit', 'them', 'out', 'when', 'nobody', 'was', 'looking', ',', 'because', 'they', 'made', 'me', 'feel', 'so', 'horrible', '.', 'my', 'eyes', 'swelled', 'up', 'painfully', 'every', 'time', 'i', 'took', 'the', 'medications', '.', 'as', 'a', 'result', ',', 'some', 'of', 'the', 'reactions', 'progressed', 'into', 'horrible', 'infections', '.', 'one', 'of', 'them', 'caused', 'some', 'damage', 'to', 'my', 'left', 'eye', '.', 'i', 'might', 'have', 'lost', 'my', 'vision', 'if', 'not', 'for', 'antibiotics', '.', 'i', 'was', 'always', 'sick', 'and', 'missing', 'school', '.', 'add', '##eral', '##l', 'was', 'the', 'only', 'drug', 'i', 'had', 'a', 'relatively', 'few', 'side', 'effects', 'with', ',', 'but', 'it', 'made', 'me', 'feel', 'very', 'distant', 'and', 'lose', 'my', 'appetite', '.', 'my', 'parents', 'thankfully', 'took', 'the', 'hint', 'and', 'stopped', 'making', 'me', 'take', 'anything', ',', 'but', 'my', 'grades', 'suffered', 'quite', 'a', 'bit', '.', 'now', ',', 'after', 'years', 'of', 'being', 'med', '-', 'free', ',', 'i', 'am', 'an', 'adult', ',', 'and', 'i', 'think', 'i', 'might', 'be', 'ready', 'to', 'explore', 'some', 'newer', 'medication', 'options', '.', 'where', 'should', 'i', 'start', '?', 'i', 'haven', \"'\", 't', 'seen', 'anyone', 'for', 'this', 'in', 'decades', ',', 'and', 'it', \"'\", 's', 'pretty', 'intimidating', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'i', 'have', 'ad', '##hd', '?', 'hey', 'y', \"'\", 'all', '.', 'i', 'am', 'debating', 'whether', 'or', 'not', 'i', 'have', 'add', '(', 'non', 'hyper', '##active', ')', 'for', 'the', 'longest', 'time', '.', 'i', 'have', 'always', 'been', 'a', 'good', 'student', ',', 'with', 'a', 'b', '+', 'average', 'all', 'through', 'university', ',', 'but', 'my', 'parents', 'are', 'both', 'high', 'achieving', 'adults', 'with', 'advanced', 'degrees', '.', 'my', 'brother', 'is', 'also', 'very', 'brilliant', '.', 'i', 'have', 'always', 'been', 'told', 'i', 'am', 'really', 'smart', ',', 'but', 'i', 'do', 'not', 'feel', 'like', 'my', 'academic', 'performance', 'is', 'up', 'to', 'what', 'i', 'think', 'i', 'am', '?', '!', '?', 'i', 'know', 'that', 'sounds', 'pre', '##ten', '##tious', ',', 'but', 'hopefully', 'you', 'see', 'what', 'i', 'mean', '.', 'i', 'also', 'got', 'mostly', 'good', 'grades', 'through', 'bullshit', '##ting', 'in', 'a', 'literature', 'major', '.', 'i', 'never', 'could', 'get', 'myself', 'to', 'read', 'a', 'book', '/', 'do', 'anything', 'for', 'longer', 'than', '15', 'minutes', '.', 'in', 'addition', 'to', 'school', ',', 'add', 'really', 'affects', 'me', 'socially', '.', 'i', 'interrupt', 'people', 'a', 'lot', 'and', 'often', 'talk', 'too', 'loudly', '.', 'i', 'am', 'also', 'incredibly', 'impatient', '.', 'i', 'am', 'currently', 'taking', '15', '##mg', 'of', 'rita', '##lin', 'and', 'i', 'just', 'think', 'it', 'makes', 'me', 'hyper', '##fo', '##cus', 'sometimes', ',', 'that', \"'\", 's', 'it', '.', 'i', 'took', 'focal', '##in', 'x', '##r', 'when', 'i', 'was', 'in', 'un', '##i', 'and', 'it', 'just', 'made', 'me', 'sleepy', '.', 'what', 'are', 'everyone', \"'\", 's', 'thoughts', '?']\n",
      "INFO:__main__:Number of tokens: 224\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'i', 'have', 'ad', '##hd', '?', 'hey', 'y', \"'\", 'all', '.', 'i', 'am', 'debating', 'whether', 'or', 'not', 'i', 'have', 'add', '(', 'non', 'hyper', '##active', ')', 'for', 'the', 'longest', 'time', '.', 'i', 'have', 'always', 'been', 'a', 'good', 'student', ',', 'with', 'a', 'b', '+', 'average', 'all', 'through', 'university', ',', 'but', 'my', 'parents', 'are', 'both', 'high', 'achieving', 'adults', 'with', 'advanced', 'degrees', '.', 'my', 'brother', 'is', 'also', 'very', 'brilliant', '.', 'i', 'have', 'always', 'been', 'told', 'i', 'am', 'really', 'smart', ',', 'but', 'i', 'do', 'not', 'feel', 'like', 'my', 'academic', 'performance', 'is', 'up', 'to', 'what', 'i', 'think', 'i', 'am', '?', '!', '?', 'i', 'know', 'that', 'sounds', 'pre', '##ten', '##tious', ',', 'but', 'hopefully', 'you', 'see', 'what', 'i', 'mean', '.', 'i', 'also', 'got', 'mostly', 'good', 'grades', 'through', 'bullshit', '##ting', 'in', 'a', 'literature', 'major', '.', 'i', 'never', 'could', 'get', 'myself', 'to', 'read', 'a', 'book', '/', 'do', 'anything', 'for', 'longer', 'than', '15', 'minutes', '.', 'in', 'addition', 'to', 'school', ',', 'add', 'really', 'affects', 'me', 'socially', '.', 'i', 'interrupt', 'people', 'a', 'lot', 'and', 'often', 'talk', 'too', 'loudly', '.', 'i', 'am', 'also', 'incredibly', 'impatient', '.', 'i', 'am', 'currently', 'taking', '15', '##mg', 'of', 'rita', '##lin', 'and', 'i', 'just', 'think', 'it', 'makes', 'me', 'hyper', '##fo', '##cus', 'sometimes', ',', 'that', \"'\", 's', 'it', '.', 'i', 'took', 'focal', '##in', 'x', '##r', 'when', 'i', 'was', 'in', 'un', '##i', 'and', 'it', 'just', 'made', 'me', 'sleepy', '.', 'what', 'are', 'everyone', \"'\", 's', 'thoughts', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['techniques', 'for', 'getting', 'to', 'sleep', 'at', 'night', '?']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['techniques', 'for', 'getting', 'to', 'sleep', 'at', 'night', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['normally', 'i', 'love', 'skeptical', 'in', '##qui', '##rer', ',', 'but', 'this', 'article', 'ir', '##rita', '##tes', 'me', '.']\n",
      "INFO:__main__:Number of tokens: 16\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['normally', 'i', 'love', 'skeptical', 'in', '##qui', '##rer', ',', 'but', 'this', 'article', 'ir', '##rita', '##tes', 'me', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', '##pa', '##mine', 'impacts', 'your', 'willingness', 'to', 'work']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', '##pa', '##mine', 'impacts', 'your', 'willingness', 'to', 'work']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['za', '##p', 'your', 'brain', 'into', 'the', 'zone', ':', 'fast', 'track', 'to', 'pure', 'focus']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['za', '##p', 'your', 'brain', 'into', 'the', 'zone', ':', 'fast', 'track', 'to', 'pure', 'focus']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['problems', 'with', 'prescription', ',', 'looking', 'for', 'alternatives', '(', 'more', 'inside', ')', '*', 'i', \"'\", 'm', 'on', 'v', '##y', '##van', '##se', ',', 'and', 'my', 'mom', 'has', 'been', 'having', 'problems', 'with', 'getting', 'my', 'prescription', 'approved', '.', 'my', 'doctor', 'prescribed', 'me', 'taking', 'two', 'doses', 'a', 'day', ',', '20', 'mg', 'each', 'dose', ',', 'in', 'the', 'morning', 'and', 'in', 'the', 'afternoon', '.', 'this', 'means', 'that', 'i', 'need', 'to', 'have', '60', 'capsule', '##s', 'for', 'every', 'month', ',', 'but', 'our', 'insurance', 'does', 'not', 'allow', 'that', '(', 'they', 'cap', 'it', 'at', '30', 'a', 'month', ')', '.', '*', 'a', 'possible', 'solution', 'we', 'came', 'up', 'with', 'is', 'to', 'try', 'to', 'get', 'the', '40', 'mg', 'capsule', '##s', 'and', 'have', 'me', 'take', 'half', 'a', 'capsule', 'a', 'day', ',', 'keeping', 'with', 'the', '30', 'capsule', '##s', 'a', 'day', 'that', 'our', 'insurance', 'allows', '.', 'the', 'problem', 'lies', 'with', 'how', 'i', \"'\", 'm', 'going', 'to', 'be', 'able', 'to', 'take', 'exactly', 'half', 'of', 'the', 'capsule', 'in', 'the', 'first', 'place', '.', '*', 'what', 'we', 'came', 'up', 'with', 'is', 'using', 'a', 'water', 'bottle', 'with', 'volume', 'measurements', 'and', 'filling', 'it', 'to', 'a', 'certain', 'point', '.', 'we', 'would', 'then', 'pour', 'the', 'contents', 'of', 'the', 'capsule', 'into', 'the', 'water', 'bottle', 'and', 'have', 'me', 'drink', 'half', 'of', 'it', 'for', 'my', 'first', 'dose', 'and', 'the', 'other', 'half', 'later', 'on', 'for', 'my', 'second', 'dose', '.', '*', 'now', ',', 'i', \"'\", 'm', 'not', 'a', 'bio', '##chemist', 'or', 'anything', ',', 'but', 'i', \"'\", 'm', 'aware', 'that', 'water', 'is', 'a', 'universal', 'solvent', '.', 'what', 'we', \"'\", 're', 'worried', 'about', 'is', 'that', 'if', 'we', 'put', 'the', 'contents', 'of', 'the', 'capsule', 'in', 'water', 'it', 'will', 'den', '##at', '##ure', 'the', 'medication', 'and', 'its', 'effectiveness', '.', 'this', 'concern', 'is', 'mostly', 'for', 'the', 'second', 'dose', ',', 'which', 'would', 'be', 'sitting', 'in', 'the', 'water', 'for', 'several', 'hours', 'until', 'my', 'second', 'dose', '.', '*', '*', '*', 'here', \"'\", 's', 'my', 'question', ':', 'if', 'you', 'put', 'the', 'powder', '##ed', 'contents', 'of', 'the', 'medication', 'capsule', '(', 'v', '##y', '##van', '##se', ')', 'in', 'water', 'for', 'extended', 'periods', 'of', 'time', ',', 'will', 'the', 'medicine', 'cease', 'to', 'function', '?', '*', '*']\n",
      "INFO:__main__:Number of tokens: 331\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['problems', 'with', 'prescription', ',', 'looking', 'for', 'alternatives', '(', 'more', 'inside', ')', '*', 'i', \"'\", 'm', 'on', 'v', '##y', '##van', '##se', ',', 'and', 'my', 'mom', 'has', 'been', 'having', 'problems', 'with', 'getting', 'my', 'prescription', 'approved', '.', 'my', 'doctor', 'prescribed', 'me', 'taking', 'two', 'doses', 'a', 'day', ',', '20', 'mg', 'each', 'dose', ',', 'in', 'the', 'morning', 'and', 'in', 'the', 'afternoon', '.', 'this', 'means', 'that', 'i', 'need', 'to', 'have', '60', 'capsule', '##s', 'for', 'every', 'month', ',', 'but', 'our', 'insurance', 'does', 'not', 'allow', 'that', '(', 'they', 'cap', 'it', 'at', '30', 'a', 'month', ')', '.', '*', 'a', 'possible', 'solution', 'we', 'came', 'up', 'with', 'is', 'to', 'try', 'to', 'get', 'the', '40', 'mg', 'capsule', '##s', 'and', 'have', 'me', 'take', 'half', 'a', 'capsule', 'a', 'day', ',', 'keeping', 'with', 'the', '30', 'capsule', '##s', 'a', 'day', 'that', 'our', 'insurance', 'allows', '.', 'the', 'problem', 'lies', 'with', 'how', 'i', \"'\", 'm', 'going', 'to', 'be', 'able', 'to', 'take', 'exactly', 'half', 'of', 'the', 'capsule', 'in', 'the', 'first', 'place', '.', '*', 'what', 'we', 'came', 'up', 'with', 'is', 'using', 'a', 'water', 'bottle', 'with', 'volume', 'measurements', 'and', 'filling', 'it', 'to', 'a', 'certain', 'point', '.', 'we', 'would', 'then', 'pour', 'the', 'contents', 'of', 'the', 'capsule', 'into', 'the', 'water', 'bottle', 'and', 'have', 'me', 'drink', 'half', 'of', 'it', 'for', 'my', 'first', 'dose', 'and', 'the', 'other', 'half', 'later', 'on', 'for', 'my', 'second', 'dose', '.', '*', 'now', ',', 'i', \"'\", 'm', 'not', 'a', 'bio', '##chemist', 'or', 'anything', ',', 'but', 'i', \"'\", 'm', 'aware', 'that', 'water', 'is', 'a', 'universal', 'solvent', '.', 'what', 'we', \"'\", 're', 'worried', 'about', 'is', 'that', 'if', 'we', 'put', 'the', 'contents', 'of', 'the', 'capsule', 'in', 'water', 'it', 'will', 'den', '##at', '##ure', 'the', 'medication', 'and', 'its', 'effectiveness', '.', 'this', 'concern', 'is', 'mostly', 'for', 'the', 'second', 'dose', ',', 'which', 'would', 'be', 'sitting', 'in', 'the', 'water', 'for', 'several', 'hours', 'until', 'my', 'second', 'dose', '.', '*', '*', '*', 'here', \"'\", 's', 'my', 'question', ':', 'if', 'you', 'put', 'the', 'powder', '##ed', 'contents', 'of', 'the', 'medication', 'capsule', '(', 'v', '##y', '##van', '##se', ')', 'in', 'water', 'for', 'extended', 'periods', 'of', 'time', ',', 'will', 'the', 'medicine', 'cease', 'to', 'function', '?', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', 'moment', 'made', 'you', 'realise', 'med', '##s', 'were', 'working', 'for', 'you', '?', 'hey', 'folks', ',', 'i', 'was', 'diagnosed', 'with', 'add', 'last', 'friday', '.', 'i', \"'\", 've', 'been', 'given', '18', 'mg', 'of', 'concert', '##a', 'for', 'the', 'next', '4', 'weeks', '.', 'after', 'three', 'days', 'i', 'haven', \"'\", 't', 'really', 'noticed', 'any', 'significant', 'difference', 'in', 'my', 'concentration', ',', 'organisation', 'or', 'motivation', 'to', 'start', 'my', 'assignments', '.', 'i', 'was', 'just', 'wondering', 'what', 'event', ',', 'situation', 'or', 'task', 'made', 'you', 'realise', 'that', 'med', '##s', 'were', 'working', 'for', 'you', '?']\n",
      "INFO:__main__:Number of tokens: 84\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', 'moment', 'made', 'you', 'realise', 'med', '##s', 'were', 'working', 'for', 'you', '?', 'hey', 'folks', ',', 'i', 'was', 'diagnosed', 'with', 'add', 'last', 'friday', '.', 'i', \"'\", 've', 'been', 'given', '18', 'mg', 'of', 'concert', '##a', 'for', 'the', 'next', '4', 'weeks', '.', 'after', 'three', 'days', 'i', 'haven', \"'\", 't', 'really', 'noticed', 'any', 'significant', 'difference', 'in', 'my', 'concentration', ',', 'organisation', 'or', 'motivation', 'to', 'start', 'my', 'assignments', '.', 'i', 'was', 'just', 'wondering', 'what', 'event', ',', 'situation', 'or', 'task', 'made', 'you', 'realise', 'that', 'med', '##s', 'were', 'working', 'for', 'you', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['anatomy', 'of', 'the', 'great', 'add', '##eral', '##l', 'drought']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['anatomy', 'of', 'the', 'great', 'add', '##eral', '##l', 'drought']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['question', 'about', 'ad', '##hd', 'symptoms']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['question', 'about', 'ad', '##hd', 'symptoms']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['update', 'to', ':', '\"', 'i', 'am', '*', 'this', '##cl', '##ose', '*', 'to', 'being', 'fired', 'from', 'my', 'job', 'for', 'arriving', 'late', 'too', 'often', '.', '.', '.', '\"', '[', 'original', 'post', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 's', '##9', '##cu', '##9', '/', 'i', '_', 'am', '_', 'this', '##cl', '##ose', '_', 'to', '_', 'being', '_', 'fired', '_', 'from', '_', 'my', '_', 'job', '_', 'for', '/', ')', 'you', 'guys', '.', 'i', 'am', 'proud', 'to', 'announce', 'that', 'in', 'the', 'past', '~', '3', 'weeks', ',', 'i', 'haven', \"'\", 't', 'been', 'late', 'to', 'work', '*', 'a', 'single', 'time', '*', '.', 'in', 'fact', ',', 'i', \"'\", 've', 'been', 'early', 'because', ',', 'as', 'many', 'of', 'you', 'pointed', 'out', ',', 'it', \"'\", 's', 'the', 'only', 'way', 'to', 'not', 'be', 'late', 'for', 'us', 'chronic', '##ally', '-', 'challenged', 'individuals', '.', 'i', 'basically', 'just', 'force', 'myself', 'out', 'the', 'door', 'by', 'a', 'certain', 'time', 'and', 'try', 'really', 'really', 'hard', 'to', 'not', 'leave', 'a', 'minute', 'later', 'because', 'then', 'i', 'spend', 'the', 'com', '##mute', 'to', 'work', 'freaking', 'out', 'and', 'road', '-', 'raging', 'at', 'the', 'seemingly', 'infinite', 'amount', 'of', 'slow', '##po', '##kes', 'on', 'the', 'road', '.', 'so', 'far', ',', 'it', \"'\", 's', 'been', 'successful', 'and', 'i', 'haven', \"'\", 't', 'heard', 'anything', 'negative', 'from', 'the', 'office', 'of', 'the', 'boss', '(', 'or', 'anything', 'positive', ',', 'for', 'that', 'matter', ',', 'but', 'whatever', '-', 'ill', 'give', 'myself', 'a', 'damn', 'cookie', 'and', 'pat', 'on', 'the', 'back', '.', '.', '.', ')', '.', 'so', 'i', \"'\", 'm', 'keeping', 'my', 'fingers', 'crossed', 'that', 'i', 'can', 'keep', 'this', 'up', '.', '.', '.', 'this', 'is', 'the', 'longest', 'i', \"'\", 've', 'made', 'it', 'so', 'far', ',', 'so', 'things', 'are', 'looking', 'good', '!', '*', '*', 'edit', ':', 'holy', 'crap', '##ola', '!', 'i', 'kept', 'getting', 'an', 'error', 'message', 'when', 'i', 'tried', 'submit', '##ting', 'this', 'post', ',', 'so', 'i', 'gave', 'up', 'thinking', 'it', 'didn', \"'\", 't', 'go', 'through', '.', 'lo', 'and', 'behold', '!', 'a', 'fantastic', 'surprise', '!', 'thanks', 'for', 'the', 'encouragement', 'and', 'high', 'five', '##s', ',', 'friends', '!', 'you', 'all', 'just', 'made', 'my', 'freaking', 'day', '.', '.', '.', '*', '*']\n",
      "INFO:__main__:Number of tokens: 343\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['update', 'to', ':', '\"', 'i', 'am', '*', 'this', '##cl', '##ose', '*', 'to', 'being', 'fired', 'from', 'my', 'job', 'for', 'arriving', 'late', 'too', 'often', '.', '.', '.', '\"', '[', 'original', 'post', ']', '(', 'http', ':', '/', '/', 'www', '.', 'red', '##dit', '.', 'com', '/', 'r', '/', 'ad', '##hd', '/', 'comments', '/', 's', '##9', '##cu', '##9', '/', 'i', '_', 'am', '_', 'this', '##cl', '##ose', '_', 'to', '_', 'being', '_', 'fired', '_', 'from', '_', 'my', '_', 'job', '_', 'for', '/', ')', 'you', 'guys', '.', 'i', 'am', 'proud', 'to', 'announce', 'that', 'in', 'the', 'past', '~', '3', 'weeks', ',', 'i', 'haven', \"'\", 't', 'been', 'late', 'to', 'work', '*', 'a', 'single', 'time', '*', '.', 'in', 'fact', ',', 'i', \"'\", 've', 'been', 'early', 'because', ',', 'as', 'many', 'of', 'you', 'pointed', 'out', ',', 'it', \"'\", 's', 'the', 'only', 'way', 'to', 'not', 'be', 'late', 'for', 'us', 'chronic', '##ally', '-', 'challenged', 'individuals', '.', 'i', 'basically', 'just', 'force', 'myself', 'out', 'the', 'door', 'by', 'a', 'certain', 'time', 'and', 'try', 'really', 'really', 'hard', 'to', 'not', 'leave', 'a', 'minute', 'later', 'because', 'then', 'i', 'spend', 'the', 'com', '##mute', 'to', 'work', 'freaking', 'out', 'and', 'road', '-', 'raging', 'at', 'the', 'seemingly', 'infinite', 'amount', 'of', 'slow', '##po', '##kes', 'on', 'the', 'road', '.', 'so', 'far', ',', 'it', \"'\", 's', 'been', 'successful', 'and', 'i', 'haven', \"'\", 't', 'heard', 'anything', 'negative', 'from', 'the', 'office', 'of', 'the', 'boss', '(', 'or', 'anything', 'positive', ',', 'for', 'that', 'matter', ',', 'but', 'whatever', '-', 'ill', 'give', 'myself', 'a', 'damn', 'cookie', 'and', 'pat', 'on', 'the', 'back', '.', '.', '.', ')', '.', 'so', 'i', \"'\", 'm', 'keeping', 'my', 'fingers', 'crossed', 'that', 'i', 'can', 'keep', 'this', 'up', '.', '.', '.', 'this', 'is', 'the', 'longest', 'i', \"'\", 've', 'made', 'it', 'so', 'far', ',', 'so', 'things', 'are', 'looking', 'good', '!', '*', '*', 'edit', ':', 'holy', 'crap', '##ola', '!', 'i', 'kept', 'getting', 'an', 'error', 'message', 'when', 'i', 'tried', 'submit', '##ting', 'this', 'post', ',', 'so', 'i', 'gave', 'up', 'thinking', 'it', 'didn', \"'\", 't', 'go', 'through', '.', 'lo', 'and', 'behold', '!', 'a', 'fantastic', 'surprise', '!', 'thanks', 'for', 'the', 'encouragement', 'and', 'high', 'five', '##s', ',', 'friends', '!', 'you', 'all', 'just', 'made', 'my', 'freaking', 'day', '.', '.', '.', '*', '*']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['add', '##eral', '##l', '/', 'v', '##y', '##van', '##se', 'and', 'hair', 'loss', '?', 'i', \"'\", 'm', '21', 'y', '##r', 'old', 'male', 'in', 'university', 'and', 'have', 'been', 'on', 'some', 'sort', 'of', 'amp', '##het', '##amine', '##s', 'for', 'the', 'past', '4', '-', '5', 'years', '.', 'over', 'the', 'past', 'year', 'or', 'so', 'i', 'have', 'noticed', 'that', 'days', 'when', 'i', 'am', 'on', 'my', 'v', '##y', '##van', '##se', '(', '60', '##mg', ')', 'or', 'add', '##eral', '##l', 'that', 'i', 'have', 'been', 'experiencing', 'some', 'hair', 'loss', ',', 'lo', '##osing', 'a', 'few', 'strands', 'here', 'and', 'there', 'which', 'is', 'unusual', 'for', 'me', '.', 'i', 'have', 'no', 'family', 'history', 'of', 'hair', 'loss', 'and', 'naturally', 'have', 'very', 'thick', 'hair', '.', 'what', 'i', 'also', 'find', 'unusual', 'is', 'its', 'mainly', 'on', 'my', 'left', 'side', 'hair', '##line', 'and', 'the', 'right', 'seems', 'unaffected', '.', 'i', 'have', 'been', 'writing', 'it', 'off', 'as', 'nothing', 'and', 'have', 'tried', 'to', 'improve', 'my', 'diet', 'which', 'was', 'fairly', 'good', 'already', ',', 'never', 'experienced', 'a', 'problem', 'with', 'eating', ',', 'but', 'the', 'past', 'few', 'weeks', 'i', 'have', 'become', 'worried', 'that', 'its', 'becoming', 'noticeable', 'and', 'started', 'taking', 'a', 'bio', '##tin', 'supplement', ',', 'fish', 'oil', ',', 'and', 'a', 'multi', '##vita', '##min', 'daily', 'but', 'still', 'experiencing', 'the', 'same', 'problems', '.', 'has', 'anyone', 'else', 'experienced', 'hair', 'loss', 'as', 'a', 'side', 'effect', 'of', 'add', '##eral', '##l', '/', 'v', '##y', '##van', '##se', 'or', 'know', 'of', 'any', 're', '##med', '##ies', 'to', 'the', 'problem', 'beside', 'abs', '##tain', '##ing', '?', 'i', 'have', 'a', 'very', 'vigorous', 'course', 'load', 'and', 'am', 'unsure', 'if', 'i', 'could', 'keep', 'up', 'without', 'the', 'medication', ',', 'that', 'said', ',', 'i', 'also', 'obviously', 'don', \"'\", 't', 'want', 'to', 'go', 'bald', 'either', '!', 'there', '##s', 'a', 'fair', 'amount', 'of', 'results', 'on', 'google', 'relating', 'to', 'the', 'topic', 'but', 'no', 'company', 'has', 'officially', 'recognized', 'it', 'as', 'a', 'possible', 'side', 'effect', '.']\n",
      "INFO:__main__:Number of tokens: 286\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['add', '##eral', '##l', '/', 'v', '##y', '##van', '##se', 'and', 'hair', 'loss', '?', 'i', \"'\", 'm', '21', 'y', '##r', 'old', 'male', 'in', 'university', 'and', 'have', 'been', 'on', 'some', 'sort', 'of', 'amp', '##het', '##amine', '##s', 'for', 'the', 'past', '4', '-', '5', 'years', '.', 'over', 'the', 'past', 'year', 'or', 'so', 'i', 'have', 'noticed', 'that', 'days', 'when', 'i', 'am', 'on', 'my', 'v', '##y', '##van', '##se', '(', '60', '##mg', ')', 'or', 'add', '##eral', '##l', 'that', 'i', 'have', 'been', 'experiencing', 'some', 'hair', 'loss', ',', 'lo', '##osing', 'a', 'few', 'strands', 'here', 'and', 'there', 'which', 'is', 'unusual', 'for', 'me', '.', 'i', 'have', 'no', 'family', 'history', 'of', 'hair', 'loss', 'and', 'naturally', 'have', 'very', 'thick', 'hair', '.', 'what', 'i', 'also', 'find', 'unusual', 'is', 'its', 'mainly', 'on', 'my', 'left', 'side', 'hair', '##line', 'and', 'the', 'right', 'seems', 'unaffected', '.', 'i', 'have', 'been', 'writing', 'it', 'off', 'as', 'nothing', 'and', 'have', 'tried', 'to', 'improve', 'my', 'diet', 'which', 'was', 'fairly', 'good', 'already', ',', 'never', 'experienced', 'a', 'problem', 'with', 'eating', ',', 'but', 'the', 'past', 'few', 'weeks', 'i', 'have', 'become', 'worried', 'that', 'its', 'becoming', 'noticeable', 'and', 'started', 'taking', 'a', 'bio', '##tin', 'supplement', ',', 'fish', 'oil', ',', 'and', 'a', 'multi', '##vita', '##min', 'daily', 'but', 'still', 'experiencing', 'the', 'same', 'problems', '.', 'has', 'anyone', 'else', 'experienced', 'hair', 'loss', 'as', 'a', 'side', 'effect', 'of', 'add', '##eral', '##l', '/', 'v', '##y', '##van', '##se', 'or', 'know', 'of', 'any', 're', '##med', '##ies', 'to', 'the', 'problem', 'beside', 'abs', '##tain', '##ing', '?', 'i', 'have', 'a', 'very', 'vigorous', 'course', 'load', 'and', 'am', 'unsure', 'if', 'i', 'could', 'keep', 'up', 'without', 'the', 'medication', ',', 'that', 'said', ',', 'i', 'also', 'obviously', 'don', \"'\", 't', 'want', 'to', 'go', 'bald', 'either', '!', 'there', '##s', 'a', 'fair', 'amount', 'of', 'results', 'on', 'google', 'relating', 'to', 'the', 'topic', 'but', 'no', 'company', 'has', 'officially', 'recognized', 'it', 'as', 'a', 'possible', 'side', 'effect', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'was', 'just', 'diagnosed', 'with', 'adult', '-', 'onset', 'ad', '##hd', '.', 'this', 'worries', 'me', '.', 'i', 'am', 'a', 'male', 'in', 'my', 'young', '20s', 'attending', 'law', 'school', '.', 'i', \"'\", 'd', 'been', 'having', 'trouble', 'focusing', 'on', 'my', 'readings', 'over', 'the', 'past', 'academic', 'year', 'so', 'i', 'went', 'and', 'saw', 'a', 'psychiatrist', '.', 'based', 'on', 'an', 'hour', 'long', 'visit', 'with', 'him', 'where', 'he', 'asked', 'me', 'questions', 'and', 'observed', 'my', 'body', 'movements', '(', 'i', 'like', 'to', 'tap', '/', 'bounce', 'my', 'leg', 'often', ')', 'he', 'concluded', 'that', 'i', 'had', 'ad', '##hd', '.', 'i', 'am', 'assuming', 'it', 'was', 'a', 'mild', 'form', 'of', 'ad', '##hd', 'because', 'he', 'did', 'not', 'seem', 'that', 'worried', 'about', 'me', '.', 'he', 'said', 'that', 'i', 'would', 'get', 'to', 'pick', 'the', 'drug', 'and', 'the', 'type', 'of', 'dos', '##age', '(', 'extended', 'release', ',', 'immediate', ')', 'so', 'that', 'we', 'could', 'get', 'a', 'handle', 'on', 'the', 'type', 'of', 'reactions', 'i', 'would', 'have', 'to', 'the', 'drug', '.', 'based', 'on', 'initial', 'research', ',', 'i', 'opted', 'for', 'add', '##eral', '##l', '(', 'generic', ')', 'extended', 'release', '.', 'i', 'am', 'not', 'really', 'keen', 'on', 'taking', 'medication', 'daily', ',', 'but', 'mainly', 'only', 'when', 'i', 'have', 'substantial', 'amounts', 'of', 'studying', 'to', 'do', '(', 'maybe', 'once', 'per', 'week', ',', 'and', 'then', 'regularly', 'during', 'finals', 'week', ')', '.', 'did', 'i', 'make', 'the', 'right', 'decision', '?', 'i', 'know', 'that', 'some', 'illegally', 'pro', '##cured', 'generic', 'rita', '##lin', 'extended', 'release', 'seems', 'to', 'be', 'more', 'effective', 'for', 'me', 'than', 'immediate', 'release', 'add', '##eral', '##l', '(', 'at', 'least', 'i', 'think', 'it', 'is', ',', 'it', \"'\", 's', 'hard', 'to', 'tell', 'what', 'is', 'working', 'and', 'what', 'is', 'place', '##bo', 'effect', ')', 'and', 'i', 'have', 'not', 'yet', 'had', 'the', 'chance', 'to', 'pick', 'up', 'extended', 'release', 'add', '##eral', '##l', '.', 'web', '##md', 'seems', 'to', 'suggest', '(', 'and', 'some', 'other', 'sites', ')', 'that', 'that', 'side', 'effects', 'are', 'mild', '##er', 'with', 'add', '##eral', '##l', '.', 'random', 'an', '##ec', '##dot', '##al', 'sites', 'seem', 'to', 'indicate', 'it', 'is', 'also', 'more', 'effective', '.', 'does', 'anyone', 'have', 'any', 'recommendations', 'on', 'which', 'i', 'should', 'pick', '(', 'outside', 'of', '\"', 'try', 'them', 'all', '?', '\"', ')', '.', 'this', 'has', 'been', 'really', 'nerve', 'wr', '##ack', '##ing', 'and', 'is', 'seriously', '(', 'ironically', ')', 'imp', '##eding', 'my', 'ability', 'to', 'focus', 'on', 'my', 'exams', '.']\n",
      "INFO:__main__:Number of tokens: 358\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'was', 'just', 'diagnosed', 'with', 'adult', '-', 'onset', 'ad', '##hd', '.', 'this', 'worries', 'me', '.', 'i', 'am', 'a', 'male', 'in', 'my', 'young', '20s', 'attending', 'law', 'school', '.', 'i', \"'\", 'd', 'been', 'having', 'trouble', 'focusing', 'on', 'my', 'readings', 'over', 'the', 'past', 'academic', 'year', 'so', 'i', 'went', 'and', 'saw', 'a', 'psychiatrist', '.', 'based', 'on', 'an', 'hour', 'long', 'visit', 'with', 'him', 'where', 'he', 'asked', 'me', 'questions', 'and', 'observed', 'my', 'body', 'movements', '(', 'i', 'like', 'to', 'tap', '/', 'bounce', 'my', 'leg', 'often', ')', 'he', 'concluded', 'that', 'i', 'had', 'ad', '##hd', '.', 'i', 'am', 'assuming', 'it', 'was', 'a', 'mild', 'form', 'of', 'ad', '##hd', 'because', 'he', 'did', 'not', 'seem', 'that', 'worried', 'about', 'me', '.', 'he', 'said', 'that', 'i', 'would', 'get', 'to', 'pick', 'the', 'drug', 'and', 'the', 'type', 'of', 'dos', '##age', '(', 'extended', 'release', ',', 'immediate', ')', 'so', 'that', 'we', 'could', 'get', 'a', 'handle', 'on', 'the', 'type', 'of', 'reactions', 'i', 'would', 'have', 'to', 'the', 'drug', '.', 'based', 'on', 'initial', 'research', ',', 'i', 'opted', 'for', 'add', '##eral', '##l', '(', 'generic', ')', 'extended', 'release', '.', 'i', 'am', 'not', 'really', 'keen', 'on', 'taking', 'medication', 'daily', ',', 'but', 'mainly', 'only', 'when', 'i', 'have', 'substantial', 'amounts', 'of', 'studying', 'to', 'do', '(', 'maybe', 'once', 'per', 'week', ',', 'and', 'then', 'regularly', 'during', 'finals', 'week', ')', '.', 'did', 'i', 'make', 'the', 'right', 'decision', '?', 'i', 'know', 'that', 'some', 'illegally', 'pro', '##cured', 'generic', 'rita', '##lin', 'extended', 'release', 'seems', 'to', 'be', 'more', 'effective', 'for', 'me', 'than', 'immediate', 'release', 'add', '##eral', '##l', '(', 'at', 'least', 'i', 'think', 'it', 'is', ',', 'it', \"'\", 's', 'hard', 'to', 'tell', 'what', 'is', 'working', 'and', 'what', 'is', 'place', '##bo', 'effect', ')', 'and', 'i', 'have', 'not', 'yet', 'had', 'the', 'chance', 'to', 'pick', 'up', 'extended', 'release', 'add', '##eral', '##l', '.', 'web', '##md', 'seems', 'to', 'suggest', '(', 'and', 'some', 'other', 'sites', ')', 'that', 'that', 'side', 'effects', 'are', 'mild', '##er', 'with', 'add', '##eral', '##l', '.', 'random', 'an', '##ec', '##dot', '##al', 'sites', 'seem', 'to', 'indicate', 'it', 'is', 'also', 'more', 'effective', '.', 'does', 'anyone', 'have', 'any', 'recommendations', 'on', 'which', 'i', 'should', 'pick', '(', 'outside', 'of', '\"', 'try', 'them', 'all', '?', '\"', ')', '.', 'this', 'has', 'been', 'really', 'nerve', 'wr', '##ack', '##ing', 'and', 'is', 'seriously', '(', 'ironically', ')', 'imp', '##eding', 'my', 'ability', 'to', 'focus', 'on', 'my', 'exams', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'need', 'a', 'kick', 'in', 'the', 'butt', ',', 'but', 'how', '?', 'i', 'have', 'too', 'much', 'self', 'pity', '.', 'for', 'example', ',', 'last', 'night', 'i', 'couldn', \"'\", 't', 'fall', 'asleep', 'at', 'all', ',', 'went', 'to', 'bed', 'at', '11', ',', 'finally', 'fell', 'asleep', 'around', '3', '.', 'tomorrow', ',', 'i', 'should', \"'\", 've', 'got', 'out', 'bi', '##j', '9', ',', 'but', 'said', 'to', 'myself', ':', 'ah', 'you', 'couldn', \"'\", 't', 'sleep', 'last', 'night', ',', 'you', 'can', 'sleep', 'as', 'long', 'as', 'you', 'want', '.', 'i', 'slept', 'only', \"'\", 'till', '10', ',', 'but', 'still', '.', 'i', \"'\", 'm', 'a', 'bit', 'spoil', '##t', '(', 'spoiled', '?', ')', ',', 'it', 'isn', \"'\", 't', 'like', 'i', 'got', 'anything', 'i', 'wanted', ',', 'and', 'i', 'had', 'to', 'work', ',', 'but', 'i', 'didn', \"'\", 't', 'have', 'enough', 'kicks', 'in', 'the', 'butt', ',', 'because', 'i', \"'\", 'm', 'pretty', 'smart', 'and', 'i', 'didn', \"'\", 't', 'need', 'to', 'do', 'a', 'lot', 'for', 'good', 'grades', '.', 'it', \"'\", 's', 'changed', ',', 'but', 'my', 'attitude', 'isn', \"'\", 't', '.', 'how', 'to', 'give', 'myself', 'a', 'kick', 'in', 'the', 'butt', '/', 'get', 'tough', '##er', '?']\n",
      "INFO:__main__:Number of tokens: 176\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'need', 'a', 'kick', 'in', 'the', 'butt', ',', 'but', 'how', '?', 'i', 'have', 'too', 'much', 'self', 'pity', '.', 'for', 'example', ',', 'last', 'night', 'i', 'couldn', \"'\", 't', 'fall', 'asleep', 'at', 'all', ',', 'went', 'to', 'bed', 'at', '11', ',', 'finally', 'fell', 'asleep', 'around', '3', '.', 'tomorrow', ',', 'i', 'should', \"'\", 've', 'got', 'out', 'bi', '##j', '9', ',', 'but', 'said', 'to', 'myself', ':', 'ah', 'you', 'couldn', \"'\", 't', 'sleep', 'last', 'night', ',', 'you', 'can', 'sleep', 'as', 'long', 'as', 'you', 'want', '.', 'i', 'slept', 'only', \"'\", 'till', '10', ',', 'but', 'still', '.', 'i', \"'\", 'm', 'a', 'bit', 'spoil', '##t', '(', 'spoiled', '?', ')', ',', 'it', 'isn', \"'\", 't', 'like', 'i', 'got', 'anything', 'i', 'wanted', ',', 'and', 'i', 'had', 'to', 'work', ',', 'but', 'i', 'didn', \"'\", 't', 'have', 'enough', 'kicks', 'in', 'the', 'butt', ',', 'because', 'i', \"'\", 'm', 'pretty', 'smart', 'and', 'i', 'didn', \"'\", 't', 'need', 'to', 'do', 'a', 'lot', 'for', 'good', 'grades', '.', 'it', \"'\", 's', 'changed', ',', 'but', 'my', 'attitude', 'isn', \"'\", 't', '.', 'how', 'to', 'give', 'myself', 'a', 'kick', 'in', 'the', 'butt', '/', 'get', 'tough', '##er', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'is', 'af', '##fra', '##id', 'like', 'me', 'to', 'become', 'mad', '/', 'dumb', 'because', 'of', 'medication', '?']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'is', 'af', '##fra', '##id', 'like', 'me', 'to', 'become', 'mad', '/', 'dumb', 'because', 'of', 'medication', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['psychiatry', \"'\", 's', '\"', 'bible', '\"', 'gets', 'an', 'overhaul', ':', 'scientific', 'american']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['psychiatry', \"'\", 's', '\"', 'bible', '\"', 'gets', 'an', 'overhaul', ':', 'scientific', 'american']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['temperature', 'vs', 'concentration']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['temperature', 'vs', 'concentration']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['damn', 'you', 'do', '-', 'good', '##ers', '.', '.', '.', 'damn', 'you', '!']\n",
      "INFO:__main__:Number of tokens: 12\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['damn', 'you', 'do', '-', 'good', '##ers', '.', '.', '.', 'damn', 'you', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['any', 'time', 'anyone', 'asks', 'me', 'a', 'simple', 'question', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 11\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['any', 'time', 'anyone', 'asks', 'me', 'a', 'simple', 'question', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['motivation', '/', 'support', 'hang', '-', 'out', 'for', 'adults', 'with', 'add', '/', 'ad', '##hd', 'on', 'google', '+', ',', 'thursday', '5', '/', '10', 'at', '9', '##am', 'ps', '##t', '?', 'the', 'idea', 'of', 'a', 'video', 'chat', 'support', 'group', 'got', 'kicked', 'around', 'a', 'month', 'or', 'so', 'ago', '.', 'i', \"'\", 'd', 'really', 'like', 'to', 'plug', 'into', 'a', 'group', ',', 'but', 'all', 'the', 'local', 'options', 'seem', 'to', 'be', 'lecture', 'oriented', 'or', 'cater', 'to', 'parents', '-', 'of', '-', 'kids', '-', 'with', '.', 'are', 'you', 'interested', '?', '5', 'or', '6', 'of', 'us', 'is', 'all', 'it', 'takes', 'to', 'start', 'a', 'good', 'conversation', ',', 'i', 'think', '.', 'are', 'you', 'interested', '?', 'edit', ':', '9', '##am', 'pd', '##t', '!', 'thanks', 'to', 'sc', '##hmi', '##n', 'for', 'the', 'correction']\n",
      "INFO:__main__:Number of tokens: 117\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['motivation', '/', 'support', 'hang', '-', 'out', 'for', 'adults', 'with', 'add', '/', 'ad', '##hd', 'on', 'google', '+', ',', 'thursday', '5', '/', '10', 'at', '9', '##am', 'ps', '##t', '?', 'the', 'idea', 'of', 'a', 'video', 'chat', 'support', 'group', 'got', 'kicked', 'around', 'a', 'month', 'or', 'so', 'ago', '.', 'i', \"'\", 'd', 'really', 'like', 'to', 'plug', 'into', 'a', 'group', ',', 'but', 'all', 'the', 'local', 'options', 'seem', 'to', 'be', 'lecture', 'oriented', 'or', 'cater', 'to', 'parents', '-', 'of', '-', 'kids', '-', 'with', '.', 'are', 'you', 'interested', '?', '5', 'or', '6', 'of', 'us', 'is', 'all', 'it', 'takes', 'to', 'start', 'a', 'good', 'conversation', ',', 'i', 'think', '.', 'are', 'you', 'interested', '?', 'edit', ':', '9', '##am', 'pd', '##t', '!', 'thanks', 'to', 'sc', '##hmi', '##n', 'for', 'the', 'correction']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['fellow', 'add', '##ers', 'of', 'red', '##dit', '.', '.', '.', 'i', \"'\", 'm', 'buying', 'my', 'first', 'smartphone', ',', 'and', 'i', 'need', 'your', 'help', '!', 'officially', 'diagnosed', 'with', 'add', 'from', 'an', 'early', 'age', '.', '.', '.', 'it', \"'\", 's', 'not', 'too', 'bad', ',', 'but', 'i', \"'\", 'm', 'incredibly', 'forget', '##ful', 'and', 'di', '##sor', '##gan', '##ized', '.', 'i', \"'\", 'm', 'getting', 'a', 'smartphone', 'before', 'i', 'head', 'off', 'to', 'college', ',', 'but', 'i', 'would', 'like', 'to', 'pick', 'one', 'that', ',', 'in', 'addition', 'to', 'playing', 'music', ',', 'has', 'good', 'internet', ',', 'and', 'a', 'load', 'of', 'apps', ',', 'etc', '.', 'has', 'a', 'convenient', 'interface', 'full', 'of', 'the', 'following', ':', 'a', 'connection', 'with', 'my', 'google', 'calendar', ',', 'a', 'system', 'of', 'reminder', '##s', ',', 'a', 'system', 'for', 'entering', 'in', 'new', 'er', '##rand', '##s', '/', 'requests', 'i', 'pick', 'up', 'randomly', 'from', 'conversations', ',', 'and', 'any', 'more', 'apps', 'that', 'would', 'otherwise', 'help', 'me', 'to', 'remember', 'my', 'responsibilities', '/', 'allow', 'me', 'to', 'manage', 'my', 'schedule', '.', 'many', 'thanks', 'in', 'advance', '!', 'i', 'apologize', 'if', 'this', 'is', 'difficult', 'to', 'read', '.', '*', '*', 'edit', ':', 'sprint', 'is', 'my', 'service', 'provider', ',', 'sorry', 'to', 'the', 'folks', 'who', 'recommended', 'an', 'iphone', 'already', ':', '/', 'thanks', 'anyway', '!', '*', '*', 'edit', '##2', ':', 'hu', '##ur', '##rr', 'du', '##ur', '##rr', '.', '.', '.', 'did', 'some', 'research', '(', 'as', 'in', ',', 'simple', 'google', 'search', ')', ',', 'found', 'that', 'sprint', 'does', 'offer', 'plans', 'for', 'iphone', '##s', '.', 'i', \"'\", 'll', 'look', 'into', 'it', ',', 'from', 'the', 'sounds', 'of', 'it', 'a', '4', '##s', 'would', 'be', 'perfect', 'for', 'me', '!']\n",
      "INFO:__main__:Number of tokens: 250\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['fellow', 'add', '##ers', 'of', 'red', '##dit', '.', '.', '.', 'i', \"'\", 'm', 'buying', 'my', 'first', 'smartphone', ',', 'and', 'i', 'need', 'your', 'help', '!', 'officially', 'diagnosed', 'with', 'add', 'from', 'an', 'early', 'age', '.', '.', '.', 'it', \"'\", 's', 'not', 'too', 'bad', ',', 'but', 'i', \"'\", 'm', 'incredibly', 'forget', '##ful', 'and', 'di', '##sor', '##gan', '##ized', '.', 'i', \"'\", 'm', 'getting', 'a', 'smartphone', 'before', 'i', 'head', 'off', 'to', 'college', ',', 'but', 'i', 'would', 'like', 'to', 'pick', 'one', 'that', ',', 'in', 'addition', 'to', 'playing', 'music', ',', 'has', 'good', 'internet', ',', 'and', 'a', 'load', 'of', 'apps', ',', 'etc', '.', 'has', 'a', 'convenient', 'interface', 'full', 'of', 'the', 'following', ':', 'a', 'connection', 'with', 'my', 'google', 'calendar', ',', 'a', 'system', 'of', 'reminder', '##s', ',', 'a', 'system', 'for', 'entering', 'in', 'new', 'er', '##rand', '##s', '/', 'requests', 'i', 'pick', 'up', 'randomly', 'from', 'conversations', ',', 'and', 'any', 'more', 'apps', 'that', 'would', 'otherwise', 'help', 'me', 'to', 'remember', 'my', 'responsibilities', '/', 'allow', 'me', 'to', 'manage', 'my', 'schedule', '.', 'many', 'thanks', 'in', 'advance', '!', 'i', 'apologize', 'if', 'this', 'is', 'difficult', 'to', 'read', '.', '*', '*', 'edit', ':', 'sprint', 'is', 'my', 'service', 'provider', ',', 'sorry', 'to', 'the', 'folks', 'who', 'recommended', 'an', 'iphone', 'already', ':', '/', 'thanks', 'anyway', '!', '*', '*', 'edit', '##2', ':', 'hu', '##ur', '##rr', 'du', '##ur', '##rr', '.', '.', '.', 'did', 'some', 'research', '(', 'as', 'in', ',', 'simple', 'google', 'search', ')', ',', 'found', 'that', 'sprint', 'does', 'offer', 'plans', 'for', 'iphone', '##s', '.', 'i', \"'\", 'll', 'look', 'into', 'it', ',', 'from', 'the', 'sounds', 'of', 'it', 'a', '4', '##s', 'would', 'be', 'perfect', 'for', 'me', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', 'ad', '##hd', 'over', 'diagnosed', '?', 'i', 'need', 'a', 'little', 'help', 'writing', 'a', 'research', 'paper', '.', 'i', 'hope', 'this', 'is', 'the', 'right', 'place', 'for', 'this', 'but', 'i', 'am', 'currently', 'writing', 'a', 'research', 'paper', 'about', 'diagnostic', 'process', 'of', 'ad', '##hd', '.', 'i', 'need', 'to', 'present', 'information', 'from', 'scholarly', 'articles', 'supporting', 'the', 'over', 'diagnosis', 'theory', '.', 'does', 'anybody', 'have', 'and', 'articles', 'that', 'i', 'can', 'cite', 'supporting', 'this', 'theory', '?']\n",
      "INFO:__main__:Number of tokens: 67\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', 'ad', '##hd', 'over', 'diagnosed', '?', 'i', 'need', 'a', 'little', 'help', 'writing', 'a', 'research', 'paper', '.', 'i', 'hope', 'this', 'is', 'the', 'right', 'place', 'for', 'this', 'but', 'i', 'am', 'currently', 'writing', 'a', 'research', 'paper', 'about', 'diagnostic', 'process', 'of', 'ad', '##hd', '.', 'i', 'need', 'to', 'present', 'information', 'from', 'scholarly', 'articles', 'supporting', 'the', 'over', 'diagnosis', 'theory', '.', 'does', 'anybody', 'have', 'and', 'articles', 'that', 'i', 'can', 'cite', 'supporting', 'this', 'theory', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['hello', 'all', '!', 'i', 'just', 'found', 'this', 'sub', '##red', '##dit', 'a', 'few', 'minutes', 'ago', ',', 'i', 'am', 'very', 'happy', 'and', 'very', 'relieved', 'that', 'i', 'don', \"'\", 't', 'have', 'to', 'feel', 'alone', 'or', 'as', 'different', 'as', 'everyone', 'makes', 'me', 'as', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'since', 'i', 'was', 'around', '7', '(', 'now', '20', ')', 'its', 'been', 'a', 'constant', 'struggle', 'in', 'school', ',', 'family', 'life', ',', 'and', 'relationships', '.', 'i', 'am', 'very', 'forget', '##ful', ',', 'constantly', 'tired', 'at', 'times', ',', 'and', 'overall', 'have', 'a', 'hard', 'time', 'keeping', 'shit', 'organized', '.', 'i', \"'\", 've', 'been', 'off', 'ri', '##ti', '##lin', 'since', 'i', 'was', '10', 'so', 'i', 'cannot', 'recall', 'my', 'med', '##icated', 'history', '.', 'i', 'have', 'managed', 'this', 'far', 'with', 'people', 'telling', 'me', 'im', 'an', 'idiot', ',', 'your', 'parents', 'didn', \"'\", 't', 'do', 'this', 'or', 'that', 'for', 'or', 'to', 'you', ',', 'its', 'all', 'in', 'your', 'head', ',', 'it', 'doesn', \"'\", 't', 'exist', ',', 'and', 'so', 'on', '.', 'hell', 'the', 'love', 'of', 'my', 'life', 'doesn', \"'\", 't', 'even', 'accept', 'it', ',', 'but', 'i', 'grew', 'to', 'live', 'with', 'it', 'and', 'moved', 'on', '.', 'sometimes', 'i', 'feel', 'like', 'i', 'do', 'have', 'r', '##ls', 'as', 'i', 'constantly', 'shake', 'and', 'tap', 'my', 'legs', '.', 'i', 'see', 'this', 'as', 'a', 'ventilation', 'for', 'excess', 'energy', 'when', 'sitting', '.', 'anyway', '.', '.', '.', 'i', 'am', 'happy', 'to', 'be', 'here', 'and', 'happy', 'to', 'find', 'others', 'that', 'understand', '.']\n",
      "INFO:__main__:Number of tokens: 226\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['hello', 'all', '!', 'i', 'just', 'found', 'this', 'sub', '##red', '##dit', 'a', 'few', 'minutes', 'ago', ',', 'i', 'am', 'very', 'happy', 'and', 'very', 'relieved', 'that', 'i', 'don', \"'\", 't', 'have', 'to', 'feel', 'alone', 'or', 'as', 'different', 'as', 'everyone', 'makes', 'me', 'as', '.', 'i', 'was', 'diagnosed', 'with', 'ad', '##hd', 'since', 'i', 'was', 'around', '7', '(', 'now', '20', ')', 'its', 'been', 'a', 'constant', 'struggle', 'in', 'school', ',', 'family', 'life', ',', 'and', 'relationships', '.', 'i', 'am', 'very', 'forget', '##ful', ',', 'constantly', 'tired', 'at', 'times', ',', 'and', 'overall', 'have', 'a', 'hard', 'time', 'keeping', 'shit', 'organized', '.', 'i', \"'\", 've', 'been', 'off', 'ri', '##ti', '##lin', 'since', 'i', 'was', '10', 'so', 'i', 'cannot', 'recall', 'my', 'med', '##icated', 'history', '.', 'i', 'have', 'managed', 'this', 'far', 'with', 'people', 'telling', 'me', 'im', 'an', 'idiot', ',', 'your', 'parents', 'didn', \"'\", 't', 'do', 'this', 'or', 'that', 'for', 'or', 'to', 'you', ',', 'its', 'all', 'in', 'your', 'head', ',', 'it', 'doesn', \"'\", 't', 'exist', ',', 'and', 'so', 'on', '.', 'hell', 'the', 'love', 'of', 'my', 'life', 'doesn', \"'\", 't', 'even', 'accept', 'it', ',', 'but', 'i', 'grew', 'to', 'live', 'with', 'it', 'and', 'moved', 'on', '.', 'sometimes', 'i', 'feel', 'like', 'i', 'do', 'have', 'r', '##ls', 'as', 'i', 'constantly', 'shake', 'and', 'tap', 'my', 'legs', '.', 'i', 'see', 'this', 'as', 'a', 'ventilation', 'for', 'excess', 'energy', 'when', 'sitting', '.', 'anyway', '.', '.', '.', 'i', 'am', 'happy', 'to', 'be', 'here', 'and', 'happy', 'to', 'find', 'others', 'that', 'understand', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['think', 'transatlantic', 'group', '2', ':', 'prescription', 'drugs', 'in', 'the', 'u', '.', 's', '.']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['think', 'transatlantic', 'group', '2', ':', 'prescription', 'drugs', 'in', 'the', 'u', '.', 's', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', \"'\", 're', 'proud', 'of', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '!', 'remember', 'the', 'good', '!', '[', 'week', '7', ']', '*', '*', 'welcome', 'to', 'the', '7th', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', '*', 'wow', '.', 'wednesday', 'is', 'already', 'here', 'again', '.', 'time', 'flies', 'when', 'you', 'have', 'ad', '##hd', '.', 'lets', 'do', 'this', '!', '*', 'so', 'here', 'is', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', 'if', 'you', 'can', \"'\", 't', 'think', 'of', 'any', '\"', 'wins', '\"', 'you', 'may', 'put', 'something', 'you', 'are', 'grateful', 'for', '.', 'we', 'all', 'can', 'express', 'some', 'gratitude', '.', '*', '*', '*', '*', '*', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', '*', '*', '*', 'some', 'examples', 'from', 'past', 'weeks', '[UNK]', '*', '*', 'started', 'taking', 'ad', '##hd', 'medication', '*', '*', 'x', '##2', '[UNK]', '*', '*', 'called', 'doctor', '*', '*', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '[UNK]', '*', '*', 'working', 'out', '*', '*', 'and', 'eating', 'a', 'healthy', 'diet', '[UNK]', 'got', 'to', 'work', '.', '.', '.', '*', '*', 'on', 'time', '*', '*', '!', '[UNK]', 'went', 'to', '*', '*', 'sleep', 'by', '2a', '##m', '*', '*', 'for', '6', 'nights', '[UNK]', 'finally', 'got', 'a', '*', '*', 'diagnosis', '*', '*', '(', 'a', 'few', 'people', ')', '*', '*', 'very', 'awesome', '*', '*', '[UNK]', 'started', '*', '*', 'working', 'out', '*', '*', 'again', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', '*', '*', '*', 'i', \"'\", 'll', 'start', '*', 'ran', 'a', '5', '##k', 'my', 'first', 'timed', 'race', 'sunday', '.', 'took', 'me', '22', ':', '30', 'and', 'got', '27th', '/', '360', 'and', '4th', 'in', 'my', 'age', 'group', '.', 'i', 'had', 'never', 'even', 'timed', 'myself', 'running', '5', '##k', 'before', 'so', 'i', 'had', 'no', 'idea', 'how', 'to', 'pace', 'myself', '.', '7', ':', '30', 'mile', 'splits', '.', 'yeah', '##hh', '##hh', '!', '*', 'signed', 'up', 'for', 'a', '12', '##k', 'in', 'a', 'month', '.', 'having', 'an', 'event', 'to', 'train', 'for', 'mo', '##tiv', '##ates', 'me', 'to', 'workout', '.', '*', 'went', 'to', 'the', 'bank', 'and', 'cash', '##ed', 'a', 'check', 'that', 'has', 'been', 'sitting', 'around', 'for', 'a', 'week', '.', '*', 'caught', 'myself', 'being', 'negative', 'and', 'self', '-', 'judgment', '##al', 'after', 'spending', 'all', 'day', 'on', 'the', 'computer', '.', 'turned', 'things', 'around', 'with', 'a', 'walk', 'outside', 'and', 'ended', 'up', 'cleaning', 'some', 'of', 'the', 'house', 'that', 'night']\n",
      "INFO:__main__:Number of tokens: 530\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['[', 'win', 'wednesday', ']', '/', 'r', '/', 'ad', '##hd', 'weekly', 'thread', 'where', 'we', \"'\", 're', 'proud', 'of', 'our', \"'\", 'wins', \"'\", 'for', 'the', 'week', '!', 'remember', 'the', 'good', '!', '[', 'week', '7', ']', '*', '*', 'welcome', 'to', 'the', '7th', 'edition', 'of', 'win', 'wednesday', '!', '*', '*', '*', 'wow', '.', 'wednesday', 'is', 'already', 'here', 'again', '.', 'time', 'flies', 'when', 'you', 'have', 'ad', '##hd', '.', 'lets', 'do', 'this', '!', '*', 'so', 'here', 'is', 'your', 'chance', 'to', '*', '*', 'bra', '##g', 'about', 'something', 'small', 'you', 'got', 'done', '.', '*', '*', 'we', 'all', 'had', 'wins', 'both', 'big', 'and', 'small', '.', '*', 'you', 'basically', 'get', 'free', 'comment', 'karma', 'as', 'well', '!', '*', '*', '*', 'if', 'you', 'can', \"'\", 't', 'think', 'of', 'any', '\"', 'wins', '\"', 'you', 'may', 'put', 'something', 'you', 'are', 'grateful', 'for', '.', 'we', 'all', 'can', 'express', 'some', 'gratitude', '.', '*', '*', '*', '*', '*', 'at', 'the', 'beginning', 'of', 'each', 'ad', '##hd', 'support', 'group', ',', 'i', 'like', 'to', 'have', 'everyone', 'share', 'their', '\"', 'win', '\"', 'for', 'the', 'month', '.', 'what', 'surprised', 'me', 'was', '*', '*', 'most', 'people', 'couldn', \"'\", 't', 'even', 'come', 'up', 'with', 'one', 'positive', 'thing', 'they', 'accomplished', 'the', 'past', '30', 'days', '*', '*', '!', '*', 'if', 'i', 'asked', 'for', 'the', 'bad', 'things', 'that', 'happened', 'each', 'person', 'probably', 'could', 'name', '20', 'before', 'pausing', '.', '.', '.', '*', '*', '*', '*', 'some', 'examples', 'from', 'past', 'weeks', '[UNK]', '*', '*', 'started', 'taking', 'ad', '##hd', 'medication', '*', '*', 'x', '##2', '[UNK]', '*', '*', 'called', 'doctor', '*', '*', 'to', 'set', 'up', 'an', 'appointment', 'x', '##4', '[UNK]', '*', '*', 'working', 'out', '*', '*', 'and', 'eating', 'a', 'healthy', 'diet', '[UNK]', 'got', 'to', 'work', '.', '.', '.', '*', '*', 'on', 'time', '*', '*', '!', '[UNK]', 'went', 'to', '*', '*', 'sleep', 'by', '2a', '##m', '*', '*', 'for', '6', 'nights', '[UNK]', 'finally', 'got', 'a', '*', '*', 'diagnosis', '*', '*', '(', 'a', 'few', 'people', ')', '*', '*', 'very', 'awesome', '*', '*', '[UNK]', 'started', '*', '*', 'working', 'out', '*', '*', 'again', '*', '*', '*', 'by', 'sharing', 'our', 'wins', 'every', 'wednesday', '.', '.', '.', '*', '*', 'i', 'hope', 'you', 'will', 'start', 'to', 'be', 'on', 'the', 'lookout', 'for', 'the', 'positive', 'things', 'you', 'do', 'each', 'week', '.', 'you', 'will', 'start', 'to', 'gain', 'awareness', 'and', 'see', 'that', 'everything', 'isn', \"'\", 't', 'all', 'bad', '!', '*', '*', '*', 'this', 'might', 'even', 'become', 'a', 'routine', 'as', 'you', 'excitedly', 'log', 'on', 'every', 'wednesday', 'night', 'to', 'share', '.', '*', '*', '*', '*', 'i', \"'\", 'll', 'start', '*', 'ran', 'a', '5', '##k', 'my', 'first', 'timed', 'race', 'sunday', '.', 'took', 'me', '22', ':', '30', 'and', 'got', '27th', '/', '360', 'and', '4th', 'in', 'my', 'age', 'group', '.', 'i', 'had', 'never', 'even', 'timed', 'myself', 'running', '5', '##k', 'before', 'so', 'i', 'had', 'no', 'idea', 'how', 'to', 'pace', 'myself', '.', '7', ':', '30', 'mile', 'splits', '.', 'yeah', '##hh', '##hh', '!', '*', 'signed', 'up', 'for', 'a', '12', '##k', 'in', 'a', 'month', '.', 'having', 'an', 'event', 'to', 'train', 'for', 'mo', '##tiv', '##ates', 'me', 'to', 'workout', '.', '*', 'went', 'to', 'the', 'bank', 'and', 'cash', '##ed', 'a', 'check', 'that', 'has', 'been', 'sitting', 'around', 'for', 'a', 'week', '.', '*', 'caught', 'myself', 'being', 'negative', 'and', 'self', '-', 'judgment', '##al', 'after', 'spending', 'all', 'day', 'on', 'the', 'computer'], ['.', 'turned', 'things', 'around', 'with', 'a', 'walk', 'outside', 'and', 'ended', 'up', 'cleaning', 'some', 'of', 'the', 'house', 'that', 'night']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['the', 'energy', 'crashes', 'in', 'the', 'middle', 'of', 'conversations']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['the', 'energy', 'crashes', 'in', 'the', 'middle', 'of', 'conversations']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['was', 'prescribed', '20', '##mg', '/', 'day', 'add', '##eral', '##l', '.', 'doc', 'wants', 'to', 'switch', 'me', 'to', '60', '##mg', '/', 'day', 'v', '##y', '##van', '##se', '.', 'is', 'this', 'too', 'high', 'of', 'a', 'dose', '?', 'i', \"'\", 've', 'been', 'taking', '10', '##mg', 'add', '##eral', '##l', 'ir', 'twice', 'a', 'day', 'for', 'the', 'last', 'month', 'or', 'so', '.', 'today', ',', 'i', 'went', 'to', 'my', 'doctor', 'and', 'told', 'him', 'that', 'i', 'didn', \"'\", 't', 'think', 'the', 'dose', 'was', 'high', 'enough', '(', 'which', 'i', 'think', 'is', 'true', ')', '.', 'he', 'then', 'gave', 'me', 'a', 'prescription', 'for', '60', '##mg', 'v', '##y', '##van', '##se', '.', 'does', 'that', 'seem', 'too', 'much', '?', 'i', 'know', 'the', 'formulation', '##s', 'are', 'different', 'but', 'going', 'from', '10', '>', '60', 'seems', 'like', 'a', 'bit', 'much', '?']\n",
      "INFO:__main__:Number of tokens: 122\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['was', 'prescribed', '20', '##mg', '/', 'day', 'add', '##eral', '##l', '.', 'doc', 'wants', 'to', 'switch', 'me', 'to', '60', '##mg', '/', 'day', 'v', '##y', '##van', '##se', '.', 'is', 'this', 'too', 'high', 'of', 'a', 'dose', '?', 'i', \"'\", 've', 'been', 'taking', '10', '##mg', 'add', '##eral', '##l', 'ir', 'twice', 'a', 'day', 'for', 'the', 'last', 'month', 'or', 'so', '.', 'today', ',', 'i', 'went', 'to', 'my', 'doctor', 'and', 'told', 'him', 'that', 'i', 'didn', \"'\", 't', 'think', 'the', 'dose', 'was', 'high', 'enough', '(', 'which', 'i', 'think', 'is', 'true', ')', '.', 'he', 'then', 'gave', 'me', 'a', 'prescription', 'for', '60', '##mg', 'v', '##y', '##van', '##se', '.', 'does', 'that', 'seem', 'too', 'much', '?', 'i', 'know', 'the', 'formulation', '##s', 'are', 'different', 'but', 'going', 'from', '10', '>', '60', 'seems', 'like', 'a', 'bit', 'much', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['question', 'about', 'ref', '##ill', '##ing', 'prescription', 'after', 'long', 'hiatus', '.']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['question', 'about', 'ref', '##ill', '##ing', 'prescription', 'after', 'long', 'hiatus', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['does', 'anyone', 'want', 'to', 'share', 'a', 'routine', 'that', 'helps', 'keep', 'you', 'focused', 'and', 'productive', '?', 'from', 'what', 'i', \"'\", 've', 'seen', 'here', '(', 'and', 'my', 'own', 'personal', 'experience', ')', ',', 'sticking', 'with', 'a', 'routine', 'is', 'one', 'of', 'the', 'harder', 'things', 'for', 'people', 'with', 'ad', '##hd', ',', 'since', 'most', 'of', 'the', 'benefits', 'are', 'in', 'the', 'long', '-', 'term', 'rather', 'than', 'instant', 'gr', '##ati', '##fication', '.', 'there', 'have', 'been', 'some', 'scattered', 'posts', 'saying', 'things', 'like', 'meditation', 'or', 'morning', 'exercise', 'help', ',', 'but', 'does', 'anyone', 'want', 'to', 'detail', 'some', 'of', 'the', 'daily', 'routines', 'that', 'have', 'been', 'really', 'helpful', '?', 'it', 'could', 'be', 'organizational', 'systems', ',', 'morning', 'rituals', ',', 'study', 'habits', '.', '.', '.', 'anything', 'you', 'trained', 'yourself', 'to', 'do', 'that', 'keeps', 'you', 'disciplined', 'and', 'productive', '.']\n",
      "INFO:__main__:Number of tokens: 123\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['does', 'anyone', 'want', 'to', 'share', 'a', 'routine', 'that', 'helps', 'keep', 'you', 'focused', 'and', 'productive', '?', 'from', 'what', 'i', \"'\", 've', 'seen', 'here', '(', 'and', 'my', 'own', 'personal', 'experience', ')', ',', 'sticking', 'with', 'a', 'routine', 'is', 'one', 'of', 'the', 'harder', 'things', 'for', 'people', 'with', 'ad', '##hd', ',', 'since', 'most', 'of', 'the', 'benefits', 'are', 'in', 'the', 'long', '-', 'term', 'rather', 'than', 'instant', 'gr', '##ati', '##fication', '.', 'there', 'have', 'been', 'some', 'scattered', 'posts', 'saying', 'things', 'like', 'meditation', 'or', 'morning', 'exercise', 'help', ',', 'but', 'does', 'anyone', 'want', 'to', 'detail', 'some', 'of', 'the', 'daily', 'routines', 'that', 'have', 'been', 'really', 'helpful', '?', 'it', 'could', 'be', 'organizational', 'systems', ',', 'morning', 'rituals', ',', 'study', 'habits', '.', '.', '.', 'anything', 'you', 'trained', 'yourself', 'to', 'do', 'that', 'keeps', 'you', 'disciplined', 'and', 'productive', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'have', 'stopped', 'taking', 'add', '##eral', '##l', 'cold', 'turkey', 'due', 'to', 'pregnancy', ',', 'anyone', 'have', 'ideas', 'on', 'how', 'to', 'cope', '?']\n",
      "INFO:__main__:Number of tokens: 21\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'have', 'stopped', 'taking', 'add', '##eral', '##l', 'cold', 'turkey', 'due', 'to', 'pregnancy', ',', 'anyone', 'have', 'ideas', 'on', 'how', 'to', 'cope', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['new', 'to', 'canada', '.', '.', '.', 'how', 'do', 'i', 'go', 'about', 'acquiring', 'ad', '##hd', 'med', '##s', 'here', '?', 'diagnosed', 'with', 'add', '25', 'y', '##rs', 'ago', '(', 'as', 'a', '2nd', 'grade', '##r', ')', '.', 'i', 'think', 'i', 'have', 'ad', '##hd', 'but', 'i', 'don', \"'\", 't', 'think', 'that', 'designation', 'existed', 'back', 'then', '.', 'anyway', ',', 'been', 'off', 'med', '##s', 'since', 'high', 'school', '.', 'pride', '/', 'stubborn', '##ness', 'and', 'the', 'rita', '##lin', 'turning', 'me', 'into', 'a', 'zombie', 'was', 'the', 'main', 'reason', 'for', 'going', 'off', 'them', '.', 'i', 'have', 'moved', 'to', 'canada', 'and', 'have', 'access', 'to', 'all', 'the', 'canadian', 'health', 'care', '.', 'add', '/', 'ad', '##hd', 'med', '##s', 'have', 'come', 'a', 'long', 'way', 'since', 'my', 'high', 'school', 'days', 'so', 'i', \"'\", 'd', 'like', 'to', 'give', 'them', 'a', 'try', 'as', 'there', 'are', 'some', 'days', 'when', 'i', 'really', 'struggle', 'with', 'my', 'symptoms', '.', 'where', 'do', 'i', 'start', 'in', 'trying', 'to', 'acquire', 'a', 'red', '##ia', '##gno', '##sis', '/', 'prescription', 'here', 'in', 'canada', '?', 'a', 'walking', 'clinic', '?', 'the', 'family', 'doc', '?', 'a', 'mental', 'health', 'doc', 'of', 'some', 'sort', '?']\n",
      "INFO:__main__:Number of tokens: 172\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['new', 'to', 'canada', '.', '.', '.', 'how', 'do', 'i', 'go', 'about', 'acquiring', 'ad', '##hd', 'med', '##s', 'here', '?', 'diagnosed', 'with', 'add', '25', 'y', '##rs', 'ago', '(', 'as', 'a', '2nd', 'grade', '##r', ')', '.', 'i', 'think', 'i', 'have', 'ad', '##hd', 'but', 'i', 'don', \"'\", 't', 'think', 'that', 'designation', 'existed', 'back', 'then', '.', 'anyway', ',', 'been', 'off', 'med', '##s', 'since', 'high', 'school', '.', 'pride', '/', 'stubborn', '##ness', 'and', 'the', 'rita', '##lin', 'turning', 'me', 'into', 'a', 'zombie', 'was', 'the', 'main', 'reason', 'for', 'going', 'off', 'them', '.', 'i', 'have', 'moved', 'to', 'canada', 'and', 'have', 'access', 'to', 'all', 'the', 'canadian', 'health', 'care', '.', 'add', '/', 'ad', '##hd', 'med', '##s', 'have', 'come', 'a', 'long', 'way', 'since', 'my', 'high', 'school', 'days', 'so', 'i', \"'\", 'd', 'like', 'to', 'give', 'them', 'a', 'try', 'as', 'there', 'are', 'some', 'days', 'when', 'i', 'really', 'struggle', 'with', 'my', 'symptoms', '.', 'where', 'do', 'i', 'start', 'in', 'trying', 'to', 'acquire', 'a', 'red', '##ia', '##gno', '##sis', '/', 'prescription', 'here', 'in', 'canada', '?', 'a', 'walking', 'clinic', '?', 'the', 'family', 'doc', '?', 'a', 'mental', 'health', 'doc', 'of', 'some', 'sort', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['it', \"'\", 's', '0', ':', '01', ',', 'assignment', 'due', 'at', '14', ':', '00', '.', 'why', ',', 'oh', 'why', ',', 'oh', 'why', '!']\n",
      "INFO:__main__:Number of tokens: 22\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['it', \"'\", 's', '0', ':', '01', ',', 'assignment', 'due', 'at', '14', ':', '00', '.', 'why', ',', 'oh', 'why', ',', 'oh', 'why', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ad', '##hd', 'new', '##b', 'intro', '+', 'anyone', 'with', 'fast', 'metabolism', 'have', 'trouble', 'with', 'ad', '##hd', 'medication', 'being', 'effective', '?', 'i', \"'\", 'm', 'an', 'ad', '##hd', '*', '*', 'new', '##b', '.', '*', '*', 'i', 'always', 'thought', 'of', 'ad', '##hd', 'as', 'something', 'over', 'prescribed', 'and', 'had', 'a', 'jade', '##d', 'view', 'towards', 'it', '.', 'reading', 'up', 'on', 'it', 'last', 'summer', 'though', 'i', 'actually', 'began', 'to', 'change', 'my', 'mind', 'and', 'open', 'up', 'to', 'the', 'idea', '.', 'i', 'brought', 'it', 'up', 'to', 'my', 'family', 'doctor', 'and', 'he', 'believed', 'that', 'my', 'schools', 'mental', 'health', 'center', 'would', 'have', 'the', 'people', 'i', 'needed', 'to', 'talk', 'to', '.', 'so', 'i', 'went', 'in', ',', 'had', 'an', 'initial', 'consultation', 'with', 'a', 'ps', '##ych', 'student', ',', 'she', 'gave', 'me', 'an', 'initial', 'pro', '##gno', '##sis', 'and', 'referred', 'me', 'to', 'the', 'medical', 'clinic', 'to', 'see', 'a', 'doctor', 'who', 'could', 'refer', 'me', 'to', 'a', 'psychiatrist', '.', 'i', 'met', 'with', 'the', 'doctor', 'at', 'school', ',', 'and', 'she', 'gave', 'me', 'the', 'option', 'of', 'doing', 'some', 'trials', 'with', 'medication', ',', 'seeing', 'a', 'psychiatrist', 'or', 'both', '.', 'i', 'went', 'with', 'both', '.', 'my', 'm', '.', 'd', 'prescribed', '*', '*', '36', '##mg', 'of', 'concert', '##a', '*', '*', 'for', '14', 'days', '.', 'after', '10', 'days', ',', 'i', 'can', 'say', 'i', \"'\", 've', 'felt', 'z', '##il', '##ch', '.', 'i', 'do', 'have', 'a', 'hyper', 'speed', 'metabolism', 'though', 'and', 'in', 'the', 'past', 'medication', 'usually', 'does', 'little', 'for', 'me', 'at', 'the', 'prescribed', 'doses', '.', 'in', 'general', ',', 'i', 'find', 'myself', 'needing', 'to', 'take', 'at', 'least', '2', 'ty', '##len', '##ols', ',', 'ad', '##vil', \"'\", 's', 'etc', '.', 'just', 'to', 'feel', 'the', 'effect', '.', 'but', 'i', 'don', '’', 't', 'have', 'much', 'ad', '##hd', 'medication', 'experience', ',', 'so', 'i', \"'\", 'm', 'thinking', 'maybe', 'a', 'different', 'brand', '/', 'dos', '##age', 'might', 'be', 'better', 'than', 'attempting', 'to', 'self', 'dia', '##gno', '##se', 'and', 'double', 'up', '.', 'another', 'thing', 'is', 'i', 'have', 'yet', 'to', 'meet', 'with', 'the', 'psychiatrist', ',', 'but', 'now', 'i', 'am', 'having', 'second', 'thoughts', '.', 'i', 'don', '’', 't', 'know', 'if', 'it', 'would', 'be', 'helpful', 'to', 'actually', 'get', 'the', 'diagnosis', ',', 'i', \"'\", 'd', 'rather', 'manage', 'it', 'without', 'being', 'on', 'my', 'record', 'in', 'case', 'i', 'ever', 'want', 'to', 'apply', 'to', 'some', 'military', 'careers', 'that', 'might', 'be', 'hesitant', 'about', 'me', 'because', 'of', 'ad', '##hd', '.', 'sorry', 'that', 'may', 'have', 'been', 'unnecessary', 'but', 'i', 'am', 'pretty', 'new', 'to', 'the', 'whole', 'idea', 'of', 'accepting', 'i', 'have', 'ad', '##hd', 'and', 'i', \"'\", 'm', 'trying', 'to', 'figure', 'it', 'out', '.', 't', '##l', ':', 'dr', 'see', 'a', 'psychiatrist', 'and', 'get', 'the', 'ad', '##hd', 'diagnosis', 'or', 'go', 'off', 'the', 'judgement', 'of', 'my', 'doctor', '+', 'initial', 'psychiatry', 'ph', '.', 'd', '.', 'student', 'pro', '##gno', '##sis', 'and', 'try', 'med', '##ica', '##ting', 'and', 'prevent', 'having', 'ad', '##hd', 'labeled', 'on', 'my', 'health', 'record', '(', 'for', 'work', ',', 'military', 'purposes', ')', '?', 'anyone', 'with', 'fast', 'metabolism', 'have', 'trouble', 'with', 'med', '##s', 'working', '?', 'bt', '##w', 'the', 'pdf', 'in', 'the', 'fa', '##q', 'from', 'consumer', '##hea', '##lth', '##re', '##ports', 'on', 'how', 'much', 'medication', 'should', 'cost', 'is', 'broken']\n",
      "INFO:__main__:Number of tokens: 483\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ad', '##hd', 'new', '##b', 'intro', '+', 'anyone', 'with', 'fast', 'metabolism', 'have', 'trouble', 'with', 'ad', '##hd', 'medication', 'being', 'effective', '?', 'i', \"'\", 'm', 'an', 'ad', '##hd', '*', '*', 'new', '##b', '.', '*', '*', 'i', 'always', 'thought', 'of', 'ad', '##hd', 'as', 'something', 'over', 'prescribed', 'and', 'had', 'a', 'jade', '##d', 'view', 'towards', 'it', '.', 'reading', 'up', 'on', 'it', 'last', 'summer', 'though', 'i', 'actually', 'began', 'to', 'change', 'my', 'mind', 'and', 'open', 'up', 'to', 'the', 'idea', '.', 'i', 'brought', 'it', 'up', 'to', 'my', 'family', 'doctor', 'and', 'he', 'believed', 'that', 'my', 'schools', 'mental', 'health', 'center', 'would', 'have', 'the', 'people', 'i', 'needed', 'to', 'talk', 'to', '.', 'so', 'i', 'went', 'in', ',', 'had', 'an', 'initial', 'consultation', 'with', 'a', 'ps', '##ych', 'student', ',', 'she', 'gave', 'me', 'an', 'initial', 'pro', '##gno', '##sis', 'and', 'referred', 'me', 'to', 'the', 'medical', 'clinic', 'to', 'see', 'a', 'doctor', 'who', 'could', 'refer', 'me', 'to', 'a', 'psychiatrist', '.', 'i', 'met', 'with', 'the', 'doctor', 'at', 'school', ',', 'and', 'she', 'gave', 'me', 'the', 'option', 'of', 'doing', 'some', 'trials', 'with', 'medication', ',', 'seeing', 'a', 'psychiatrist', 'or', 'both', '.', 'i', 'went', 'with', 'both', '.', 'my', 'm', '.', 'd', 'prescribed', '*', '*', '36', '##mg', 'of', 'concert', '##a', '*', '*', 'for', '14', 'days', '.', 'after', '10', 'days', ',', 'i', 'can', 'say', 'i', \"'\", 've', 'felt', 'z', '##il', '##ch', '.', 'i', 'do', 'have', 'a', 'hyper', 'speed', 'metabolism', 'though', 'and', 'in', 'the', 'past', 'medication', 'usually', 'does', 'little', 'for', 'me', 'at', 'the', 'prescribed', 'doses', '.', 'in', 'general', ',', 'i', 'find', 'myself', 'needing', 'to', 'take', 'at', 'least', '2', 'ty', '##len', '##ols', ',', 'ad', '##vil', \"'\", 's', 'etc', '.', 'just', 'to', 'feel', 'the', 'effect', '.', 'but', 'i', 'don', '’', 't', 'have', 'much', 'ad', '##hd', 'medication', 'experience', ',', 'so', 'i', \"'\", 'm', 'thinking', 'maybe', 'a', 'different', 'brand', '/', 'dos', '##age', 'might', 'be', 'better', 'than', 'attempting', 'to', 'self', 'dia', '##gno', '##se', 'and', 'double', 'up', '.', 'another', 'thing', 'is', 'i', 'have', 'yet', 'to', 'meet', 'with', 'the', 'psychiatrist', ',', 'but', 'now', 'i', 'am', 'having', 'second', 'thoughts', '.', 'i', 'don', '’', 't', 'know', 'if', 'it', 'would', 'be', 'helpful', 'to', 'actually', 'get', 'the', 'diagnosis', ',', 'i', \"'\", 'd', 'rather', 'manage', 'it', 'without', 'being', 'on', 'my', 'record', 'in', 'case', 'i', 'ever', 'want', 'to', 'apply', 'to', 'some', 'military', 'careers', 'that', 'might', 'be', 'hesitant', 'about', 'me', 'because', 'of', 'ad', '##hd', '.', 'sorry', 'that', 'may', 'have', 'been', 'unnecessary', 'but', 'i', 'am', 'pretty', 'new', 'to', 'the', 'whole', 'idea', 'of', 'accepting', 'i', 'have', 'ad', '##hd', 'and', 'i', \"'\", 'm', 'trying', 'to', 'figure', 'it', 'out', '.', 't', '##l', ':', 'dr', 'see', 'a', 'psychiatrist', 'and', 'get', 'the', 'ad', '##hd', 'diagnosis', 'or', 'go', 'off', 'the', 'judgement', 'of', 'my', 'doctor', '+', 'initial', 'psychiatry', 'ph', '.', 'd', '.', 'student', 'pro', '##gno', '##sis', 'and', 'try', 'med', '##ica', '##ting', 'and', 'prevent', 'having', 'ad', '##hd', 'labeled', 'on', 'my', 'health', 'record', '(', 'for', 'work', ',', 'military', 'purposes', ')', '?', 'anyone', 'with', 'fast', 'metabolism', 'have', 'trouble', 'with', 'med', '##s', 'working', '?', 'bt', '##w', 'the', 'pdf', 'in', 'the', 'fa', '##q', 'from', 'consumer', '##hea', '##lth', '##re', '##ports', 'on', 'how', 'much', 'medication', 'should', 'cost', 'is', 'broken']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['let', \"'\", 's', 'raise', 'kids', 'to', 'be', 'entrepreneurs']\n",
      "INFO:__main__:Number of tokens: 8\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['let', \"'\", 's', 'raise', 'kids', 'to', 'be', 'entrepreneurs']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['here', '’', 's', 'what', 'it', 'feels', 'like', 'as', 'i', '’', 'm', 'typing', 'this', '.', 'it', 'feels', 'like', 'i', 'cannot', 'get', 'the', 'words', 'just', 'right', ',', 'and', 'i', 'immediately', 'want', 'to', 'give', 'up', '.', 'it', 'feels', 'like', 'i', 'have', 'nothing', 'that', 'is', 'really', 'worth', 'saying', ',', 'and', 'i', 'should', 'stop', '.', 'i', 'feel', 'the', 'odds', 'of', 'me', 'actually', 'finishing', 'and', 'posting', 'this', 'are', 'very', 'low', '.', 'the', 'more', 'sentences', 'i', 'write', ',', 'the', 'slower', 'my', 'momentum', '.', 'i', 'am', 'practically', 'at', 'a', 'stands', '##till', 'now', '.', 'there', '’', 's', 'so', 'much', 'going', 'on', 'inside', 'my', 'head', ',', 'but', 'i', 'just', 'can', '’', 't', 'channel', 'it', '.', 'it', 'is', 'so', 'frustrating', '.', 'does', 'everyone', 'go', 'through', 'this', 'struggle', 'or', 'and', 'i', 'am', 'just', 'being', 'mel', '##od', '##rama', '##tic', '?', 'self', '-', 'pity', '##ing', '?', 'lazy', '?', 'i', '’', 'm', 'pretty', 'sure', 'that', '’', 's', 'what', 'other', 'people', 'think', 'of', 'me', '.', 'are', 'they', 'right', '?', 'do', 'i', 'just', 'need', 'to', 'step', 'up', 'on', 'my', 'own', '?', 'i', 'want', 'a', 'clean', 'room', '.', 'i', 'want', 'to', 'not', 'mis', '##pl', '##ace', 'my', 'keys', ',', 'wallet', 'or', 'cell', '##phone', 'on', 'a', 'close', '-', 'to', '-', 'daily', 'basis', '.', 'i', 'want', 'to', 'be', 'able', 'to', 'follow', 'the', 'plot', 'of', 'a', 'james', 'bond', 'movie', ',', 'or', 'at', 'the', 'very', 'least', ',', 'a', 'conversation', 'my', 'friend', 'is', 'trying', 'to', 'have', 'with', 'me', '.', 'i', 'want', 'to', 'be', 'decisive', 'and', 'confident', 'and', 'accomplished', '.', 'i', 'don', '’', 't', 'know', 'if', 'i', 'have', 'ad', '##hd', 'or', 'some', 'other', 'thing', '.', 'i', '’', 've', 'never', 'taken', 'med', '##s', 'before', '.', 'i', 'am', 'tre', '##pid', 'to', 'start', '.', 'but', 'all', 'these', 'days', 'that', 'i', 'waste', 'just', 'because', 'i', 'can', '’', 't', 'bring', 'myself', 'to', 'start', 'anything', '—', 'that', '’', 's', 'what', 'really', 'scares', 'me', '.', 'thanks', 'for', 'listening', '.']\n",
      "INFO:__main__:Number of tokens: 297\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['here', '’', 's', 'what', 'it', 'feels', 'like', 'as', 'i', '’', 'm', 'typing', 'this', '.', 'it', 'feels', 'like', 'i', 'cannot', 'get', 'the', 'words', 'just', 'right', ',', 'and', 'i', 'immediately', 'want', 'to', 'give', 'up', '.', 'it', 'feels', 'like', 'i', 'have', 'nothing', 'that', 'is', 'really', 'worth', 'saying', ',', 'and', 'i', 'should', 'stop', '.', 'i', 'feel', 'the', 'odds', 'of', 'me', 'actually', 'finishing', 'and', 'posting', 'this', 'are', 'very', 'low', '.', 'the', 'more', 'sentences', 'i', 'write', ',', 'the', 'slower', 'my', 'momentum', '.', 'i', 'am', 'practically', 'at', 'a', 'stands', '##till', 'now', '.', 'there', '’', 's', 'so', 'much', 'going', 'on', 'inside', 'my', 'head', ',', 'but', 'i', 'just', 'can', '’', 't', 'channel', 'it', '.', 'it', 'is', 'so', 'frustrating', '.', 'does', 'everyone', 'go', 'through', 'this', 'struggle', 'or', 'and', 'i', 'am', 'just', 'being', 'mel', '##od', '##rama', '##tic', '?', 'self', '-', 'pity', '##ing', '?', 'lazy', '?', 'i', '’', 'm', 'pretty', 'sure', 'that', '’', 's', 'what', 'other', 'people', 'think', 'of', 'me', '.', 'are', 'they', 'right', '?', 'do', 'i', 'just', 'need', 'to', 'step', 'up', 'on', 'my', 'own', '?', 'i', 'want', 'a', 'clean', 'room', '.', 'i', 'want', 'to', 'not', 'mis', '##pl', '##ace', 'my', 'keys', ',', 'wallet', 'or', 'cell', '##phone', 'on', 'a', 'close', '-', 'to', '-', 'daily', 'basis', '.', 'i', 'want', 'to', 'be', 'able', 'to', 'follow', 'the', 'plot', 'of', 'a', 'james', 'bond', 'movie', ',', 'or', 'at', 'the', 'very', 'least', ',', 'a', 'conversation', 'my', 'friend', 'is', 'trying', 'to', 'have', 'with', 'me', '.', 'i', 'want', 'to', 'be', 'decisive', 'and', 'confident', 'and', 'accomplished', '.', 'i', 'don', '’', 't', 'know', 'if', 'i', 'have', 'ad', '##hd', 'or', 'some', 'other', 'thing', '.', 'i', '’', 've', 'never', 'taken', 'med', '##s', 'before', '.', 'i', 'am', 'tre', '##pid', 'to', 'start', '.', 'but', 'all', 'these', 'days', 'that', 'i', 'waste', 'just', 'because', 'i', 'can', '’', 't', 'bring', 'myself', 'to', 'start', 'anything', '—', 'that', '’', 's', 'what', 'really', 'scares', 'me', '.', 'thanks', 'for', 'listening', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['getting', 'diagnosed', 'was', 'a', 'long', 'and', 'ar', '##du', '##ous', 'process', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 13\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['getting', 'diagnosed', 'was', 'a', 'long', 'and', 'ar', '##du', '##ous', 'process', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['nothing', 'can', 'compete', 'with', 'the', 'pain', 'i', 'have', 'add', '.', 'i', 'am', 'broken', '.', 'my', 'life', 'is', 'advancing', 'as', 'it', 'falls', 'apart', 'under', 'my', 'feet', '.', 'every', 'thought', 'takes', 'effort', '.', 'every', 'accomplishment', 'starts', 'by', 'feeling', 'like', 'i', \"'\", 'm', 'climbing', 'a', 'mountain', '.', 'there', 'are', 'good', 'hours', 'and', 'good', 'days', ',', 'and', 'twice', 'a', 'year', 'i', 'have', 'a', 'good', 'week', ',', 'but', 'the', 'victories', 'are', 'transient', '.', 'the', 'pain', 'is', 'always', 'there', '.', 'it', 'is', 'playing', 'the', 'long', 'game', '.', 'i', 'am', 'losing', '.', 'even', 'the', 'good', 'days', 'are', 'full', 'of', 'dread', ',', 'knowing', 'that', 'eventually', 'the', 'pain', 'will', 'return', ',', 'and', 'i', 'will', 'fail', '.', 'i', 'just', 'let', 'things', 'go', '.', 'i', 'know', 'i', 'need', 'to', 'do', 'things', ',', 'but', 'the', 'pain', 'of', 'starting', 'is', 'so', 'present', ',', 'so', 'intense', ',', 'it', 'makes', 'so', 'much', 'sense', 'to', 'let', 'it', 'go', '.', 'the', 'smallest', 'tasks', 'are', 'the', 'hardest', '.', 'the', 'reward', 'is', 'so', 'small', ',', 'but', 'the', 'pain', 'never', 'changes', 'size', '.', 'if', 'people', 'knew', 'what', 'was', 'in', 'my', 'head', ',', 'they', 'would', 'never', 'trust', 'me', 'with', 'anything', 'ever', 'again', ',', 'but', 'at', 'least', 'they', 'would', 'understand', '.', 'they', 'don', \"'\", 't', 'understand', '.', 'they', 'see', 'flashes', 'of', 'great', '##ness', 'on', 'the', 'good', 'days', ',', 'and', 'they', 'imagine', 'i', 'can', 'succeed', '.', 'they', 'don', \"'\", 't', 'understand', '.', 'how', 'could', 'they', 'when', 'i', 'never', 'have', '.', 'all', 'i', 'know', 'is', 'failure', 'is', 'inevitable', '.', 'the', 'earlier', 'people', 'knew', 'me', ',', 'the', 'higher', 'their', 'opinion', 'of', 'me', 'is', '.', 'that', 'can', \"'\", 't', 'be', 'a', 'good', 'sign', '.', 'the', 'pain', 'makes', 'me', 'cut', 'corners', ',', 'it', 'makes', 'me', 'make', 'mistakes', ',', 'and', 'my', 'mistakes', 'are', 'chasing', 'me', '.', 'eventually', 'they', 'always', 'catch', 'me', '.', 'sometimes', 'my', 'mind', 'fills', 'with', 'dread', 'for', '15', 'minutes', 'before', 'i', 'even', 'realize', 'what', 'is', 'happening', '.', 'i', 'am', 'anxious', '.', 'i', 'am', 'depressed', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'came', 'first', ',', 'and', 'at', 'this', 'point', 'it', 'doesn', \"'\", 't', 'matter', '.', 'i', 'take', 'a', 'powerful', 'st', '##im', '##ula', '##nt', 'every', 'morning', 'and', 'spend', 'every', 'day', 'exhausted', 'by', 'fear', '.', 'i', 'was', 'almost', 'fired', 'from', 'my', 'first', 'job', '.', 'that', 'finally', 'convinced', 'me', 'to', 'see', 'a', 'doctor', ',', 'to', 'get', 'med', '##icated', '.', 'everything', 'turned', 'around', 'for', 'me', ',', 'i', 'became', 'a', 'star', '.', 'now', 'it', \"'\", 's', '6', 'years', 'later', 'and', 'the', 'cycle', 'is', 'repeated', 'itself', '.', 'there', \"'\", 's', 'no', 'magic', 'bullet', 'this', 'time', '.', 'you', 'can', 'only', 'fail', 'so', 'many', 'times', 'before', 'people', 'give', 'up', 'on', 'you', '.', 'they', \"'\", 've', 'only', 'waited', 'this', 'long', 'because', 'they', 'don', \"'\", 't', 'understand', '.', 'i', 'desperately', 'want', 'to', 'accomplish', 'things', '.', 'they', 'just', 'makes', 'it', 'worse', '.', 'i', 'can', 'see', 'victory', 'dangling', 'in', 'front', 'of', 'my', 'face', ',', 'and', 'it', \"'\", 's', 'close', 'enough', 'to', 'touch', '.', 'it', \"'\", 's', 'taunting', 'me', '.', 'i', \"'\", 'm', 'too', 'old', 'for', 'potential', '.', 'i', \"'\", 'll', 'be', '30', 'this', 'year', '.', 'imagining', 'that', 'i', \"'\", 'll', 'figure', 'it', 'out', 'someday', '.', '.', '.', '.', 'i', 'don', \"'\", 't', 'even', 'ind', '##ul', '##ge', 'the', 'fantasy', 'anymore', '.', 'i', 'have', 'suicidal', 'thoughts', '.', 'they', 'aren', \"'\", 't', 'real', '.', 'it', 'just', 'seems', 'so', 'final', '.', 'i', \"'\", 'm', 'a', 'competitive', 'person', 'who', 'stills', 'harbor', '##s', 'the', 'del', '##usion', 'that', 'i', 'can', 'fix', 'myself', '.', 'i', \"'\", 'm', 'not', 'ready', 'to', 'admit', 'defeat', '.']\n",
      "INFO:__main__:Number of tokens: 556\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['nothing', 'can', 'compete', 'with', 'the', 'pain', 'i', 'have', 'add', '.', 'i', 'am', 'broken', '.', 'my', 'life', 'is', 'advancing', 'as', 'it', 'falls', 'apart', 'under', 'my', 'feet', '.', 'every', 'thought', 'takes', 'effort', '.', 'every', 'accomplishment', 'starts', 'by', 'feeling', 'like', 'i', \"'\", 'm', 'climbing', 'a', 'mountain', '.', 'there', 'are', 'good', 'hours', 'and', 'good', 'days', ',', 'and', 'twice', 'a', 'year', 'i', 'have', 'a', 'good', 'week', ',', 'but', 'the', 'victories', 'are', 'transient', '.', 'the', 'pain', 'is', 'always', 'there', '.', 'it', 'is', 'playing', 'the', 'long', 'game', '.', 'i', 'am', 'losing', '.', 'even', 'the', 'good', 'days', 'are', 'full', 'of', 'dread', ',', 'knowing', 'that', 'eventually', 'the', 'pain', 'will', 'return', ',', 'and', 'i', 'will', 'fail', '.', 'i', 'just', 'let', 'things', 'go', '.', 'i', 'know', 'i', 'need', 'to', 'do', 'things', ',', 'but', 'the', 'pain', 'of', 'starting', 'is', 'so', 'present', ',', 'so', 'intense', ',', 'it', 'makes', 'so', 'much', 'sense', 'to', 'let', 'it', 'go', '.', 'the', 'smallest', 'tasks', 'are', 'the', 'hardest', '.', 'the', 'reward', 'is', 'so', 'small', ',', 'but', 'the', 'pain', 'never', 'changes', 'size', '.', 'if', 'people', 'knew', 'what', 'was', 'in', 'my', 'head', ',', 'they', 'would', 'never', 'trust', 'me', 'with', 'anything', 'ever', 'again', ',', 'but', 'at', 'least', 'they', 'would', 'understand', '.', 'they', 'don', \"'\", 't', 'understand', '.', 'they', 'see', 'flashes', 'of', 'great', '##ness', 'on', 'the', 'good', 'days', ',', 'and', 'they', 'imagine', 'i', 'can', 'succeed', '.', 'they', 'don', \"'\", 't', 'understand', '.', 'how', 'could', 'they', 'when', 'i', 'never', 'have', '.', 'all', 'i', 'know', 'is', 'failure', 'is', 'inevitable', '.', 'the', 'earlier', 'people', 'knew', 'me', ',', 'the', 'higher', 'their', 'opinion', 'of', 'me', 'is', '.', 'that', 'can', \"'\", 't', 'be', 'a', 'good', 'sign', '.', 'the', 'pain', 'makes', 'me', 'cut', 'corners', ',', 'it', 'makes', 'me', 'make', 'mistakes', ',', 'and', 'my', 'mistakes', 'are', 'chasing', 'me', '.', 'eventually', 'they', 'always', 'catch', 'me', '.', 'sometimes', 'my', 'mind', 'fills', 'with', 'dread', 'for', '15', 'minutes', 'before', 'i', 'even', 'realize', 'what', 'is', 'happening', '.', 'i', 'am', 'anxious', '.', 'i', 'am', 'depressed', '.', 'i', 'don', \"'\", 't', 'know', 'what', 'came', 'first', ',', 'and', 'at', 'this', 'point', 'it', 'doesn', \"'\", 't', 'matter', '.', 'i', 'take', 'a', 'powerful', 'st', '##im', '##ula', '##nt', 'every', 'morning', 'and', 'spend', 'every', 'day', 'exhausted', 'by', 'fear', '.', 'i', 'was', 'almost', 'fired', 'from', 'my', 'first', 'job', '.', 'that', 'finally', 'convinced', 'me', 'to', 'see', 'a', 'doctor', ',', 'to', 'get', 'med', '##icated', '.', 'everything', 'turned', 'around', 'for', 'me', ',', 'i', 'became', 'a', 'star', '.', 'now', 'it', \"'\", 's', '6', 'years', 'later', 'and', 'the', 'cycle', 'is', 'repeated', 'itself', '.', 'there', \"'\", 's', 'no', 'magic', 'bullet', 'this', 'time', '.', 'you', 'can', 'only', 'fail', 'so', 'many', 'times', 'before', 'people', 'give', 'up', 'on', 'you', '.', 'they', \"'\", 've', 'only', 'waited', 'this', 'long', 'because', 'they', 'don', \"'\", 't', 'understand', '.', 'i', 'desperately', 'want', 'to', 'accomplish', 'things', '.', 'they', 'just', 'makes', 'it', 'worse', '.', 'i', 'can', 'see', 'victory', 'dangling', 'in', 'front', 'of', 'my', 'face', ',', 'and', 'it', \"'\", 's', 'close', 'enough', 'to', 'touch', '.', 'it', \"'\", 's', 'taunting', 'me', '.', 'i', \"'\", 'm', 'too', 'old', 'for', 'potential', '.', 'i', \"'\", 'll', 'be', '30', 'this', 'year', '.', 'imagining', 'that', 'i', \"'\", 'll', 'figure', 'it', 'out', 'someday', '.', '.', '.', '.', 'i', 'don', \"'\", 't', 'even', 'ind', '##ul', '##ge', 'the', 'fantasy', 'anymore', '.', 'i'], ['have', 'suicidal', 'thoughts', '.', 'they', 'aren', \"'\", 't', 'real', '.', 'it', 'just', 'seems', 'so', 'final', '.', 'i', \"'\", 'm', 'a', 'competitive', 'person', 'who', 'stills', 'harbor', '##s', 'the', 'del', '##usion', 'that', 'i', 'can', 'fix', 'myself', '.', 'i', \"'\", 'm', 'not', 'ready', 'to', 'admit', 'defeat', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'love', 'you', 'guys', '!', 'i', 'honestly', 'feel', 'at', 'home', '.', 'hugs', 'all', 'around', '!']\n",
      "INFO:__main__:Number of tokens: 15\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'love', 'you', 'guys', '!', 'i', 'honestly', 'feel', 'at', 'home', '.', 'hugs', 'all', 'around', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['ia', '##ma', 'college', 'student', 'who', \"'\", 's', 'life', 'has', 'been', 'changed', 'by', 'v', '##y', '##van', '##se', ',', 'for', 'better', 'and', 'for', 'worse', '.', 'ama']\n",
      "INFO:__main__:Number of tokens: 24\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['ia', '##ma', 'college', 'student', 'who', \"'\", 's', 'life', 'has', 'been', 'changed', 'by', 'v', '##y', '##van', '##se', ',', 'for', 'better', 'and', 'for', 'worse', '.', 'ama']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['stress', '/', 'anxiety', 'with', 'up', '##ped', 'med', '##s', 'recently', ',', 'for', 'finals', ',', 'i', \"'\", 've', 'increased', 'the', 'amount', 'of', 'medication', 'i', 'take', 'during', 'the', 'day', '-', '-', 'like', 'i', 'take', 'it', 'for', 'a', 'longer', 'period', 'of', 'time', '.', 'i', \"'\", 'm', 'finding', 'it', 'gives', 'me', 'pretty', 'bad', 'anxiety', ',', 'even', 'with', 'exercise', ',', 'and', 'i', 'am', 'terribly', 'moody', 'or', 'just', 'completely', 'out', 'of', 'it', 'when', 'i', \"'\", 'm', 'coming', 'off', 'them', 'at', 'night', 'when', 'i', \"'\", 'm', 'at', 'social', 'engagements', 'or', 'with', 'my', 'boo', '.', 'not', 'to', 'mention', 'paranoid', 'sigh', '.', 'it', \"'\", 's', 'really', 'frustrating', '.', 'i', 'feel', 'like', 'a', 'completely', 'different', 'person', '.', 'not', 'to', 'mention', 'it', \"'\", 's', 'like', 'i', \"'\", 'm', 'not', 'in', 'control', 'of', 'my', 'emotions', 'or', 'stress', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'looking', 'at', 'myself', 'outside', 'my', 'body', '.', 'any', 'suggestions', 'for', 'how', 'to', 'deal', '?']\n",
      "INFO:__main__:Number of tokens: 145\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['stress', '/', 'anxiety', 'with', 'up', '##ped', 'med', '##s', 'recently', ',', 'for', 'finals', ',', 'i', \"'\", 've', 'increased', 'the', 'amount', 'of', 'medication', 'i', 'take', 'during', 'the', 'day', '-', '-', 'like', 'i', 'take', 'it', 'for', 'a', 'longer', 'period', 'of', 'time', '.', 'i', \"'\", 'm', 'finding', 'it', 'gives', 'me', 'pretty', 'bad', 'anxiety', ',', 'even', 'with', 'exercise', ',', 'and', 'i', 'am', 'terribly', 'moody', 'or', 'just', 'completely', 'out', 'of', 'it', 'when', 'i', \"'\", 'm', 'coming', 'off', 'them', 'at', 'night', 'when', 'i', \"'\", 'm', 'at', 'social', 'engagements', 'or', 'with', 'my', 'boo', '.', 'not', 'to', 'mention', 'paranoid', 'sigh', '.', 'it', \"'\", 's', 'really', 'frustrating', '.', 'i', 'feel', 'like', 'a', 'completely', 'different', 'person', '.', 'not', 'to', 'mention', 'it', \"'\", 's', 'like', 'i', \"'\", 'm', 'not', 'in', 'control', 'of', 'my', 'emotions', 'or', 'stress', '.', 'i', 'feel', 'like', 'i', \"'\", 'm', 'looking', 'at', 'myself', 'outside', 'my', 'body', '.', 'any', 'suggestions', 'for', 'how', 'to', 'deal', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['doctor', 'problems', '.']\n",
      "INFO:__main__:Number of tokens: 3\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['doctor', 'problems', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['deciding', 'between', 'add', '##eral', '##l', 'x', '##r', '/', 'ir', '?']\n",
      "INFO:__main__:Number of tokens: 10\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['deciding', 'between', 'add', '##eral', '##l', 'x', '##r', '/', 'ir', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['what', \"'\", 's', 'a', 'good', 'evaluation', 'process', 'like', '?', 'hi', 'people', '.', 'i', \"'\", 'm', 'new', 'here', ',', 'new', 'to', 'this', 'red', '##dit', 'and', 'new', 'to', 'the', 'ad', '##hd', 'world', 'altogether', '.', 'after', 'a', 'long', 'steady', 'series', 'of', 'behavior', 'issues', 'at', 'kindergarten', ',', 'our', 'school', 'nurse', 'had', 'us', 'and', 'our', 'teachers', 'fill', 'out', 'some', 'surveys', 'about', 'our', '5', '-', 'y', '##r', '-', 'old', 'son', 'and', 'his', 'scores', 'were', 'rated', 'from', 'the', '90', '##th', '-', '98', '##th', 'percent', '##ile', 'for', 'ad', '##hd', '(', 'combined', ')', '.', 'we', \"'\", 're', 'now', 'trying', 'to', 'find', 'a', 'good', 'place', 'to', 'take', 'my', 'ds', 'to', 'get', 'him', 'properly', 'evaluated', '.', 'i', 'have', 'a', 'list', 'of', 'therapist', '##s', '&', 'psychologists', 'from', 'my', 'insurance', 'company', 'and', 'from', 'friends', '.', 'most', 'places', 'i', \"'\", 've', 'called', 'are', 'either', 'booked', 'up', 'for', 'months', 'before', 'they', 'can', 'take', 'any', 'more', 'patients', ',', 'or', 'they', 'don', \"'\", 't', 'accept', 'my', 'insurance', ',', 'or', 'they', 'just', 'aren', \"'\", 't', 'calling', 'me', 'back', '.', 'but', 'i', 'did', 'finally', 'speak', 'to', 'a', 'children', \"'\", 's', 'hospital', ',', 'and', 'now', 'i', \"'\", 'm', 'waiting', 'for', 'another', 'call', 'to', 'schedule', 'an', 'appointment', '.', 'for', 'those', 'of', 'you', 'who', 'have', 'been', 'through', 'the', 'process', 'already', ',', 'can', 'you', 'tell', 'me', 'about', 'the', 'evaluation', 'process', '?', 'i', 'feel', 'a', 'tendency', 'to', 'trust', 'a', 'large', 'children', \"'\", 's', 'hospital', 'that', 'has', 'its', 'own', 'ad', '##hd', 'clinic', 'over', 'an', 'individual', 'practitioner', 'or', 'a', 'small', 'clinic', '.', 'is', 'this', 'reasonable', '?', 'are', 'there', 'huge', 'differences', 'in', 'the', 'way', 'different', 'places', 'evaluate', 'kids', 'for', 'ad', '##hd', '?', 'i', 'think', 'the', 'biggest', 'concern', 'for', 'me', ',', 'which', 'i', 'guess', 'may', 'be', 'an', 'irrational', 'fear', ',', 'is', 'that', 'people', 'in', 'this', 'field', 'make', 'their', 'living', 'on', 'people', 'diagnosed', 'with', 'disorders', 'like', 'this', ',', 'so', 'it', \"'\", 's', 'obviously', 'in', 'their', 'best', 'interest', 'to', 'see', 'that', 'the', 'person', 'that', 'steps', 'into', 'their', 'doors', 'is', 'diagnosed', 'with', 'something', '.', 'so', 'how', 'do', 'you', 'know', 'who', 'you', 'can', 'trust', 'with', 'an', 'honest', 'evaluation', '?', 'any', 'red', 'flags', 'to', 'watch', 'out', 'for', '?']\n",
      "INFO:__main__:Number of tokens: 335\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['what', \"'\", 's', 'a', 'good', 'evaluation', 'process', 'like', '?', 'hi', 'people', '.', 'i', \"'\", 'm', 'new', 'here', ',', 'new', 'to', 'this', 'red', '##dit', 'and', 'new', 'to', 'the', 'ad', '##hd', 'world', 'altogether', '.', 'after', 'a', 'long', 'steady', 'series', 'of', 'behavior', 'issues', 'at', 'kindergarten', ',', 'our', 'school', 'nurse', 'had', 'us', 'and', 'our', 'teachers', 'fill', 'out', 'some', 'surveys', 'about', 'our', '5', '-', 'y', '##r', '-', 'old', 'son', 'and', 'his', 'scores', 'were', 'rated', 'from', 'the', '90', '##th', '-', '98', '##th', 'percent', '##ile', 'for', 'ad', '##hd', '(', 'combined', ')', '.', 'we', \"'\", 're', 'now', 'trying', 'to', 'find', 'a', 'good', 'place', 'to', 'take', 'my', 'ds', 'to', 'get', 'him', 'properly', 'evaluated', '.', 'i', 'have', 'a', 'list', 'of', 'therapist', '##s', '&', 'psychologists', 'from', 'my', 'insurance', 'company', 'and', 'from', 'friends', '.', 'most', 'places', 'i', \"'\", 've', 'called', 'are', 'either', 'booked', 'up', 'for', 'months', 'before', 'they', 'can', 'take', 'any', 'more', 'patients', ',', 'or', 'they', 'don', \"'\", 't', 'accept', 'my', 'insurance', ',', 'or', 'they', 'just', 'aren', \"'\", 't', 'calling', 'me', 'back', '.', 'but', 'i', 'did', 'finally', 'speak', 'to', 'a', 'children', \"'\", 's', 'hospital', ',', 'and', 'now', 'i', \"'\", 'm', 'waiting', 'for', 'another', 'call', 'to', 'schedule', 'an', 'appointment', '.', 'for', 'those', 'of', 'you', 'who', 'have', 'been', 'through', 'the', 'process', 'already', ',', 'can', 'you', 'tell', 'me', 'about', 'the', 'evaluation', 'process', '?', 'i', 'feel', 'a', 'tendency', 'to', 'trust', 'a', 'large', 'children', \"'\", 's', 'hospital', 'that', 'has', 'its', 'own', 'ad', '##hd', 'clinic', 'over', 'an', 'individual', 'practitioner', 'or', 'a', 'small', 'clinic', '.', 'is', 'this', 'reasonable', '?', 'are', 'there', 'huge', 'differences', 'in', 'the', 'way', 'different', 'places', 'evaluate', 'kids', 'for', 'ad', '##hd', '?', 'i', 'think', 'the', 'biggest', 'concern', 'for', 'me', ',', 'which', 'i', 'guess', 'may', 'be', 'an', 'irrational', 'fear', ',', 'is', 'that', 'people', 'in', 'this', 'field', 'make', 'their', 'living', 'on', 'people', 'diagnosed', 'with', 'disorders', 'like', 'this', ',', 'so', 'it', \"'\", 's', 'obviously', 'in', 'their', 'best', 'interest', 'to', 'see', 'that', 'the', 'person', 'that', 'steps', 'into', 'their', 'doors', 'is', 'diagnosed', 'with', 'something', '.', 'so', 'how', 'do', 'you', 'know', 'who', 'you', 'can', 'trust', 'with', 'an', 'honest', 'evaluation', '?', 'any', 'red', 'flags', 'to', 'watch', 'out', 'for', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'amp', '##het', '##amine', '##s', 'cause', 'increased', 'q', '##t', 'interval', '/', 'tor', '##sa', '##des', 'de', 'pointe', '##s', '?', 'my', 'psychiatrist', 'made', 'me', 'have', 'an', 'ek', '##g', 'before', 'pre', '##sc', '##ri', '##bing', 'me', 'v', '##y', '##van', '##se', 'and', 'has', 'told', 'me', 'that', 'i', 'will', 'have', 'to', 'have', 'regular', 'ek', '##gs', 'to', 'check', 'the', 'corrected', 'q', '##t', 'interval', ',', 'or', 'q', '##tc', '.', 'i', 'believe', 'the', 'reasoning', 'was', 'that', 'if', 'it', 'goes', 'above', '500', 'or', '550', 'i', 'could', 'have', 'tor', '##sa', '##de', 'de', 'pointe', '##s', '.', 'have', 'any', 'of', 'you', 'had', 'this', 'experience', 'or', 'is', 'my', 'doc', 'being', 'extra', 'cautious', '?', 'from', 'what', 'information', 'i', \"'\", 've', 'been', 'able', 'to', 'dig', 'up', 'so', 'far', ',', 'it', 'seems', 'this', 'would', 'be', 'more', 'a', 'problem', 'associated', 'with', 'methyl', '##ph', '##eni', '##date', 'than', 'amp', '##het', '##amine', '.']\n",
      "INFO:__main__:Number of tokens: 132\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['do', 'amp', '##het', '##amine', '##s', 'cause', 'increased', 'q', '##t', 'interval', '/', 'tor', '##sa', '##des', 'de', 'pointe', '##s', '?', 'my', 'psychiatrist', 'made', 'me', 'have', 'an', 'ek', '##g', 'before', 'pre', '##sc', '##ri', '##bing', 'me', 'v', '##y', '##van', '##se', 'and', 'has', 'told', 'me', 'that', 'i', 'will', 'have', 'to', 'have', 'regular', 'ek', '##gs', 'to', 'check', 'the', 'corrected', 'q', '##t', 'interval', ',', 'or', 'q', '##tc', '.', 'i', 'believe', 'the', 'reasoning', 'was', 'that', 'if', 'it', 'goes', 'above', '500', 'or', '550', 'i', 'could', 'have', 'tor', '##sa', '##de', 'de', 'pointe', '##s', '.', 'have', 'any', 'of', 'you', 'had', 'this', 'experience', 'or', 'is', 'my', 'doc', 'being', 'extra', 'cautious', '?', 'from', 'what', 'information', 'i', \"'\", 've', 'been', 'able', 'to', 'dig', 'up', 'so', 'far', ',', 'it', 'seems', 'this', 'would', 'be', 'more', 'a', 'problem', 'associated', 'with', 'methyl', '##ph', '##eni', '##date', 'than', 'amp', '##het', '##amine', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['just', 'a', 'courtesy', 'reminder', 'to', 'my', 'american', 'friends', ':', 'mother', \"'\", 's', 'day', 'is', 'this', 'sunday', '!']\n",
      "INFO:__main__:Number of tokens: 17\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['just', 'a', 'courtesy', 'reminder', 'to', 'my', 'american', 'friends', ':', 'mother', \"'\", 's', 'day', 'is', 'this', 'sunday', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['rough', 'week', '.', 'so', ',', 'i', 'had', 'a', 'bit', 'more', 'responsibility', 'piled', 'on', 'me', 'this', 'week', '.', 'i', \"'\", 'm', 'usually', 'a', 'very', 'responsible', 'person', 'capable', 'of', 'handling', 'business', 'in', 'most', 'ways', '.', 'i', \"'\", 've', 'always', 'pride', '##d', 'myself', 'on', 'being', 'fairly', 'accountable', '.', 'well', ',', 'i', \"'\", 've', 'been', 'suppose', 'to', 'do', '2', 'or', '3', 'extra', 'task', 'all', 'this', 'week', 'and', 'with', 'finals', 'i', \"'\", 've', 'dropped', 'the', 'ball', 'embarrassing', '##ly', 'hard', 'on', 'all', 'accounts', '.', 'it', \"'\", 's', 'just', 'funny', 'how', 'my', 'problems', 'have', 'always', 'been', 'more', 'related', 'to', 'doing', 'task', 'which', 'take', 'longer', 'to', 'do', '.', '.', 'not', 'forgetting', 'feed', 'fish', 'which', 'only', 'takes', 'a', 'couple', 'seconds', '.', '.']\n",
      "INFO:__main__:Number of tokens: 113\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['rough', 'week', '.', 'so', ',', 'i', 'had', 'a', 'bit', 'more', 'responsibility', 'piled', 'on', 'me', 'this', 'week', '.', 'i', \"'\", 'm', 'usually', 'a', 'very', 'responsible', 'person', 'capable', 'of', 'handling', 'business', 'in', 'most', 'ways', '.', 'i', \"'\", 've', 'always', 'pride', '##d', 'myself', 'on', 'being', 'fairly', 'accountable', '.', 'well', ',', 'i', \"'\", 've', 'been', 'suppose', 'to', 'do', '2', 'or', '3', 'extra', 'task', 'all', 'this', 'week', 'and', 'with', 'finals', 'i', \"'\", 've', 'dropped', 'the', 'ball', 'embarrassing', '##ly', 'hard', 'on', 'all', 'accounts', '.', 'it', \"'\", 's', 'just', 'funny', 'how', 'my', 'problems', 'have', 'always', 'been', 'more', 'related', 'to', 'doing', 'task', 'which', 'take', 'longer', 'to', 'do', '.', '.', 'not', 'forgetting', 'feed', 'fish', 'which', 'only', 'takes', 'a', 'couple', 'seconds', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['long', 'term', 'use', 'of', 'add', '##eral', '##l', 'and', 'metabolism', '-', 'any', 'notable', 'consequences', 'or', 'changes', '?', 'i', \"'\", 've', 'been', 'taking', 'add', '##eral', '##l', 'since', 'i', 'was', 'diagnosed', 'with', 'predominantly', 'ina', '##tten', '##tive', 'type', 'back', 'in', 'the', 'summer', 'of', '2009', '(', 'save', 'for', 'the', 'first', 'few', 'weeks', 'when', 'rita', '##lin', 'was', 'tried', ')', '.', 'i', \"'\", 've', 'taken', '10', '##mg', 'in', 'the', 'morning', 'and', '10', '##mg', 'in', 'the', 'afternoon', 'of', 'instant', 'release', 'dex', '##tro', '##amp', '##het', '##amine', '.', 'in', 'my', 'first', 'few', 'months', 'of', 'taking', 'the', 'drug', 'i', 'lost', '30', '##lb', '##s', 'and', 'looked', 'fantastic', ',', 'as', 'well', 'as', 'had', 'an', 'easier', 'time', 'paying', 'attention', 'in', 'my', 'college', 'lectures', 'and', 'an', 'improved', 'social', 'life', 'and', 'overall', 'increased', 'mood', '.', 'a', 'year', 'later', '(', 'and', 'for', 'the', 'past', '2', 'years', ',', 'really', ')', 'i', \"'\", 've', 'been', 'off', 'and', 'on', 'the', 'prescription', 'for', 'months', 'at', 'a', 'time', 'based', 'on', 'whether', 'or', 'not', 'i', 'can', 'afford', 'an', 'appointment', 'to', 'get', 'a', 'new', 'prescription', 'written', 'for', 'me', 'by', 'my', 'doctor', '.', 'in', 'this', 'time', 'i', \"'\", 've', 'gained', 'all', 'of', 'the', 'weight', 'back', 'that', 'i', \"'\", 've', 'lost', '.', 'i', \"'\", 've', 'tried', 'numerous', 'diet', '##s', 'and', 'exercise', 'routines', 'but', 'to', 'no', 'avail', '-', 'it', 'seems', 'no', 'matter', 'what', ',', 'i', 'cannot', 'drop', 'the', 'weight', 'i', 'gained', 'back', '.', 'i', \"'\", 'm', 'not', 'under', '##weight', 'to', 'begin', 'with', 'by', 'any', 'means', '-', 'i', \"'\", 'm', 'currently', '5', \"'\", '6', 'and', 'hovering', 'between', '150', '-', '160', '##lb', '##s', '.', '(', 'female', ',', 'bt', '##w', ')', 'i', \"'\", 'm', 'starting', 'to', 'wonder', 'if', 'taking', 'the', 'add', '##eral', '##l', '(', 'or', 'dex', '##tro', '##amp', '##het', '##amine', ',', 'in', 'my', 'case', ')', 'has', 'completely', 'killed', 'my', 'metabolism', '.', 'has', 'anyone', 'else', 'found', 'any', 'long', 'term', 'differences', 'in', 'their', 'metabolism', 'or', 'weight', 'from', 'taking', 'the', 'drug', '?', 'any', 'tips', 'or', 'advice', 'or', 'light', 'you', 'can', 'shed', '?', 'thanks', '!']\n",
      "INFO:__main__:Number of tokens: 311\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['long', 'term', 'use', 'of', 'add', '##eral', '##l', 'and', 'metabolism', '-', 'any', 'notable', 'consequences', 'or', 'changes', '?', 'i', \"'\", 've', 'been', 'taking', 'add', '##eral', '##l', 'since', 'i', 'was', 'diagnosed', 'with', 'predominantly', 'ina', '##tten', '##tive', 'type', 'back', 'in', 'the', 'summer', 'of', '2009', '(', 'save', 'for', 'the', 'first', 'few', 'weeks', 'when', 'rita', '##lin', 'was', 'tried', ')', '.', 'i', \"'\", 've', 'taken', '10', '##mg', 'in', 'the', 'morning', 'and', '10', '##mg', 'in', 'the', 'afternoon', 'of', 'instant', 'release', 'dex', '##tro', '##amp', '##het', '##amine', '.', 'in', 'my', 'first', 'few', 'months', 'of', 'taking', 'the', 'drug', 'i', 'lost', '30', '##lb', '##s', 'and', 'looked', 'fantastic', ',', 'as', 'well', 'as', 'had', 'an', 'easier', 'time', 'paying', 'attention', 'in', 'my', 'college', 'lectures', 'and', 'an', 'improved', 'social', 'life', 'and', 'overall', 'increased', 'mood', '.', 'a', 'year', 'later', '(', 'and', 'for', 'the', 'past', '2', 'years', ',', 'really', ')', 'i', \"'\", 've', 'been', 'off', 'and', 'on', 'the', 'prescription', 'for', 'months', 'at', 'a', 'time', 'based', 'on', 'whether', 'or', 'not', 'i', 'can', 'afford', 'an', 'appointment', 'to', 'get', 'a', 'new', 'prescription', 'written', 'for', 'me', 'by', 'my', 'doctor', '.', 'in', 'this', 'time', 'i', \"'\", 've', 'gained', 'all', 'of', 'the', 'weight', 'back', 'that', 'i', \"'\", 've', 'lost', '.', 'i', \"'\", 've', 'tried', 'numerous', 'diet', '##s', 'and', 'exercise', 'routines', 'but', 'to', 'no', 'avail', '-', 'it', 'seems', 'no', 'matter', 'what', ',', 'i', 'cannot', 'drop', 'the', 'weight', 'i', 'gained', 'back', '.', 'i', \"'\", 'm', 'not', 'under', '##weight', 'to', 'begin', 'with', 'by', 'any', 'means', '-', 'i', \"'\", 'm', 'currently', '5', \"'\", '6', 'and', 'hovering', 'between', '150', '-', '160', '##lb', '##s', '.', '(', 'female', ',', 'bt', '##w', ')', 'i', \"'\", 'm', 'starting', 'to', 'wonder', 'if', 'taking', 'the', 'add', '##eral', '##l', '(', 'or', 'dex', '##tro', '##amp', '##het', '##amine', ',', 'in', 'my', 'case', ')', 'has', 'completely', 'killed', 'my', 'metabolism', '.', 'has', 'anyone', 'else', 'found', 'any', 'long', 'term', 'differences', 'in', 'their', 'metabolism', 'or', 'weight', 'from', 'taking', 'the', 'drug', '?', 'any', 'tips', 'or', 'advice', 'or', 'light', 'you', 'can', 'shed', '?', 'thanks', '!']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['can', 'someone', 'help', 'me', '?']\n",
      "INFO:__main__:Number of tokens: 5\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['can', 'someone', 'help', 'me', '?']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['i', 'am', 'curious', 'about', 'something', 'regarding', 'ad', '##hd', 'and', 'how', 'it', 'manifest', '##s', 'itself', 'in', 'different', 'people', '.', '.', '.']\n",
      "INFO:__main__:Number of tokens: 20\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['i', 'am', 'curious', 'about', 'something', 'regarding', 'ad', '##hd', 'and', 'how', 'it', 'manifest', '##s', 'itself', 'in', 'different', 'people', '.', '.', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['do', 'i', 'have', 'any', 'chance', 'of', 'getting', 'prescribed', 'add', 'med', '##s', 'at', '17', 'as', 'a', '\"', 'good', '\"', 'student', '?', 'i', \"'\", 'm', '17', ',', 'very', 'intelligent', ',', 'and', 'i', 'get', 'solid', 'grades', '.', 'not', 'necessarily', 'straight', 'as', ',', 'but', 'i', 'do', 'have', 'a', '90', '-', 'something', 'average', 'at', 'a', 'private', 'school', 'that', 'is', 'highly', 'regarded', 'in', 'my', 'area', ',', 'with', '2', 'ap', 'courses', 'as', 'a', 'junior', '.', 'however', ',', 'i', 'have', 'trouble', 'focusing', 'in', 'class', 'and', 'i', \"'\", 'm', 'moderately', 'depressed', ',', 'over', '##weight', ',', 'and', 'i', \"'\", 'm', 'generally', 'in', 'a', 'crap', '##py', 'mood', 'a', 'lot', 'of', 'the', 'time', '.', 'i', 'spend', 'more', 'time', 'talking', 'with', 'friends', ',', 'day', '##dre', '##ami', '##ng', ',', 'and', 'writing', 'down', 'random', 'junk', 'than', 'doing', 'actual', 'work', 'during', 'classes', ',', 'and', 'i', 'feel', 'generally', 'un', '##mot', '##ivated', 'to', 'actually', 'do', 'any', 'work', 'when', 'i', 'get', 'home', '.', 'rather', ',', 'i', 'play', 'video', 'games', ',', 'text', 'friends', ',', 'and', 'do', 'random', 'shit', '.', 'a', 'friend', 'told', 'me', 'about', 'v', '##y', '##van', '##se', ',', 'some', 'add', '/', 'ad', '##hd', 'med', '.', 'he', 'bought', 'me', 'a', 'pill', '(', '50', '##mg', ')', 'and', 'i', 'took', 'it', 'today', '(', 'i', 'know', ',', 'i', \"'\", 'm', 'a', 'bad', 'person', 'etc', '.', '.', '.', 'spare', 'me', 'please', '.', 'first', 'time', 'i', \"'\", 've', 'ever', 'taken', 'any', 'med', '/', 'drug', 'i', 'wasn', \"'\", 't', 'supposed', 'to', 'have', ')', ',', 'and', 'everything', 'was', 'absurd', '##ly', 'clear', '.', 'i', 'was', 'happy', ',', 'in', 'a', 'great', 'mood', 'all', 'day', ',', 'i', 'got', 'a', 'ton', 'of', 'work', 'done', '.', 'i', 'didn', \"'\", 't', 'have', 'a', 'free', 'period', 'but', ',', 'without', 'missing', 'a', 'beat', 'in', 'any', 'of', 'my', 'classes', 'today', 'i', 'managed', 'to', 'get', '5', 'pages', 'of', 'a', 'hand', '##written', 'study', 'guide', 'for', 'an', 'exam', 'i', 'have', 'tomorrow', 'during', 'classes', '.', 'i', 'felt', 'motivated', 'and', ',', 'well', ',', 'pretty', 'damn', 'good', '.', 'i', 'didn', \"'\", 't', 'feel', 'eu', '##ph', '##oric', 'the', 'way', 'my', 'friend', 'described', 'it', 'when', 'he', 'took', 'it', 'for', 'the', 'first', 'time', '-', 'just', '.', '.', '.', 'good', '.', 'normal', '.', 'well', ',', 'what', 'i', 'imagine', 'normal', 'to', 'be', 'for', 'other', 'people', ',', 'at', 'least', '.', 'i', 'feel', 'like', 'a', 'medicine', 'like', 'v', '##y', '##van', '##se', 'could', 'improve', 'my', 'quality', 'of', 'life', 'overall', 'thanks', 'to', 'both', 'its', 'intended', 'effect', 'and', 'one', 'of', 'the', 'most', 'common', 'side', 'effects', ',', 'which', 'is', 'lack', 'of', 'appetite', '(', 'over', '##weight', ',', 'plus', 'i', 'might', 'have', 'motivation', 'to', 'exercise', 'too', ')', '.', 'though', 'i', 'haven', \"'\", 't', 'looked', 'into', 'it', 'extremely', 'thoroughly', 'it', 'appears', 'fairly', 'safe', 'and', 'effective', 'over', 'time', ',', 'and', 'from', 'what', 'i', 'know', 'after', 'a', 'length', 'of', 'time', 'on', 'add', 'med', '##s', 'the', 'effect', 'of', 'the', 'medicine', 'sinks', 'itself', 'into', 'the', 'user', \"'\", 's', 'life', 'in', 'general', ',', 'so', 'eventually', 'i', \"'\", 'd', 'be', 'back', 'med', '-', 'free', '.', '*', 't', '##l', ';', 'dr', '##1', ':', 'so', 'my', 'question', ',', 'as', 'a', '\"', 'good', '\"', 'student', 'who', 'gets', 'good', 'grades', 'mostly', 'by', 'just', 'being', 'smart', ',', 'but', 'who', 'has', 'many', 'of', 'the', 'effects', 'of', 'add', '(', 'inability', 'to', 'focus', 'especially', 'in', 'classes', ',', 'lack', 'of', 'motivation', ',', 'feeling', 'crap', '##py', 'about', 'myself', 'a', 'lot', 'etc', ')', ',', 'and', 'at', '17', 'years', 'old', ',', 'would', 'doctors', '/', 'psychiatrist', '##s', 'be', 'willing', 'to', 'write', 'prescription', '##s', 'for', 'me', 'for', 'add', 'med', '##s', '?', '*', 'i', 'feel', 'like', 'it', 'would', 'be', 'kind', 'of', 'hard', 'to', 'convince', 'someone', 'that', 'i', \"'\", 'm', 'not', 'performing', 'as', 'well', 'as', 'i', 'should', 'be', 'given', 'the', 'fact', 'that', 'i', 'already', 'have', 'solid', 'grades', 'and', 'such', '.', 'however', ',', 'that', \"'\", 's', 'not', 'all', '.', 'in', 'the', 'past', 'i', \"'\", 've', 'talked', 'to', 'my', 'mom', 'about', 'add', '/', 'ad', '##hd', 'med', '##s', ',', 'telling', 'her', 'that', 'i', \"'\", 'm', 'not', 'focusing', 'that', 'well', 'and', 'that', 'i', 'could', 'be', 'doing', 'a', 'lot', 'better', 'than', 'i', 'already', 'am', 'in', 'school', ',', 'but', 'she', 'sort', 'of', 'blew', 'me', 'off', 'and', 'said', 'there', \"'\", 's', 'no', 'way', 'i', 'have', 'add', '/', 'ad', '##hd', '.', 'i', 'think', 'she', 'seems', 'to', 'think', 'my', 'ability', 'to', 'concentrate', 'on', 'video', 'games', 'means', 'i', 'can', 'focus', 'on', 'whatever', 'the', 'hell', 'is', 'in', 'front', 'of', 'me', ',', 'and', 'that', \"'\", 's', 'just', 'not', 'the', 'case', '.', 'but', 'anyway', '##s', '.', '.', '.', '*', 't', '##l', ';', 'dr', '##2', ':', 'if', 'you', 'think', 'i', 'would', 'be', 'able', 'to', 'get', 'a', 'prescription', 'for', 'v', '##y', '##van', '##se', 'or', 'something', 'similar', 'despite', 'my', 'ability', 'to', 'perform', 'in', 'school', 'by', 'some', 'miracle', ',', 'how', 'should', 'i', 'approach', 'my', 'mom', 'about', 'it', '?', 'in', 'the', 'past', 'she', \"'\", 's', 'been', 'adamant', '##ly', 'against', 'it', ',', 'but', 'there', 'should', 'be', 'some', 'way', 'i', 'can', 'talk', 'to', 'her', 'about', 'it', '.', '*', 'sorry', 'that', 'was', 'so', 'long', ',', 'and', 'thanks', 'to', 'anyone', 'who', 'actually', 'bothered', 'to', 'read', 'the', 'whole', 'thing', 'and', 'responds', 'to', 'it', '!', ':', 'd', '*', '*', 'edit', ':', 'after', 'some', 'chatting', 'with', 'folks', 'in', 'ir', '##c', ',', 'i', \"'\", 'm', 'now', 'pretty', 'much', 'certain', 'that', 'i', 'have', 'some', 'form', 'of', 'ad', '##hd', ',', 'and', 'i', \"'\", 'm', 'sure', 'i', 'can', 'get', 'help', 'for', 'it', '.', 'big', 'thanks', 'to', 'op', '##iate', '##s', ',', 'over', '##thest', '##ars', ',', 'and', 'everyone', 'else', 'who', 'i', 'talked', 'to', '.', '*', '*', '*', 'but', 'now', ',', 'where', 'do', 'i', 'start', '?', '*', 'obviously', 'i', \"'\", 'll', 'have', 'to', 'talk', 'to', 'my', 'mom', ',', 'but', 'i', 'feel', 'like', 'i', 'should', 'know', 'the', 'basic', 'process', 'that', 'i', \"'\", 'll', 'be', 'going', 'through', 'for', 'getting', 'evaluated', 'before', 'i', 'try', 'and', 'approach', 'someone', 'who', \"'\", 's', 'already', 'hesitant', 'when', 'it', 'comes', 'to', 'this', 'topic', '.', 'i', 'think', 'if', 'i', 'explain', 'it', 'well', 'i', \"'\", 'll', 'be', 'more', 'likely', 'to', 'be', 'taken', 'seriously', 'about', 'it', 'and', 'i', 'should', 'actually', 'be', 'able', 'to', 'get', 'some', 'help', '.']\n",
      "INFO:__main__:Number of tokens: 946\n",
      "INFO:__main__:Number of chunks: 2\n",
      "INFO:__main__:Chunks: [['do', 'i', 'have', 'any', 'chance', 'of', 'getting', 'prescribed', 'add', 'med', '##s', 'at', '17', 'as', 'a', '\"', 'good', '\"', 'student', '?', 'i', \"'\", 'm', '17', ',', 'very', 'intelligent', ',', 'and', 'i', 'get', 'solid', 'grades', '.', 'not', 'necessarily', 'straight', 'as', ',', 'but', 'i', 'do', 'have', 'a', '90', '-', 'something', 'average', 'at', 'a', 'private', 'school', 'that', 'is', 'highly', 'regarded', 'in', 'my', 'area', ',', 'with', '2', 'ap', 'courses', 'as', 'a', 'junior', '.', 'however', ',', 'i', 'have', 'trouble', 'focusing', 'in', 'class', 'and', 'i', \"'\", 'm', 'moderately', 'depressed', ',', 'over', '##weight', ',', 'and', 'i', \"'\", 'm', 'generally', 'in', 'a', 'crap', '##py', 'mood', 'a', 'lot', 'of', 'the', 'time', '.', 'i', 'spend', 'more', 'time', 'talking', 'with', 'friends', ',', 'day', '##dre', '##ami', '##ng', ',', 'and', 'writing', 'down', 'random', 'junk', 'than', 'doing', 'actual', 'work', 'during', 'classes', ',', 'and', 'i', 'feel', 'generally', 'un', '##mot', '##ivated', 'to', 'actually', 'do', 'any', 'work', 'when', 'i', 'get', 'home', '.', 'rather', ',', 'i', 'play', 'video', 'games', ',', 'text', 'friends', ',', 'and', 'do', 'random', 'shit', '.', 'a', 'friend', 'told', 'me', 'about', 'v', '##y', '##van', '##se', ',', 'some', 'add', '/', 'ad', '##hd', 'med', '.', 'he', 'bought', 'me', 'a', 'pill', '(', '50', '##mg', ')', 'and', 'i', 'took', 'it', 'today', '(', 'i', 'know', ',', 'i', \"'\", 'm', 'a', 'bad', 'person', 'etc', '.', '.', '.', 'spare', 'me', 'please', '.', 'first', 'time', 'i', \"'\", 've', 'ever', 'taken', 'any', 'med', '/', 'drug', 'i', 'wasn', \"'\", 't', 'supposed', 'to', 'have', ')', ',', 'and', 'everything', 'was', 'absurd', '##ly', 'clear', '.', 'i', 'was', 'happy', ',', 'in', 'a', 'great', 'mood', 'all', 'day', ',', 'i', 'got', 'a', 'ton', 'of', 'work', 'done', '.', 'i', 'didn', \"'\", 't', 'have', 'a', 'free', 'period', 'but', ',', 'without', 'missing', 'a', 'beat', 'in', 'any', 'of', 'my', 'classes', 'today', 'i', 'managed', 'to', 'get', '5', 'pages', 'of', 'a', 'hand', '##written', 'study', 'guide', 'for', 'an', 'exam', 'i', 'have', 'tomorrow', 'during', 'classes', '.', 'i', 'felt', 'motivated', 'and', ',', 'well', ',', 'pretty', 'damn', 'good', '.', 'i', 'didn', \"'\", 't', 'feel', 'eu', '##ph', '##oric', 'the', 'way', 'my', 'friend', 'described', 'it', 'when', 'he', 'took', 'it', 'for', 'the', 'first', 'time', '-', 'just', '.', '.', '.', 'good', '.', 'normal', '.', 'well', ',', 'what', 'i', 'imagine', 'normal', 'to', 'be', 'for', 'other', 'people', ',', 'at', 'least', '.', 'i', 'feel', 'like', 'a', 'medicine', 'like', 'v', '##y', '##van', '##se', 'could', 'improve', 'my', 'quality', 'of', 'life', 'overall', 'thanks', 'to', 'both', 'its', 'intended', 'effect', 'and', 'one', 'of', 'the', 'most', 'common', 'side', 'effects', ',', 'which', 'is', 'lack', 'of', 'appetite', '(', 'over', '##weight', ',', 'plus', 'i', 'might', 'have', 'motivation', 'to', 'exercise', 'too', ')', '.', 'though', 'i', 'haven', \"'\", 't', 'looked', 'into', 'it', 'extremely', 'thoroughly', 'it', 'appears', 'fairly', 'safe', 'and', 'effective', 'over', 'time', ',', 'and', 'from', 'what', 'i', 'know', 'after', 'a', 'length', 'of', 'time', 'on', 'add', 'med', '##s', 'the', 'effect', 'of', 'the', 'medicine', 'sinks', 'itself', 'into', 'the', 'user', \"'\", 's', 'life', 'in', 'general', ',', 'so', 'eventually', 'i', \"'\", 'd', 'be', 'back', 'med', '-', 'free', '.', '*', 't', '##l', ';', 'dr', '##1', ':', 'so', 'my', 'question', ',', 'as', 'a', '\"', 'good', '\"', 'student', 'who', 'gets', 'good', 'grades', 'mostly', 'by', 'just', 'being', 'smart', ',', 'but', 'who', 'has', 'many', 'of', 'the', 'effects', 'of', 'add', '(', 'inability', 'to', 'focus', 'especially', 'in', 'classes', ',', 'lack', 'of', 'motivation', ',', 'feeling'], ['crap', '##py', 'about', 'myself', 'a', 'lot', 'etc', ')', ',', 'and', 'at', '17', 'years', 'old', ',', 'would', 'doctors', '/', 'psychiatrist', '##s', 'be', 'willing', 'to', 'write', 'prescription', '##s', 'for', 'me', 'for', 'add', 'med', '##s', '?', '*', 'i', 'feel', 'like', 'it', 'would', 'be', 'kind', 'of', 'hard', 'to', 'convince', 'someone', 'that', 'i', \"'\", 'm', 'not', 'performing', 'as', 'well', 'as', 'i', 'should', 'be', 'given', 'the', 'fact', 'that', 'i', 'already', 'have', 'solid', 'grades', 'and', 'such', '.', 'however', ',', 'that', \"'\", 's', 'not', 'all', '.', 'in', 'the', 'past', 'i', \"'\", 've', 'talked', 'to', 'my', 'mom', 'about', 'add', '/', 'ad', '##hd', 'med', '##s', ',', 'telling', 'her', 'that', 'i', \"'\", 'm', 'not', 'focusing', 'that', 'well', 'and', 'that', 'i', 'could', 'be', 'doing', 'a', 'lot', 'better', 'than', 'i', 'already', 'am', 'in', 'school', ',', 'but', 'she', 'sort', 'of', 'blew', 'me', 'off', 'and', 'said', 'there', \"'\", 's', 'no', 'way', 'i', 'have', 'add', '/', 'ad', '##hd', '.', 'i', 'think', 'she', 'seems', 'to', 'think', 'my', 'ability', 'to', 'concentrate', 'on', 'video', 'games', 'means', 'i', 'can', 'focus', 'on', 'whatever', 'the', 'hell', 'is', 'in', 'front', 'of', 'me', ',', 'and', 'that', \"'\", 's', 'just', 'not', 'the', 'case', '.', 'but', 'anyway', '##s', '.', '.', '.', '*', 't', '##l', ';', 'dr', '##2', ':', 'if', 'you', 'think', 'i', 'would', 'be', 'able', 'to', 'get', 'a', 'prescription', 'for', 'v', '##y', '##van', '##se', 'or', 'something', 'similar', 'despite', 'my', 'ability', 'to', 'perform', 'in', 'school', 'by', 'some', 'miracle', ',', 'how', 'should', 'i', 'approach', 'my', 'mom', 'about', 'it', '?', 'in', 'the', 'past', 'she', \"'\", 's', 'been', 'adamant', '##ly', 'against', 'it', ',', 'but', 'there', 'should', 'be', 'some', 'way', 'i', 'can', 'talk', 'to', 'her', 'about', 'it', '.', '*', 'sorry', 'that', 'was', 'so', 'long', ',', 'and', 'thanks', 'to', 'anyone', 'who', 'actually', 'bothered', 'to', 'read', 'the', 'whole', 'thing', 'and', 'responds', 'to', 'it', '!', ':', 'd', '*', '*', 'edit', ':', 'after', 'some', 'chatting', 'with', 'folks', 'in', 'ir', '##c', ',', 'i', \"'\", 'm', 'now', 'pretty', 'much', 'certain', 'that', 'i', 'have', 'some', 'form', 'of', 'ad', '##hd', ',', 'and', 'i', \"'\", 'm', 'sure', 'i', 'can', 'get', 'help', 'for', 'it', '.', 'big', 'thanks', 'to', 'op', '##iate', '##s', ',', 'over', '##thest', '##ars', ',', 'and', 'everyone', 'else', 'who', 'i', 'talked', 'to', '.', '*', '*', '*', 'but', 'now', ',', 'where', 'do', 'i', 'start', '?', '*', 'obviously', 'i', \"'\", 'll', 'have', 'to', 'talk', 'to', 'my', 'mom', ',', 'but', 'i', 'feel', 'like', 'i', 'should', 'know', 'the', 'basic', 'process', 'that', 'i', \"'\", 'll', 'be', 'going', 'through', 'for', 'getting', 'evaluated', 'before', 'i', 'try', 'and', 'approach', 'someone', 'who', \"'\", 's', 'already', 'hesitant', 'when', 'it', 'comes', 'to', 'this', 'topic', '.', 'i', 'think', 'if', 'i', 'explain', 'it', 'well', 'i', \"'\", 'll', 'be', 'more', 'likely', 'to', 'be', 'taken', 'seriously', 'about', 'it', 'and', 'i', 'should', 'actually', 'be', 'able', 'to', 'get', 'some', 'help', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting chunk embedding...\n",
      "INFO:__main__:Chunk embedding shape: torch.Size([1, 768])\n",
      "INFO:__main__:Getting embedding...\n",
      "INFO:__main__:Chunking text...\n",
      "INFO:__main__:Tokenized text: ['is', '60', '##mg', '/', 'day', 'of', 'add', '##eral', '##l', 'x', '##r', 'too', 'much', '?', 'that', ',', 'plus', 'my', 'ina', '##d', '##vert', '##ent', 'encyclopedia', 'of', 'a', 'backs', '##tory', '.']\n",
      "INFO:__main__:Number of tokens: 28\n",
      "INFO:__main__:Number of chunks: 1\n",
      "INFO:__main__:Chunks: [['is', '60', '##mg', '/', 'day', 'of', 'add', '##eral', '##l', 'x', '##r', 'too', 'much', '?', 'that', ',', 'plus', 'my', 'ina', '##d', '##vert', '##ent', 'encyclopedia', 'of', 'a', 'backs', '##tory', '.']]\n",
      "INFO:__main__:Getting chunk embedding...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the function to the 'body' column of the DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mstart_embedding_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mstart_embedding_workflow\u001b[0;34m(file_path, text_column)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Apply the embedding function to the specified column\u001b[39;00m\n\u001b[1;32m      9\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying embedding function to column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_full_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Save the DataFrame with embeddings to a new CSV file\u001b[39;00m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mget_full_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m chunk_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m---> 11\u001b[0m     chunk_emb \u001b[38;5;241m=\u001b[39m \u001b[43mget_chunk_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     chunk_embeddings\u001b[38;5;241m.\u001b[39mappend(chunk_emb)\n\u001b[1;32m     13\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk embedding shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mget_chunk_embedding\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m      5\u001b[0m     tokens,\n\u001b[1;32m      6\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/adhd-linguistic-patterns-beliefs-portuguese-women/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:325\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    328\u001b[0m     query_length, key_length \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], key_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply the function to the 'body' column of the DataFrame\n",
    "df = start_embedding_workflow(file_path, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                               body  \\\n",
      "0  29kaf8               Adult Women Are the New Face of ADHD   \n",
      "1  2ip2ra                 Why Women Hide Their ADHD Symptoms   \n",
      "2  2q6jdk        Adult ADHD and Burnout: Success or Failure?   \n",
      "3  2sc7fa                  How Am I And My ADHD Still Alive?   \n",
      "4  3296xx  I'd like to see this subreddit grow! Hello, I'...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [[0.1278188, -0.20812818, 0.54785943, 0.262456...  \n",
      "1  [[0.1341694, -0.04314984, 0.11840491, 0.153770...  \n",
      "2  [[0.2748738, -0.34320945, 0.29193562, 0.353753...  \n",
      "3  [[0.17047045, 0.09128848, 0.4974976, 0.2096059...  \n",
      "4  [[0.09756681, 0.011357546, 0.49866802, 0.06258...  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
